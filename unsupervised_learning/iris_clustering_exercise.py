"""
é¸¢å°¾èŠ±(Iris)èšç±»ç»ƒä¹ 
===================

ç»ƒä¹ ç›®æ ‡ï¼š
1. ä½¿ç”¨å¤šç§èšç±»ç®—æ³•å¯¹Irisæ•°æ®é›†è¿›è¡Œèšç±»
2. å¯¹æ¯”ä¸åŒç®—æ³•çš„èšç±»æ•ˆæžœ
3. ç†è§£èšç±»è¯„ä¼°æŒ‡æ ‡çš„å«ä¹‰

è¿è¡Œæ–¹å¼ï¼š
python iris_clustering_exercise.py
"""

# ==================== å¯¼å…¥å¿…è¦çš„åº“ ====================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# èšç±»ç®—æ³•
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import (
    silhouette_score,           # è½®å»“ç³»æ•°ï¼šè¡¡é‡ç°‡å†…ç´§å¯†åº¦å’Œç°‡é—´åˆ†ç¦»åº¦
    davies_bouldin_score,       # DBæŒ‡æ•°ï¼šç°‡å†…è·ç¦»ä¸Žç°‡é—´è·ç¦»çš„æ¯”å€¼ï¼Œè¶Šå°è¶Šå¥½
    adjusted_rand_score,        # è°ƒæ•´å…°å¾·ç³»æ•°ï¼šä¸ŽçœŸå®žæ ‡ç­¾çš„ä¸€è‡´æ€§
    calinski_harabasz_score     # CHæŒ‡æ•°ï¼šç°‡é—´æ–¹å·®ä¸Žç°‡å†…æ–¹å·®çš„æ¯”å€¼ï¼Œè¶Šå¤§è¶Šå¥½
)

import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾é£Žæ ¼
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['font.sans-serif'] = [
    'Arial Unicode MS',  # macOSé€šç”¨
    'PingFang SC',       # macOSç³»ç»Ÿå­—ä½“
    'STHeiti',           # åŽæ–‡é»‘ä½“
    'Heiti TC',          # é»‘ä½“-ç¹
    'SimHei',            # é»‘ä½“
]
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

print("=" * 80)
print("é¸¢å°¾èŠ±(Iris)èšç±»åˆ†æžç»ƒä¹ ".center(70))
print("=" * 80)


# ==================== 1. æ•°æ®åŠ è½½ä¸ŽæŽ¢ç´¢ ====================
print("\nã€æ­¥éª¤1ã€‘æ•°æ®åŠ è½½ä¸ŽæŽ¢ç´¢")
print("-" * 80)

# åŠ è½½Irisæ•°æ®é›†
# Irisæ•°æ®é›†åŒ…å«150ä¸ªæ ·æœ¬ï¼Œ3ä¸ªç±»åˆ«(Setosaã€Versicolorã€Virginica)
# æ¯ä¸ªæ ·æœ¬æœ‰4ä¸ªç‰¹å¾ï¼šèŠ±è¼é•¿åº¦ã€èŠ±è¼å®½åº¦ã€èŠ±ç“£é•¿åº¦ã€èŠ±ç“£å®½åº¦
iris = load_iris()
X = iris.data           # ç‰¹å¾æ•°æ® (150, 4)
y_true = iris.target    # çœŸå®žæ ‡ç­¾ (150,) - ç”¨äºŽåŽç»­è¯„ä¼°å¯¹æ¯”

# åˆ›å»ºDataFrameæ–¹ä¾¿æŸ¥çœ‹
feature_names = iris.feature_names
df = pd.DataFrame(X, columns=feature_names)
df['species'] = iris.target_names[y_true]

print(f"æ•°æ®é›†å½¢çŠ¶: {X.shape}")
print(f"ç‰¹å¾åç§°: {feature_names}")
print(f"ç±»åˆ«åç§°: {iris.target_names}")
print(f"\nå„ç±»åˆ«æ ·æœ¬æ•°é‡:")
print(df['species'].value_counts())

print("\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
print(df.describe())


# ==================== 2. æ•°æ®é¢„å¤„ç† ====================
print("\nã€æ­¥éª¤2ã€‘æ•°æ®é¢„å¤„ç†")
print("-" * 80)

# æ•°æ®æ ‡å‡†åŒ–
# åŽŸå› ï¼šä¸åŒç‰¹å¾çš„é‡çº²å¯èƒ½ä¸åŒï¼Œæ ‡å‡†åŒ–å¯ä»¥æ¶ˆé™¤é‡çº²å½±å“
# æ–¹æ³•ï¼šZ-scoreæ ‡å‡†åŒ–ï¼Œä½¿æ¯ä¸ªç‰¹å¾å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1
# å…¬å¼ï¼šx_scaled = (x - mean) / std
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"æ ‡å‡†åŒ–å‰ç‰¹å¾èŒƒå›´:")
for i, name in enumerate(feature_names):
    print(f"  {name}: [{X[:, i].min():.2f}, {X[:, i].max():.2f}]")

print(f"\næ ‡å‡†åŒ–åŽç‰¹å¾èŒƒå›´:")
for i, name in enumerate(feature_names):
    print(f"  {name}: [{X_scaled[:, i].min():.2f}, {X_scaled[:, i].max():.2f}]")

# PCAé™ç»´ç”¨äºŽå¯è§†åŒ–
# åŽŸå› ï¼šIrisæœ‰4ä¸ªç‰¹å¾ï¼Œæ— æ³•ç›´æŽ¥åœ¨2Då¹³é¢ä¸Šå¯è§†åŒ–
# æ–¹æ³•ï¼šä½¿ç”¨PCAæå–å‰2ä¸ªä¸»æˆåˆ†ï¼Œä¿ç•™æœ€å¤šçš„æ–¹å·®ä¿¡æ¯
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

print(f"\nPCAé™ç»´åŽå½¢çŠ¶: {X_pca.shape}")
print(f"å‰2ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_}")
print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")


# ==================== 3. K-Meansèšç±» ====================
print("\nã€æ­¥éª¤3ã€‘K-Meansèšç±»")
print("-" * 80)

# K-Meansç®—æ³•åŽŸç†ï¼š
# 1. éšæœºåˆå§‹åŒ–kä¸ªç°‡ä¸­å¿ƒ
# 2. å°†æ¯ä¸ªæ ·æœ¬åˆ†é…åˆ°æœ€è¿‘çš„ç°‡ä¸­å¿ƒ
# 3. é‡æ–°è®¡ç®—æ¯ä¸ªç°‡çš„ä¸­å¿ƒç‚¹ï¼ˆå‡å€¼ï¼‰
# 4. é‡å¤æ­¥éª¤2-3ï¼Œç›´åˆ°æ”¶æ•›

# å‚æ•°è¯´æ˜Žï¼š
# - n_clusters=3: ç°‡çš„æ•°é‡ï¼ˆæˆ‘ä»¬çŸ¥é“Irisæœ‰3ä¸ªç±»åˆ«ï¼‰
# - n_init=10: ç”¨ä¸åŒçš„åˆå§‹ä¸­å¿ƒè¿è¡Œ10æ¬¡ï¼Œé€‰æ‹©æœ€å¥½çš„ç»“æžœ
# - max_iter=300: æœ€å¤§è¿­ä»£æ¬¡æ•°
# - random_state=42: éšæœºç§å­ï¼Œä¿è¯ç»“æžœå¯å¤çŽ°
kmeans = KMeans(n_clusters=3, n_init=10, max_iter=300, random_state=42)
y_kmeans = kmeans.fit_predict(X_scaled)

# è¯„ä¼°K-Meansèšç±»æ•ˆæžœ
silhouette_kmeans = silhouette_score(X_scaled, y_kmeans)
db_kmeans = davies_bouldin_score(X_scaled, y_kmeans)
ari_kmeans = adjusted_rand_score(y_true, y_kmeans)
ch_kmeans = calinski_harabasz_score(X_scaled, y_kmeans)

print(f"K-Meansèšç±»ç»“æžœ:")
print(f"  Silhouette Score (è½®å»“ç³»æ•°): {silhouette_kmeans:.4f}  # èŒƒå›´[-1,1]ï¼Œè¶Šå¤§è¶Šå¥½")
print(f"  Davies-Bouldin Index (DBæŒ‡æ•°): {db_kmeans:.4f}  # è¶Šå°è¶Šå¥½")
print(f"  Adjusted Rand Index (ARI): {ari_kmeans:.4f}  # èŒƒå›´[-1,1]ï¼Œ1è¡¨ç¤ºå®Œå…¨ä¸€è‡´")
print(f"  Calinski-Harabasz Index (CHæŒ‡æ•°): {ch_kmeans:.4f}  # è¶Šå¤§è¶Šå¥½")
print(f"  Inertia (æƒ¯æ€§): {kmeans.inertia_:.4f}  # ç°‡å†…è·ç¦»å¹³æ–¹å’Œï¼Œè¶Šå°è¶Šå¥½")

print(f"\næ¯ä¸ªç°‡çš„æ ·æœ¬æ•°é‡:")
for i in range(3):
    count = np.sum(y_kmeans == i)
    print(f"  ç°‡ {i}: {count} ä¸ªæ ·æœ¬")


# ==================== 4. DBSCANèšç±» ====================
print("\nã€æ­¥éª¤4ã€‘DBSCANèšç±»")
print("-" * 80)

# DBSCANç®—æ³•åŽŸç†ï¼š
# - åŸºäºŽå¯†åº¦çš„èšç±»ï¼Œä¸éœ€è¦é¢„å…ˆæŒ‡å®šç°‡çš„æ•°é‡
# - æ ¸å¿ƒæ€æƒ³ï¼šé«˜å¯†åº¦åŒºåŸŸå½¢æˆç°‡ï¼Œä½Žå¯†åº¦åŒºåŸŸä¸ºå™ªå£°ç‚¹
# - å‚æ•°ï¼š
#   1. eps: é‚»åŸŸåŠå¾„ï¼ˆä¸¤ç‚¹ä¹‹é—´çš„æœ€å¤§è·ç¦»ï¼‰
#   2. min_samples: æ ¸å¿ƒç‚¹çš„æœ€å°é‚»å±…æ•°é‡

# å‚æ•°é€‰æ‹©ï¼š
# - eps=0.5: ç»è¿‡å°è¯•ï¼Œ0.5å¯¹Irisæ•°æ®æ•ˆæžœè¾ƒå¥½
# - min_samples=5: ä¸€èˆ¬è®¾ç½®ä¸ºç‰¹å¾æ•°+1ï¼Œè¿™é‡Œ4+1=5
dbscan = DBSCAN(eps=0.5, min_samples=5)
y_dbscan = dbscan.fit_predict(X_scaled)

# DBSCANä¼šå°†å™ªå£°ç‚¹æ ‡è®°ä¸º-1
n_clusters_dbscan = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)
n_noise = list(y_dbscan).count(-1)

print(f"DBSCANèšç±»ç»“æžœ:")
print(f"  è¯†åˆ«çš„ç°‡æ•°é‡: {n_clusters_dbscan}")
print(f"  å™ªå£°ç‚¹æ•°é‡: {n_noise}")

# åªå¯¹éžå™ªå£°ç‚¹è®¡ç®—è¯„ä¼°æŒ‡æ ‡
if n_clusters_dbscan > 1 and n_noise < len(y_dbscan):
    # ç­›é€‰éžå™ªå£°ç‚¹
    mask = y_dbscan != -1
    X_no_noise = X_scaled[mask]
    y_dbscan_no_noise = y_dbscan[mask]
    y_true_no_noise = y_true[mask]

    silhouette_dbscan = silhouette_score(X_no_noise, y_dbscan_no_noise)
    db_dbscan = davies_bouldin_score(X_no_noise, y_dbscan_no_noise)
    ari_dbscan = adjusted_rand_score(y_true_no_noise, y_dbscan_no_noise)
    ch_dbscan = calinski_harabasz_score(X_no_noise, y_dbscan_no_noise)

    print(f"  Silhouette Score: {silhouette_dbscan:.4f}")
    print(f"  Davies-Bouldin Index: {db_dbscan:.4f}")
    print(f"  Adjusted Rand Index: {ari_dbscan:.4f}")
    print(f"  Calinski-Harabasz Index: {ch_dbscan:.4f}")
else:
    print("  âš ï¸ DBSCANæœªèƒ½æœ‰æ•ˆèšç±»ï¼ˆç°‡æ•°é‡<=1æˆ–å™ªå£°ç‚¹è¿‡å¤šï¼‰")
    silhouette_dbscan = db_dbscan = ari_dbscan = ch_dbscan = 0

print(f"\næ¯ä¸ªç°‡çš„æ ·æœ¬æ•°é‡:")
for i in sorted(set(y_dbscan)):
    count = np.sum(y_dbscan == i)
    label = "å™ªå£°ç‚¹" if i == -1 else f"ç°‡ {i}"
    print(f"  {label}: {count} ä¸ªæ ·æœ¬")


# ==================== 5. å±‚æ¬¡èšç±» ====================
print("\nã€æ­¥éª¤5ã€‘å±‚æ¬¡èšç±» (Hierarchical Clustering)")
print("-" * 80)

# å±‚æ¬¡èšç±»ç®—æ³•åŽŸç†ï¼š
# - å‡èšåž‹(Agglomerative)ï¼šè‡ªåº•å‘ä¸Šï¼Œæ¯ä¸ªç‚¹åˆå§‹ä¸ºä¸€ä¸ªç°‡ï¼Œé€æ­¥åˆå¹¶
# - å‚æ•°ï¼š
#   1. n_clusters: æœ€ç»ˆç°‡çš„æ•°é‡
#   2. linkage: ç°‡é—´è·ç¦»è®¡ç®—æ–¹æ³•
#      - 'ward': æœ€å°åŒ–ç°‡å†…æ–¹å·®ï¼ˆæœ€å¸¸ç”¨ï¼‰
#      - 'complete': æœ€å¤§è·ç¦»
#      - 'average': å¹³å‡è·ç¦»
#      - 'single': æœ€å°è·ç¦»

# ä½¿ç”¨wardè¿žæŽ¥æ–¹æ³•
hierarchical = AgglomerativeClustering(n_clusters=3, linkage='ward')
y_hierarchical = hierarchical.fit_predict(X_scaled)

# è¯„ä¼°å±‚æ¬¡èšç±»æ•ˆæžœ
silhouette_hier = silhouette_score(X_scaled, y_hierarchical)
db_hier = davies_bouldin_score(X_scaled, y_hierarchical)
ari_hier = adjusted_rand_score(y_true, y_hierarchical)
ch_hier = calinski_harabasz_score(X_scaled, y_hierarchical)

print(f"å±‚æ¬¡èšç±»ç»“æžœ (linkage='ward'):")
print(f"  Silhouette Score: {silhouette_hier:.4f}")
print(f"  Davies-Bouldin Index: {db_hier:.4f}")
print(f"  Adjusted Rand Index: {ari_hier:.4f}")
print(f"  Calinski-Harabasz Index: {ch_hier:.4f}")

print(f"\næ¯ä¸ªç°‡çš„æ ·æœ¬æ•°é‡:")
for i in range(3):
    count = np.sum(y_hierarchical == i)
    print(f"  ç°‡ {i}: {count} ä¸ªæ ·æœ¬")


# ==================== 6. ç»“æžœå¯¹æ¯” ====================
print("\nã€æ­¥éª¤6ã€‘ä¸‰ç§ç®—æ³•æ€§èƒ½å¯¹æ¯”")
print("-" * 80)

# åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
results = pd.DataFrame({
    'Algorithm': ['K-Means', 'DBSCAN', 'Hierarchical'],
    'Silhouette Score â†‘': [silhouette_kmeans, silhouette_dbscan, silhouette_hier],
    'Davies-Bouldin â†“': [db_kmeans, db_dbscan, db_hier],
    'Adjusted Rand Index â†‘': [ari_kmeans, ari_dbscan, ari_hier],
    'Calinski-Harabasz â†‘': [ch_kmeans, ch_dbscan, ch_hier]
})

print("\næ€§èƒ½å¯¹æ¯” (â†‘è¶Šå¤§è¶Šå¥½ï¼Œâ†“è¶Šå°è¶Šå¥½):")
print(results.to_string(index=False))

# æ‰¾å‡ºæœ€ä½³ç®—æ³•
print("\nå„æŒ‡æ ‡æœ€ä½³ç®—æ³•:")
print(f"  Silhouette Score: {results.loc[results['Silhouette Score â†‘'].idxmax(), 'Algorithm']}")
print(f"  Davies-Bouldin: {results.loc[results['Davies-Bouldin â†“'].idxmin(), 'Algorithm']}")
print(f"  Adjusted Rand Index: {results.loc[results['Adjusted Rand Index â†‘'].idxmax(), 'Algorithm']}")
print(f"  Calinski-Harabasz: {results.loc[results['Calinski-Harabasz â†‘'].idxmax(), 'Algorithm']}")


# ==================== 7. å¯è§†åŒ– ====================
print("\nã€æ­¥éª¤7ã€‘å¯è§†åŒ–èšç±»ç»“æžœ")
print("-" * 80)

# åˆ›å»º2x2å­å›¾
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
fig.suptitle('é¸¢å°¾èŠ±(Iris)èšç±»ç»“æžœå¯¹æ¯” (åŸºäºŽPCAé™ç»´)', fontsize=16, fontweight='bold')

# å®šä¹‰é¢œè‰²æ˜ å°„
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']

# å­å›¾1: çœŸå®žæ ‡ç­¾
ax1 = axes[0, 0]
for i, species in enumerate(iris.target_names):
    mask = y_true == i
    ax1.scatter(X_pca[mask, 0], X_pca[mask, 1],
               c=colors[i], label=species, s=50, alpha=0.7, edgecolors='black', linewidth=0.5)
ax1.set_title('çœŸå®žæ ‡ç­¾ (Ground Truth)', fontsize=14, fontweight='bold')
ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)
ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)
ax1.legend(loc='best')
ax1.grid(True, alpha=0.3)

# å­å›¾2: K-Meansèšç±»
ax2 = axes[0, 1]
for i in range(3):
    mask = y_kmeans == i
    ax2.scatter(X_pca[mask, 0], X_pca[mask, 1],
               c=colors[i], label=f'Cluster {i}', s=50, alpha=0.7, edgecolors='black', linewidth=0.5)
# ç»˜åˆ¶ç°‡ä¸­å¿ƒ (éœ€è¦å°†ç°‡ä¸­å¿ƒæŠ•å½±åˆ°PCAç©ºé—´)
centers_pca = pca.transform(kmeans.cluster_centers_)
ax2.scatter(centers_pca[:, 0], centers_pca[:, 1],
           c='red', marker='X', s=300, edgecolors='black', linewidth=2,
           label='Centroids', zorder=10)
ax2.set_title(f'K-Means (ARI={ari_kmeans:.3f})', fontsize=14, fontweight='bold')
ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)
ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)
ax2.legend(loc='best')
ax2.grid(True, alpha=0.3)

# å­å›¾3: DBSCANèšç±»
ax3 = axes[1, 0]
unique_labels = set(y_dbscan)
for i, label in enumerate(sorted(unique_labels)):
    mask = y_dbscan == label
    if label == -1:
        # å™ªå£°ç‚¹ç”¨ç°è‰²Xæ ‡è®°
        ax3.scatter(X_pca[mask, 0], X_pca[mask, 1],
                   c='gray', marker='x', s=50, alpha=0.5, label='Noise')
    else:
        ax3.scatter(X_pca[mask, 0], X_pca[mask, 1],
                   c=colors[label], label=f'Cluster {label}', s=50, alpha=0.7,
                   edgecolors='black', linewidth=0.5)
ax3.set_title(f'DBSCAN (ARI={ari_dbscan:.3f}, eps=0.5)', fontsize=14, fontweight='bold')
ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)
ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)
ax3.legend(loc='best')
ax3.grid(True, alpha=0.3)

# å­å›¾4: å±‚æ¬¡èšç±»
ax4 = axes[1, 1]
for i in range(3):
    mask = y_hierarchical == i
    ax4.scatter(X_pca[mask, 0], X_pca[mask, 1],
               c=colors[i], label=f'Cluster {i}', s=50, alpha=0.7, edgecolors='black', linewidth=0.5)
ax4.set_title(f'Hierarchical (ARI={ari_hier:.3f}, ward)', fontsize=14, fontweight='bold')
ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)
ax4.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)
ax4.legend(loc='best')
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('/Users/lyh/Desktop/ Machine Learning/unsupervised_learning/iris_clustering_results.png',
            dpi=300, bbox_inches='tight')
print("âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: iris_clustering_results.png")
plt.show()


# ==================== 8. è¯„ä¼°æŒ‡æ ‡é›·è¾¾å›¾ ====================
print("\nã€æ­¥éª¤8ã€‘ç»˜åˆ¶è¯„ä¼°æŒ‡æ ‡é›·è¾¾å›¾")
print("-" * 80)

# å½’ä¸€åŒ–æŒ‡æ ‡ (ä½¿å…¶åœ¨0-1èŒƒå›´å†…ï¼Œä¾¿äºŽå¯¹æ¯”)
def normalize_score(score, metric_type='maximize'):
    """
    å½’ä¸€åŒ–è¯„åˆ†åˆ°0-1èŒƒå›´
    metric_type: 'maximize' è¡¨ç¤ºè¶Šå¤§è¶Šå¥½, 'minimize' è¡¨ç¤ºè¶Šå°è¶Šå¥½
    """
    if metric_type == 'minimize':
        # å¯¹äºŽè¶Šå°è¶Šå¥½çš„æŒ‡æ ‡ï¼Œä½¿ç”¨å€’æ•°
        return 1 / (1 + score)
    else:
        # å¯¹äºŽè¶Šå¤§è¶Šå¥½çš„æŒ‡æ ‡ï¼Œç›´æŽ¥ä½¿ç”¨
        return max(0, min(1, (score + 1) / 2))  # ARIèŒƒå›´[-1,1]ï¼Œå½’ä¸€åŒ–åˆ°[0,1]

# å‡†å¤‡é›·è¾¾å›¾æ•°æ®
categories = ['Silhouette\n(å†…èšæ€§)', 'Davies-Bouldin\n(åˆ†ç¦»åº¦)',
              'Adjusted Rand\n(å‡†ç¡®æ€§)', 'Calinski-Harabasz\n(å¯¹æ¯”åº¦)']

# å½’ä¸€åŒ–å„æŒ‡æ ‡
kmeans_scores = [
    normalize_score(silhouette_kmeans),
    normalize_score(db_kmeans, 'minimize'),
    normalize_score(ari_kmeans),
    normalize_score(ch_kmeans / 1000)  # CHæŒ‡æ•°è¾ƒå¤§ï¼Œç¼©æ”¾ä¸€ä¸‹
]

dbscan_scores = [
    normalize_score(silhouette_dbscan),
    normalize_score(db_dbscan, 'minimize'),
    normalize_score(ari_dbscan),
    normalize_score(ch_dbscan / 1000)
]

hierarchical_scores = [
    normalize_score(silhouette_hier),
    normalize_score(db_hier, 'minimize'),
    normalize_score(ari_hier),
    normalize_score(ch_hier / 1000)
]

# é›·è¾¾å›¾éœ€è¦é—­åˆï¼Œå¤åˆ¶ç¬¬ä¸€ä¸ªå€¼åˆ°æœ€åŽ
angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
kmeans_scores += kmeans_scores[:1]
dbscan_scores += dbscan_scores[:1]
hierarchical_scores += hierarchical_scores[:1]
angles += angles[:1]

# ç»˜åˆ¶é›·è¾¾å›¾
fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

ax.plot(angles, kmeans_scores, 'o-', linewidth=2, label='K-Means', color='#FF6B6B')
ax.fill(angles, kmeans_scores, alpha=0.25, color='#FF6B6B')

ax.plot(angles, dbscan_scores, 'o-', linewidth=2, label='DBSCAN', color='#4ECDC4')
ax.fill(angles, dbscan_scores, alpha=0.25, color='#4ECDC4')

ax.plot(angles, hierarchical_scores, 'o-', linewidth=2, label='Hierarchical', color='#45B7D1')
ax.fill(angles, hierarchical_scores, alpha=0.25, color='#45B7D1')

ax.set_theta_offset(np.pi / 2)
ax.set_theta_direction(-1)
ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories, fontsize=11)
ax.set_ylim(0, 1)
ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)
ax.grid(True, linestyle='--', alpha=0.7)

plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)
plt.title('èšç±»ç®—æ³•è¯„ä¼°æŒ‡æ ‡å¯¹æ¯” (å½’ä¸€åŒ–)', fontsize=16, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig('/Users/lyh/Desktop/ Machine Learning/unsupervised_learning/iris_clustering_radar.png',
            dpi=300, bbox_inches='tight')
print("âœ… é›·è¾¾å›¾å·²ä¿å­˜: iris_clustering_radar.png")
plt.show()


# ==================== 9. æ€»ç»“ ====================
print("\n" + "=" * 80)
print("ã€æ€»ç»“ã€‘èšç±»åˆ†æžç»“æžœ".center(70))
print("=" * 80)

print("""
ðŸ“Š Irisæ•°æ®é›†èšç±»åˆ†æžæ€»ç»“:

1ï¸âƒ£  K-Meansèšç±»:
   âœ… ä¼˜ç‚¹: é€Ÿåº¦å¿«ï¼Œç»“æžœç¨³å®šï¼Œé€‚åˆçƒå½¢ç°‡
   âš ï¸  ç¼ºç‚¹: éœ€è¦é¢„å…ˆæŒ‡å®škå€¼ï¼Œå¯¹åˆå§‹å€¼æ•æ„Ÿ
   ðŸ“ˆ æ€§èƒ½: Silhouette={:.3f}, ARI={:.3f}
   ðŸ’¡ é€‚ç”¨åœºæ™¯: ç°‡å¤§å°ç›¸è¿‘ã€å½¢çŠ¶è§„åˆ™çš„æ•°æ®

2ï¸âƒ£  DBSCANèšç±»:
   âœ… ä¼˜ç‚¹: ä¸éœ€è¦æŒ‡å®škå€¼ï¼Œå¯ä»¥è¯†åˆ«å™ªå£°ç‚¹å’Œä»»æ„å½¢çŠ¶çš„ç°‡
   âš ï¸  ç¼ºç‚¹: å¯¹epså’Œmin_sampleså‚æ•°æ•æ„Ÿï¼Œå¯†åº¦å·®å¼‚å¤§æ—¶æ•ˆæžœå·®
   ðŸ“ˆ æ€§èƒ½: Silhouette={:.3f}, ARI={:.3f}
   ðŸ’¡ é€‚ç”¨åœºæ™¯: ç°‡å¯†åº¦ç›¸è¿‘ã€å­˜åœ¨å™ªå£°çš„æ•°æ®

3ï¸âƒ£  å±‚æ¬¡èšç±»:
   âœ… ä¼˜ç‚¹: å¯ä»¥ç”Ÿæˆå±‚æ¬¡ç»“æž„æ ‘ï¼Œä¸éœ€è¦é¢„å…ˆæŒ‡å®škå€¼
   âš ï¸  ç¼ºç‚¹: è®¡ç®—å¤æ‚åº¦é«˜O(nÂ²)ï¼Œä¸é€‚åˆå¤§æ•°æ®é›†
   ðŸ“ˆ æ€§èƒ½: Silhouette={:.3f}, ARI={:.3f}
   ðŸ’¡ é€‚ç”¨åœºæ™¯: éœ€è¦å¤šå±‚æ¬¡ç»“æž„åˆ†æžçš„å°æ•°æ®é›†

ðŸ† å¯¹äºŽIrisæ•°æ®é›†ï¼Œ{}è¡¨çŽ°æœ€ä½³ï¼

ðŸ’¡ å…³é”®å¯ç¤º:
   - Irisæ•°æ®é›†çš„3ä¸ªç±»åˆ«ä¸­ï¼ŒSetosaä¸Žå…¶ä»–ä¸¤ç±»åˆ†ç¦»æ˜Žæ˜¾
   - Versicolorå’ŒVirginicaå­˜åœ¨ä¸€å®šé‡å ï¼Œèšç±»éš¾åº¦è¾ƒå¤§
   - é€‰æ‹©èšç±»ç®—æ³•éœ€è¦æ ¹æ®æ•°æ®ç‰¹ç‚¹å’Œä¸šåŠ¡éœ€æ±‚
   - è¯„ä¼°æŒ‡æ ‡åº”ç»¼åˆè€ƒè™‘ï¼Œä¸èƒ½åªçœ‹å•ä¸€æŒ‡æ ‡
""".format(
    silhouette_kmeans, ari_kmeans,
    silhouette_dbscan, ari_dbscan,
    silhouette_hier, ari_hier,
    results.loc[results['Adjusted Rand Index â†‘'].idxmax(), 'Algorithm']
))

print("=" * 80)
print("âœ… ç»ƒä¹ å®Œæˆï¼".center(70))
print("=" * 80)
