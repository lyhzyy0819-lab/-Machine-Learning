"""
ğŸ¯ å®¢æˆ·åˆ†ç¾¤é¡¹ç›® (Customer Segmentation)
======================================

é¡¹ç›®ç›®æ ‡ï¼š
    ä½¿ç”¨K-Meanså’ŒGMMèšç±»ç®—æ³•å¯¹å•†åœºå®¢æˆ·è¿›è¡Œåˆ†ç¾¤åˆ†æï¼Œ
    ä»¥ä¾¿åˆ¶å®šé’ˆå¯¹æ€§çš„è¥é”€ç­–ç•¥ã€‚

æ•°æ®é›†ï¼š
    Kaggle Mall Customer Segmentation Dataset
    https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python

æ•°æ®å­—æ®µï¼š
    - CustomerID: å®¢æˆ·ID
    - Gender: æ€§åˆ« (Male/Female)
    - Age: å¹´é¾„
    - Annual Income (k$): å¹´æ”¶å…¥ï¼ˆåƒç¾å…ƒï¼‰
    - Spending Score (1-100): æ¶ˆè´¹è¯„åˆ†ï¼ˆ1-100ï¼Œç”±å•†åœºæ ¹æ®å®¢æˆ·è¡Œä¸ºè¯„å®šï¼‰

ä½œè€…: Machine Learning å­¦ä¹ é¡¹ç›®
æ—¥æœŸ: 2024å¹´11æœˆ
"""

# ============================================================================
# ç¬¬1éƒ¨åˆ†ï¼šå¯¼å…¥å¿…è¦çš„åº“
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# èšç±»ç®—æ³•
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

# æ•°æ®é¢„å¤„ç†
from sklearn.preprocessing import StandardScaler, LabelEncoder

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# æ¨¡å‹ä¿å­˜
import joblib
import json

# å¿½ç•¥è­¦å‘Š
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['font.sans-serif'] = [
    'Arial Unicode MS',  # macOSé€šç”¨
    'PingFang SC',       # macOSç³»ç»Ÿå­—ä½“
    'STHeiti',           # åæ–‡é»‘ä½“
    'SimHei',            # Windowsé»‘ä½“
]
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

# è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
np.random.seed(42)

print("âœ… åº“å¯¼å…¥å®Œæˆï¼")
print("=" * 60)


# ============================================================================
# ç¬¬2éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ä¸æ¢ç´¢ (EDA)
# ============================================================================

def load_data(filepath):
    """
    åŠ è½½å®¢æˆ·æ•°æ®

    Parameters:
    -----------
    filepath : str
        æ•°æ®æ–‡ä»¶è·¯å¾„

    Returns:
    --------
    df : DataFrame
        åŠ è½½çš„æ•°æ®æ¡†
    """
    # è¯»å–CSVæ–‡ä»¶
    df = pd.read_csv(filepath)
    return df


def explore_data(df):
    """
    æ•°æ®æ¢ç´¢æ€§åˆ†æ (EDA)

    ç›®çš„ï¼šäº†è§£æ•°æ®çš„åŸºæœ¬æƒ…å†µï¼ŒåŒ…æ‹¬æ•°æ®ç±»å‹ã€ç¼ºå¤±å€¼ã€ç»Ÿè®¡ç‰¹å¾ç­‰

    Parameters:
    -----------
    df : DataFrame
        å®¢æˆ·æ•°æ®
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®æ¢ç´¢æ€§åˆ†æ (EDA)")
    print("=" * 60)

    # ----- 1. åŸºæœ¬ä¿¡æ¯ -----
    print("\nã€1. æ•°æ®åŸºæœ¬ä¿¡æ¯ã€‘")
    print(f"  â€¢ æ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"  â€¢ åˆ—å: {list(df.columns)}")
    print("\næ•°æ®ç±»å‹:")
    print(df.dtypes)

    # ----- 2. å‰5è¡Œæ•°æ® -----
    print("\nã€2. æ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰ã€‘")
    print(df.head())

    # ----- 3. ç¼ºå¤±å€¼æ£€æŸ¥ -----
    print("\nã€3. ç¼ºå¤±å€¼æ£€æŸ¥ã€‘")
    missing = df.isnull().sum()
    if missing.sum() == 0:
        print("  âœ… æ•°æ®æ— ç¼ºå¤±å€¼")
    else:
        print(missing[missing > 0])

    # ----- 4. ç»Ÿè®¡æè¿° -----
    print("\nã€4. æ•°å€¼ç‰¹å¾ç»Ÿè®¡æè¿°ã€‘")
    print(df.describe())

    # ----- 5. ç±»åˆ«ç‰¹å¾åˆ†å¸ƒ -----
    print("\nã€5. æ€§åˆ«åˆ†å¸ƒã€‘")
    gender_counts = df['Gender'].value_counts()
    print(gender_counts)
    print(f"  ç”·æ€§å æ¯”: {gender_counts['Male']/len(df)*100:.1f}%")
    print(f"  å¥³æ€§å æ¯”: {gender_counts['Female']/len(df)*100:.1f}%")


def visualize_distributions(df):
    """
    å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ

    ç›®çš„ï¼šé€šè¿‡å›¾è¡¨ç›´è§‚äº†è§£å„ç‰¹å¾çš„åˆ†å¸ƒæƒ…å†µ

    Parameters:
    -----------
    df : DataFrame
        å®¢æˆ·æ•°æ®
    """
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–")
    print("=" * 60)

    fig, axes = plt.subplots(2, 3, figsize=(16, 10))

    # ----- 1. å¹´é¾„åˆ†å¸ƒ -----
    # ç›´æ–¹å›¾å±•ç¤ºå¹´é¾„çš„æ•´ä½“åˆ†å¸ƒ
    ax1 = axes[0, 0]
    ax1.hist(df['Age'], bins=20, color='steelblue', edgecolor='white', alpha=0.7)
    ax1.axvline(df['Age'].mean(), color='red', linestyle='--', linewidth=2, label=f'å‡å€¼={df["Age"].mean():.1f}')
    ax1.set_xlabel('å¹´é¾„')
    ax1.set_ylabel('é¢‘æ•°')
    ax1.set_title('å¹´é¾„åˆ†å¸ƒç›´æ–¹å›¾')
    ax1.legend()

    # ----- 2. å¹´æ”¶å…¥åˆ†å¸ƒ -----
    ax2 = axes[0, 1]
    ax2.hist(df['Annual Income (k$)'], bins=20, color='seagreen', edgecolor='white', alpha=0.7)
    ax2.axvline(df['Annual Income (k$)'].mean(), color='red', linestyle='--', linewidth=2,
                label=f'å‡å€¼={df["Annual Income (k$)"].mean():.1f}k$')
    ax2.set_xlabel('å¹´æ”¶å…¥ (k$)')
    ax2.set_ylabel('é¢‘æ•°')
    ax2.set_title('å¹´æ”¶å…¥åˆ†å¸ƒç›´æ–¹å›¾')
    ax2.legend()

    # ----- 3. æ¶ˆè´¹è¯„åˆ†åˆ†å¸ƒ -----
    ax3 = axes[0, 2]
    ax3.hist(df['Spending Score (1-100)'], bins=20, color='coral', edgecolor='white', alpha=0.7)
    ax3.axvline(df['Spending Score (1-100)'].mean(), color='red', linestyle='--', linewidth=2,
                label=f'å‡å€¼={df["Spending Score (1-100)"].mean():.1f}')
    ax3.set_xlabel('æ¶ˆè´¹è¯„åˆ† (1-100)')
    ax3.set_ylabel('é¢‘æ•°')
    ax3.set_title('æ¶ˆè´¹è¯„åˆ†åˆ†å¸ƒç›´æ–¹å›¾')
    ax3.legend()

    # ----- 4. æ€§åˆ«åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰-----
    ax4 = axes[1, 0]
    gender_counts = df['Gender'].value_counts()
    colors = ['#66b3ff', '#ff9999']
    explode = (0.05, 0)  # çªå‡ºæ˜¾ç¤ºç¬¬ä¸€å—
    ax4.pie(gender_counts, labels=['å¥³æ€§', 'ç”·æ€§'], autopct='%1.1f%%',
            colors=colors, explode=explode, startangle=90, shadow=True)
    ax4.set_title('æ€§åˆ«åˆ†å¸ƒ')

    # ----- 5. å¹´é¾„ç®±çº¿å›¾ï¼ˆæŒ‰æ€§åˆ«ï¼‰-----
    ax5 = axes[1, 1]
    df.boxplot(column='Age', by='Gender', ax=ax5)
    ax5.set_xlabel('æ€§åˆ«')
    ax5.set_ylabel('å¹´é¾„')
    ax5.set_title('å¹´é¾„åˆ†å¸ƒï¼ˆæŒ‰æ€§åˆ«ï¼‰')
    plt.suptitle('')  # ç§»é™¤è‡ªåŠ¨ç”Ÿæˆçš„æ ‡é¢˜

    # ----- 6. æ”¶å…¥ vs æ¶ˆè´¹è¯„åˆ† æ•£ç‚¹å›¾ -----
    # è¿™æ˜¯èšç±»åˆ†æçš„æ ¸å¿ƒç‰¹å¾ç»„åˆ
    ax6 = axes[1, 2]
    scatter = ax6.scatter(df['Annual Income (k$)'], df['Spending Score (1-100)'],
                         c=df['Age'], cmap='viridis', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)
    ax6.set_xlabel('å¹´æ”¶å…¥ (k$)')
    ax6.set_ylabel('æ¶ˆè´¹è¯„åˆ† (1-100)')
    ax6.set_title('æ”¶å…¥ vs æ¶ˆè´¹è¯„åˆ†ï¼ˆé¢œè‰²=å¹´é¾„ï¼‰')
    plt.colorbar(scatter, ax=ax6, label='å¹´é¾„')

    plt.tight_layout()
    plt.savefig('output/01_feature_distributions.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("  âœ… å›¾è¡¨å·²ä¿å­˜: output/01_feature_distributions.png")


def plot_correlation_heatmap(df):
    """
    ç»˜åˆ¶ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾

    ç›®çš„ï¼šäº†è§£å„æ•°å€¼ç‰¹å¾ä¹‹é—´çš„ç›¸å…³å…³ç³»

    Parameters:
    -----------
    df : DataFrame
        å®¢æˆ·æ•°æ®
    """
    print("\nã€ç›¸å…³æ€§åˆ†æã€‘")

    # é€‰æ‹©æ•°å€¼ç‰¹å¾
    numeric_cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
    corr_matrix = df[numeric_cols].corr()

    plt.figure(figsize=(8, 6))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                fmt='.2f', square=True, linewidths=0.5,
                annot_kws={'size': 14, 'weight': 'bold'})
    plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('output/02_correlation_heatmap.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("  âœ… å›¾è¡¨å·²ä¿å­˜: output/02_correlation_heatmap.png")

    # è§£è¯»ç›¸å…³æ€§
    print("\nã€ç›¸å…³æ€§è§£è¯»ã€‘")
    print(f"  â€¢ å¹´é¾„ vs å¹´æ”¶å…¥: {corr_matrix.loc['Age', 'Annual Income (k$)']:.3f}")
    print(f"  â€¢ å¹´é¾„ vs æ¶ˆè´¹è¯„åˆ†: {corr_matrix.loc['Age', 'Spending Score (1-100)']:.3f}")
    print(f"  â€¢ å¹´æ”¶å…¥ vs æ¶ˆè´¹è¯„åˆ†: {corr_matrix.loc['Annual Income (k$)', 'Spending Score (1-100)']:.3f}")
    print("\n  ğŸ’¡ ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§è¾ƒä½ï¼Œè¯´æ˜å®ƒä»¬å„è‡ªæä¾›äº†ä¸åŒçš„ä¿¡æ¯ç»´åº¦")


# ============================================================================
# ç¬¬3éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†
# ============================================================================

def preprocess_data(df):
    """
    æ•°æ®é¢„å¤„ç†

    æ­¥éª¤ï¼š
        1. æ€§åˆ«ç¼–ç ï¼ˆLabel Encodingï¼‰
        2. é€‰æ‹©èšç±»ç‰¹å¾
        3. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰

    Parameters:
    -----------
    df : DataFrame
        åŸå§‹å®¢æˆ·æ•°æ®

    Returns:
    --------
    X : array
        ç”¨äºèšç±»çš„åŸå§‹ç‰¹å¾ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
    X_scaled : array
        æ ‡å‡†åŒ–åçš„ç‰¹å¾
    feature_names : list
        ç‰¹å¾åç§°
    df_processed : DataFrame
        å¤„ç†åçš„æ•°æ®æ¡†ï¼ˆåŒ…å«ç¼–ç åçš„æ€§åˆ«ï¼‰
    scaler : StandardScaler
        è®­ç»ƒå¥½çš„æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºæ–°æ•°æ®é¢„æµ‹ï¼‰
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†")
    print("=" * 60)

    # å¤åˆ¶æ•°æ®ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    df_processed = df.copy()

    # ----- 1. æ€§åˆ«ç¼–ç  -----
    # å°†ç±»åˆ«å˜é‡è½¬æ¢ä¸ºæ•°å€¼ï¼šFemale=0, Male=1
    print("\nã€1. æ€§åˆ«ç¼–ç ã€‘")
    le = LabelEncoder()
    df_processed['Gender_encoded'] = le.fit_transform(df_processed['Gender'])
    print(f"  â€¢ ç¼–ç æ˜ å°„: {dict(zip(le.classes_, le.transform(le.classes_)))}")

    # ----- 2. é€‰æ‹©èšç±»ç‰¹å¾ -----
    # æ ¹æ®ä¸šåŠ¡åœºæ™¯ï¼Œé€‰æ‹©"å¹´æ”¶å…¥"å’Œ"æ¶ˆè´¹è¯„åˆ†"ä½œä¸ºä¸»è¦èšç±»ç‰¹å¾
    # ç†ç”±ï¼šè¿™ä¸¤ä¸ªç‰¹å¾ç›´æ¥åæ˜ å®¢æˆ·çš„æ¶ˆè´¹èƒ½åŠ›å’Œæ¶ˆè´¹æ„æ„¿
    print("\nã€2. é€‰æ‹©èšç±»ç‰¹å¾ã€‘")
    feature_names = ['Annual Income (k$)', 'Spending Score (1-100)']
    X = df_processed[feature_names].values
    print(f"  â€¢ é€‰æ‹©çš„ç‰¹å¾: {feature_names}")
    print(f"  â€¢ ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")

    # ----- 3. ç‰¹å¾æ ‡å‡†åŒ– -----
    # ä¸ºä»€ä¹ˆè¦æ ‡å‡†åŒ–ï¼Ÿ
    # K-Meansä½¿ç”¨æ¬§æ°è·ç¦»ï¼Œå¦‚æœç‰¹å¾å°ºåº¦ä¸åŒï¼Œå¤§å°ºåº¦ç‰¹å¾ä¼šä¸»å¯¼è·ç¦»è®¡ç®—
    # æ ‡å‡†åŒ–å…¬å¼: z = (x - Î¼) / Ïƒï¼Œä½¿æ¯ä¸ªç‰¹å¾å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1
    print("\nã€3. ç‰¹å¾æ ‡å‡†åŒ–ã€‘")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    print("  æ ‡å‡†åŒ–å‰åå¯¹æ¯”:")
    print(f"  â€¢ å¹´æ”¶å…¥ - åŸå§‹å‡å€¼: {X[:, 0].mean():.2f}, æ ‡å‡†åŒ–åå‡å€¼: {X_scaled[:, 0].mean():.6f}")
    print(f"  â€¢ å¹´æ”¶å…¥ - åŸå§‹æ ‡å‡†å·®: {X[:, 0].std():.2f}, æ ‡å‡†åŒ–åæ ‡å‡†å·®: {X_scaled[:, 0].std():.6f}")
    print(f"  â€¢ æ¶ˆè´¹è¯„åˆ† - åŸå§‹å‡å€¼: {X[:, 1].mean():.2f}, æ ‡å‡†åŒ–åå‡å€¼: {X_scaled[:, 1].mean():.6f}")
    print(f"  â€¢ æ¶ˆè´¹è¯„åˆ† - åŸå§‹æ ‡å‡†å·®: {X[:, 1].std():.2f}, æ ‡å‡†åŒ–åæ ‡å‡†å·®: {X_scaled[:, 1].std():.6f}")

    print("\n  âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")

    return X, X_scaled, feature_names, df_processed, scaler


# ============================================================================
# ç¬¬4éƒ¨åˆ†ï¼šç¡®å®šæœ€ä½³Kå€¼
# ============================================================================

def find_optimal_k(X_scaled, k_range=range(2, 11)):
    """
    ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°ç¡®å®šæœ€ä½³Kå€¼

    æ–¹æ³•è¯´æ˜ï¼š
    ---------
    1. è‚˜éƒ¨æ³•åˆ™ (Elbow Method)
       - è®¡ç®—ä¸åŒKå€¼ä¸‹çš„æƒ¯æ€§ï¼ˆInertiaï¼Œå³ç°‡å†…è·ç¦»å¹³æ–¹å’Œï¼‰
       - å¯»æ‰¾æƒ¯æ€§ä¸‹é™é€Ÿåº¦æ˜æ˜¾å˜ç¼“çš„"è‚˜éƒ¨"ç‚¹
       - æƒ¯æ€§å…¬å¼: J = Î£ Î£ ||x - Î¼_i||Â²

    2. è½®å»“ç³»æ•° (Silhouette Score)
       - è¡¡é‡ç°‡å†…ç´§å¯†åº¦å’Œç°‡é—´åˆ†ç¦»åº¦
       - èŒƒå›´: [-1, 1]ï¼Œè¶Šå¤§è¶Šå¥½
       - å…¬å¼: s(i) = (b(i) - a(i)) / max(a(i), b(i))
         å…¶ä¸­ a(i) = æ ·æœ¬iåˆ°åŒç°‡å…¶ä»–ç‚¹çš„å¹³å‡è·ç¦»
              b(i) = æ ·æœ¬iåˆ°æœ€è¿‘å…¶ä»–ç°‡çš„å¹³å‡è·ç¦»

    Parameters:
    -----------
    X_scaled : array
        æ ‡å‡†åŒ–åçš„ç‰¹å¾çŸ©é˜µ
    k_range : range
        è¦æµ‹è¯•çš„Kå€¼èŒƒå›´

    Returns:
    --------
    optimal_k : int
        æ¨èçš„æœ€ä½³Kå€¼
    """
    print("\n" + "=" * 60)
    print("ğŸ” ç¡®å®šæœ€ä½³èšç±»æ•° K")
    print("=" * 60)

    # å­˜å‚¨å„æŒ‡æ ‡
    inertias = []           # æƒ¯æ€§ï¼ˆç°‡å†…è·ç¦»å¹³æ–¹å’Œï¼‰
    silhouette_scores = []  # è½®å»“ç³»æ•°
    db_scores = []          # Davies-BouldinæŒ‡æ•°ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    ch_scores = []          # Calinski-HarabaszæŒ‡æ•°ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰

    print("\nã€è®¡ç®—å„Kå€¼çš„è¯„ä¼°æŒ‡æ ‡ã€‘")
    print("-" * 60)
    print(f"{'K':^5} {'Inertia':^12} {'Silhouette':^12} {'Davies-Bouldin':^15} {'Calinski-Harabasz':^18}")
    print("-" * 60)

    for k in k_range:
        # è®­ç»ƒK-Meansæ¨¡å‹
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)
        labels = kmeans.fit_predict(X_scaled)

        # è®¡ç®—æƒ¯æ€§
        inertias.append(kmeans.inertia_)

        # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆK>=2æ‰èƒ½è®¡ç®—ï¼‰
        sil_score = silhouette_score(X_scaled, labels)
        silhouette_scores.append(sil_score)

        # è®¡ç®—Davies-BouldinæŒ‡æ•°
        db_score = davies_bouldin_score(X_scaled, labels)
        db_scores.append(db_score)

        # è®¡ç®—Calinski-HarabaszæŒ‡æ•°
        ch_score = calinski_harabasz_score(X_scaled, labels)
        ch_scores.append(ch_score)

        print(f"{k:^5} {kmeans.inertia_:^12.2f} {sil_score:^12.4f} {db_score:^15.4f} {ch_score:^18.2f}")

    print("-" * 60)

    # ----- å¯è§†åŒ– -----
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # 1. è‚˜éƒ¨æ³•åˆ™å›¾
    ax1 = axes[0, 0]
    ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)
    ax1.set_xlabel('èšç±»æ•° K')
    ax1.set_ylabel('æƒ¯æ€§ (Inertia)')
    ax1.set_title('è‚˜éƒ¨æ³•åˆ™ - æƒ¯æ€§ vs K', fontweight='bold')
    ax1.grid(True, alpha=0.3)
    # æ ‡è®°K=5çš„ä½ç½®ï¼ˆé€šå¸¸æ˜¯è‚˜éƒ¨ï¼‰
    ax1.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='K=5 (å»ºè®®)')
    ax1.legend()

    # 2. è½®å»“ç³»æ•°å›¾
    ax2 = axes[0, 1]
    ax2.plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)
    ax2.set_xlabel('èšç±»æ•° K')
    ax2.set_ylabel('è½®å»“ç³»æ•°')
    ax2.set_title('è½®å»“ç³»æ•° vs Kï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰', fontweight='bold')
    ax2.grid(True, alpha=0.3)
    # æ‰¾åˆ°æœ€å¤§è½®å»“ç³»æ•°å¯¹åº”çš„K
    best_k_silhouette = list(k_range)[np.argmax(silhouette_scores)]
    ax2.axvline(x=best_k_silhouette, color='red', linestyle='--', alpha=0.7,
                label=f'æœ€ä½³ K={best_k_silhouette}')
    ax2.legend()

    # 3. Davies-BouldinæŒ‡æ•°å›¾
    ax3 = axes[1, 0]
    ax3.plot(k_range, db_scores, 'ro-', linewidth=2, markersize=8)
    ax3.set_xlabel('èšç±»æ•° K')
    ax3.set_ylabel('Davies-Bouldin æŒ‡æ•°')
    ax3.set_title('Davies-BouldinæŒ‡æ•° vs Kï¼ˆè¶Šå°è¶Šå¥½ï¼‰', fontweight='bold')
    ax3.grid(True, alpha=0.3)
    best_k_db = list(k_range)[np.argmin(db_scores)]
    ax3.axvline(x=best_k_db, color='red', linestyle='--', alpha=0.7, label=f'æœ€ä½³ K={best_k_db}')
    ax3.legend()

    # 4. Calinski-HarabaszæŒ‡æ•°å›¾
    ax4 = axes[1, 1]
    ax4.plot(k_range, ch_scores, 'mo-', linewidth=2, markersize=8)
    ax4.set_xlabel('èšç±»æ•° K')
    ax4.set_ylabel('Calinski-Harabasz æŒ‡æ•°')
    ax4.set_title('Calinski-HarabaszæŒ‡æ•° vs Kï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰', fontweight='bold')
    ax4.grid(True, alpha=0.3)
    best_k_ch = list(k_range)[np.argmax(ch_scores)]
    ax4.axvline(x=best_k_ch, color='red', linestyle='--', alpha=0.7, label=f'æœ€ä½³ K={best_k_ch}')
    ax4.legend()

    plt.tight_layout()
    plt.savefig('output/03_optimal_k_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("\n  âœ… å›¾è¡¨å·²ä¿å­˜: output/03_optimal_k_analysis.png")

    # ----- ç»¼åˆåˆ†æ -----
    print("\nã€ç»¼åˆåˆ†æã€‘")
    print(f"  â€¢ è½®å»“ç³»æ•°æœ€å¤§: K = {best_k_silhouette}")
    print(f"  â€¢ Davies-Bouldinæœ€å°: K = {best_k_db}")
    print(f"  â€¢ Calinski-Harabaszæœ€å¤§: K = {best_k_ch}")
    print(f"  â€¢ è‚˜éƒ¨æ³•åˆ™è§‚å¯Ÿ: K = 5 é™„è¿‘å‡ºç°æ˜æ˜¾æ‹ç‚¹")

    # ç»¼åˆå„æŒ‡æ ‡ï¼Œé€‰æ‹©K=5
    optimal_k = 5
    print(f"\n  ğŸ¯ æ¨èä½¿ç”¨ K = {optimal_k}")
    print("     ç†ç”±ï¼šå¤šä¸ªæŒ‡æ ‡ç»¼åˆæŒ‡å‘K=5ï¼Œä¸”ä»ä¸šåŠ¡è§’åº¦å¯è§£é‡Šæ€§å¼º")

    return optimal_k


# ============================================================================
# ç¬¬5éƒ¨åˆ†ï¼šK-Means èšç±»
# ============================================================================

def kmeans_clustering(X, X_scaled, optimal_k, feature_names):
    """
    æ‰§è¡ŒK-Meansèšç±»

    K-Meansç®—æ³•æ­¥éª¤ï¼š
    ----------------
    1. åˆå§‹åŒ–ï¼šéšæœºé€‰æ‹©Kä¸ªæ•°æ®ç‚¹ä½œä¸ºåˆå§‹ç°‡ä¸­å¿ƒ
    2. åˆ†é…ï¼šå°†æ¯ä¸ªæ•°æ®ç‚¹åˆ†é…åˆ°æœ€è¿‘çš„ç°‡ä¸­å¿ƒ
    3. æ›´æ–°ï¼šé‡æ–°è®¡ç®—æ¯ä¸ªç°‡çš„ä¸­å¿ƒï¼ˆç°‡å†…æ‰€æœ‰ç‚¹çš„å‡å€¼ï¼‰
    4. è¿­ä»£ï¼šé‡å¤æ­¥éª¤2-3ï¼Œç›´åˆ°æ”¶æ•›

    Parameters:
    -----------
    X : array
        åŸå§‹ç‰¹å¾çŸ©é˜µï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    X_scaled : array
        æ ‡å‡†åŒ–åçš„ç‰¹å¾çŸ©é˜µï¼ˆç”¨äºèšç±»ï¼‰
    optimal_k : int
        æœ€ä½³èšç±»æ•°
    feature_names : list
        ç‰¹å¾åç§°

    Returns:
    --------
    kmeans : KMeans
        è®­ç»ƒå¥½çš„K-Meansæ¨¡å‹
    labels : array
        æ¯ä¸ªæ ·æœ¬çš„ç°‡æ ‡ç­¾
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ K-Means èšç±»")
    print("=" * 60)

    # ----- è®­ç»ƒK-Meansæ¨¡å‹ -----
    print(f"\nã€æ¨¡å‹è®­ç»ƒã€‘")
    print(f"  â€¢ èšç±»æ•° K = {optimal_k}")
    print(f"  â€¢ åˆå§‹åŒ–æ–¹æ³•: k-means++ (æ™ºèƒ½åˆå§‹åŒ–)")
    print(f"  â€¢ åˆå§‹åŒ–æ¬¡æ•°: 10 (é€‰æ‹©æœ€ä¼˜ç»“æœ)")

    kmeans = KMeans(
        n_clusters=optimal_k,   # ç°‡çš„æ•°é‡
        init='k-means++',       # åˆå§‹åŒ–æ–¹æ³•ï¼šk-means++æ¯”éšæœºåˆå§‹åŒ–æ›´ä¼˜
        n_init=10,              # ä¸åŒåˆå§‹åŒ–çš„è¿è¡Œæ¬¡æ•°
        max_iter=300,           # æœ€å¤§è¿­ä»£æ¬¡æ•°
        random_state=42         # éšæœºç§å­
    )

    # è®­ç»ƒå¹¶é¢„æµ‹
    labels = kmeans.fit_predict(X_scaled)

    print(f"  â€¢ æ”¶æ•›è¿­ä»£æ¬¡æ•°: {kmeans.n_iter_}")
    print(f"  â€¢ æƒ¯æ€§ (Inertia): {kmeans.inertia_:.2f}")

    # ----- è¯„ä¼°èšç±»æ•ˆæœ -----
    print("\nã€èšç±»è¯„ä¼°ã€‘")
    sil_score = silhouette_score(X_scaled, labels)
    db_score = davies_bouldin_score(X_scaled, labels)
    ch_score = calinski_harabasz_score(X_scaled, labels)

    print(f"  â€¢ è½®å»“ç³»æ•°: {sil_score:.4f} (èŒƒå›´[-1,1]ï¼Œè¶Šå¤§è¶Šå¥½)")
    print(f"  â€¢ Davies-BouldinæŒ‡æ•°: {db_score:.4f} (è¶Šå°è¶Šå¥½)")
    print(f"  â€¢ Calinski-HarabaszæŒ‡æ•°: {ch_score:.2f} (è¶Šå¤§è¶Šå¥½)")

    # ----- ç°‡åˆ†å¸ƒç»Ÿè®¡ -----
    print("\nã€å„ç°‡æ ·æœ¬æ•°ã€‘")
    unique, counts = np.unique(labels, return_counts=True)
    for cluster, count in zip(unique, counts):
        print(f"  â€¢ ç°‡ {cluster}: {count} ä¸ªå®¢æˆ· ({count/len(labels)*100:.1f}%)")

    # ----- ç°‡ä¸­å¿ƒåˆ†æï¼ˆè¿˜åŸåˆ°åŸå§‹å°ºåº¦ï¼‰-----
    # æ³¨æ„ï¼škmeans.cluster_centers_ æ˜¯æ ‡å‡†åŒ–åçš„åæ ‡ï¼Œéœ€è¦è¿˜åŸ
    print("\nã€ç°‡ä¸­å¿ƒï¼ˆåŸå§‹å°ºåº¦ï¼‰ã€‘")

    # è®¡ç®—åŸå§‹æ•°æ®ä¸­æ¯ä¸ªç°‡çš„å‡å€¼ä½œä¸ºç°‡ä¸­å¿ƒ
    centers_original = np.array([X[labels == i].mean(axis=0) for i in range(optimal_k)])

    print(f"  {'ç°‡':<5} {feature_names[0]:<25} {feature_names[1]:<25}")
    print("  " + "-" * 55)
    for i in range(optimal_k):
        print(f"  {i:<5} {centers_original[i, 0]:<25.2f} {centers_original[i, 1]:<25.2f}")

    # ----- å¯è§†åŒ–èšç±»ç»“æœ -----
    plt.figure(figsize=(12, 8))

    # å®šä¹‰é¢œè‰²
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']

    # ç»˜åˆ¶å„ç°‡çš„æ•°æ®ç‚¹
    for i in range(optimal_k):
        mask = labels == i
        plt.scatter(X[mask, 0], X[mask, 1],
                   c=colors[i], s=80, alpha=0.6,
                   edgecolors='white', linewidth=0.5,
                   label=f'ç°‡ {i} (n={mask.sum()})')

    # ç»˜åˆ¶ç°‡ä¸­å¿ƒ
    plt.scatter(centers_original[:, 0], centers_original[:, 1],
               c='black', s=300, marker='*',
               edgecolors='white', linewidths=2,
               label='ç°‡ä¸­å¿ƒ', zorder=10)

    # ä¸ºæ¯ä¸ªç°‡ä¸­å¿ƒæ·»åŠ æ ‡ç­¾
    for i, center in enumerate(centers_original):
        plt.annotate(f'C{i}', xy=center, xytext=(5, 5),
                    textcoords='offset points', fontsize=12, fontweight='bold')

    plt.xlabel(feature_names[0], fontsize=12)
    plt.ylabel(feature_names[1], fontsize=12)
    plt.title(f'K-Means èšç±»ç»“æœ (K={optimal_k})\nè½®å»“ç³»æ•°={sil_score:.4f}',
              fontsize=14, fontweight='bold')
    plt.legend(loc='upper right')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('output/04_kmeans_clustering_result.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("\n  âœ… å›¾è¡¨å·²ä¿å­˜: output/04_kmeans_clustering_result.png")

    return kmeans, labels


# ============================================================================
# ç¬¬6éƒ¨åˆ†ï¼šGMM èšç±»å¯¹æ¯”
# ============================================================================

def gmm_clustering(X, X_scaled, optimal_k, feature_names, kmeans_labels):
    """
    ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ (GMM) è¿›è¡Œèšç±»ï¼Œå¹¶ä¸K-Meanså¯¹æ¯”

    GMM vs K-Means:
    ---------------
    | ç‰¹æ€§         | K-Means        | GMM                |
    |--------------|----------------|---------------------|
    | åˆ†é…æ–¹å¼     | ç¡¬åˆ†é…         | è½¯åˆ†é…ï¼ˆæ¦‚ç‡ï¼‰      |
    | ç°‡å½¢çŠ¶       | çƒå½¢           | æ¤­åœ†å½¢ï¼ˆæ›´çµæ´»ï¼‰    |
    | ç®—æ³•         | è·ç¦»æœ€å°åŒ–     | EMç®—æ³•ï¼ˆä¼¼ç„¶æœ€å¤§åŒ–ï¼‰|

    Parameters:
    -----------
    X : array
        åŸå§‹ç‰¹å¾çŸ©é˜µ
    X_scaled : array
        æ ‡å‡†åŒ–åçš„ç‰¹å¾çŸ©é˜µ
    optimal_k : int
        èšç±»æ•°
    feature_names : list
        ç‰¹å¾åç§°
    kmeans_labels : array
        K-Meansçš„èšç±»æ ‡ç­¾ï¼ˆç”¨äºå¯¹æ¯”ï¼‰

    Returns:
    --------
    gmm : GaussianMixture
        è®­ç»ƒå¥½çš„GMMæ¨¡å‹
    gmm_labels : array
        GMMçš„èšç±»æ ‡ç­¾
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ é«˜æ–¯æ··åˆæ¨¡å‹ (GMM) èšç±»")
    print("=" * 60)

    # ----- 1. ä½¿ç”¨BICé€‰æ‹©æœ€ä¼˜ç»„ä»¶æ•° -----
    print("\nã€1. BIC/AICæ¨¡å‹é€‰æ‹©ã€‘")

    n_components_range = range(2, 11)
    bic_scores = []
    aic_scores = []

    for n in n_components_range:
        gmm_test = GaussianMixture(n_components=n, covariance_type='full',
                                   random_state=42, n_init=5)
        gmm_test.fit(X_scaled)
        bic_scores.append(gmm_test.bic(X_scaled))
        aic_scores.append(gmm_test.aic(X_scaled))

    # BICæœ€å°å€¼å¯¹åº”çš„ç»„ä»¶æ•°
    best_n_bic = list(n_components_range)[np.argmin(bic_scores)]
    best_n_aic = list(n_components_range)[np.argmin(aic_scores)]

    print(f"  â€¢ BICæ¨èç»„ä»¶æ•°: {best_n_bic}")
    print(f"  â€¢ AICæ¨èç»„ä»¶æ•°: {best_n_aic}")

    # ----- 2. è®­ç»ƒGMMæ¨¡å‹ -----
    print(f"\nã€2. è®­ç»ƒGMMæ¨¡å‹ã€‘(ä½¿ç”¨K={optimal_k}è¿›è¡Œå…¬å¹³å¯¹æ¯”)")

    gmm = GaussianMixture(
        n_components=optimal_k,     # ä¸K-Meansç›¸åŒçš„ç°‡æ•°
        covariance_type='full',     # å®Œæ•´åæ–¹å·®çŸ©é˜µï¼ˆæœ€çµæ´»ï¼‰
        max_iter=100,
        n_init=10,
        random_state=42
    )

    gmm.fit(X_scaled)
    gmm_labels = gmm.predict(X_scaled)
    gmm_proba = gmm.predict_proba(X_scaled)  # è½¯åˆ†é…æ¦‚ç‡

    print(f"  â€¢ æ˜¯å¦æ”¶æ•›: {gmm.converged_}")
    print(f"  â€¢ è¿­ä»£æ¬¡æ•°: {gmm.n_iter_}")

    # ----- 3. è¯„ä¼°å¯¹æ¯” -----
    print("\nã€3. GMM vs K-Means å¯¹æ¯”ã€‘")
    print("-" * 50)

    sil_gmm = silhouette_score(X_scaled, gmm_labels)
    sil_kmeans = silhouette_score(X_scaled, kmeans_labels)

    db_gmm = davies_bouldin_score(X_scaled, gmm_labels)
    db_kmeans = davies_bouldin_score(X_scaled, kmeans_labels)

    print(f"  {'æŒ‡æ ‡':<20} {'K-Means':<15} {'GMM':<15}")
    print("  " + "-" * 50)
    print(f"  {'è½®å»“ç³»æ•°':<20} {sil_kmeans:<15.4f} {sil_gmm:<15.4f}")
    print(f"  {'Davies-Bouldin':<20} {db_kmeans:<15.4f} {db_gmm:<15.4f}")
    print("  " + "-" * 50)

    # ----- 4. å¯è§†åŒ–å¯¹æ¯” -----
    fig, axes = plt.subplots(2, 2, figsize=(16, 14))

    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']

    # å›¾1: BIC/AICæ›²çº¿
    ax1 = axes[0, 0]
    ax1.plot(n_components_range, bic_scores, 'o-', label='BIC', linewidth=2, markersize=8)
    ax1.plot(n_components_range, aic_scores, 's-', label='AIC', linewidth=2, markersize=8)
    ax1.axvline(best_n_bic, color='red', linestyle='--', alpha=0.7, label=f'BICæœ€ä¼˜: {best_n_bic}')
    ax1.set_xlabel('ç»„ä»¶æ•°')
    ax1.set_ylabel('ä¿¡æ¯å‡†åˆ™å€¼')
    ax1.set_title('BIC/AIC æ¨¡å‹é€‰æ‹©', fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # å›¾2: K-Meansç»“æœ
    ax2 = axes[0, 1]
    for i in range(optimal_k):
        mask = kmeans_labels == i
        ax2.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=60, alpha=0.6,
                   edgecolors='white', linewidth=0.5, label=f'ç°‡ {i}')
    ax2.set_xlabel(feature_names[0])
    ax2.set_ylabel(feature_names[1])
    ax2.set_title(f'K-Means (è½®å»“ç³»æ•°={sil_kmeans:.4f})', fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # å›¾3: GMMç»“æœ
    ax3 = axes[1, 0]
    for i in range(optimal_k):
        mask = gmm_labels == i
        ax3.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=60, alpha=0.6,
                   edgecolors='white', linewidth=0.5, label=f'ç°‡ {i}')
    ax3.set_xlabel(feature_names[0])
    ax3.set_ylabel(feature_names[1])
    ax3.set_title(f'GMM (è½®å»“ç³»æ•°={sil_gmm:.4f})', fontweight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # å›¾4: GMMæ¦‚ç‡ä¸ç¡®å®šæ€§
    # ç”¨æœ€å¤§æ¦‚ç‡çš„å€¼æ¥è¡¨ç¤ºç¡®å®šæ€§ï¼Œé¢œè‰²æ·±æµ…è¡¨ç¤ºåˆ†é…çš„ç¡®å®šç¨‹åº¦
    ax4 = axes[1, 1]
    max_proba = gmm_proba.max(axis=1)  # æ¯ä¸ªç‚¹æœ€å¤§æ¦‚ç‡
    scatter = ax4.scatter(X[:, 0], X[:, 1], c=max_proba, cmap='RdYlGn',
                         s=60, alpha=0.8, edgecolors='white', linewidth=0.5)
    plt.colorbar(scatter, ax=ax4, label='æœ€å¤§å½’å±æ¦‚ç‡')
    ax4.set_xlabel(feature_names[0])
    ax4.set_ylabel(feature_names[1])
    ax4.set_title('GMM åˆ†é…ç¡®å®šæ€§ï¼ˆç»¿è‰²=é«˜ç¡®å®šæ€§ï¼‰', fontweight='bold')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('output/05_gmm_vs_kmeans.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("\n  âœ… å›¾è¡¨å·²ä¿å­˜: output/05_gmm_vs_kmeans.png")

    # ----- 5. å±•ç¤ºè½¯åˆ†é…ç¤ºä¾‹ -----
    print("\nã€4. GMMè½¯åˆ†é…ç¤ºä¾‹ï¼ˆå‰5ä¸ªæ ·æœ¬ï¼‰ã€‘")
    print("  æ¯ä¸ªæ ·æœ¬å±äºå„ç°‡çš„æ¦‚ç‡:")
    proba_df = pd.DataFrame(gmm_proba[:5],
                           columns=[f'ç°‡{i}' for i in range(optimal_k)])
    proba_df['æœ€å¤§æ¦‚ç‡ç°‡'] = gmm_labels[:5]
    proba_df['ç¡®å®šæ€§'] = max_proba[:5]
    print(proba_df.to_string(index=False))

    return gmm, gmm_labels


# ============================================================================
# ç¬¬7éƒ¨åˆ†ï¼šå®¢æˆ·ç¾¤ç”»åƒåˆ†æ
# ============================================================================

def analyze_customer_segments(df, labels, optimal_k):
    """
    å®¢æˆ·ç¾¤ç”»åƒåˆ†æ

    ä¸ºæ¯ä¸ªå®¢æˆ·ç¾¤è®¡ç®—ç»Ÿè®¡ç‰¹å¾ï¼Œè¿›è¡Œå‘½åï¼Œå¹¶æä¾›è¥é”€å»ºè®®

    Parameters:
    -----------
    df : DataFrame
        åŸå§‹å®¢æˆ·æ•°æ®
    labels : array
        èšç±»æ ‡ç­¾
    optimal_k : int
        èšç±»æ•°

    Returns:
    --------
    segment_profiles : DataFrame
        å„å®¢æˆ·ç¾¤çš„ç”»åƒç»Ÿè®¡
    """
    print("\n" + "=" * 60)
    print("ğŸ‘¥ å®¢æˆ·ç¾¤ç”»åƒåˆ†æ")
    print("=" * 60)

    # æ·»åŠ èšç±»æ ‡ç­¾åˆ°æ•°æ®æ¡†
    df_analysis = df.copy()
    df_analysis['Cluster'] = labels

    # ----- 1. è®¡ç®—å„ç°‡ç»Ÿè®¡ç‰¹å¾ -----
    print("\nã€1. å„å®¢æˆ·ç¾¤ç»Ÿè®¡ç‰¹å¾ã€‘")

    # æ•°å€¼ç‰¹å¾ç»Ÿè®¡
    numeric_stats = df_analysis.groupby('Cluster').agg({
        'Age': ['mean', 'std', 'min', 'max'],
        'Annual Income (k$)': ['mean', 'std', 'min', 'max'],
        'Spending Score (1-100)': ['mean', 'std', 'min', 'max']
    }).round(2)

    # æ€§åˆ«æ¯”ä¾‹
    gender_ratio = df_analysis.groupby('Cluster')['Gender'].apply(
        lambda x: f"ç”·{(x=='Male').sum()}/å¥³{(x=='Female').sum()}"
    )

    # å®¢æˆ·æ•°é‡
    cluster_counts = df_analysis['Cluster'].value_counts().sort_index()

    # åˆ›å»ºç»¼åˆç”»åƒè¡¨
    segment_profiles = []
    for cluster in range(optimal_k):
        cluster_data = df_analysis[df_analysis['Cluster'] == cluster]
        profile = {
            'å®¢æˆ·ç¾¤': cluster,
            'å®¢æˆ·æ•°é‡': len(cluster_data),
            'å æ¯”(%)': round(len(cluster_data) / len(df_analysis) * 100, 1),
            'å¹³å‡å¹´é¾„': round(cluster_data['Age'].mean(), 1),
            'å¹³å‡å¹´æ”¶å…¥(k$)': round(cluster_data['Annual Income (k$)'].mean(), 1),
            'å¹³å‡æ¶ˆè´¹è¯„åˆ†': round(cluster_data['Spending Score (1-100)'].mean(), 1),
            'ç”·æ€§å æ¯”(%)': round((cluster_data['Gender'] == 'Male').sum() / len(cluster_data) * 100, 1)
        }
        segment_profiles.append(profile)

    profiles_df = pd.DataFrame(segment_profiles)
    print(profiles_df.to_string(index=False))

    # ----- 2. å®¢æˆ·ç¾¤å‘½å -----
    print("\nã€2. å®¢æˆ·ç¾¤å‘½åä¸ç‰¹å¾è§£è¯»ã€‘")
    print("-" * 70)

    # æ ¹æ®æ”¶å…¥å’Œæ¶ˆè´¹è¯„åˆ†çš„å‡å€¼è¿›è¡Œåˆ†ç±»å‘½å
    cluster_names = {}
    cluster_descriptions = {}
    marketing_strategies = {}

    for cluster in range(optimal_k):
        cluster_data = df_analysis[df_analysis['Cluster'] == cluster]
        avg_income = cluster_data['Annual Income (k$)'].mean()
        avg_spending = cluster_data['Spending Score (1-100)'].mean()
        avg_age = cluster_data['Age'].mean()

        # æ ¹æ®æ”¶å…¥å’Œæ¶ˆè´¹è¯„åˆ†è¿›è¡Œåˆ†ç±»
        # å®šä¹‰é˜ˆå€¼ï¼šæ”¶å…¥60k$ä¸ºåˆ†ç•Œï¼Œæ¶ˆè´¹è¯„åˆ†50ä¸ºåˆ†ç•Œ
        if avg_income >= 70 and avg_spending >= 60:
            name = "ğŸ’ VIPå®¢æˆ· (é«˜æ”¶å…¥é«˜æ¶ˆè´¹)"
            desc = "é«˜æ”¶å…¥ä¸”æ¶ˆè´¹æ„æ„¿å¼ºï¼Œæ˜¯å•†åœºæœ€æœ‰ä»·å€¼çš„å®¢æˆ·ç¾¤"
            strategy = "æä¾›ä¸“å±VIPæœåŠ¡ã€é«˜ç«¯å“ç‰Œæ¨èã€ä¼šå‘˜ç§¯åˆ†å¥–åŠ±ã€ä¼˜å…ˆä½“éªŒæ–°å“"
        elif avg_income >= 70 and avg_spending < 40:
            name = "ğŸ¯ æ½œåŠ›å®¢æˆ· (é«˜æ”¶å…¥ä½æ¶ˆè´¹)"
            desc = "æœ‰æ¶ˆè´¹èƒ½åŠ›ä½†æ¶ˆè´¹æ„æ„¿ä½ï¼Œå¯èƒ½æ˜¯ç†æ€§æ¶ˆè´¹è€…æˆ–å¯¹å•†åœºäº§å“ä¸æ„Ÿå…´è¶£"
            strategy = "ç²¾å‡†è¥é”€é«˜å“è´¨å•†å“ã€ä¸ªæ€§åŒ–æ¨èã€äº†è§£æ¶ˆè´¹åå¥½ã€æä¾›å®šåˆ¶æœåŠ¡"
        elif avg_income < 40 and avg_spending >= 60:
            name = "ğŸ”¥ å†²åŠ¨æ¶ˆè´¹å‹ (ä½æ”¶å…¥é«˜æ¶ˆè´¹)"
            desc = "æ”¶å…¥ä¸é«˜ä½†æ¶ˆè´¹æ„æ„¿å¼ºï¼Œå¯èƒ½æ˜¯å¹´è½»äººæˆ–æ³¨é‡ç”Ÿæ´»å“è´¨è€…"
            strategy = "æ¨é€ä¿ƒé”€æ´»åŠ¨ã€åˆ†æœŸä»˜æ¬¾é€‰é¡¹ã€æ€§ä»·æ¯”å•†å“æ¨èã€ä¼šå‘˜æŠ˜æ‰£"
        elif avg_income < 40 and avg_spending < 40:
            name = "ğŸ’° ä»·æ ¼æ•æ„Ÿå‹ (ä½æ”¶å…¥ä½æ¶ˆè´¹)"
            desc = "æ¶ˆè´¹èƒ½åŠ›å’Œæ„æ„¿éƒ½è¾ƒä½ï¼Œæ³¨é‡ä»·æ ¼"
            strategy = "æ‰“æŠ˜ä¿ƒé”€ä¿¡æ¯ã€ç‰¹ä»·å•†å“æ¨èã€ä¼˜æƒ åˆ¸å‘æ”¾ã€åŸºç¡€ä¼šå‘˜æœåŠ¡"
        else:
            name = "ğŸ“Š æ™®é€šå®¢æˆ· (ä¸­ç­‰æ°´å¹³)"
            desc = "æ”¶å…¥å’Œæ¶ˆè´¹éƒ½å¤„äºä¸­ç­‰æ°´å¹³ï¼Œæ˜¯å•†åœºçš„ä¸»åŠ›å®¢æˆ·ç¾¤"
            strategy = "å¸¸è§„ä¿ƒé”€æ´»åŠ¨ã€ä¼šå‘˜æƒç›Šä»‹ç»ã€å¤šæ ·åŒ–å•†å“æ¨èã€æå‡æ¶ˆè´¹ä½“éªŒ"

        cluster_names[cluster] = name
        cluster_descriptions[cluster] = desc
        marketing_strategies[cluster] = strategy

        print(f"\nç°‡ {cluster}: {name}")
        print(f"  ğŸ“Œ ç‰¹å¾: å¹³å‡å¹´é¾„{avg_age:.0f}å², å¹´æ”¶å…¥{avg_income:.0f}k$, æ¶ˆè´¹è¯„åˆ†{avg_spending:.0f}")
        print(f"  ğŸ“ æè¿°: {desc}")
        print(f"  ğŸ’¡ è¥é”€ç­–ç•¥: {strategy}")

    # ----- 3. å¯è§†åŒ–å®¢æˆ·ç”»åƒ -----
    fig, axes = plt.subplots(2, 2, figsize=(16, 14))

    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']

    # å›¾1: å„ç°‡åœ¨æ”¶å…¥-æ¶ˆè´¹ç©ºé—´çš„åˆ†å¸ƒ
    ax1 = axes[0, 0]
    for i in range(optimal_k):
        mask = labels == i
        ax1.scatter(df.loc[mask, 'Annual Income (k$)'],
                   df.loc[mask, 'Spending Score (1-100)'],
                   c=colors[i], s=80, alpha=0.6,
                   edgecolors='white', linewidth=0.5,
                   label=f'ç°‡{i}: {cluster_names[i].split("(")[0].strip()}')
    ax1.set_xlabel('å¹´æ”¶å…¥ (k$)', fontsize=12)
    ax1.set_ylabel('æ¶ˆè´¹è¯„åˆ†', fontsize=12)
    ax1.set_title('å®¢æˆ·åˆ†ç¾¤åˆ†å¸ƒå›¾', fontsize=14, fontweight='bold')
    ax1.legend(loc='upper left', fontsize=9)
    ax1.grid(True, alpha=0.3)

    # å›¾2: å„ç°‡å¹³å‡ç‰¹å¾é›·è¾¾å›¾ï¼ˆç®€åŒ–ä¸ºæ¡å½¢å›¾å¯¹æ¯”ï¼‰
    ax2 = axes[0, 1]
    x = np.arange(optimal_k)
    width = 0.25

    ax2.bar(x - width, profiles_df['å¹³å‡å¹´é¾„'], width, label='å¹³å‡å¹´é¾„', color='#3498db')
    ax2.bar(x, profiles_df['å¹³å‡å¹´æ”¶å…¥(k$)'], width, label='å¹³å‡å¹´æ”¶å…¥(k$)', color='#2ecc71')
    ax2.bar(x + width, profiles_df['å¹³å‡æ¶ˆè´¹è¯„åˆ†'], width, label='å¹³å‡æ¶ˆè´¹è¯„åˆ†', color='#e74c3c')

    ax2.set_xlabel('å®¢æˆ·ç¾¤')
    ax2.set_ylabel('æ•°å€¼')
    ax2.set_title('å„å®¢æˆ·ç¾¤å¹³å‡ç‰¹å¾å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels([f'ç°‡{i}' for i in range(optimal_k)])
    ax2.legend()
    ax2.grid(True, alpha=0.3, axis='y')

    # å›¾3: å„ç°‡å®¢æˆ·æ•°é‡
    ax3 = axes[1, 0]
    bars = ax3.bar(range(optimal_k), profiles_df['å®¢æˆ·æ•°é‡'], color=colors[:optimal_k],
                  edgecolor='white', linewidth=2)
    ax3.set_xlabel('å®¢æˆ·ç¾¤')
    ax3.set_ylabel('å®¢æˆ·æ•°é‡')
    ax3.set_title('å„å®¢æˆ·ç¾¤è§„æ¨¡', fontsize=14, fontweight='bold')
    ax3.set_xticks(range(optimal_k))
    ax3.set_xticklabels([f'ç°‡{i}' for i in range(optimal_k)])

    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, count, pct in zip(bars, profiles_df['å®¢æˆ·æ•°é‡'], profiles_df['å æ¯”(%)']):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                f'{count}\n({pct}%)', ha='center', va='bottom', fontsize=10)

    ax3.grid(True, alpha=0.3, axis='y')

    # å›¾4: å„ç°‡å¹´é¾„åˆ†å¸ƒç®±çº¿å›¾
    ax4 = axes[1, 1]
    bp = ax4.boxplot([df_analysis[df_analysis['Cluster']==i]['Age'] for i in range(optimal_k)],
                     patch_artist=True)
    for patch, color in zip(bp['boxes'], colors[:optimal_k]):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    ax4.set_xlabel('å®¢æˆ·ç¾¤')
    ax4.set_ylabel('å¹´é¾„')
    ax4.set_title('å„å®¢æˆ·ç¾¤å¹´é¾„åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax4.set_xticklabels([f'ç°‡{i}' for i in range(optimal_k)])
    ax4.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig('output/06_customer_segments_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("\n  âœ… å›¾è¡¨å·²ä¿å­˜: output/06_customer_segments_analysis.png")

    # ----- 4. è¾“å‡ºè¥é”€å»ºè®®æ±‡æ€» -----
    print("\n" + "=" * 60)
    print("ğŸ’¼ è¥é”€ç­–ç•¥æ±‡æ€»")
    print("=" * 60)

    for cluster in range(optimal_k):
        print(f"\nã€{cluster_names[cluster]}ã€‘")
        print(f"  å®¢æˆ·æ•°: {cluster_counts[cluster]} ({profiles_df.loc[cluster, 'å æ¯”(%)']:.1f}%)")
        print(f"  ç­–ç•¥: {marketing_strategies[cluster]}")

    return profiles_df, cluster_names


# ============================================================================
# ç¬¬8éƒ¨åˆ†ï¼šæ¨¡å‹ä¿å­˜ä¸åŠ è½½
# ============================================================================

def save_models(kmeans, gmm, scaler, feature_names, cluster_names, profiles_df, model_dir="models"):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯

    ä¿å­˜å†…å®¹ï¼š
    ---------
    1. kmeans_model.pkl - K-Meansèšç±»æ¨¡å‹
    2. gmm_model.pkl - GMMèšç±»æ¨¡å‹
    3. scaler.pkl - ç‰¹å¾æ ‡å‡†åŒ–å™¨
    4. cluster_info.json - ç°‡ä¿¡æ¯ï¼ˆåç§°ã€ç‰¹å¾ã€è¥é”€ç­–ç•¥ï¼‰

    Parameters:
    -----------
    kmeans : KMeans
        è®­ç»ƒå¥½çš„K-Meansæ¨¡å‹
    gmm : GaussianMixture
        è®­ç»ƒå¥½çš„GMMæ¨¡å‹
    scaler : StandardScaler
        è®­ç»ƒå¥½çš„æ ‡å‡†åŒ–å™¨
    feature_names : list
        ç‰¹å¾åç§°
    cluster_names : dict
        å„ç°‡çš„å‘½å
    profiles_df : DataFrame
        å„ç°‡çš„ç»Ÿè®¡ç”»åƒ
    model_dir : str
        æ¨¡å‹ä¿å­˜ç›®å½•
    """
    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹")
    print("=" * 60)

    # åˆ›å»ºæ¨¡å‹ç›®å½•
    model_path = Path(model_dir)
    model_path.mkdir(exist_ok=True)

    # ----- 1. ä¿å­˜K-Meansæ¨¡å‹ -----
    kmeans_path = model_path / "kmeans_model.pkl"
    joblib.dump(kmeans, kmeans_path)
    print(f"  âœ… K-Meansæ¨¡å‹å·²ä¿å­˜: {kmeans_path}")

    # ----- 2. ä¿å­˜GMMæ¨¡å‹ -----
    gmm_path = model_path / "gmm_model.pkl"
    joblib.dump(gmm, gmm_path)
    print(f"  âœ… GMMæ¨¡å‹å·²ä¿å­˜: {gmm_path}")

    # ----- 3. ä¿å­˜Scaler -----
    scaler_path = model_path / "scaler.pkl"
    joblib.dump(scaler, scaler_path)
    print(f"  âœ… æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")

    # ----- 4. ä¿å­˜ç°‡ä¿¡æ¯ -----
    cluster_info = {
        "feature_names": feature_names,
        "n_clusters": kmeans.n_clusters,
        "cluster_names": {str(k): v for k, v in cluster_names.items()},
        "cluster_profiles": profiles_df.to_dict(orient='records')
    }

    info_path = model_path / "cluster_info.json"
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(cluster_info, f, ensure_ascii=False, indent=2)
    print(f"  âœ… ç°‡ä¿¡æ¯å·²ä¿å­˜: {info_path}")

    print(f"\n  ğŸ“ æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ° '{model_dir}/' ç›®å½•")


def load_models(model_dir="models"):
    """
    åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹

    Parameters:
    -----------
    model_dir : str
        æ¨¡å‹ä¿å­˜ç›®å½•

    Returns:
    --------
    kmeans : KMeans
        K-Meansæ¨¡å‹
    gmm : GaussianMixture
        GMMæ¨¡å‹
    scaler : StandardScaler
        æ ‡å‡†åŒ–å™¨
    cluster_info : dict
        ç°‡ä¿¡æ¯
    """
    model_path = Path(model_dir)

    # åŠ è½½æ¨¡å‹
    kmeans = joblib.load(model_path / "kmeans_model.pkl")
    gmm = joblib.load(model_path / "gmm_model.pkl")
    scaler = joblib.load(model_path / "scaler.pkl")

    # åŠ è½½ç°‡ä¿¡æ¯
    with open(model_path / "cluster_info.json", 'r', encoding='utf-8') as f:
        cluster_info = json.load(f)

    print(f"âœ… æ¨¡å‹å·²ä» '{model_dir}/' åŠ è½½")

    return kmeans, gmm, scaler, cluster_info


def predict_new_customer(annual_income, spending_score, model_dir="models"):
    """
    å¯¹æ–°å®¢æˆ·è¿›è¡Œåˆ†ç¾¤é¢„æµ‹

    ä½¿ç”¨æ–¹æ³•ï¼š
    --------
    >>> cluster, cluster_name, proba = predict_new_customer(
    ...     annual_income=75,      # å¹´æ”¶å…¥ 75k$
    ...     spending_score=60      # æ¶ˆè´¹è¯„åˆ† 60
    ... )
    >>> print(f"å®¢æˆ·åˆ†ç¾¤: {cluster_name}")

    Parameters:
    -----------
    annual_income : float
        å¹´æ”¶å…¥ï¼ˆåƒç¾å…ƒï¼‰
    spending_score : float
        æ¶ˆè´¹è¯„åˆ†ï¼ˆ1-100ï¼‰
    model_dir : str
        æ¨¡å‹ç›®å½•

    Returns:
    --------
    cluster : int
        ç°‡æ ‡ç­¾
    cluster_name : str
        ç°‡åç§°
    proba : array
        å±äºå„ç°‡çš„æ¦‚ç‡ï¼ˆGMMè½¯åˆ†é…ï¼‰
    """
    # åŠ è½½æ¨¡å‹
    kmeans, gmm, scaler, cluster_info = load_models(model_dir)

    # æ„é€ ç‰¹å¾å‘é‡
    X_new = np.array([[annual_income, spending_score]])

    # æ ‡å‡†åŒ–
    X_new_scaled = scaler.transform(X_new)

    # K-Meansé¢„æµ‹
    cluster = kmeans.predict(X_new_scaled)[0]

    # GMMè½¯åˆ†é…æ¦‚ç‡
    proba = gmm.predict_proba(X_new_scaled)[0]

    # è·å–ç°‡åç§°
    cluster_name = cluster_info["cluster_names"].get(str(cluster), f"ç°‡{cluster}")

    print(f"\nğŸ¯ æ–°å®¢æˆ·åˆ†ç¾¤é¢„æµ‹ç»“æœ")
    print("=" * 50)
    print(f"  è¾“å…¥ç‰¹å¾:")
    print(f"    â€¢ å¹´æ”¶å…¥: {annual_income}k$")
    print(f"    â€¢ æ¶ˆè´¹è¯„åˆ†: {spending_score}")
    print(f"\n  é¢„æµ‹ç»“æœ:")
    print(f"    â€¢ æ‰€å±å®¢æˆ·ç¾¤: ç°‡{cluster}")
    print(f"    â€¢ å®¢æˆ·ç¾¤åç§°: {cluster_name}")
    print(f"\n  å„ç°‡å½’å±æ¦‚ç‡ (GMM):")
    for i, p in enumerate(proba):
        print(f"    â€¢ ç°‡{i}: {p*100:.2f}%")

    return cluster, cluster_name, proba


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„å®¢æˆ·åˆ†ç¾¤åˆ†ææµç¨‹
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ å®¢æˆ·åˆ†ç¾¤é¡¹ç›® (Customer Segmentation)")
    print("=" * 60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    # ----- Step 1: åŠ è½½æ•°æ® -----
    data_path = "data/Mall_Customers.csv"
    df = load_data(data_path)

    # ----- Step 2: æ•°æ®æ¢ç´¢ -----
    explore_data(df)
    visualize_distributions(df)
    plot_correlation_heatmap(df)

    # ----- Step 3: æ•°æ®é¢„å¤„ç† -----
    X, X_scaled, feature_names, df_processed, scaler = preprocess_data(df)

    # ----- Step 4: ç¡®å®šæœ€ä½³Kå€¼ -----
    optimal_k = find_optimal_k(X_scaled)

    # ----- Step 5: K-Meansèšç±» -----
    kmeans, kmeans_labels = kmeans_clustering(X, X_scaled, optimal_k, feature_names)

    # ----- Step 6: GMMèšç±»å¯¹æ¯” -----
    gmm, gmm_labels = gmm_clustering(X, X_scaled, optimal_k, feature_names, kmeans_labels)

    # ----- Step 7: å®¢æˆ·ç”»åƒåˆ†æ -----
    # ä½¿ç”¨K-Meansçš„ç»“æœè¿›è¡Œç”»åƒåˆ†æï¼ˆä¸¤è€…ç»“æœç›¸è¿‘ï¼‰
    profiles_df, cluster_names = analyze_customer_segments(df, kmeans_labels, optimal_k)

    # ----- Step 8: ä¿å­˜æ¨¡å‹ -----
    save_models(kmeans, gmm, scaler, feature_names, cluster_names, profiles_df)

    # ----- é¡¹ç›®æ€»ç»“ -----
    print("\n" + "=" * 60)
    print("ğŸ“‹ é¡¹ç›®æ€»ç»“")
    print("=" * 60)
    print("""
    æœ¬é¡¹ç›®å®Œæˆäº†ä»¥ä¸‹å·¥ä½œï¼š

    1. âœ… æ•°æ®æ¢ç´¢ (EDA)
       - åˆ†æäº†200ä¸ªå®¢æˆ·çš„æ•°æ®
       - å¯è§†åŒ–äº†å¹´é¾„ã€æ”¶å…¥ã€æ¶ˆè´¹è¯„åˆ†çš„åˆ†å¸ƒ
       - å‘ç°ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§è¾ƒä½ï¼Œå„è‡ªæä¾›ä¸åŒä¿¡æ¯

    2. âœ… æ•°æ®é¢„å¤„ç†
       - å¯¹æ€§åˆ«è¿›è¡Œäº†æ ‡ç­¾ç¼–ç 
       - é€‰æ‹©å¹´æ”¶å…¥å’Œæ¶ˆè´¹è¯„åˆ†ä½œä¸ºèšç±»ç‰¹å¾
       - ä½¿ç”¨StandardScalerè¿›è¡Œç‰¹å¾æ ‡å‡†åŒ–

    3. âœ… èšç±»åˆ†æ
       - ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°ç¡®å®šK=5
       - å®ŒæˆK-Meansèšç±»
       - å®ŒæˆGMMèšç±»å¹¶ä¸K-Meanså¯¹æ¯”

    4. âœ… å®¢æˆ·ç”»åƒ
       - è¯†åˆ«å‡º5ä¸ªä¸åŒçš„å®¢æˆ·ç¾¤ä½“
       - ä¸ºæ¯ä¸ªç¾¤ä½“è¿›è¡Œå‘½åå’Œç‰¹å¾æè¿°
       - æä¾›é’ˆå¯¹æ€§çš„è¥é”€å»ºè®®

    5. âœ… æ¨¡å‹ä¿å­˜
       - ä¿å­˜äº†K-Meanså’ŒGMMæ¨¡å‹
       - ä¿å­˜äº†æ ‡å‡†åŒ–å™¨å’Œç°‡ä¿¡æ¯
       - å¯ç”¨äºæ–°å®¢æˆ·åˆ†ç¾¤é¢„æµ‹

    ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š
       - output/01_feature_distributions.png
       - output/02_correlation_heatmap.png
       - output/03_optimal_k_analysis.png
       - output/04_kmeans_clustering_result.png
       - output/05_gmm_vs_kmeans.png
       - output/06_customer_segments_analysis.png

    ğŸ“¦ ä¿å­˜çš„æ¨¡å‹ï¼š
       - models/kmeans_model.pkl
       - models/gmm_model.pkl
       - models/scaler.pkl
       - models/cluster_info.json

    ğŸ”® æ–°å®¢æˆ·é¢„æµ‹ç¤ºä¾‹ï¼š
       from customer_segmentation import predict_new_customer
       cluster, name, proba = predict_new_customer(
           annual_income=75,    # å¹´æ”¶å…¥ 75k$
           spending_score=60    # æ¶ˆè´¹è¯„åˆ† 60
       )
    """)

    print("\nğŸ‰ é¡¹ç›®å®Œæˆï¼")
    print("=" * 60)


# ============================================================================
# ç¨‹åºå…¥å£
# ============================================================================

if __name__ == "__main__":
    # åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
    import os
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)

    # æ‰§è¡Œä¸»å‡½æ•°
    main()
