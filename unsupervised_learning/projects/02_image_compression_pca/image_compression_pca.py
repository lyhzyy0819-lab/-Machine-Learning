"""
ğŸ¯ PCAå›¾åƒå‹ç¼©é¡¹ç›® (Image Compression with PCA)
================================================

é¡¹ç›®ç›®æ ‡ï¼š
    ä½¿ç”¨ä¸»æˆåˆ†åˆ†æ(PCA)å®ç°å›¾åƒå‹ç¼©ï¼Œåˆ†æä¸åŒå‹ç¼©ç‡å¯¹å›¾åƒè´¨é‡çš„å½±å“ï¼Œ
    ç†è§£PCAåœ¨é™ç»´å’Œæ•°æ®å‹ç¼©ä¸­çš„å®é™…åº”ç”¨ã€‚

æ•°æ®é›†ï¼š
    Olivetti Faces Dataset (sklearnè‡ªå¸¦)
    - 400å¼ 64x64åƒç´ çš„äººè„¸ç°åº¦å›¾åƒ
    - 40ä¸ªä¸åŒçš„äººï¼Œæ¯äºº10å¼ ä¸åŒè¡¨æƒ…/è§’åº¦çš„ç…§ç‰‡
    - æ•°æ®é›†ç½‘å€: https://scikit-learn.org/stable/datasets/real_world.html#olivetti-faces-dataset

æ ¸å¿ƒæ¦‚å¿µï¼š
    - PCAé™ç»´åŸç†ï¼šå°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°ä½ç»´ç©ºé—´
    - æ–¹å·®è§£é‡Šç‡ï¼šè¡¡é‡ä¸»æˆåˆ†ä¿ç•™çš„ä¿¡æ¯é‡
    - å›¾åƒå‹ç¼©ï¼šå‡å°‘å­˜å‚¨ç©ºé—´çš„åŒæ—¶ä¿æŒè§†è§‰è´¨é‡
    - å‹ç¼©æ¯” vs é‡æ„è´¨é‡çš„æƒè¡¡

ä½œè€…: Machine Learning å­¦ä¹ é¡¹ç›®
æ—¥æœŸ: 2024å¹´11æœˆ
"""

# ============================================================================
# ç¬¬1éƒ¨åˆ†ï¼šå¯¼å…¥å¿…è¦çš„åº“
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time

# PCAç®—æ³•
from sklearn.decomposition import PCA

# æ•°æ®é›†ï¼ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œæ— éœ€ä»è¿œç¨‹ä¸‹è½½ï¼‰
# from sklearn.datasets import fetch_olivetti_faces  # å·²æ”¹ä¸ºæœ¬åœ°åŠ è½½

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import mean_squared_error

# æ¨¡å‹ä¿å­˜
import joblib
import json

# å¿½ç•¥è­¦å‘Š
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['font.sans-serif'] = [
    'Arial Unicode MS',  # macOSé€šç”¨
    'PingFang SC',       # macOSç³»ç»Ÿå­—ä½“
    'STHeiti',           # åæ–‡é»‘ä½“
    'SimHei',            # Windowsé»‘ä½“
]
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 11

# è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
np.random.seed(42)

print("âœ… åº“å¯¼å…¥å®Œæˆï¼")
print("=" * 80)


# ============================================================================
# ç¬¬2éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ä¸æ¢ç´¢
# ============================================================================

def load_olivetti_faces():
    """
    åŠ è½½Olivetti Facesäººè„¸æ•°æ®é›†

    æ•°æ®é›†è¯´æ˜ï¼š
        - 400å¼ 64x64åƒç´ çš„äººè„¸ç°åº¦å›¾åƒ
        - 40ä¸ªä¸åŒçš„äººï¼Œæ¯äºº10å¼ ç…§ç‰‡
        - åƒç´ å€¼èŒƒå›´ï¼š[0, 1]ï¼Œå·²å½’ä¸€åŒ–

    Returns:
    --------
    faces : ndarray, shape (400, 4096)
        äººè„¸å›¾åƒæ•°æ®ï¼Œæ¯è¡Œæ˜¯ä¸€å¼ 64x64å›¾åƒå±•å¹³åçš„å‘é‡
    targets : ndarray, shape (400,)
        äººè„¸æ ‡ç­¾ï¼Œè¡¨ç¤ºæ˜¯å“ªä¸ªäººï¼ˆ0-39ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸ“‚ æ­£åœ¨åŠ è½½Olivetti Facesæ•°æ®é›†...")
    print("=" * 80)

    # ========================================================================
    # ä»æœ¬åœ° archive/ ç›®å½•åŠ è½½æ•°æ®ï¼ˆKaggle ä¸‹è½½çš„æ•°æ®é›†ï¼‰
    # è§£å†³äº†ä»è¿œç¨‹æœåŠ¡å™¨ä¸‹è½½æ—¶çš„ 403 é”™è¯¯é—®é¢˜
    # ========================================================================
    data_dir = Path(__file__).parent / 'archive'

    # åŠ è½½åŸå§‹æ•°æ®
    # olivetti_faces.npy: shape (400, 64, 64) - 400å¼ 64x64çš„ç°åº¦äººè„¸å›¾åƒ
    # olivetti_faces_target.npy: shape (400,) - æ¯å¼ å›¾åƒå¯¹åº”çš„äººç‰©ID (0-39)
    faces_raw = np.load(data_dir / 'olivetti_faces.npy')
    targets = np.load(data_dir / 'olivetti_faces_target.npy')

    # è½¬æ¢æ ¼å¼ä»¥åŒ¹é…åŸå§‹ sklearn API çš„è¾“å‡ºæ ¼å¼
    images = faces_raw                    # shape: (400, 64, 64) åŸå§‹å›¾åƒæ ¼å¼
    faces = faces_raw.reshape(400, -1)    # shape: (400, 4096) å±•å¹³ä¸ºå‘é‡

    # æ‰“ä¹±æ•°æ®é¡ºåºï¼ˆä¿æŒä¸åŸä»£ç ä¸€è‡´çš„éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°ï¼‰
    np.random.seed(42)
    shuffle_idx = np.random.permutation(len(faces))
    faces = faces[shuffle_idx]
    targets = targets[shuffle_idx]
    images = images[shuffle_idx]

    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
    print(f"   - å›¾åƒæ•°é‡: {faces.shape[0]}")
    print(f"   - å›¾åƒå°ºå¯¸: 64 x 64 åƒç´ ")
    print(f"   - ç‰¹å¾ç»´åº¦: {faces.shape[1]} (64*64åƒç´ å±•å¹³)")
    print(f"   - äººç‰©æ•°é‡: {len(np.unique(targets))}")
    print(f"   - åƒç´ å€¼èŒƒå›´: [{faces.min():.2f}, {faces.max():.2f}]")
    print(f"   - æ•°æ®ç±»å‹: {faces.dtype}")

    return faces, targets, images


def explore_data(faces, targets, images):
    """
    æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)

    ç›®çš„ï¼š
        é€šè¿‡å¯è§†åŒ–äº†è§£æ•°æ®é›†çš„åŸºæœ¬æƒ…å†µ

    Parameters:
    -----------
    faces : ndarray, shape (400, 4096)
        å±•å¹³çš„å›¾åƒæ•°æ®
    targets : ndarray, shape (400,)
        äººè„¸æ ‡ç­¾
    images : ndarray, shape (400, 64, 64)
        åŸå§‹å›¾åƒæ ¼å¼
    """
    print("\n" + "=" * 80)
    print("ğŸ” æ•°æ®æ¢ç´¢åˆ†æ")
    print("=" * 80)

    # 1. æ•°æ®åŸºæœ¬ç»Ÿè®¡
    print("\nã€æ•°æ®ç»Ÿè®¡ã€‘")
    print(f"   - æ€»æ ·æœ¬æ•°: {len(faces)}")
    print(f"   - åŸå§‹ç»´åº¦: {faces.shape[1]} (æœªå‹ç¼©)")
    print(f"   - æ•°æ®å¤§å°: {faces.nbytes / 1024:.2f} KB")
    print(f"   - å¹³å‡åƒç´ å€¼: {faces.mean():.4f}")
    print(f"   - åƒç´ æ ‡å‡†å·®: {faces.std():.4f}")

    # 2. å¯è§†åŒ–éƒ¨åˆ†æ ·æœ¬
    print("\nã€å¯è§†åŒ–æ ·æœ¬ã€‘")
    print("   æ­£åœ¨ç”Ÿæˆæ ·æœ¬å›¾åƒ...")

    fig, axes = plt.subplots(4, 10, figsize=(15, 6))
    fig.suptitle('Olivetti Faces æ•°æ®é›†æ ·æœ¬ (å‰40å¼ å›¾åƒ)',
                 fontsize=14, fontweight='bold', y=1.00)

    for i in range(40):
        ax = axes[i // 10, i % 10]
        # æ˜¾ç¤ºå›¾åƒï¼ˆ64x64ç°åº¦å›¾ï¼‰
        ax.imshow(images[i], cmap='gray')
        ax.set_title(f'ID:{targets[i]}', fontsize=8)
        ax.axis('off')

    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    output_dir = Path('outputs')
    output_dir.mkdir(exist_ok=True)
    plt.savefig(output_dir / '01_data_samples.png', dpi=150, bbox_inches='tight')
    print(f"   âœ… æ ·æœ¬å›¾åƒå·²ä¿å­˜åˆ°: outputs/01_data_samples.png")
    plt.show()

    # 3. åƒç´ åˆ†å¸ƒåˆ†æ
    print("\nã€åƒç´ å€¼åˆ†å¸ƒã€‘")
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # åƒç´ å€¼ç›´æ–¹å›¾
    axes[0].hist(faces.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')
    axes[0].set_xlabel('åƒç´ å€¼', fontsize=11)
    axes[0].set_ylabel('é¢‘æ•°', fontsize=11)
    axes[0].set_title('æ‰€æœ‰å›¾åƒçš„åƒç´ å€¼åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[0].grid(True, alpha=0.3)

    # å•å¼ å›¾åƒçš„åƒç´ å€¼åˆ†å¸ƒ
    sample_idx = 0
    axes[1].hist(faces[sample_idx], bins=50, color='coral', alpha=0.7, edgecolor='black')
    axes[1].set_xlabel('åƒç´ å€¼', fontsize=11)
    axes[1].set_ylabel('é¢‘æ•°', fontsize=11)
    axes[1].set_title(f'å•å¼ å›¾åƒçš„åƒç´ å€¼åˆ†å¸ƒ (ID: {targets[sample_idx]})',
                      fontsize=12, fontweight='bold')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / '02_pixel_distribution.png', dpi=150, bbox_inches='tight')
    print(f"   âœ… åƒç´ åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: outputs/02_pixel_distribution.png")
    plt.show()


# ============================================================================
# ç¬¬3éƒ¨åˆ†ï¼šPCAå›¾åƒå‹ç¼©å®ç°
# ============================================================================

def apply_pca_compression(faces, n_components_list):
    """
    åº”ç”¨PCAå¯¹å›¾åƒè¿›è¡Œå‹ç¼©

    åŸç†ï¼š
        PCAé€šè¿‡æ‰¾åˆ°æ•°æ®æ–¹å·®æœ€å¤§çš„æ–¹å‘ï¼ˆä¸»æˆåˆ†ï¼‰ï¼Œå°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°ä½ç»´ç©ºé—´ã€‚
        å¯¹äºå›¾åƒå‹ç¼©ï¼š
        1. åŸå§‹å›¾åƒ: 64Ã—64 = 4096 ç»´
        2. PCAå‹ç¼©: æŠ•å½±åˆ° n_components ç»´
        3. é‡æ„å›¾åƒ: ä»ä½ç»´ç©ºé—´è¿˜åŸåˆ°åŸå§‹ç©ºé—´

        å‹ç¼©æ¯” = 1 - (n_components / 4096)
        ä¾‹å¦‚ï¼šn_components=100 æ—¶ï¼Œå‹ç¼©æ¯” = 1 - 100/4096 â‰ˆ 97.6%

    Parameters:
    -----------
    faces : ndarray, shape (400, 4096)
        åŸå§‹å›¾åƒæ•°æ®
    n_components_list : list
        è¦æµ‹è¯•çš„ä¸»æˆåˆ†æ•°é‡åˆ—è¡¨ï¼Œå¦‚ [10, 50, 100, ...]

    Returns:
    --------
    results : dict
        åŒ…å«æ¯ä¸ªn_componentså¯¹åº”çš„PCAæ¨¡å‹ã€é‡æ„å›¾åƒã€è¯„ä¼°æŒ‡æ ‡ç­‰
    """
    print("\n" + "=" * 80)
    print("ğŸ”§ åº”ç”¨PCAå‹ç¼©")
    print("=" * 80)

    results = {}
    n_features = faces.shape[1]  # 4096

    for n_comp in n_components_list:
        print(f"\nã€å‹ç¼©åˆ° {n_comp} ä¸ªä¸»æˆåˆ†ã€‘")

        # åˆ›å»ºPCAæ¨¡å‹
        # n_components: ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡
        # svd_solver='randomized': ä½¿ç”¨éšæœºåŒ–ç®—æ³•ï¼Œé€‚åˆå¤§æ•°æ®é›†
        # whiten=False: ä¸è¿›è¡Œç™½åŒ–å¤„ç†
        start_time = time.time()
        pca = PCA(n_components=n_comp, svd_solver='randomized', random_state=42)

        # æ­¥éª¤1ï¼šæ‹ŸåˆPCAæ¨¡å‹å¹¶è½¬æ¢æ•°æ®ï¼ˆé™ç»´ï¼‰
        # fit_transform ä¼šï¼š
        #   1. è®¡ç®—æ•°æ®çš„åæ–¹å·®çŸ©é˜µ
        #   2. æ±‚è§£ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
        #   3. é€‰æ‹©å‰n_compä¸ªæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡
        #   4. å°†åŸå§‹æ•°æ®æŠ•å½±åˆ°è¿™äº›ä¸»æˆåˆ†ä¸Š
        # ç»“æœå½¢çŠ¶: (400, n_comp)
        faces_compressed = pca.fit_transform(faces)

        # æ­¥éª¤2ï¼šä»å‹ç¼©æ•°æ®é‡æ„åŸå§‹å›¾åƒï¼ˆå‡ç»´ï¼‰
        # inverse_transform ä¼šï¼š
        #   å°†ä½ç»´æ•°æ®æŠ•å½±å›åŸå§‹é«˜ç»´ç©ºé—´
        # å…¬å¼: X_reconstructed = X_compressed @ components + mean
        # ç»“æœå½¢çŠ¶: (400, 4096)
        faces_reconstructed = pca.inverse_transform(faces_compressed)

        compression_time = time.time() - start_time

        # è®¡ç®—å‹ç¼©æ¯”
        # åŸå§‹å­˜å‚¨: n_samples Ã— n_features
        # å‹ç¼©å­˜å‚¨: n_samples Ã— n_components + n_components Ã— n_features (PCAå‚æ•°)
        # ç®€åŒ–è®¡ç®—: 1 - (n_components / n_features)
        compression_ratio = 1 - (n_comp / n_features)

        # è®¡ç®—æ–¹å·®è§£é‡Šç‡
        # explained_variance_ratio_: æ¯ä¸ªä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®å æ¯”
        # cumsum: ç´¯ç§¯æ–¹å·®è§£é‡Šç‡
        variance_explained = pca.explained_variance_ratio_
        cumsum_variance = np.cumsum(variance_explained)
        total_variance_explained = cumsum_variance[-1]

        # è®¡ç®—é‡æ„è¯¯å·® (MSE)
        # MSE = mean((åŸå§‹ - é‡æ„)^2)
        # è¶Šå°è¡¨ç¤ºé‡æ„è´¨é‡è¶Šå¥½
        mse = mean_squared_error(faces, faces_reconstructed)

        # è®¡ç®—å³°å€¼ä¿¡å™ªæ¯” (PSNR)
        # PSNR = 10 * log10(MAX^2 / MSE)
        # MAX æ˜¯åƒç´ æœ€å¤§å€¼ï¼Œè¿™é‡Œæ˜¯1.0ï¼ˆå½’ä¸€åŒ–åï¼‰
        # PSNRè¶Šå¤§è¡¨ç¤ºå›¾åƒè´¨é‡è¶Šå¥½ï¼Œé€šå¸¸ >30dB è®¤ä¸ºè´¨é‡è¾ƒå¥½
        psnr = 10 * np.log10(1.0**2 / mse) if mse > 0 else float('inf')

        # ä¿å­˜ç»“æœ
        results[n_comp] = {
            'pca_model': pca,
            'compressed_data': faces_compressed,
            'reconstructed_data': faces_reconstructed,
            'compression_ratio': compression_ratio,
            'variance_explained': variance_explained,
            'total_variance_explained': total_variance_explained,
            'mse': mse,
            'psnr': psnr,
            'compression_time': compression_time
        }

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"   âœ… å‹ç¼©å®Œæˆï¼")
        print(f"      - å‹ç¼©åç»´åº¦: {faces_compressed.shape}")
        print(f"      - å‹ç¼©æ¯”: {compression_ratio:.2%}")
        print(f"      - æ–¹å·®è§£é‡Šç‡: {total_variance_explained:.2%}")
        print(f"      - é‡æ„è¯¯å·® (MSE): {mse:.6f}")
        print(f"      - å³°å€¼ä¿¡å™ªæ¯” (PSNR): {psnr:.2f} dB")
        print(f"      - å‹ç¼©ç”¨æ—¶: {compression_time:.3f} ç§’")

    return results


def calculate_variance_curve(faces, max_components=400):
    """
    è®¡ç®—æ–¹å·®è§£é‡Šç‡æ›²çº¿

    ç›®çš„ï¼š
        äº†è§£éœ€è¦å¤šå°‘ä¸ªä¸»æˆåˆ†æ‰èƒ½ä¿ç•™è¶³å¤Ÿçš„ä¿¡æ¯ï¼ˆå¦‚90%ã€95%ã€99%æ–¹å·®ï¼‰

    Parameters:
    -----------
    faces : ndarray, shape (400, 4096)
        åŸå§‹å›¾åƒæ•°æ®
    max_components : int
        æœ€å¤§ä¸»æˆåˆ†æ•°é‡ï¼ˆä¸è¶…è¿‡æ ·æœ¬æ•°ï¼‰

    Returns:
    --------
    pca_full : PCAå¯¹è±¡
        å®Œæ•´çš„PCAæ¨¡å‹
    cumsum_variance : ndarray
        ç´¯ç§¯æ–¹å·®è§£é‡Šç‡
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š è®¡ç®—å®Œæ•´æ–¹å·®è§£é‡Šç‡æ›²çº¿")
    print("=" * 80)

    # ä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„ä¸»æˆåˆ†ï¼ˆæœ€å¤šmin(n_samples, n_features)ï¼‰
    pca_full = PCA(n_components=max_components, random_state=42)
    pca_full.fit(faces)

    # ç´¯ç§¯æ–¹å·®è§£é‡Šç‡
    cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)

    # æ‰¾åˆ°ä¿ç•™ä¸åŒæ–¹å·®æ¯”ä¾‹æ‰€éœ€çš„ä¸»æˆåˆ†æ•°
    variance_thresholds = [0.80, 0.90, 0.95, 0.99]
    print("\nã€æ–¹å·®è§£é‡Šç‡åˆ†æã€‘")
    for threshold in variance_thresholds:
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¶…è¿‡é˜ˆå€¼çš„ç´¢å¼•
        n_comp_needed = np.argmax(cumsum_variance >= threshold) + 1
        compression_ratio = 1 - (n_comp_needed / faces.shape[1])
        print(f"   ä¿ç•™ {threshold:.0%} æ–¹å·®éœ€è¦: {n_comp_needed:3d} ä¸ªä¸»æˆåˆ† "
              f"(å‹ç¼©æ¯”: {compression_ratio:.2%})")

    return pca_full, cumsum_variance


# ============================================================================
# ç¬¬4éƒ¨åˆ†ï¼šå¯è§†åŒ–
# ============================================================================

def visualize_compression_comparison(faces, images, results, n_components_list):
    """
    å¯è§†åŒ–åŸå§‹å›¾åƒä¸ä¸åŒå‹ç¼©ç‡çš„é‡æ„å›¾åƒå¯¹æ¯”

    Parameters:
    -----------
    faces : ndarray, shape (400, 4096)
        åŸå§‹å›¾åƒæ•°æ®ï¼ˆå±•å¹³ï¼‰
    images : ndarray, shape (400, 64, 64)
        åŸå§‹å›¾åƒæ ¼å¼
    results : dict
        PCAå‹ç¼©ç»“æœ
    n_components_list : list
        ä¸»æˆåˆ†æ•°é‡åˆ—è¡¨
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š å¯è§†åŒ–å‹ç¼©æ•ˆæœå¯¹æ¯”")
    print("=" * 80)

    # é€‰æ‹©å‡ å¼ ä»£è¡¨æ€§å›¾åƒ
    sample_indices = [0, 10, 50, 100, 200]
    n_samples = len(sample_indices)
    n_compressions = len(n_components_list)

    # åˆ›å»ºå­å›¾ï¼šç¬¬ä¸€è¡Œæ˜¯åŸå§‹å›¾åƒï¼Œåç»­è¡Œæ˜¯ä¸åŒå‹ç¼©ç‡
    fig, axes = plt.subplots(n_compressions + 1, n_samples,
                             figsize=(15, 2.5 * (n_compressions + 1)))

    fig.suptitle('PCAå›¾åƒå‹ç¼©æ•ˆæœå¯¹æ¯”', fontsize=16, fontweight='bold', y=0.995)

    # ç¬¬ä¸€è¡Œï¼šåŸå§‹å›¾åƒ
    for j, idx in enumerate(sample_indices):
        axes[0, j].imshow(images[idx], cmap='gray', vmin=0, vmax=1)
        axes[0, j].set_title(f'åŸå§‹å›¾åƒ\n(4096ç»´)', fontsize=10, fontweight='bold')
        axes[0, j].axis('off')

    # åç»­è¡Œï¼šä¸åŒå‹ç¼©ç‡çš„é‡æ„å›¾åƒ
    for i, n_comp in enumerate(n_components_list):
        reconstructed = results[n_comp]['reconstructed_data']
        compression_ratio = results[n_comp]['compression_ratio']
        variance = results[n_comp]['total_variance_explained']
        psnr = results[n_comp]['psnr']

        for j, idx in enumerate(sample_indices):
            # é‡æ„å›¾åƒreshapeå›64x64
            img_reconstructed = reconstructed[idx].reshape(64, 64)

            axes[i + 1, j].imshow(img_reconstructed, cmap='gray', vmin=0, vmax=1)

            # ç¬¬ä¸€åˆ—æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            if j == 0:
                title = (f'{n_comp}ä¸ªä¸»æˆåˆ†\n'
                        f'å‹ç¼©æ¯”:{compression_ratio:.1%}\n'
                        f'æ–¹å·®:{variance:.1%}|PSNR:{psnr:.1f}dB')
            else:
                title = f'{n_comp}ç»´'

            axes[i + 1, j].set_title(title, fontsize=9)
            axes[i + 1, j].axis('off')

    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    output_dir = Path('outputs')
    plt.savefig(output_dir / '03_compression_comparison.png', dpi=200, bbox_inches='tight')
    print("   âœ… å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: outputs/03_compression_comparison.png")
    plt.show()


def visualize_variance_curve(cumsum_variance):
    """
    å¯è§†åŒ–ç´¯ç§¯æ–¹å·®è§£é‡Šç‡æ›²çº¿

    ç›®çš„ï¼š
        äº†è§£ä¸»æˆåˆ†æ•°é‡ä¸ä¿¡æ¯ä¿ç•™é‡çš„å…³ç³»

    Parameters:
    -----------
    cumsum_variance : ndarray
        ç´¯ç§¯æ–¹å·®è§£é‡Šç‡
    """
    print("\nã€ç»˜åˆ¶æ–¹å·®è§£é‡Šç‡æ›²çº¿ã€‘")

    fig, ax = plt.subplots(figsize=(12, 6))

    n_components = len(cumsum_variance)

    # ç»˜åˆ¶ç´¯ç§¯æ–¹å·®æ›²çº¿
    ax.plot(range(1, n_components + 1), cumsum_variance * 100,
            linewidth=2.5, color='steelblue', label='ç´¯ç§¯æ–¹å·®è§£é‡Šç‡')

    # æ·»åŠ å‚è€ƒçº¿
    variance_thresholds = [80, 90, 95, 99]
    colors = ['green', 'orange', 'red', 'purple']

    for threshold, color in zip(variance_thresholds, colors):
        ax.axhline(y=threshold, color=color, linestyle='--', alpha=0.6, linewidth=1.5,
                  label=f'{threshold}% æ–¹å·®é˜ˆå€¼')

        # æ‰¾åˆ°å¯¹åº”çš„ä¸»æˆåˆ†æ•°
        n_comp_needed = np.argmax(cumsum_variance >= threshold/100) + 1
        ax.axvline(x=n_comp_needed, color=color, linestyle=':', alpha=0.4, linewidth=1.5)

        # æ ‡æ³¨ç‚¹
        ax.plot(n_comp_needed, threshold, 'o', color=color, markersize=8)
        ax.text(n_comp_needed + 5, threshold - 3,
               f'n={n_comp_needed}', fontsize=9, color=color, fontweight='bold')

    ax.set_xlabel('ä¸»æˆåˆ†æ•°é‡', fontsize=12, fontweight='bold')
    ax.set_ylabel('ç´¯ç§¯æ–¹å·®è§£é‡Šç‡ (%)', fontsize=12, fontweight='bold')
    ax.set_title('PCAç´¯ç§¯æ–¹å·®è§£é‡Šç‡æ›²çº¿', fontsize=14, fontweight='bold', pad=15)
    ax.legend(loc='lower right', fontsize=10)
    ax.grid(True, alpha=0.3)
    ax.set_xlim(0, n_components)
    ax.set_ylim(0, 105)

    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    output_dir = Path('outputs')
    plt.savefig(output_dir / '04_variance_curve.png', dpi=150, bbox_inches='tight')
    print("   âœ… æ–¹å·®æ›²çº¿å·²ä¿å­˜åˆ°: outputs/04_variance_curve.png")
    plt.show()


def visualize_metrics_comparison(results, n_components_list):
    """
    å¯è§†åŒ–å‹ç¼©æ¯”ã€æ–¹å·®è§£é‡Šç‡ã€MSEã€PSNRçš„å¯¹æ¯”

    Parameters:
    -----------
    results : dict
        PCAå‹ç¼©ç»“æœ
    n_components_list : list
        ä¸»æˆåˆ†æ•°é‡åˆ—è¡¨
    """
    print("\nã€ç»˜åˆ¶è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”å›¾ã€‘")

    # æå–æŒ‡æ ‡
    compression_ratios = [results[n]['compression_ratio'] * 100 for n in n_components_list]
    variance_explained = [results[n]['total_variance_explained'] * 100 for n in n_components_list]
    mse_values = [results[n]['mse'] for n in n_components_list]
    psnr_values = [results[n]['psnr'] for n in n_components_list]

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('PCAå›¾åƒå‹ç¼©è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold', y=0.995)

    # 1. å‹ç¼©æ¯” vs ä¸»æˆåˆ†æ•°
    axes[0, 0].plot(n_components_list, compression_ratios,
                   marker='o', linewidth=2.5, markersize=8, color='steelblue')
    axes[0, 0].set_xlabel('ä¸»æˆåˆ†æ•°é‡', fontsize=11, fontweight='bold')
    axes[0, 0].set_ylabel('å‹ç¼©æ¯” (%)', fontsize=11, fontweight='bold')
    axes[0, 0].set_title('å‹ç¼©æ¯” vs ä¸»æˆåˆ†æ•°é‡', fontsize=12, fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_ylim(0, 105)

    # 2. æ–¹å·®è§£é‡Šç‡ vs ä¸»æˆåˆ†æ•°
    axes[0, 1].plot(n_components_list, variance_explained,
                   marker='s', linewidth=2.5, markersize=8, color='coral')
    axes[0, 1].axhline(y=95, color='red', linestyle='--', alpha=0.6, label='95%é˜ˆå€¼')
    axes[0, 1].set_xlabel('ä¸»æˆåˆ†æ•°é‡', fontsize=11, fontweight='bold')
    axes[0, 1].set_ylabel('æ–¹å·®è§£é‡Šç‡ (%)', fontsize=11, fontweight='bold')
    axes[0, 1].set_title('æ–¹å·®è§£é‡Šç‡ vs ä¸»æˆåˆ†æ•°é‡', fontsize=12, fontweight='bold')
    axes[0, 1].legend(fontsize=10)
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_ylim(0, 105)

    # 3. MSE vs ä¸»æˆåˆ†æ•°
    axes[1, 0].plot(n_components_list, mse_values,
                   marker='^', linewidth=2.5, markersize=8, color='green')
    axes[1, 0].set_xlabel('ä¸»æˆåˆ†æ•°é‡', fontsize=11, fontweight='bold')
    axes[1, 0].set_ylabel('å‡æ–¹è¯¯å·® (MSE)', fontsize=11, fontweight='bold')
    axes[1, 0].set_title('é‡æ„è¯¯å·® vs ä¸»æˆåˆ†æ•°é‡', fontsize=12, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åæ ‡

    # 4. PSNR vs ä¸»æˆåˆ†æ•°
    axes[1, 1].plot(n_components_list, psnr_values,
                   marker='D', linewidth=2.5, markersize=8, color='purple')
    axes[1, 1].axhline(y=30, color='orange', linestyle='--', alpha=0.6, label='30dBé˜ˆå€¼')
    axes[1, 1].set_xlabel('ä¸»æˆåˆ†æ•°é‡', fontsize=11, fontweight='bold')
    axes[1, 1].set_ylabel('å³°å€¼ä¿¡å™ªæ¯” (PSNR, dB)', fontsize=11, fontweight='bold')
    axes[1, 1].set_title('å›¾åƒè´¨é‡ vs ä¸»æˆåˆ†æ•°é‡', fontsize=12, fontweight='bold')
    axes[1, 1].legend(fontsize=10)
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    output_dir = Path('outputs')
    plt.savefig(output_dir / '05_metrics_comparison.png', dpi=150, bbox_inches='tight')
    print("   âœ… æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: outputs/05_metrics_comparison.png")
    plt.show()


def visualize_eigenfaces(pca_model, n_eigenfaces=16):
    """
    å¯è§†åŒ–ç‰¹å¾è„¸ï¼ˆä¸»æˆåˆ†ï¼‰

    ç‰¹å¾è„¸ (Eigenfaces)ï¼š
        - PCAçš„ä¸»æˆåˆ†å¯¹åº”äºäººè„¸çš„"åŸºæœ¬ç‰¹å¾"
        - å‰å‡ ä¸ªä¸»æˆåˆ†æ•è·æœ€æ˜¾è‘—çš„äººè„¸å˜åŒ–ï¼ˆå¦‚å…‰ç…§ã€è¡¨æƒ…ã€è§’åº¦ï¼‰
        - ä»»ä½•äººè„¸éƒ½å¯ä»¥è¡¨ç¤ºä¸ºè¿™äº›ç‰¹å¾è„¸çš„çº¿æ€§ç»„åˆ

    Parameters:
    -----------
    pca_model : PCAå¯¹è±¡
        è®­ç»ƒå¥½çš„PCAæ¨¡å‹
    n_eigenfaces : int
        è¦æ˜¾ç¤ºçš„ç‰¹å¾è„¸æ•°é‡
    """
    print("\nã€å¯è§†åŒ–ç‰¹å¾è„¸ (Eigenfaces)ã€‘")

    # è·å–ä¸»æˆåˆ†ï¼ˆç‰¹å¾å‘é‡ï¼‰
    # components_ çš„å½¢çŠ¶: (n_components, n_features)
    # æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªä¸»æˆåˆ†ï¼Œä»£è¡¨ä¸€ä¸ª"ç‰¹å¾è„¸"
    components = pca_model.components_[:n_eigenfaces]

    # ç»˜åˆ¶ç‰¹å¾è„¸
    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    fig.suptitle('å‰16ä¸ªç‰¹å¾è„¸ (Eigenfaces) - PCAä¸»æˆåˆ†å¯è§†åŒ–',
                 fontsize=14, fontweight='bold', y=0.995)

    for i, ax in enumerate(axes.flat):
        if i < n_eigenfaces:
            # å°†ç‰¹å¾å‘é‡reshapeæˆ64x64å›¾åƒ
            eigenface = components[i].reshape(64, 64)

            # æ˜¾ç¤ºç‰¹å¾è„¸
            # ä½¿ç”¨'RdBu_r'è‰²å›¾æ›´å¥½åœ°æ˜¾ç¤ºæ­£è´Ÿç‰¹å¾
            im = ax.imshow(eigenface, cmap='RdBu_r')
            ax.set_title(f'ç‰¹å¾è„¸ #{i+1}', fontsize=10, fontweight='bold')
            ax.axis('off')

            # æ·»åŠ é¢œè‰²æ¡
            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        else:
            ax.axis('off')

    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    output_dir = Path('outputs')
    plt.savefig(output_dir / '06_eigenfaces.png', dpi=150, bbox_inches='tight')
    print("   âœ… ç‰¹å¾è„¸å›¾å·²ä¿å­˜åˆ°: outputs/06_eigenfaces.png")
    plt.show()


# ============================================================================
# ç¬¬5éƒ¨åˆ†ï¼šæ¨¡å‹ä¿å­˜
# ============================================================================

def save_models(results, n_components_list):
    """
    ä¿å­˜PCAæ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡

    Parameters:
    -----------
    results : dict
        PCAå‹ç¼©ç»“æœ
    n_components_list : list
        ä¸»æˆåˆ†æ•°é‡åˆ—è¡¨
    """
    print("\n" + "=" * 80)
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡")
    print("=" * 80)

    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    models_dir = Path('models')
    models_dir.mkdir(exist_ok=True)

    # ä¿å­˜æ¯ä¸ªPCAæ¨¡å‹
    for n_comp in n_components_list:
        pca_model = results[n_comp]['pca_model']
        model_path = models_dir / f'pca_model_{n_comp}_components.pkl'
        joblib.dump(pca_model, model_path)
        print(f"   âœ… å·²ä¿å­˜æ¨¡å‹: {model_path}")

    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡åˆ°JSON
    metrics = {}
    for n_comp in n_components_list:
        metrics[f'{n_comp}_components'] = {
            'n_components': n_comp,
            'compression_ratio': float(results[n_comp]['compression_ratio']),
            'total_variance_explained': float(results[n_comp]['total_variance_explained']),
            'mse': float(results[n_comp]['mse']),
            'psnr': float(results[n_comp]['psnr']),
            'compression_time_seconds': float(results[n_comp]['compression_time'])
        }

    metrics_path = models_dir / 'pca_compression_metrics.json'
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=4, ensure_ascii=False)

    print(f"   âœ… å·²ä¿å­˜è¯„ä¼°æŒ‡æ ‡: {metrics_path}")
    print("\n" + "=" * 80)


# ============================================================================
# ç¬¬6éƒ¨åˆ†ï¼šä¸»å‡½æ•°
# ============================================================================

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„PCAå›¾åƒå‹ç¼©æµç¨‹
    """
    print("\n" + "=" * 80)
    print("ğŸš€ PCAå›¾åƒå‹ç¼©é¡¹ç›®å¼€å§‹")
    print("=" * 80)

    # Step 1: åŠ è½½æ•°æ®
    faces, targets, images = load_olivetti_faces()

    # Step 2: æ•°æ®æ¢ç´¢
    explore_data(faces, targets, images)

    # Step 3: å®šä¹‰è¦æµ‹è¯•çš„ä¸»æˆåˆ†æ•°é‡
    # ä»æåº¦å‹ç¼©åˆ°è½»åº¦å‹ç¼©
    n_components_list = [10, 25, 50, 75, 100, 150, 200, 300]

    # Step 4: åº”ç”¨PCAå‹ç¼©
    results = apply_pca_compression(faces, n_components_list)

    # Step 5: è®¡ç®—å®Œæ•´çš„æ–¹å·®è§£é‡Šç‡æ›²çº¿
    pca_full, cumsum_variance = calculate_variance_curve(faces, max_components=400)

    # Step 6: å¯è§†åŒ–
    visualize_compression_comparison(faces, images, results, n_components_list)
    visualize_variance_curve(cumsum_variance)
    visualize_metrics_comparison(results, n_components_list)

    # ä½¿ç”¨æœ€å¤§ä¸»æˆåˆ†æ•°çš„æ¨¡å‹å¯è§†åŒ–ç‰¹å¾è„¸
    best_pca = results[300]['pca_model']
    visualize_eigenfaces(best_pca, n_eigenfaces=16)

    # Step 7: ä¿å­˜æ¨¡å‹
    save_models(results, n_components_list)

    # Step 8: æ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“Š é¡¹ç›®æ€»ç»“æŠ¥å‘Š")
    print("=" * 80)

    print("\nã€å…³é”®å‘ç°ã€‘")

    # æ‰¾åˆ°95%æ–¹å·®çš„æœ€å°ä¸»æˆåˆ†æ•°
    idx_95 = np.argmax(cumsum_variance >= 0.95)
    n_comp_95 = idx_95 + 1
    compression_95 = 1 - (n_comp_95 / faces.shape[1])

    print(f"   1. ä¿ç•™95%æ–¹å·®æ‰€éœ€ä¸»æˆåˆ†: {n_comp_95} (å‹ç¼©æ¯”: {compression_95:.2%})")

    # æ‰¾åˆ°PSNR > 30dBçš„æœ€å°ä¸»æˆåˆ†æ•°
    psnr_30_candidates = [n for n in n_components_list if results[n]['psnr'] >= 30]
    if psnr_30_candidates:
        n_comp_30db = min(psnr_30_candidates)
        print(f"   2. PSNR>30dBçš„æœ€å°ä¸»æˆåˆ†: {n_comp_30db} "
              f"(PSNR: {results[n_comp_30db]['psnr']:.2f} dB)")

    # å¯¹æ¯”æç«¯æƒ…å†µ
    n_min = min(n_components_list)
    n_max = max(n_components_list)
    print(f"\n   3. æåº¦å‹ç¼© (n={n_min}):")
    print(f"      - å‹ç¼©æ¯”: {results[n_min]['compression_ratio']:.2%}")
    print(f"      - æ–¹å·®è§£é‡Šç‡: {results[n_min]['total_variance_explained']:.2%}")
    print(f"      - PSNR: {results[n_min]['psnr']:.2f} dB")

    print(f"\n   4. è½»åº¦å‹ç¼© (n={n_max}):")
    print(f"      - å‹ç¼©æ¯”: {results[n_max]['compression_ratio']:.2%}")
    print(f"      - æ–¹å·®è§£é‡Šç‡: {results[n_max]['total_variance_explained']:.2%}")
    print(f"      - PSNR: {results[n_max]['psnr']:.2f} dB")

    print("\nã€å®è·µå»ºè®®ã€‘")
    print("   - äººè„¸è¯†åˆ«åº”ç”¨: å»ºè®®ä½¿ç”¨100-150ä¸ªä¸»æˆåˆ†ï¼ˆä¿ç•™>95%æ–¹å·®ï¼‰")
    print("   - ç¼©ç•¥å›¾ç”Ÿæˆ: å¯ä½¿ç”¨25-50ä¸ªä¸»æˆåˆ†ï¼ˆå‹ç¼©æ¯”>98%ï¼‰")
    print("   - æ•°æ®å­˜å‚¨ä¼˜åŒ–: æ ¹æ®è´¨é‡è¦æ±‚åœ¨50-200ä¸ªä¸»æˆåˆ†é—´æƒè¡¡")

    print("\n" + "=" * 80)
    print("âœ… PCAå›¾åƒå‹ç¼©é¡¹ç›®å®Œæˆï¼")
    print("=" * 80)
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("   - outputs/01_data_samples.png           # æ•°æ®æ ·æœ¬")
    print("   - outputs/02_pixel_distribution.png      # åƒç´ åˆ†å¸ƒ")
    print("   - outputs/03_compression_comparison.png  # å‹ç¼©æ•ˆæœå¯¹æ¯”")
    print("   - outputs/04_variance_curve.png          # æ–¹å·®è§£é‡Šç‡æ›²çº¿")
    print("   - outputs/05_metrics_comparison.png      # è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”")
    print("   - outputs/06_eigenfaces.png              # ç‰¹å¾è„¸å¯è§†åŒ–")
    print("   - models/pca_model_*_components.pkl      # PCAæ¨¡å‹æ–‡ä»¶")
    print("   - models/pca_compression_metrics.json    # è¯„ä¼°æŒ‡æ ‡JSON")
    print("\n" + "=" * 80)


# ============================================================================
# ç¨‹åºå…¥å£
# ============================================================================

if __name__ == "__main__":
    main()
