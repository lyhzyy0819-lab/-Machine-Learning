"""
ğŸ¯ ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹é¡¹ç›® (Credit Card Fraud Detection)
===================================================

é¡¹ç›®ç›®æ ‡ï¼š
    ä½¿ç”¨å¼‚å¸¸æ£€æµ‹ç®—æ³•è¯†åˆ«ä¿¡ç”¨å¡æ¬ºè¯ˆäº¤æ˜“ï¼Œå¤„ç†é«˜åº¦ä¸å¹³è¡¡çš„æ•°æ®ï¼Œ
    å¯¹æ¯”Isolation Forestã€One-Class SVMã€LOFä¸‰ç§ç®—æ³•çš„æ€§èƒ½ã€‚

ä¸šåŠ¡åœºæ™¯ï¼š
    ä¿¡ç”¨å¡æ¬ºè¯ˆæ¯å¹´é€ æˆæ•°åäº¿ç¾å…ƒæŸå¤±ã€‚é€šè¿‡æœºå™¨å­¦ä¹ å®æ—¶æ£€æµ‹å¼‚å¸¸äº¤æ˜“ï¼Œ
    å¯ä»¥å¸®åŠ©é“¶è¡ŒåŠæ—¶å‘ç°æ¬ºè¯ˆè¡Œä¸ºï¼Œä¿æŠ¤å®¢æˆ·èµ„é‡‘å®‰å…¨ã€‚

æ•°æ®ç‰¹ç‚¹ï¼š
    - é«˜åº¦ä¸å¹³è¡¡ï¼šæ¬ºè¯ˆäº¤æ˜“ä»…å  ~0.2%
    - ç‰¹å¾å·²è„±æ•ï¼šä½¿ç”¨PCAé™ç»´ä¿æŠ¤éšç§
    - çœŸå®åœºæ™¯ï¼šéœ€è¦æƒè¡¡è¯¯æŠ¥å’Œæ¼æŠ¥

æ ¸å¿ƒç®—æ³•ï¼š
    - Isolation Forest (éš”ç¦»æ£®æ—)ï¼šåŸºäºå†³ç­–æ ‘çš„å¿«é€Ÿå¼‚å¸¸æ£€æµ‹
    - One-Class SVM (å•ç±»SVM)ï¼šå­¦ä¹ æ­£å¸¸æ ·æœ¬çš„è¾¹ç•Œ
    - LOF (å±€éƒ¨ç¦»ç¾¤å› å­)ï¼šåŸºäºå¯†åº¦çš„å¼‚å¸¸æ£€æµ‹

ä½œè€…: Machine Learning å­¦ä¹ é¡¹ç›®
æ—¥æœŸ: 2024å¹´11æœˆ
"""

# ============================================================================
# ç¬¬1éƒ¨åˆ†ï¼šå¯¼å…¥å¿…è¦çš„åº“
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time

# æ•°æ®é¢„å¤„ç†
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# å¼‚å¸¸æ£€æµ‹ç®—æ³•
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor

# é™ç»´å¯è§†åŒ–
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    precision_recall_curve,
    average_precision_score,
    precision_score,
    recall_score,
    f1_score
)

# æ¨¡å‹ä¿å­˜
import joblib
import json

# å¿½ç•¥è­¦å‘Š
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['font.sans-serif'] = [
    'Arial Unicode MS',  # macOSé€šç”¨
    'PingFang SC',       # macOSç³»ç»Ÿå­—ä½“
    'STHeiti',           # åæ–‡é»‘ä½“
    'SimHei',            # Windowsé»‘ä½“
]
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 11

# è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
np.random.seed(42)

print("âœ… åº“å¯¼å…¥å®Œæˆï¼")
print("=" * 80)


# ============================================================================
# ç¬¬2éƒ¨åˆ†ï¼šåŠ è½½çœŸå®ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†
# ============================================================================

def load_credit_card_data(file_path=None):
    """
    åŠ è½½ Kaggle ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†

    æ•°æ®é›†æ¥æºï¼š
        https://www.kaggle.com/mlg-ulb/creditcardfraud

    æ•°æ®é›†è¯´æ˜ï¼š
        è¯¥æ•°æ®é›†åŒ…å«2013å¹´9æœˆæ¬§æ´²æŒå¡äººçš„ä¿¡ç”¨å¡äº¤æ˜“è®°å½•ã€‚
        æ•°æ®é›†ç»è¿‡PCAé™ç»´å¤„ç†ä»¥ä¿æŠ¤ç”¨æˆ·éšç§ã€‚

    æ•°æ®ç‰¹ç‚¹ï¼š
        - 284,807 ç¬”äº¤æ˜“è®°å½•
        - 492 ç¬”æ¬ºè¯ˆäº¤æ˜“ï¼ˆå æ¯”çº¦ 0.172%ï¼‰
        - é«˜åº¦ä¸å¹³è¡¡æ•°æ®é›†ï¼ˆæ­£å¸¸:æ¬ºè¯ˆ â‰ˆ 578:1ï¼‰

    ç‰¹å¾è¯´æ˜ï¼š
        - Time: è¯¥äº¤æ˜“è·ç¦»æ•°æ®é›†ç¬¬ä¸€ç¬”äº¤æ˜“çš„ç§’æ•°
                å¯ç”¨äºåˆ†æäº¤æ˜“æ—¶é—´æ¨¡å¼
        - V1-V28: PCAé™ç»´åçš„ç‰¹å¾ï¼ˆåŸå§‹ç‰¹å¾å› éšç§ä¿æŠ¤ä¸å¯çŸ¥ï¼‰
                  è¿™äº›ç‰¹å¾å·²ç»è¿‡æ ‡å‡†åŒ–å¤„ç†
        - Amount: äº¤æ˜“é‡‘é¢ï¼ˆæœªç»æ ‡å‡†åŒ–ï¼‰
        - Class: æ ‡ç­¾ï¼Œ0=æ­£å¸¸äº¤æ˜“ï¼Œ1=æ¬ºè¯ˆäº¤æ˜“

    Parameters:
    -----------
    file_path : str, optional
        CSVæ–‡ä»¶è·¯å¾„ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„ 'creditcard.csv'

    Returns:
    --------
    df : DataFrame
        åŒ…å«æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾çš„æ•°æ®æ¡†

    Raises:
    -------
    FileNotFoundError
        å¦‚æœæ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨
    """
    print("\n" + "=" * 80)
    print("ğŸ“‚ åŠ è½½ Kaggle ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†")
    print("=" * 80)

    # ç¡®å®šæ–‡ä»¶è·¯å¾„
    if file_path is None:
        # é»˜è®¤åœ¨å½“å‰ç›®å½•ä¸‹æŸ¥æ‰¾
        file_path = Path(__file__).parent / 'creditcard.csv'

    file_path = Path(file_path)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not file_path.exists():
        raise FileNotFoundError(
            f"\nâŒ æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}\n"
            f"   è¯·ä» Kaggle ä¸‹è½½æ•°æ®é›†:\n"
            f"   https://www.kaggle.com/mlg-ulb/creditcardfraud\n"
            f"   å¹¶å°† creditcard.csv æ”¾ç½®åœ¨å½“å‰ç›®å½•ä¸‹ã€‚"
        )

    print(f"\n   æ•°æ®é›†è·¯å¾„: {file_path}")
    print(f"   æ­£åœ¨åŠ è½½æ•°æ®...")

    # è¯»å–CSVæ–‡ä»¶
    # æ³¨æ„ï¼šClassåˆ—åœ¨CSVä¸­å¯èƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œéœ€è¦è½¬æ¢
    df = pd.read_csv(file_path)

    # ç¡®ä¿Classåˆ—æ˜¯æ•´æ•°ç±»å‹
    # Kaggleæ•°æ®é›†ä¸­Classåˆ—å¯èƒ½æ˜¯"0"/"1"å­—ç¬¦ä¸²
    df['Class'] = df['Class'].astype(int)

    # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
    n_samples = len(df)
    n_features = df.shape[1] - 1  # å‡å»Classåˆ—
    n_fraud = df['Class'].sum()
    n_normal = n_samples - n_fraud
    fraud_ratio = n_fraud / n_samples

    print(f"\n   âœ… æ•°æ®åŠ è½½å®Œæˆï¼")
    print(f"\n   ã€æ•°æ®é›†ç»Ÿè®¡ã€‘")
    print(f"      - æ€»äº¤æ˜“æ•°: {n_samples:,}")
    print(f"      - æ­£å¸¸äº¤æ˜“: {n_normal:,} ({(1-fraud_ratio)*100:.3f}%)")
    print(f"      - æ¬ºè¯ˆäº¤æ˜“: {n_fraud:,} ({fraud_ratio*100:.3f}%)")
    print(f"      - ä¸å¹³è¡¡æ¯”ä¾‹: 1:{n_normal//n_fraud}")
    print(f"      - ç‰¹å¾æ•°é‡: {n_features}")

    print(f"\n   ã€ç‰¹å¾åˆ—è¡¨ã€‘")
    print(f"      - Time: è·ç¦»ç¬¬ä¸€ç¬”äº¤æ˜“çš„ç§’æ•°")
    print(f"      - V1-V28: PCAé™ç»´åçš„åŒ¿åç‰¹å¾")
    print(f"      - Amount: äº¤æ˜“é‡‘é¢")
    print(f"      - Class: æ ‡ç­¾ (0=æ­£å¸¸, 1=æ¬ºè¯ˆ)")

    # æ˜¾ç¤ºæ•°æ®é›†çš„åŸºæœ¬ç»Ÿè®¡
    print(f"\n   ã€æ•°å€¼ç»Ÿè®¡æ‘˜è¦ã€‘")
    print(f"      TimeèŒƒå›´: {df['Time'].min():.0f} - {df['Time'].max():.0f} ç§’")
    print(f"      Timeè·¨åº¦: {df['Time'].max()/3600:.1f} å°æ—¶")
    print(f"      AmountèŒƒå›´: ${df['Amount'].min():.2f} - ${df['Amount'].max():.2f}")
    print(f"      Amountå‡å€¼: ${df['Amount'].mean():.2f}")
    print(f"      Amountä¸­ä½æ•°: ${df['Amount'].median():.2f}")

    return df


# ============================================================================
# ç¬¬3éƒ¨åˆ†ï¼šæ•°æ®æ¢ç´¢åˆ†æ (EDA)
# ============================================================================

def explore_data(df):
    """
    æ¢ç´¢æ€§æ•°æ®åˆ†æ

    ç›®çš„ï¼š
        äº†è§£æ•°æ®çš„åŸºæœ¬æƒ…å†µã€ç±»åˆ«åˆ†å¸ƒã€ç‰¹å¾å·®å¼‚ç­‰
        é’ˆå¯¹çœŸå® Kaggle æ•°æ®é›†è¿›è¡Œå…¨é¢çš„ EDA

    åˆ†æå†…å®¹ï¼š
        1. æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼ˆå½¢çŠ¶ã€ç¼ºå¤±å€¼ã€æ•°æ®ç±»å‹ï¼‰
        2. ç±»åˆ«åˆ†å¸ƒåˆ†æï¼ˆæ­£å¸¸ vs æ¬ºè¯ˆï¼‰
        3. äº¤æ˜“é‡‘é¢åˆ†æï¼ˆæ­£å¸¸ vs æ¬ºè¯ˆçš„é‡‘é¢å·®å¼‚ï¼‰
        4. æ—¶é—´åˆ†å¸ƒåˆ†æï¼ˆäº¤æ˜“æ—¶é—´æ¨¡å¼ï¼‰
        5. V1-V28 ç‰¹å¾åˆ†æï¼ˆPCAç‰¹å¾çš„åˆ†å¸ƒå·®å¼‚ï¼‰

    Parameters:
    -----------
    df : DataFrame
        äº¤æ˜“æ•°æ®ï¼ˆåŒ…å« Time, V1-V28, Amount, Class åˆ—ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸ” æ•°æ®æ¢ç´¢åˆ†æ (EDA)")
    print("=" * 80)

    # 1. åŸºæœ¬ä¿¡æ¯
    print("\nã€æ•°æ®åŸºæœ¬ä¿¡æ¯ã€‘")
    print(f"   - æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"   - ç‰¹å¾æ•°é‡: {df.shape[1] - 1}")
    print(f"   - æ ·æœ¬æ•°é‡: {df.shape[0]:,}")
    print(f"   - ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
    print(f"   - æ•°æ®ç±»å‹:")
    for col in ['Time', 'Amount', 'Class']:
        print(f"      {col}: {df[col].dtype}")
    print(f"\n   å‰5è¡Œæ•°æ®:")
    print(df.head())

    # 2. ç±»åˆ«åˆ†å¸ƒ
    print("\nã€ç±»åˆ«åˆ†å¸ƒã€‘")
    class_counts = df['Class'].value_counts()
    print(f"   - æ­£å¸¸äº¤æ˜“ (Class=0): {class_counts[0]:,} ({class_counts[0]/len(df)*100:.3f}%)")
    print(f"   - æ¬ºè¯ˆäº¤æ˜“ (Class=1): {class_counts[1]:,} ({class_counts[1]/len(df)*100:.3f}%)")
    print(f"   - ä¸å¹³è¡¡æ¯”ä¾‹: 1:{class_counts[0]//class_counts[1]}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path('outputs')
    output_dir.mkdir(exist_ok=True)

    # 3. å¯è§†åŒ–ç±»åˆ«åˆ†å¸ƒ
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # 3.1 æŸ±çŠ¶å›¾
    class_counts.plot(kind='bar', ax=axes[0], color=['steelblue', 'coral'], alpha=0.8, edgecolor='black')
    axes[0].set_xlabel('äº¤æ˜“ç±»åˆ«', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('äº¤æ˜“æ•°é‡', fontsize=12, fontweight='bold')
    axes[0].set_title('äº¤æ˜“ç±»åˆ«åˆ†å¸ƒï¼ˆæŸ±çŠ¶å›¾ï¼‰', fontsize=13, fontweight='bold')
    axes[0].set_xticklabels(['æ­£å¸¸ (0)', 'æ¬ºè¯ˆ (1)'], rotation=0)
    axes[0].grid(True, alpha=0.3, axis='y')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(class_counts):
        axes[0].text(i, v + max(class_counts)*0.02, f'{v:,}\n({v/len(df)*100:.3f}%)',
                    ha='center', fontsize=10, fontweight='bold')

    # 3.2 é¥¼å›¾
    colors = ['steelblue', 'coral']
    explode = (0, 0.1)  # çªå‡ºæ˜¾ç¤ºæ¬ºè¯ˆéƒ¨åˆ†
    axes[1].pie(class_counts, labels=['æ­£å¸¸äº¤æ˜“', 'æ¬ºè¯ˆäº¤æ˜“'], autopct='%1.3f%%',
               colors=colors, explode=explode, startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})
    axes[1].set_title('äº¤æ˜“ç±»åˆ«åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰', fontsize=13, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_dir / '01_class_distribution.png', dpi=150, bbox_inches='tight')
    print("\n   âœ… ç±»åˆ«åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: outputs/01_class_distribution.png")
    plt.show()

    # 4. äº¤æ˜“é‡‘é¢åˆ†æ
    print("\nã€äº¤æ˜“é‡‘é¢åˆ†æã€‘")
    normal_amounts = df[df['Class'] == 0]['Amount']
    fraud_amounts = df[df['Class'] == 1]['Amount']

    print(f"   æ­£å¸¸äº¤æ˜“é‡‘é¢:")
    print(f"      - å¹³å‡å€¼: ${normal_amounts.mean():.2f}")
    print(f"      - ä¸­ä½æ•°: ${normal_amounts.median():.2f}")
    print(f"      - æ ‡å‡†å·®: ${normal_amounts.std():.2f}")

    print(f"\n   æ¬ºè¯ˆäº¤æ˜“é‡‘é¢:")
    print(f"      - å¹³å‡å€¼: ${fraud_amounts.mean():.2f}")
    print(f"      - ä¸­ä½æ•°: ${fraud_amounts.median():.2f}")
    print(f"      - æ ‡å‡†å·®: ${fraud_amounts.std():.2f}")

    # å¯è§†åŒ–é‡‘é¢åˆ†å¸ƒ
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('äº¤æ˜“é‡‘é¢åˆ†å¸ƒåˆ†æ', fontsize=16, fontweight='bold', y=0.995)

    # 4.1 æ­£å¸¸äº¤æ˜“é‡‘é¢ç›´æ–¹å›¾
    axes[0, 0].hist(normal_amounts, bins=50, color='steelblue', alpha=0.7, edgecolor='black')
    axes[0, 0].set_xlabel('äº¤æ˜“é‡‘é¢ ($)', fontsize=11, fontweight='bold')
    axes[0, 0].set_ylabel('é¢‘æ•°', fontsize=11, fontweight='bold')
    axes[0, 0].set_title('æ­£å¸¸äº¤æ˜“é‡‘é¢åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[0, 0].axvline(normal_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'å‡å€¼: ${normal_amounts.mean():.2f}')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # 4.2 æ¬ºè¯ˆäº¤æ˜“é‡‘é¢ç›´æ–¹å›¾
    axes[0, 1].hist(fraud_amounts, bins=30, color='coral', alpha=0.7, edgecolor='black')
    axes[0, 1].set_xlabel('äº¤æ˜“é‡‘é¢ ($)', fontsize=11, fontweight='bold')
    axes[0, 1].set_ylabel('é¢‘æ•°', fontsize=11, fontweight='bold')
    axes[0, 1].set_title('æ¬ºè¯ˆäº¤æ˜“é‡‘é¢åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[0, 1].axvline(fraud_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'å‡å€¼: ${fraud_amounts.mean():.2f}')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # 4.3 ç®±çº¿å›¾å¯¹æ¯”
    data_to_plot = [normal_amounts, fraud_amounts]
    bp = axes[1, 0].boxplot(data_to_plot, labels=['æ­£å¸¸äº¤æ˜“', 'æ¬ºè¯ˆäº¤æ˜“'],
                           patch_artist=True, showmeans=True)
    for patch, color in zip(bp['boxes'], ['steelblue', 'coral']):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    axes[1, 0].set_ylabel('äº¤æ˜“é‡‘é¢ ($)', fontsize=11, fontweight='bold')
    axes[1, 0].set_title('äº¤æ˜“é‡‘é¢ç®±çº¿å›¾å¯¹æ¯”', fontsize=12, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3, axis='y')

    # 4.4 å¯¹æ•°å°ºåº¦å¯¹æ¯”
    axes[1, 1].hist([normal_amounts, fraud_amounts], bins=50, label=['æ­£å¸¸äº¤æ˜“', 'æ¬ºè¯ˆäº¤æ˜“'],
                   color=['steelblue', 'coral'], alpha=0.6, edgecolor='black')
    axes[1, 1].set_xlabel('äº¤æ˜“é‡‘é¢ ($)', fontsize=11, fontweight='bold')
    axes[1, 1].set_ylabel('é¢‘æ•° (å¯¹æ•°å°ºåº¦)', fontsize=11, fontweight='bold')
    axes[1, 1].set_title('äº¤æ˜“é‡‘é¢åˆ†å¸ƒå¯¹æ¯”ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰', fontsize=12, fontweight='bold')
    axes[1, 1].set_yscale('log')
    axes[1, 1].legend(fontsize=10)
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / '02_amount_distribution.png', dpi=150, bbox_inches='tight')
    print("   âœ… é‡‘é¢åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: outputs/02_amount_distribution.png")
    plt.show()

    # 5. æ—¶é—´åˆ†å¸ƒåˆ†æ
    print("\nã€æ—¶é—´åˆ†å¸ƒåˆ†æã€‘")
    normal_times = df[df['Class'] == 0]['Time']
    fraud_times = df[df['Class'] == 1]['Time']

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # 5.1 æ­£å¸¸äº¤æ˜“æ—¶é—´åˆ†å¸ƒ
    axes[0].hist(normal_times / 3600, bins=48, color='steelblue', alpha=0.7, edgecolor='black')
    axes[0].set_xlabel('æ—¶é—´ï¼ˆå°æ—¶ï¼‰', fontsize=11, fontweight='bold')
    axes[0].set_ylabel('äº¤æ˜“æ•°é‡', fontsize=11, fontweight='bold')
    axes[0].set_title('æ­£å¸¸äº¤æ˜“æ—¶é—´åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[0].grid(True, alpha=0.3)

    # 5.2 æ¬ºè¯ˆäº¤æ˜“æ—¶é—´åˆ†å¸ƒ
    axes[1].hist(fraud_times / 3600, bins=48, color='coral', alpha=0.7, edgecolor='black')
    axes[1].set_xlabel('æ—¶é—´ï¼ˆå°æ—¶ï¼‰', fontsize=11, fontweight='bold')
    axes[1].set_ylabel('äº¤æ˜“æ•°é‡', fontsize=11, fontweight='bold')
    axes[1].set_title('æ¬ºè¯ˆäº¤æ˜“æ—¶é—´åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / '03_time_distribution.png', dpi=150, bbox_inches='tight')
    print("   âœ… æ—¶é—´åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: outputs/03_time_distribution.png")
    plt.show()

    # 6. V1-V28 PCAç‰¹å¾åˆ†æ
    # è¿™äº›ç‰¹å¾æ˜¯åŸå§‹äº¤æ˜“ç‰¹å¾ç»è¿‡PCAé™ç»´åçš„ç»“æœ
    # åˆ†ææ­£å¸¸äº¤æ˜“å’Œæ¬ºè¯ˆäº¤æ˜“åœ¨è¿™äº›ç‰¹å¾ä¸Šçš„å·®å¼‚
    print("\nã€V1-V28 PCAç‰¹å¾åˆ†æã€‘")
    print("   è¯´æ˜: V1-V28æ˜¯åŸå§‹ç‰¹å¾ç»PCAé™ç»´åçš„åŒ¿åç‰¹å¾")
    print("   åˆ†ææ­£å¸¸äº¤æ˜“ä¸æ¬ºè¯ˆäº¤æ˜“åœ¨è¿™äº›ç‰¹å¾ä¸Šçš„åˆ†å¸ƒå·®å¼‚...")

    # è·å–V1-V28ç‰¹å¾åˆ—
    v_features = [f'V{i}' for i in range(1, 29)]

    # è®¡ç®—æ­£å¸¸å’Œæ¬ºè¯ˆäº¤æ˜“çš„ç‰¹å¾å‡å€¼å·®å¼‚
    normal_means = df[df['Class'] == 0][v_features].mean()
    fraud_means = df[df['Class'] == 1][v_features].mean()
    mean_diff = fraud_means - normal_means

    # æ‰¾å‡ºå·®å¼‚æœ€å¤§çš„ç‰¹å¾ï¼ˆå¯¹æ¬ºè¯ˆæ£€æµ‹æœ€æœ‰ä»·å€¼çš„ç‰¹å¾ï¼‰
    abs_diff = mean_diff.abs().sort_values(ascending=False)
    top_features = abs_diff.head(10).index.tolist()

    print(f"\n   æ¬ºè¯ˆäº¤æ˜“ä¸æ­£å¸¸äº¤æ˜“å‡å€¼å·®å¼‚æœ€å¤§çš„10ä¸ªç‰¹å¾:")
    for i, feat in enumerate(top_features, 1):
        diff = mean_diff[feat]
        direction = "â†‘ æ¬ºè¯ˆæ›´é«˜" if diff > 0 else "â†“ æ¬ºè¯ˆæ›´ä½"
        print(f"      {i}. {feat}: å·®å¼‚ {diff:+.4f} ({direction})")

    # å¯è§†åŒ–å·®å¼‚æœ€å¤§çš„4ä¸ªç‰¹å¾åˆ†å¸ƒ
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('æ¬ºè¯ˆäº¤æ˜“ä¸æ­£å¸¸äº¤æ˜“çš„PCAç‰¹å¾åˆ†å¸ƒå¯¹æ¯”ï¼ˆå·®å¼‚æœ€å¤§çš„4ä¸ªç‰¹å¾ï¼‰',
                 fontsize=14, fontweight='bold', y=0.995)

    for idx, feat in enumerate(top_features[:4]):
        row, col = idx // 2, idx % 2
        ax = axes[row, col]

        # ç»˜åˆ¶æ­£å¸¸äº¤æ˜“çš„åˆ†å¸ƒ
        normal_data = df[df['Class'] == 0][feat]
        fraud_data = df[df['Class'] == 1][feat]

        ax.hist(normal_data, bins=50, alpha=0.6, color='steelblue',
                label=f'æ­£å¸¸ (n={len(normal_data):,})', density=True, edgecolor='none')
        ax.hist(fraud_data, bins=50, alpha=0.7, color='coral',
                label=f'æ¬ºè¯ˆ (n={len(fraud_data):,})', density=True, edgecolor='none')

        # æ·»åŠ å‡å€¼çº¿
        ax.axvline(normal_data.mean(), color='blue', linestyle='--', linewidth=2,
                   label=f'æ­£å¸¸å‡å€¼: {normal_data.mean():.2f}')
        ax.axvline(fraud_data.mean(), color='red', linestyle='--', linewidth=2,
                   label=f'æ¬ºè¯ˆå‡å€¼: {fraud_data.mean():.2f}')

        ax.set_xlabel(f'{feat} å€¼', fontsize=11, fontweight='bold')
        ax.set_ylabel('å¯†åº¦', fontsize=11, fontweight='bold')
        ax.set_title(f'{feat} ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / '03b_feature_distribution.png', dpi=150, bbox_inches='tight')
    print("\n   âœ… ç‰¹å¾åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: outputs/03b_feature_distribution.png")
    plt.show()

    # 7. ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼ˆä»…å±•ç¤ºä¸Classç›¸å…³æ€§è¾ƒé«˜çš„ç‰¹å¾ï¼‰
    print("\nã€ç‰¹å¾ä¸æ¬ºè¯ˆæ ‡ç­¾çš„ç›¸å…³æ€§åˆ†æã€‘")

    # è®¡ç®—æ‰€æœ‰ç‰¹å¾ä¸Classçš„ç›¸å…³æ€§
    correlations = df.corr()['Class'].drop('Class').sort_values(key=abs, ascending=False)

    print(f"\n   ä¸æ¬ºè¯ˆæ ‡ç­¾ç›¸å…³æ€§æœ€é«˜çš„10ä¸ªç‰¹å¾:")
    for i, (feat, corr) in enumerate(correlations.head(10).items(), 1):
        direction = "æ­£ç›¸å…³" if corr > 0 else "è´Ÿç›¸å…³"
        print(f"      {i}. {feat}: {corr:+.4f} ({direction})")

    # ç»˜åˆ¶ç›¸å…³æ€§æ¡å½¢å›¾
    fig, ax = plt.subplots(figsize=(12, 8))

    # é€‰æ‹©ç›¸å…³æ€§ç»å¯¹å€¼æœ€é«˜çš„15ä¸ªç‰¹å¾
    top_corr = correlations.head(15)
    colors = ['coral' if x > 0 else 'steelblue' for x in top_corr.values]

    bars = ax.barh(range(len(top_corr)), top_corr.values, color=colors, alpha=0.8, edgecolor='black')
    ax.set_yticks(range(len(top_corr)))
    ax.set_yticklabels(top_corr.index)
    ax.set_xlabel('ä¸æ¬ºè¯ˆæ ‡ç­¾çš„ç›¸å…³ç³»æ•°', fontsize=12, fontweight='bold')
    ax.set_title('ç‰¹å¾ä¸æ¬ºè¯ˆæ ‡ç­¾(Class)çš„ç›¸å…³æ€§æ’å', fontsize=14, fontweight='bold')
    ax.axvline(0, color='black', linewidth=1)
    ax.grid(True, alpha=0.3, axis='x')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, val in zip(bars, top_corr.values):
        ax.text(val + 0.01 if val > 0 else val - 0.01,
                bar.get_y() + bar.get_height()/2,
                f'{val:.3f}',
                va='center', ha='left' if val > 0 else 'right',
                fontsize=9, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_dir / '03c_feature_correlation.png', dpi=150, bbox_inches='tight')
    print("   âœ… ç›¸å…³æ€§åˆ†æå›¾å·²ä¿å­˜åˆ°: outputs/03c_feature_correlation.png")
    plt.show()


# ============================================================================
# ç¬¬4éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†
# ============================================================================

def preprocess_data(df):
    """
    æ•°æ®é¢„å¤„ç†

    é’ˆå¯¹ Kaggle ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†çš„é¢„å¤„ç†æ­¥éª¤ï¼š

    æ­¥éª¤ï¼š
        1. åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
           - ç‰¹å¾: Time, V1-V28, Amountï¼ˆå…±30ç»´ï¼‰
           - æ ‡ç­¾: Classï¼ˆ0=æ­£å¸¸, 1=æ¬ºè¯ˆï¼‰

        2. æ ‡å‡†åŒ–å¤„ç†
           - V1-V28 å·²ç»è¿‡ PCA å¤„ç†ï¼Œä½†ä»éœ€æ ‡å‡†åŒ–ä»¥ç»Ÿä¸€å°ºåº¦
           - Time å’Œ Amount ç‰¹å¾å°¤å…¶éœ€è¦æ ‡å‡†åŒ–
           - ä½¿ç”¨ StandardScaler: (x - mean) / std

        3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
           - ä½¿ç”¨åˆ†å±‚é‡‡æ ·ä¿æŒç±»åˆ«æ¯”ä¾‹ä¸€è‡´
           - 70% è®­ç»ƒï¼Œ30% æµ‹è¯•

        4. æå–æ­£å¸¸äº¤æ˜“æ ·æœ¬
           - ç”¨äº One-Class æ–¹æ³•ï¼ˆåªç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒï¼‰

    Parameters:
    -----------
    df : DataFrame
        åŸå§‹æ•°æ®ï¼ˆåŒ…å« Time, V1-V28, Amount, Class åˆ—ï¼‰

    Returns:
    --------
    X_train : ndarray
        è®­ç»ƒé›†ç‰¹å¾ï¼Œå½¢çŠ¶ (n_train_samples, 30)
    X_test : ndarray
        æµ‹è¯•é›†ç‰¹å¾ï¼Œå½¢çŠ¶ (n_test_samples, 30)
    y_train : ndarray
        è®­ç»ƒé›†æ ‡ç­¾ï¼Œå½¢çŠ¶ (n_train_samples,)
    y_test : ndarray
        æµ‹è¯•é›†æ ‡ç­¾ï¼Œå½¢çŠ¶ (n_test_samples,)
    X_train_normal : ndarray
        è®­ç»ƒé›†ä¸­çš„æ­£å¸¸äº¤æ˜“ï¼ˆç”¨äº One-Class æ–¹æ³•ï¼‰
    scaler : StandardScaler
        æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºåç»­æ–°æ•°æ®é¢„å¤„ç†ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†")
    print("=" * 80)

    # 1. åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    # ç‰¹å¾åˆ—: Time, V1-V28, Amount
    # æ ‡ç­¾åˆ—: Class
    X = df.drop('Class', axis=1).values
    y = df['Class'].values

    print(f"\n   ã€æ•°æ®åˆ†ç¦»ã€‘")
    print(f"      - ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    print(f"      - æ ‡ç­¾å‘é‡å½¢çŠ¶: {y.shape}")
    print(f"      - ç‰¹å¾åˆ—: Time, V1-V28, Amount")

    # 2. æ ‡å‡†åŒ–å¤„ç†
    # ä¸ºä»€ä¹ˆéœ€è¦æ ‡å‡†åŒ–ï¼Ÿ
    # - Amountç‰¹å¾çš„æ•°å€¼èŒƒå›´ä¸V1-V28ä¸åŒ
    # - SVMç­‰ç®—æ³•å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿ
    # - æ ‡å‡†åŒ–å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½
    print("\n   æ­£åœ¨è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    print(f"   âœ… æ ‡å‡†åŒ–å®Œæˆï¼")
    print(f"      - ç‰¹å¾å‡å€¼: {X_scaled.mean(axis=0)[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
    print(f"      - ç‰¹å¾æ ‡å‡†å·®: {X_scaled.std(axis=0)[:5]}...")

    # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # stratify=y: ä¿æŒè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç±»åˆ«æ¯”ä¾‹ä¸€è‡´
    # test_size=0.3: 30%ä½œä¸ºæµ‹è¯•é›†
    print("\n   æ­£åœ¨åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42, stratify=y
    )

    print(f"   âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼")
    print(f"      - è®­ç»ƒé›†å¤§å°: {X_train.shape[0]:,}")
    print(f"      - æµ‹è¯•é›†å¤§å°: {X_test.shape[0]:,}")
    print(f"      - è®­ç»ƒé›†æ¬ºè¯ˆæ¯”ä¾‹: {y_train.mean():.4f}")
    print(f"      - æµ‹è¯•é›†æ¬ºè¯ˆæ¯”ä¾‹: {y_test.mean():.4f}")

    # 4. æå–æ­£å¸¸äº¤æ˜“ï¼ˆç”¨äºOne-Classæ–¹æ³•ï¼‰
    # One-Classæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼š
    # - åªä½¿ç”¨æ­£å¸¸æ ·æœ¬å­¦ä¹ "æ­£å¸¸"çš„æ¨¡å¼
    # - æµ‹è¯•æ—¶ï¼Œä¸ç¬¦åˆæ­£å¸¸æ¨¡å¼çš„è§†ä¸ºå¼‚å¸¸
    # - é€‚åˆæåº¦ä¸å¹³è¡¡çš„æ•°æ®
    X_train_normal = X_train[y_train == 0]

    print(f"\n   æå–æ­£å¸¸äº¤æ˜“ç”¨äºOne-Classè®­ç»ƒ:")
    print(f"      - æ­£å¸¸äº¤æ˜“æ•°é‡: {X_train_normal.shape[0]:,}")
    print(f"      - å è®­ç»ƒé›†æ¯”ä¾‹: {X_train_normal.shape[0]/X_train.shape[0]*100:.2f}%")

    return X_train, X_test, y_train, y_test, X_train_normal, scaler


# ============================================================================
# ç¬¬5éƒ¨åˆ†ï¼šå¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ
# ============================================================================

def train_isolation_forest(X_train_normal, contamination=0.002):
    """
    è®­ç»ƒIsolation Forestæ¨¡å‹

    ç®—æ³•åŸç†ï¼š
        Isolation Forestï¼ˆéš”ç¦»æ£®æ—ï¼‰åŸºäºä»¥ä¸‹ç›´è§‰ï¼š
        - å¼‚å¸¸ç‚¹æ›´å®¹æ˜“è¢«"éš”ç¦»"ï¼ˆä¸å…¶ä»–ç‚¹åˆ†ç¦»ï¼‰
        - ä½¿ç”¨éšæœºå†³ç­–æ ‘ï¼Œå¼‚å¸¸ç‚¹é€šå¸¸åœ¨æ ‘çš„è¾ƒæµ…å±‚å°±è¢«éš”ç¦»
        - æ­£å¸¸ç‚¹éœ€è¦æ›´æ·±çš„è·¯å¾„æ‰èƒ½è¢«éš”ç¦»

        ç®—æ³•æ­¥éª¤ï¼š
        1. éšæœºé€‰æ‹©ç‰¹å¾å’Œåˆ†å‰²ç‚¹æ„å»ºå†³ç­–æ ‘
        2. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡è·¯å¾„é•¿åº¦ï¼ˆä»æ ¹åˆ°å¶ï¼‰
        3. è·¯å¾„é•¿åº¦çŸ­çš„æ ·æœ¬ä¸ºå¼‚å¸¸ï¼ˆå®¹æ˜“è¢«éš”ç¦»ï¼‰

    å‚æ•°è¯´æ˜ï¼š
        - contamination: è®­ç»ƒæ•°æ®ä¸­å¼‚å¸¸çš„æ¯”ä¾‹ï¼ˆé¢„æœŸï¼‰
          è®¾ç½®ä¸º0.002è¡¨ç¤ºé¢„æœŸ0.2%çš„æ•°æ®ä¸ºå¼‚å¸¸
        - n_estimators: å†³ç­–æ ‘æ•°é‡ï¼Œé»˜è®¤100
        - max_samples: æ„å»ºæ¯æ£µæ ‘ä½¿ç”¨çš„æ ·æœ¬æ•°ï¼Œ'auto'è¡¨ç¤ºmin(256, n_samples)
        - random_state: éšæœºç§å­

    Parameters:
    -----------
    X_train_normal : ndarray
        æ­£å¸¸äº¤æ˜“æ•°æ®
    contamination : float
        é¢„æœŸçš„å¼‚å¸¸æ¯”ä¾‹

    Returns:
    --------
    model : IsolationForest
        è®­ç»ƒå¥½çš„æ¨¡å‹
    train_time : float
        è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸŒ² è®­ç»ƒ Isolation Forest æ¨¡å‹")
    print("=" * 80)

    print(f"\n   æ¨¡å‹å‚æ•°:")
    print(f"      - contamination: {contamination} (é¢„æœŸå¼‚å¸¸æ¯”ä¾‹)")
    print(f"      - n_estimators: 100 (å†³ç­–æ ‘æ•°é‡)")
    print(f"      - max_samples: 'auto'")

    # åˆ›å»ºæ¨¡å‹
    model = IsolationForest(
        contamination=contamination,
        n_estimators=100,
        max_samples='auto',
        random_state=42,
        n_jobs=-1  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    )

    # è®­ç»ƒæ¨¡å‹
    print("\n   æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
    start_time = time.time()
    model.fit(X_train_normal)
    train_time = time.time() - start_time

    print(f"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"      - è®­ç»ƒæ ·æœ¬æ•°: {X_train_normal.shape[0]:,}")
    print(f"      - è®­ç»ƒæ—¶é—´: {train_time:.3f} ç§’")

    return model, train_time


def train_one_class_svm(X_train_normal, nu=0.002):
    """
    è®­ç»ƒOne-Class SVMæ¨¡å‹

    ç®—æ³•åŸç†ï¼š
        One-Class SVMï¼ˆå•ç±»æ”¯æŒå‘é‡æœºï¼‰é€šè¿‡ä»¥ä¸‹æ–¹å¼æ£€æµ‹å¼‚å¸¸ï¼š
        - åœ¨ç‰¹å¾ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªæœ€å°çš„è¶…çƒé¢ï¼ˆæˆ–è¶…å¹³é¢ï¼‰åŒ…å›´æ­£å¸¸æ•°æ®
        - ä½¿ç”¨æ ¸æŠ€å·§å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´
        - åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ­£å¸¸æ•°æ®æ›´å®¹æ˜“è¢«åˆ†ç¦»

        å…³é”®æ¦‚å¿µï¼š
        - nuå‚æ•°ï¼šæ§åˆ¶å¼‚å¸¸çš„ä¸Šç•Œå’Œæ”¯æŒå‘é‡çš„ä¸‹ç•Œ
          è®¾ç½®ä¸º0.002è¡¨ç¤ºå…è®¸æœ€å¤š0.2%çš„è®­ç»ƒæ•°æ®ä¸ºå¼‚å¸¸
        - RBFæ ¸ï¼šå¾„å‘åŸºå‡½æ•°æ ¸ï¼Œé€‚åˆéçº¿æ€§æ•°æ®

    å‚æ•°è¯´æ˜ï¼š
        - nu: å¼‚å¸¸å€¼çš„ä¸Šç•Œï¼Œå–å€¼èŒƒå›´(0, 1]
        - kernel: æ ¸å‡½æ•°ç±»å‹ï¼Œ'rbf'æ˜¯å¸¸ç”¨é€‰æ‹©
        - gamma: RBFæ ¸çš„å‚æ•°ï¼Œ'scale'è¡¨ç¤º1/(n_features * X.var())

    æ³¨æ„ï¼š
        One-Class SVMè®­ç»ƒè¾ƒæ…¢ï¼Œç‰¹åˆ«æ˜¯å¤§æ•°æ®é›†
        å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´

    Parameters:
    -----------
    X_train_normal : ndarray
        æ­£å¸¸äº¤æ˜“æ•°æ®
    nu : float
        å¼‚å¸¸å€¼ä¸Šç•Œ

    Returns:
    --------
    model : OneClassSVM
        è®­ç»ƒå¥½çš„æ¨¡å‹
    train_time : float
        è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸ”µ è®­ç»ƒ One-Class SVM æ¨¡å‹")
    print("=" * 80)

    print(f"\n   æ¨¡å‹å‚æ•°:")
    print(f"      - nu: {nu} (å¼‚å¸¸å€¼ä¸Šç•Œ)")
    print(f"      - kernel: 'rbf' (å¾„å‘åŸºå‡½æ•°æ ¸)")
    print(f"      - gamma: 'scale'")

    # ç”±äºOne-Class SVMè®­ç»ƒè¾ƒæ…¢ï¼Œå¯¹å¤§æ•°æ®é›†è¿›è¡Œé‡‡æ ·
    max_samples = 10000
    if X_train_normal.shape[0] > max_samples:
        print(f"\n   âš ï¸  æ•°æ®é‡è¾ƒå¤§ï¼Œé‡‡æ · {max_samples:,} ä¸ªæ ·æœ¬ä»¥åŠ é€Ÿè®­ç»ƒ")
        indices = np.random.choice(X_train_normal.shape[0], max_samples, replace=False)
        X_train_sample = X_train_normal[indices]
    else:
        X_train_sample = X_train_normal

    # åˆ›å»ºæ¨¡å‹
    model = OneClassSVM(
        nu=nu,
        kernel='rbf',
        gamma='scale'
    )

    # è®­ç»ƒæ¨¡å‹
    print("\n   æ­£åœ¨è®­ç»ƒæ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
    start_time = time.time()
    model.fit(X_train_sample)
    train_time = time.time() - start_time

    print(f"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"      - è®­ç»ƒæ ·æœ¬æ•°: {X_train_sample.shape[0]:,}")
    print(f"      - è®­ç»ƒæ—¶é—´: {train_time:.3f} ç§’")

    return model, train_time


def train_local_outlier_factor(X_train_normal, contamination=0.002):
    """
    è®­ç»ƒLocal Outlier Factoræ¨¡å‹

    ç®—æ³•åŸç†ï¼š
        LOFï¼ˆå±€éƒ¨ç¦»ç¾¤å› å­ï¼‰é€šè¿‡æ¯”è¾ƒæ ·æœ¬çš„å±€éƒ¨å¯†åº¦æ¥æ£€æµ‹å¼‚å¸¸ï¼š
        - è®¡ç®—æ¯ä¸ªç‚¹ä¸å…¶kè¿‘é‚»çš„å¯†åº¦
        - æ¯”è¾ƒè¯¥ç‚¹çš„å¯†åº¦ä¸å…¶é‚»å±…çš„å¯†åº¦
        - å¯†åº¦æ˜æ˜¾ä½äºé‚»å±…çš„ç‚¹è¢«è®¤ä¸ºæ˜¯å¼‚å¸¸

        æ ¸å¿ƒæ¦‚å¿µï¼š
        - å±€éƒ¨å¯†åº¦ï¼šç‚¹åˆ°å…¶kè¿‘é‚»çš„å¹³å‡è·ç¦»çš„å€’æ•°
        - LOFå€¼ï¼šç‚¹çš„å±€éƒ¨å¯†åº¦ä¸å…¶é‚»å±…å±€éƒ¨å¯†åº¦çš„æ¯”å€¼
        - LOF >> 1: å¼‚å¸¸ç‚¹ï¼ˆå¯†åº¦è¿œä½äºé‚»å±…ï¼‰
        - LOF â‰ˆ 1: æ­£å¸¸ç‚¹ï¼ˆå¯†åº¦ä¸é‚»å±…ç›¸ä¼¼ï¼‰

    å‚æ•°è¯´æ˜ï¼š
        - n_neighbors: è€ƒè™‘çš„é‚»å±…æ•°é‡ï¼Œé»˜è®¤20
        - contamination: æ•°æ®é›†ä¸­å¼‚å¸¸çš„æ¯”ä¾‹
        - novelty: Trueè¡¨ç¤ºç”¨äºæ–°æ•°æ®æ£€æµ‹ï¼ŒFalseè¡¨ç¤ºç”¨äºå·²æœ‰æ•°æ®

    æ³¨æ„ï¼š
        LOFæœ‰ä¸¤ç§æ¨¡å¼ï¼š
        - novelty=False: åªèƒ½å¯¹è®­ç»ƒæ•°æ®æ‰“åˆ†ï¼Œä¸èƒ½é¢„æµ‹æ–°æ•°æ®
        - novelty=True: å¯ä»¥é¢„æµ‹æ–°æ•°æ®ï¼ˆæˆ‘ä»¬ä½¿ç”¨è¿™ç§æ¨¡å¼ï¼‰

    Parameters:
    -----------
    X_train_normal : ndarray
        æ­£å¸¸äº¤æ˜“æ•°æ®
    contamination : float
        é¢„æœŸçš„å¼‚å¸¸æ¯”ä¾‹

    Returns:
    --------
    model : LocalOutlierFactor
        è®­ç»ƒå¥½çš„æ¨¡å‹
    train_time : float
        è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸ“ è®­ç»ƒ Local Outlier Factor (LOF) æ¨¡å‹")
    print("=" * 80)

    print(f"\n   æ¨¡å‹å‚æ•°:")
    print(f"      - n_neighbors: 20 (é‚»å±…æ•°é‡)")
    print(f"      - contamination: {contamination} (é¢„æœŸå¼‚å¸¸æ¯”ä¾‹)")
    print(f"      - novelty: True (ç”¨äºæ–°æ•°æ®æ£€æµ‹)")

    # åˆ›å»ºæ¨¡å‹
    # novelty=True å…è®¸æ¨¡å‹é¢„æµ‹æ–°æ•°æ®
    model = LocalOutlierFactor(
        n_neighbors=20,
        contamination=contamination,
        novelty=True,  # é‡è¦ï¼šå…è®¸é¢„æµ‹æ–°æ•°æ®
        n_jobs=-1
    )

    # è®­ç»ƒæ¨¡å‹
    print("\n   æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
    start_time = time.time()
    model.fit(X_train_normal)
    train_time = time.time() - start_time

    print(f"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"      - è®­ç»ƒæ ·æœ¬æ•°: {X_train_normal.shape[0]:,}")
    print(f"      - è®­ç»ƒæ—¶é—´: {train_time:.3f} ç§’")

    return model, train_time


# ============================================================================
# ç¬¬6éƒ¨åˆ†ï¼šæ¨¡å‹è¯„ä¼°
# ============================================================================

def evaluate_model(model, model_name, X_test, y_test):
    """
    è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ¨¡å‹

    è¯„ä¼°æŒ‡æ ‡è¯´æ˜ï¼š
        - Precision (ç²¾ç¡®ç‡): é¢„æµ‹ä¸ºæ¬ºè¯ˆçš„äº¤æ˜“ä¸­ï¼ŒçœŸæ­£æ¬ºè¯ˆçš„æ¯”ä¾‹
          å…¬å¼: TP / (TP + FP)
          é‡è¦æ€§: é«˜ç²¾ç¡®ç‡å‡å°‘è¯¯æŠ¥ï¼Œé¿å…é”™è¯¯å†»ç»“æ­£å¸¸äº¤æ˜“

        - Recall (å¬å›ç‡): çœŸå®æ¬ºè¯ˆäº¤æ˜“ä¸­ï¼Œè¢«æ£€æµ‹å‡ºçš„æ¯”ä¾‹
          å…¬å¼: TP / (TP + FN)
          é‡è¦æ€§: é«˜å¬å›ç‡å‡å°‘æ¼æŠ¥ï¼Œé™ä½ç»æµæŸå¤±

        - F1-Score: Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡
          å…¬å¼: 2 * (Precision * Recall) / (Precision + Recall)
          ç”¨é€”: ç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½

        - ROC-AUC: ROCæ›²çº¿ä¸‹é¢ç§¯
          ç”¨é€”: è¡¡é‡æ¨¡å‹åŒºåˆ†æ­£è´Ÿæ ·æœ¬çš„èƒ½åŠ›
          æ³¨æ„: å¯¹ä¸å¹³è¡¡æ•°æ®å¯èƒ½ä¸å¤Ÿæ•æ„Ÿ

        - PR-AUC: Precision-Recallæ›²çº¿ä¸‹é¢ç§¯
          ç”¨é€”: æ›´é€‚åˆä¸å¹³è¡¡æ•°æ®çš„è¯„ä¼°æŒ‡æ ‡

    Parameters:
    -----------
    model : å¼‚å¸¸æ£€æµ‹æ¨¡å‹
        å·²è®­ç»ƒçš„æ¨¡å‹
    model_name : str
        æ¨¡å‹åç§°
    X_test : ndarray
        æµ‹è¯•é›†ç‰¹å¾
    y_test : ndarray
        æµ‹è¯•é›†æ ‡ç­¾

    Returns:
    --------
    metrics : dict
        åŒ…å«å„é¡¹è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    y_pred : ndarray
        é¢„æµ‹ç»“æœ (0æˆ–1)
    y_scores : ndarray
        å¼‚å¸¸åˆ†æ•°
    """
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š è¯„ä¼° {model_name} æ¨¡å‹")
    print("=" * 80)

    # 1. é¢„æµ‹
    print("\n   æ­£åœ¨è¿›è¡Œé¢„æµ‹...")
    start_time = time.time()

    # predictè¿”å›ï¼š1è¡¨ç¤ºæ­£å¸¸ï¼Œ-1è¡¨ç¤ºå¼‚å¸¸
    y_pred_raw = model.predict(X_test)

    # è½¬æ¢ä¸º0/1æ ‡ç­¾ï¼š0=æ­£å¸¸ï¼Œ1=å¼‚å¸¸
    y_pred = np.where(y_pred_raw == -1, 1, 0)

    # è·å–å¼‚å¸¸åˆ†æ•°
    # decision_functionè¿”å›ï¼šå€¼è¶Šå°è¶Šå¯èƒ½æ˜¯å¼‚å¸¸
    y_scores = -model.decision_function(X_test)  # å–è´Ÿæ•°ï¼Œä½¿å¾—åˆ†æ•°è¶Šå¤§è¶Šå¼‚å¸¸

    predict_time = time.time() - start_time
    print(f"   âœ… é¢„æµ‹å®Œæˆï¼ç”¨æ—¶: {predict_time:.3f} ç§’")

    # 2. è®¡ç®—åŸºæœ¬æŒ‡æ ‡
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    print(f"\n   ã€åˆ†ç±»æŒ‡æ ‡ã€‘")
    print(f"      - Precision (ç²¾ç¡®ç‡): {precision:.4f}")
    print(f"      - Recall (å¬å›ç‡):    {recall:.4f}")
    print(f"      - F1-Score:          {f1:.4f}")

    # 3. è®¡ç®—ROC-AUC
    fpr, tpr, roc_thresholds = roc_curve(y_test, y_scores)
    roc_auc = auc(fpr, tpr)

    print(f"\n   ã€ROCæŒ‡æ ‡ã€‘")
    print(f"      - ROC-AUC: {roc_auc:.4f}")

    # 4. è®¡ç®—PR-AUC
    pr_precision, pr_recall, pr_thresholds = precision_recall_curve(y_test, y_scores)
    pr_auc = average_precision_score(y_test, y_scores)

    print(f"\n   ã€PRæŒ‡æ ‡ã€‘")
    print(f"      - PR-AUC (Average Precision): {pr_auc:.4f}")

    # 5. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    print(f"\n   ã€æ··æ·†çŸ©é˜µã€‘")
    print(f"      çœŸè´Ÿä¾‹ (TN): {tn:,}  |  å‡æ­£ä¾‹ (FP): {fp:,}")
    print(f"      å‡è´Ÿä¾‹ (FN): {fn:,}  |  çœŸæ­£ä¾‹ (TP): {tp:,}")

    # 6. æ£€æµ‹ç»Ÿè®¡
    n_detected = (y_pred == 1).sum()
    n_actual = (y_test == 1).sum()

    print(f"\n   ã€æ£€æµ‹ç»Ÿè®¡ã€‘")
    print(f"      - å®é™…æ¬ºè¯ˆæ•°: {n_actual:,}")
    print(f"      - æ£€æµ‹æ¬ºè¯ˆæ•°: {n_detected:,}")
    print(f"      - æ£€æµ‹ç‡: {tp/n_actual*100:.2f}% (å¬å›ç‡)")
    print(f"      - è¯¯æŠ¥æ•°: {fp:,}")
    print(f"      - æ¼æŠ¥æ•°: {fn:,}")

    # 7. ç»„ç»‡è¿”å›ç»“æœ
    metrics = {
        'model_name': model_name,
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'roc_auc': float(roc_auc),
        'pr_auc': float(pr_auc),
        'confusion_matrix': cm.tolist(),
        'true_positives': int(tp),
        'false_positives': int(fp),
        'true_negatives': int(tn),
        'false_negatives': int(fn),
        'predict_time': float(predict_time),
        'fpr': fpr.tolist(),
        'tpr': tpr.tolist(),
        'pr_precision': pr_precision.tolist(),
        'pr_recall': pr_recall.tolist()
    }

    return metrics, y_pred, y_scores


# ============================================================================
# ç¬¬7éƒ¨åˆ†ï¼šå¯è§†åŒ–
# ============================================================================

def plot_confusion_matrices(all_metrics):
    """
    ç»˜åˆ¶ä¸‰ä¸ªæ¨¡å‹çš„æ··æ·†çŸ©é˜µå¯¹æ¯”

    Parameters:
    -----------
    all_metrics : list of dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„åˆ—è¡¨
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š ç»˜åˆ¶æ··æ·†çŸ©é˜µå¯¹æ¯”")
    print("=" * 80)

    output_dir = Path('outputs')

    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    fig.suptitle('ä¸‰ç§å¼‚å¸¸æ£€æµ‹ç®—æ³•çš„æ··æ·†çŸ©é˜µå¯¹æ¯”', fontsize=16, fontweight='bold', y=1.02)

    for i, metrics in enumerate(all_metrics):
        cm = np.array(metrics['confusion_matrix'])
        model_name = metrics['model_name']

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],
                   cbar=True, square=True, linewidths=2, linecolor='black',
                   annot_kws={'fontsize': 14, 'fontweight': 'bold'})

        axes[i].set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=11, fontweight='bold')
        axes[i].set_ylabel('çœŸå®æ ‡ç­¾', fontsize=11, fontweight='bold')
        axes[i].set_title(f'{model_name}\n(F1={metrics["f1_score"]:.4f})',
                         fontsize=12, fontweight='bold')
        axes[i].set_xticklabels(['æ­£å¸¸', 'æ¬ºè¯ˆ'])
        axes[i].set_yticklabels(['æ­£å¸¸', 'æ¬ºè¯ˆ'])

    plt.tight_layout()
    plt.savefig(output_dir / '04_confusion_matrices.png', dpi=150, bbox_inches='tight')
    print("   âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: outputs/04_confusion_matrices.png")
    plt.show()


def plot_roc_curves(all_metrics):
    """
    ç»˜åˆ¶ROCæ›²çº¿å¯¹æ¯”

    ROCæ›²çº¿è¯´æ˜ï¼š
        - æ¨ªè½´ï¼šå‡æ­£ä¾‹ç‡ (FPR) = FP / (FP + TN)
        - çºµè½´ï¼šçœŸæ­£ä¾‹ç‡ (TPR) = TP / (TP + FN) = Recall
        - å¯¹è§’çº¿ï¼šéšæœºçŒœæµ‹çš„æ€§èƒ½
        - æ›²çº¿è¶Šé è¿‘å·¦ä¸Šè§’ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå¥½
        - AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰ï¼š0.5=éšæœºï¼Œ1.0=å®Œç¾

    Parameters:
    -----------
    all_metrics : list of dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„åˆ—è¡¨
    """
    print("\nã€ç»˜åˆ¶ROCæ›²çº¿ã€‘")

    output_dir = Path('outputs')

    plt.figure(figsize=(10, 8))

    colors = ['steelblue', 'coral', 'green']

    for i, metrics in enumerate(all_metrics):
        fpr = np.array(metrics['fpr'])
        tpr = np.array(metrics['tpr'])
        roc_auc = metrics['roc_auc']
        model_name = metrics['model_name']

        plt.plot(fpr, tpr, color=colors[i], linewidth=2.5,
                label=f'{model_name} (AUC = {roc_auc:.4f})')

    # ç»˜åˆ¶å¯¹è§’çº¿ï¼ˆéšæœºçŒœæµ‹ï¼‰
    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='éšæœºçŒœæµ‹ (AUC = 0.5000)')

    plt.xlabel('å‡æ­£ä¾‹ç‡ (FPR)', fontsize=12, fontweight='bold')
    plt.ylabel('çœŸæ­£ä¾‹ç‡ (TPR / Recall)', fontsize=12, fontweight='bold')
    plt.title('ROCæ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold', pad=15)
    plt.legend(loc='lower right', fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])

    plt.tight_layout()
    plt.savefig(output_dir / '05_roc_curves.png', dpi=150, bbox_inches='tight')
    print("   âœ… ROCæ›²çº¿å·²ä¿å­˜åˆ°: outputs/05_roc_curves.png")
    plt.show()


def plot_pr_curves(all_metrics):
    """
    ç»˜åˆ¶Precision-Recallæ›²çº¿å¯¹æ¯”

    PRæ›²çº¿è¯´æ˜ï¼š
        - æ¨ªè½´ï¼šRecall (å¬å›ç‡) = TP / (TP + FN)
        - çºµè½´ï¼šPrecision (ç²¾ç¡®ç‡) = TP / (TP + FP)
        - å¯¹äºä¸å¹³è¡¡æ•°æ®ï¼ŒPRæ›²çº¿æ¯”ROCæ›²çº¿æ›´æœ‰æ„ä¹‰
        - æ›²çº¿è¶Šé è¿‘å³ä¸Šè§’ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå¥½
        - PR-AUCï¼ˆå¹³å‡ç²¾ç¡®ç‡ï¼‰ï¼šè¶Šæ¥è¿‘1è¶Šå¥½

    ä¸ºä»€ä¹ˆPRæ›²çº¿æ›´é€‚åˆä¸å¹³è¡¡æ•°æ®ï¼Ÿ
        - ROCæ›²çº¿ä¸­çš„FPRåœ¨è´Ÿæ ·æœ¬æå¤šæ—¶å˜åŒ–ä¸æ•æ„Ÿ
        - PRæ›²çº¿ç›´æ¥å…³æ³¨æ­£æ ·æœ¬çš„æ£€æµ‹è´¨é‡
        - æ›´ç¬¦åˆæ¬ºè¯ˆæ£€æµ‹çš„ä¸šåŠ¡éœ€æ±‚

    Parameters:
    -----------
    all_metrics : list of dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„åˆ—è¡¨
    """
    print("\nã€ç»˜åˆ¶PRæ›²çº¿ã€‘")

    output_dir = Path('outputs')

    plt.figure(figsize=(10, 8))

    colors = ['steelblue', 'coral', 'green']

    for i, metrics in enumerate(all_metrics):
        pr_recall = np.array(metrics['pr_recall'])
        pr_precision = np.array(metrics['pr_precision'])
        pr_auc = metrics['pr_auc']
        model_name = metrics['model_name']

        plt.plot(pr_recall, pr_precision, color=colors[i], linewidth=2.5,
                label=f'{model_name} (AP = {pr_auc:.4f})')

    # ç»˜åˆ¶åŸºå‡†çº¿ï¼ˆéšæœºçŒœæµ‹ï¼‰
    # å¯¹äºä¸å¹³è¡¡æ•°æ®ï¼ŒéšæœºçŒœæµ‹çš„PR = æ­£æ ·æœ¬æ¯”ä¾‹
    baseline = all_metrics[0]['true_positives'] + all_metrics[0]['false_negatives']
    total = baseline + all_metrics[0]['true_negatives'] + all_metrics[0]['false_positives']
    baseline_precision = baseline / total

    plt.axhline(y=baseline_precision, color='k', linestyle='--', linewidth=2,
               label=f'éšæœºçŒœæµ‹ (AP = {baseline_precision:.4f})')

    plt.xlabel('å¬å›ç‡ (Recall)', fontsize=12, fontweight='bold')
    plt.ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12, fontweight='bold')
    plt.title('Precision-Recallæ›²çº¿å¯¹æ¯”ï¼ˆæ›´é€‚åˆä¸å¹³è¡¡æ•°æ®ï¼‰', fontsize=14, fontweight='bold', pad=15)
    plt.legend(loc='best', fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])

    plt.tight_layout()
    plt.savefig(output_dir / '06_pr_curves.png', dpi=150, bbox_inches='tight')
    print("   âœ… PRæ›²çº¿å·²ä¿å­˜åˆ°: outputs/06_pr_curves.png")
    plt.show()


def plot_metrics_comparison(all_metrics):
    """
    ç»˜åˆ¶æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾

    Parameters:
    -----------
    all_metrics : list of dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„åˆ—è¡¨
    """
    print("\nã€ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”ã€‘")

    output_dir = Path('outputs')

    # æå–æŒ‡æ ‡
    model_names = [m['model_name'] for m in all_metrics]
    precisions = [m['precision'] for m in all_metrics]
    recalls = [m['recall'] for m in all_metrics]
    f1_scores = [m['f1_score'] for m in all_metrics]
    pr_aucs = [m['pr_auc'] for m in all_metrics]

    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('å¼‚å¸¸æ£€æµ‹æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold', y=0.995)

    x = np.arange(len(model_names))
    width = 0.6

    # 1. Precisionå¯¹æ¯”
    bars1 = axes[0, 0].bar(x, precisions, width, color='steelblue', alpha=0.8, edgecolor='black')
    axes[0, 0].set_ylabel('Precision (ç²¾ç¡®ç‡)', fontsize=11, fontweight='bold')
    axes[0, 0].set_title('ç²¾ç¡®ç‡å¯¹æ¯”ï¼ˆå‡å°‘è¯¯æŠ¥ï¼‰', fontsize=12, fontweight='bold')
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(model_names, rotation=15, ha='right')
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    axes[0, 0].set_ylim([0, 1])
    for i, v in enumerate(precisions):
        axes[0, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')

    # 2. Recallå¯¹æ¯”
    bars2 = axes[0, 1].bar(x, recalls, width, color='coral', alpha=0.8, edgecolor='black')
    axes[0, 1].set_ylabel('Recall (å¬å›ç‡)', fontsize=11, fontweight='bold')
    axes[0, 1].set_title('å¬å›ç‡å¯¹æ¯”ï¼ˆå‡å°‘æ¼æŠ¥ï¼‰', fontsize=12, fontweight='bold')
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(model_names, rotation=15, ha='right')
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    axes[0, 1].set_ylim([0, 1])
    for i, v in enumerate(recalls):
        axes[0, 1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')

    # 3. F1-Scoreå¯¹æ¯”
    bars3 = axes[1, 0].bar(x, f1_scores, width, color='green', alpha=0.8, edgecolor='black')
    axes[1, 0].set_ylabel('F1-Score', fontsize=11, fontweight='bold')
    axes[1, 0].set_title('F1-Scoreå¯¹æ¯”ï¼ˆç»¼åˆæŒ‡æ ‡ï¼‰', fontsize=12, fontweight='bold')
    axes[1, 0].set_xticks(x)
    axes[1, 0].set_xticklabels(model_names, rotation=15, ha='right')
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    axes[1, 0].set_ylim([0, 1])
    for i, v in enumerate(f1_scores):
        axes[1, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')

    # 4. PR-AUCå¯¹æ¯”
    bars4 = axes[1, 1].bar(x, pr_aucs, width, color='purple', alpha=0.8, edgecolor='black')
    axes[1, 1].set_ylabel('PR-AUC', fontsize=11, fontweight='bold')
    axes[1, 1].set_title('PR-AUCå¯¹æ¯”ï¼ˆä¸å¹³è¡¡æ•°æ®æ¨èæŒ‡æ ‡ï¼‰', fontsize=12, fontweight='bold')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels(model_names, rotation=15, ha='right')
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    axes[1, 1].set_ylim([0, 1])
    for i, v in enumerate(pr_aucs):
        axes[1, 1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_dir / '07_metrics_comparison.png', dpi=150, bbox_inches='tight')
    print("   âœ… æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: outputs/07_metrics_comparison.png")
    plt.show()


def visualize_anomalies_tsne(X_test, y_test, y_pred_if, y_pred_svm, y_pred_lof):
    """
    ä½¿ç”¨t-SNEé™ç»´å¯è§†åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœ

    t-SNEè¯´æ˜ï¼š
        - t-SNE (t-Distributed Stochastic Neighbor Embedding)
        - å°†é«˜ç»´æ•°æ®é™ç»´åˆ°2D/3Dç”¨äºå¯è§†åŒ–
        - ä¿æŒæ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§å…³ç³»
        - ç›¸ä¼¼çš„ç‚¹åœ¨ä½ç»´ç©ºé—´ä¸­ä¹Ÿä¼šé è¿‘

    Parameters:
    -----------
    X_test : ndarray
        æµ‹è¯•é›†ç‰¹å¾
    y_test : ndarray
        çœŸå®æ ‡ç­¾
    y_pred_if : ndarray
        Isolation Foresté¢„æµ‹ç»“æœ
    y_pred_svm : ndarray
        One-Class SVMé¢„æµ‹ç»“æœ
    y_pred_lof : ndarray
        LOFé¢„æµ‹ç»“æœ
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š ä½¿ç”¨t-SNEå¯è§†åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœ")
    print("=" * 80)

    output_dir = Path('outputs')

    # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œt-SNEï¼ˆt-SNEè®¡ç®—è¾ƒæ…¢ï¼‰
    max_samples = 5000
    if X_test.shape[0] > max_samples:
        print(f"   é‡‡æ · {max_samples:,} ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–...")
        indices = np.random.choice(X_test.shape[0], max_samples, replace=False)
        X_sample = X_test[indices]
        y_sample = y_test[indices]
        y_if_sample = y_pred_if[indices]
        y_svm_sample = y_pred_svm[indices]
        y_lof_sample = y_pred_lof[indices]
    else:
        X_sample = X_test
        y_sample = y_test
        y_if_sample = y_pred_if
        y_svm_sample = y_pred_svm
        y_lof_sample = y_pred_lof

    # åº”ç”¨t-SNEé™ç»´
    print("\n   æ­£åœ¨è¿›è¡Œt-SNEé™ç»´ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
    X_tsne = tsne.fit_transform(X_sample)

    print("   âœ… t-SNEé™ç»´å®Œæˆï¼")

    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(16, 14))
    fig.suptitle('å¼‚å¸¸æ£€æµ‹ç»“æœå¯è§†åŒ– (t-SNEé™ç»´)', fontsize=16, fontweight='bold', y=0.995)

    # 1. çœŸå®æ ‡ç­¾
    scatter1 = axes[0, 0].scatter(X_tsne[y_sample == 0, 0], X_tsne[y_sample == 0, 1],
                                  c='steelblue', s=20, alpha=0.5, label='æ­£å¸¸äº¤æ˜“', edgecolors='none')
    scatter2 = axes[0, 0].scatter(X_tsne[y_sample == 1, 0], X_tsne[y_sample == 1, 1],
                                  c='red', s=50, alpha=0.8, label='æ¬ºè¯ˆäº¤æ˜“', marker='X', edgecolors='black')
    axes[0, 0].set_title('çœŸå®æ ‡ç­¾', fontsize=13, fontweight='bold')
    axes[0, 0].set_xlabel('t-SNE ç»´åº¦ 1', fontsize=11)
    axes[0, 0].set_ylabel('t-SNE ç»´åº¦ 2', fontsize=11)
    axes[0, 0].legend(fontsize=10)
    axes[0, 0].grid(True, alpha=0.3)

    # 2. Isolation Foresté¢„æµ‹
    axes[0, 1].scatter(X_tsne[y_if_sample == 0, 0], X_tsne[y_if_sample == 0, 1],
                      c='lightblue', s=20, alpha=0.5, label='é¢„æµ‹æ­£å¸¸', edgecolors='none')
    axes[0, 1].scatter(X_tsne[y_if_sample == 1, 0], X_tsne[y_if_sample == 1, 1],
                      c='orange', s=50, alpha=0.8, label='é¢„æµ‹æ¬ºè¯ˆ', marker='X', edgecolors='black')
    # æ ‡è®°çœŸå®æ¬ºè¯ˆ
    axes[0, 1].scatter(X_tsne[y_sample == 1, 0], X_tsne[y_sample == 1, 1],
                      c='none', s=80, marker='o', edgecolors='red', linewidths=2, label='çœŸå®æ¬ºè¯ˆ')
    axes[0, 1].set_title('Isolation Forest é¢„æµ‹', fontsize=13, fontweight='bold')
    axes[0, 1].set_xlabel('t-SNE ç»´åº¦ 1', fontsize=11)
    axes[0, 1].set_ylabel('t-SNE ç»´åº¦ 2', fontsize=11)
    axes[0, 1].legend(fontsize=9)
    axes[0, 1].grid(True, alpha=0.3)

    # 3. One-Class SVMé¢„æµ‹
    axes[1, 0].scatter(X_tsne[y_svm_sample == 0, 0], X_tsne[y_svm_sample == 0, 1],
                      c='lightblue', s=20, alpha=0.5, label='é¢„æµ‹æ­£å¸¸', edgecolors='none')
    axes[1, 0].scatter(X_tsne[y_svm_sample == 1, 0], X_tsne[y_svm_sample == 1, 1],
                      c='orange', s=50, alpha=0.8, label='é¢„æµ‹æ¬ºè¯ˆ', marker='X', edgecolors='black')
    axes[1, 0].scatter(X_tsne[y_sample == 1, 0], X_tsne[y_sample == 1, 1],
                      c='none', s=80, marker='o', edgecolors='red', linewidths=2, label='çœŸå®æ¬ºè¯ˆ')
    axes[1, 0].set_title('One-Class SVM é¢„æµ‹', fontsize=13, fontweight='bold')
    axes[1, 0].set_xlabel('t-SNE ç»´åº¦ 1', fontsize=11)
    axes[1, 0].set_ylabel('t-SNE ç»´åº¦ 2', fontsize=11)
    axes[1, 0].legend(fontsize=9)
    axes[1, 0].grid(True, alpha=0.3)

    # 4. LOFé¢„æµ‹
    axes[1, 1].scatter(X_tsne[y_lof_sample == 0, 0], X_tsne[y_lof_sample == 0, 1],
                      c='lightblue', s=20, alpha=0.5, label='é¢„æµ‹æ­£å¸¸', edgecolors='none')
    axes[1, 1].scatter(X_tsne[y_lof_sample == 1, 0], X_tsne[y_lof_sample == 1, 1],
                      c='orange', s=50, alpha=0.8, label='é¢„æµ‹æ¬ºè¯ˆ', marker='X', edgecolors='black')
    axes[1, 1].scatter(X_tsne[y_sample == 1, 0], X_tsne[y_sample == 1, 1],
                      c='none', s=80, marker='o', edgecolors='red', linewidths=2, label='çœŸå®æ¬ºè¯ˆ')
    axes[1, 1].set_title('Local Outlier Factor é¢„æµ‹', fontsize=13, fontweight='bold')
    axes[1, 1].set_xlabel('t-SNE ç»´åº¦ 1', fontsize=11)
    axes[1, 1].set_ylabel('t-SNE ç»´åº¦ 2', fontsize=11)
    axes[1, 1].legend(fontsize=9)
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / '08_tsne_visualization.png', dpi=150, bbox_inches='tight')
    print("   âœ… t-SNEå¯è§†åŒ–å·²ä¿å­˜åˆ°: outputs/08_tsne_visualization.png")
    plt.show()


# ============================================================================
# ç¬¬8éƒ¨åˆ†ï¼šæ¨¡å‹ä¿å­˜
# ============================================================================

def save_models_and_metrics(models, all_metrics, scaler):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡

    Parameters:
    -----------
    models : dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹çš„å­—å…¸
    all_metrics : list of dict
        æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡
    scaler : StandardScaler
        æ•°æ®æ ‡å‡†åŒ–å™¨
    """
    print("\n" + "=" * 80)
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡")
    print("=" * 80)

    # åˆ›å»ºæ¨¡å‹ç›®å½•
    models_dir = Path('models')
    models_dir.mkdir(exist_ok=True)

    # ä¿å­˜æ¨¡å‹
    for name, model in models.items():
        model_path = models_dir / f'{name.lower().replace(" ", "_")}_model.pkl'
        joblib.dump(model, model_path)
        print(f"   âœ… å·²ä¿å­˜æ¨¡å‹: {model_path}")

    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    scaler_path = models_dir / 'scaler.pkl'
    joblib.dump(scaler, scaler_path)
    print(f"   âœ… å·²ä¿å­˜æ ‡å‡†åŒ–å™¨: {scaler_path}")

    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼Œå»é™¤å¤§æ•°ç»„ï¼‰
    metrics_simplified = []
    for m in all_metrics:
        metrics_simple = {
            'model_name': m['model_name'],
            'precision': m['precision'],
            'recall': m['recall'],
            'f1_score': m['f1_score'],
            'roc_auc': m['roc_auc'],
            'pr_auc': m['pr_auc'],
            'confusion_matrix': m['confusion_matrix'],
            'true_positives': m['true_positives'],
            'false_positives': m['false_positives'],
            'true_negatives': m['true_negatives'],
            'false_negatives': m['false_negatives'],
            'predict_time': m['predict_time']
        }
        metrics_simplified.append(metrics_simple)

    metrics_path = models_dir / 'evaluation_metrics.json'
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_simplified, f, indent=4, ensure_ascii=False)

    print(f"   âœ… å·²ä¿å­˜è¯„ä¼°æŒ‡æ ‡: {metrics_path}")
    print("\n" + "=" * 80)


# ============================================================================
# ç¬¬8.5éƒ¨åˆ†ï¼šè®­ç»ƒæ¨¡å¼å¯¹æ¯”å®éªŒï¼ˆåŠç›‘ç£ vs æ— ç›‘ç£ï¼‰
# ============================================================================

def compare_training_modes(X_train, X_train_normal, X_test, y_train, y_test, contamination):
    """
    å¯¹æ¯”å®éªŒï¼šåŠç›‘ç£æ¨¡å¼ vs æ— ç›‘ç£æ¨¡å¼

    æ ¸å¿ƒæ¦‚å¿µè¯´æ˜ï¼š
    ==============

    å¼‚å¸¸æ£€æµ‹ç®—æ³•æœ‰ä¸¤ç§ä¸»è¦çš„è®­ç»ƒæ¨¡å¼ï¼Œç†è§£å®ƒä»¬çš„åŒºåˆ«å¯¹äºé€‰æ‹©æ­£ç¡®çš„æ–¹æ³•è‡³å…³é‡è¦ï¼š

    ã€åŠç›‘ç£æ¨¡å¼ (Semi-supervised)ã€‘
    --------------------------------
    - è®­ç»ƒæ•°æ®ï¼šåªä½¿ç”¨æ­£å¸¸æ ·æœ¬ (X_train_normal)
    - æ ¸å¿ƒæ€æƒ³ï¼šå­¦ä¹ "ä»€ä¹ˆæ˜¯æ­£å¸¸"ï¼Œç„¶åå°†ä¸ç¬¦åˆæ­£å¸¸æ¨¡å¼çš„åˆ¤å®šä¸ºå¼‚å¸¸
    - é€‚ç”¨åœºæ™¯ï¼š
        * æœ‰æ ‡ç­¾æ•°æ®ï¼ŒçŸ¥é“å“ªäº›æ˜¯æ­£å¸¸æ ·æœ¬
        * æ­£å¸¸æ ·æœ¬æ•°é‡å……è¶³
        * å¼‚å¸¸æ ·æœ¬å¤ªå°‘ä¸è¶³ä»¥å­¦ä¹ å¼‚å¸¸æ¨¡å¼
    - contamination å‚æ•°å«ä¹‰ï¼šé¢„æµ‹æ—¶æœŸæœ›çš„å¼‚å¸¸æ¯”ä¾‹ï¼ˆå½±å“å†³ç­–é˜ˆå€¼ï¼‰

    ã€æ— ç›‘ç£æ¨¡å¼ (Unsupervised)ã€‘
    ----------------------------
    - è®­ç»ƒæ•°æ®ï¼šä½¿ç”¨å…¨éƒ¨æ•°æ® (X_train)ï¼ŒåŒ…å«å°‘é‡å¼‚å¸¸
    - æ ¸å¿ƒæ€æƒ³ï¼šå‡è®¾æ•°æ®ä¸­æœ‰ contamination æ¯”ä¾‹çš„å¼‚å¸¸ï¼Œæ‰¾å‡ºæœ€"ä¸æ­£å¸¸"çš„é‚£éƒ¨åˆ†
    - é€‚ç”¨åœºæ™¯ï¼š
        * æ— æ ‡ç­¾æ•°æ®
        * ä¸ç¡®å®šå“ªäº›æ˜¯æ­£å¸¸æ ·æœ¬
        * çœŸå®ç”Ÿäº§ç¯å¢ƒä¸­å¸¸è§
    - contamination å‚æ•°å«ä¹‰ï¼šè®­ç»ƒæ•°æ®ä¸­å®é™…çš„å¼‚å¸¸æ¯”ä¾‹

    ã€ä¸ºä»€ä¹ˆè¦å¯¹æ¯”ï¼Ÿã€‘
    -----------------
    åœ¨è¿™ä¸ªä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†ä¸­ï¼š
    - æˆ‘ä»¬æœ‰æ ‡ç­¾ï¼Œå¯ä»¥ä½¿ç”¨åŠç›‘ç£æ¨¡å¼
    - ä½†çœŸå®ä¸šåŠ¡ä¸­å¾€å¾€æ²¡æœ‰å®Œæ•´æ ‡ç­¾ï¼Œéœ€è¦ç”¨æ— ç›‘ç£æ¨¡å¼
    - å¯¹æ¯”ä¸¤ç§æ¨¡å¼å¯ä»¥å¸®åŠ©ç†è§£å„è‡ªçš„ä¼˜ç¼ºç‚¹

    Parameters:
    -----------
    X_train : ndarray
        å®Œæ•´è®­ç»ƒé›†ï¼ˆåŒ…å«æ­£å¸¸+æ¬ºè¯ˆæ ·æœ¬ï¼‰
    X_train_normal : ndarray
        ä»…åŒ…å«æ­£å¸¸æ ·æœ¬çš„è®­ç»ƒé›†
    X_test : ndarray
        æµ‹è¯•é›†
    y_train : ndarray
        è®­ç»ƒé›†æ ‡ç­¾
    y_test : ndarray
        æµ‹è¯•é›†æ ‡ç­¾
    contamination : float
        å¼‚å¸¸æ¯”ä¾‹

    Returns:
    --------
    comparison_results : dict
        åŒ…å«ä¸¤ç§æ¨¡å¼çš„å¯¹æ¯”ç»“æœ
    """
    print("\n" + "=" * 80)
    print("ğŸ”¬ è®­ç»ƒæ¨¡å¼å¯¹æ¯”å®éªŒï¼šåŠç›‘ç£ vs æ— ç›‘ç£")
    print("=" * 80)

    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  æœ¬å®éªŒå¯¹æ¯” Isolation Forest åœ¨ä¸¤ç§è®­ç»ƒæ¨¡å¼ä¸‹çš„æ€§èƒ½å·®å¼‚              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  åŠç›‘ç£æ¨¡å¼ï¼šåªç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒ â†’ å­¦ä¹ "æ­£å¸¸æ˜¯ä»€ä¹ˆæ ·"                   â”‚
    â”‚  æ— ç›‘ç£æ¨¡å¼ï¼šç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ   â†’ æ‰¾å‡º"æœ€ä¸æ­£å¸¸çš„é‚£éƒ¨åˆ†"               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    output_dir = Path('outputs')
    results = {}

    # =========================================================================
    # å®éªŒ1ï¼šåŠç›‘ç£æ¨¡å¼ï¼ˆåªç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒï¼‰
    # =========================================================================
    print("\nã€å®éªŒ1ã€‘åŠç›‘ç£æ¨¡å¼ (Semi-supervised)")
    print("-" * 60)
    print(f"   è®­ç»ƒæ•°æ®ï¼šä»…æ­£å¸¸æ ·æœ¬")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°ï¼š{X_train_normal.shape[0]:,}")
    print(f"   contaminationï¼š{contamination:.5f}ï¼ˆä½œä¸ºé¢„æµ‹é˜ˆå€¼ï¼‰")

    # è®­ç»ƒåŠç›‘ç£æ¨¡å‹
    model_semi = IsolationForest(
        contamination=contamination,
        n_estimators=100,
        max_samples='auto',
        random_state=42,
        n_jobs=-1
    )

    start_time = time.time()
    model_semi.fit(X_train_normal)  # åªç”¨æ­£å¸¸æ ·æœ¬
    train_time_semi = time.time() - start_time

    # é¢„æµ‹
    y_pred_semi_raw = model_semi.predict(X_test)
    y_pred_semi = np.where(y_pred_semi_raw == -1, 1, 0)
    y_scores_semi = -model_semi.decision_function(X_test)

    # è¯„ä¼°
    precision_semi = precision_score(y_test, y_pred_semi, zero_division=0)
    recall_semi = recall_score(y_test, y_pred_semi, zero_division=0)
    f1_semi = f1_score(y_test, y_pred_semi, zero_division=0)
    pr_auc_semi = average_precision_score(y_test, y_scores_semi)

    print(f"\n   è®­ç»ƒæ—¶é—´ï¼š{train_time_semi:.3f} ç§’")
    print(f"   Precisionï¼š{precision_semi:.4f}")
    print(f"   Recallï¼š{recall_semi:.4f}")
    print(f"   F1-Scoreï¼š{f1_semi:.4f}")
    print(f"   PR-AUCï¼š{pr_auc_semi:.4f}")

    results['semi_supervised'] = {
        'precision': precision_semi,
        'recall': recall_semi,
        'f1_score': f1_semi,
        'pr_auc': pr_auc_semi,
        'train_time': train_time_semi
    }

    # =========================================================================
    # å®éªŒ2ï¼šæ— ç›‘ç£æ¨¡å¼ï¼ˆç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒï¼‰
    # =========================================================================
    print("\nã€å®éªŒ2ã€‘æ— ç›‘ç£æ¨¡å¼ (Unsupervised)")
    print("-" * 60)
    print(f"   è®­ç»ƒæ•°æ®ï¼šå…¨éƒ¨è®­ç»ƒæ ·æœ¬ï¼ˆåŒ…å«å°‘é‡æ¬ºè¯ˆï¼‰")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°ï¼š{X_train.shape[0]:,}")
    print(f"   å…¶ä¸­æ¬ºè¯ˆæ ·æœ¬ï¼š{y_train.sum():,} ({y_train.mean()*100:.3f}%)")
    print(f"   contaminationï¼š{contamination:.5f}ï¼ˆè®­ç»ƒæ•°æ®ä¸­çš„å®é™…å¼‚å¸¸æ¯”ä¾‹ï¼‰")

    # è®­ç»ƒæ— ç›‘ç£æ¨¡å‹
    model_unsup = IsolationForest(
        contamination=contamination,
        n_estimators=100,
        max_samples='auto',
        random_state=42,
        n_jobs=-1
    )

    start_time = time.time()
    model_unsup.fit(X_train)  # ç”¨å…¨éƒ¨æ•°æ®
    train_time_unsup = time.time() - start_time

    # é¢„æµ‹
    y_pred_unsup_raw = model_unsup.predict(X_test)
    y_pred_unsup = np.where(y_pred_unsup_raw == -1, 1, 0)
    y_scores_unsup = -model_unsup.decision_function(X_test)

    # è¯„ä¼°
    precision_unsup = precision_score(y_test, y_pred_unsup, zero_division=0)
    recall_unsup = recall_score(y_test, y_pred_unsup, zero_division=0)
    f1_unsup = f1_score(y_test, y_pred_unsup, zero_division=0)
    pr_auc_unsup = average_precision_score(y_test, y_scores_unsup)

    print(f"\n   è®­ç»ƒæ—¶é—´ï¼š{train_time_unsup:.3f} ç§’")
    print(f"   Precisionï¼š{precision_unsup:.4f}")
    print(f"   Recallï¼š{recall_unsup:.4f}")
    print(f"   F1-Scoreï¼š{f1_unsup:.4f}")
    print(f"   PR-AUCï¼š{pr_auc_unsup:.4f}")

    results['unsupervised'] = {
        'precision': precision_unsup,
        'recall': recall_unsup,
        'f1_score': f1_unsup,
        'pr_auc': pr_auc_unsup,
        'train_time': train_time_unsup
    }

    # =========================================================================
    # å¯¹æ¯”åˆ†æ
    # =========================================================================
    print("\n" + "=" * 60)
    print("ğŸ“Š å¯¹æ¯”åˆ†æç»“æœ")
    print("=" * 60)

    print(f"\n{'æŒ‡æ ‡':<15} {'åŠç›‘ç£æ¨¡å¼':<15} {'æ— ç›‘ç£æ¨¡å¼':<15} {'å·®å¼‚':<15}")
    print("-" * 60)

    metrics_names = ['Precision', 'Recall', 'F1-Score', 'PR-AUC']
    semi_values = [precision_semi, recall_semi, f1_semi, pr_auc_semi]
    unsup_values = [precision_unsup, recall_unsup, f1_unsup, pr_auc_unsup]

    for name, semi_val, unsup_val in zip(metrics_names, semi_values, unsup_values):
        diff = unsup_val - semi_val
        diff_str = f"+{diff:.4f}" if diff > 0 else f"{diff:.4f}"
        winner = "â† èƒœ" if semi_val > unsup_val else "èƒœ â†’" if unsup_val > semi_val else "å¹³"
        print(f"{name:<15} {semi_val:<15.4f} {unsup_val:<15.4f} {diff_str:<10} {winner}")

    # ç»“è®º
    print("\nã€ç»“è®ºä¸å»ºè®®ã€‘")
    if f1_semi > f1_unsup:
        print("   âœ… åŠç›‘ç£æ¨¡å¼è¡¨ç°æ›´å¥½")
        print("   åŸå› åˆ†æï¼š")
        print("      - è®­ç»ƒæ•°æ®ä¸­åªæœ‰çº¯æ­£å¸¸æ ·æœ¬ï¼Œæ¨¡å‹å­¦åˆ°äº†æ¸…æ™°çš„'æ­£å¸¸'è¾¹ç•Œ")
        print("      - æ— ç›‘ç£æ¨¡å¼ä¸­ï¼Œå°‘é‡æ¬ºè¯ˆæ ·æœ¬å¯èƒ½å¹²æ‰°äº†æ¨¡å‹å¯¹'æ­£å¸¸'çš„å­¦ä¹ ")
    elif f1_unsup > f1_semi:
        print("   âœ… æ— ç›‘ç£æ¨¡å¼è¡¨ç°æ›´å¥½")
        print("   åŸå› åˆ†æï¼š")
        print("      - è®­ç»ƒæ•°æ®ä¸­çš„æ¬ºè¯ˆæ ·æœ¬å¸®åŠ©æ¨¡å‹å­¦ä¹ äº†å¼‚å¸¸æ¨¡å¼")
        print("      - Isolation Forest åœ¨æ— ç›‘ç£åœºæ™¯ä¸‹çš„è®¾è®¡ä½¿å…¶èƒ½è‡ªåŠ¨è¯†åˆ«å¼‚å¸¸")
    else:
        print("   âš–ï¸ ä¸¤ç§æ¨¡å¼è¡¨ç°ç›¸å½“")

    print("\n   ã€ä½•æ—¶ä½¿ç”¨å“ªç§æ¨¡å¼ï¼Ÿã€‘")
    print("   - åŠç›‘ç£æ¨¡å¼ï¼šæœ‰æ ‡ç­¾ã€ç¡®ä¿¡è®­ç»ƒæ•°æ®æ˜¯å¹²å‡€çš„")
    print("   - æ— ç›‘ç£æ¨¡å¼ï¼šæ— æ ‡ç­¾ã€æ•°æ®å¯èƒ½å·²è¢«æ±¡æŸ“ã€ç”Ÿäº§ç¯å¢ƒ")

    # =========================================================================
    # å¯è§†åŒ–å¯¹æ¯”
    # =========================================================================
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    fig.suptitle('Isolation Forest è®­ç»ƒæ¨¡å¼å¯¹æ¯”ï¼šåŠç›‘ç£ vs æ— ç›‘ç£',
                 fontsize=14, fontweight='bold', y=1.02)

    # å›¾1ï¼šæ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    x = np.arange(len(metrics_names))
    width = 0.35

    bars1 = axes[0].bar(x - width/2, semi_values, width, label='åŠç›‘ç£æ¨¡å¼',
                        color='steelblue', alpha=0.8, edgecolor='black')
    bars2 = axes[0].bar(x + width/2, unsup_values, width, label='æ— ç›‘ç£æ¨¡å¼',
                        color='coral', alpha=0.8, edgecolor='black')

    axes[0].set_xlabel('è¯„ä¼°æŒ‡æ ‡', fontsize=11, fontweight='bold')
    axes[0].set_ylabel('åˆ†æ•°', fontsize=11, fontweight='bold')
    axes[0].set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=12, fontweight='bold')
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(metrics_names)
    axes[0].legend(fontsize=10)
    axes[0].set_ylim(0, max(max(semi_values), max(unsup_values)) * 1.2)
    axes[0].grid(True, alpha=0.3, axis='y')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width()/2., height,
                     f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    for bar in bars2:
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width()/2., height,
                     f'{height:.3f}', ha='center', va='bottom', fontsize=9)

    # å›¾2ï¼šPRæ›²çº¿å¯¹æ¯”
    pr_precision_semi, pr_recall_semi, _ = precision_recall_curve(y_test, y_scores_semi)
    pr_precision_unsup, pr_recall_unsup, _ = precision_recall_curve(y_test, y_scores_unsup)

    axes[1].plot(pr_recall_semi, pr_precision_semi, color='steelblue', linewidth=2.5,
                 label=f'åŠç›‘ç£ (AP={pr_auc_semi:.4f})')
    axes[1].plot(pr_recall_unsup, pr_precision_unsup, color='coral', linewidth=2.5,
                 label=f'æ— ç›‘ç£ (AP={pr_auc_unsup:.4f})')

    axes[1].set_xlabel('å¬å›ç‡ (Recall)', fontsize=11, fontweight='bold')
    axes[1].set_ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=11, fontweight='bold')
    axes[1].set_title('Precision-Recall æ›²çº¿å¯¹æ¯”', fontsize=12, fontweight='bold')
    axes[1].legend(loc='best', fontsize=10)
    axes[1].grid(True, alpha=0.3)
    axes[1].set_xlim([0, 1])
    axes[1].set_ylim([0, 1])

    plt.tight_layout()
    plt.savefig(output_dir / '09_training_mode_comparison.png', dpi=150, bbox_inches='tight')
    print(f"\n   âœ… å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: outputs/09_training_mode_comparison.png")
    plt.show()

    return results


# ============================================================================
# ç¬¬9éƒ¨åˆ†ï¼šä¸»å‡½æ•°
# ============================================================================

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æ¬ºè¯ˆæ£€æµ‹æµç¨‹

    ä½¿ç”¨ Kaggle ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†ï¼Œå®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š
        1. åŠ è½½æ•°æ®é›†
        2. æ•°æ®æ¢ç´¢åˆ†æ (EDA)
        3. æ•°æ®é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–ã€åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†ï¼‰
        4. è®­ç»ƒä¸‰ç§å¼‚å¸¸æ£€æµ‹æ¨¡å‹
           - Isolation Forest
           - One-Class SVM
           - Local Outlier Factor (LOF)
        5. æ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”
        6. å¯è§†åŒ–ç»“æœ
        7. ä¿å­˜æ¨¡å‹
        8. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    """
    print("\n" + "=" * 80)
    print("ğŸš€ ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹é¡¹ç›®å¼€å§‹")
    print("   ä½¿ç”¨ Kaggle çœŸå®æ•°æ®é›† (creditcard.csv)")
    print("=" * 80)

    # Step 1: åŠ è½½çœŸå®æ•°æ®é›†
    # æ•°æ®é›†æ¥æº: https://www.kaggle.com/mlg-ulb/creditcardfraud
    df = load_credit_card_data()

    # Step 2: æ•°æ®æ¢ç´¢åˆ†æ
    explore_data(df)

    # Step 3: æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test, X_train_normal, scaler = preprocess_data(df)

    # Step 4: è®­ç»ƒä¸‰ç§å¼‚å¸¸æ£€æµ‹æ¨¡å‹
    # æ ¹æ®çœŸå®æ•°æ®é›†çš„æ¬ºè¯ˆæ¯”ä¾‹è®¾ç½® contamination å‚æ•°
    # å®é™…æ¬ºè¯ˆæ¯”ä¾‹çº¦ä¸º 492/284807 â‰ˆ 0.00173
    fraud_ratio = df['Class'].mean()
    contamination = fraud_ratio
    print(f"\n   ä½¿ç”¨æ¬ºè¯ˆæ¯”ä¾‹ä½œä¸º contamination å‚æ•°: {contamination:.5f}")

    # 4.1 Isolation Forest
    model_if, train_time_if = train_isolation_forest(X_train_normal, contamination)

    # 4.2 One-Class SVM
    model_svm, train_time_svm = train_one_class_svm(X_train_normal, nu=contamination)

    # 4.3 Local Outlier Factor
    model_lof, train_time_lof = train_local_outlier_factor(X_train_normal, contamination)

    # Step 5: è¯„ä¼°æ¨¡å‹
    metrics_if, y_pred_if, y_scores_if = evaluate_model(model_if, "Isolation Forest", X_test, y_test)
    metrics_svm, y_pred_svm, y_scores_svm = evaluate_model(model_svm, "One-Class SVM", X_test, y_test)
    metrics_lof, y_pred_lof, y_scores_lof = evaluate_model(model_lof, "Local Outlier Factor", X_test, y_test)

    all_metrics = [metrics_if, metrics_svm, metrics_lof]

    # Step 6: å¯è§†åŒ–å¯¹æ¯”
    print("\n" + "=" * 80)
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾")
    print("=" * 80)

    plot_confusion_matrices(all_metrics)
    plot_roc_curves(all_metrics)
    plot_pr_curves(all_metrics)
    plot_metrics_comparison(all_metrics)
    visualize_anomalies_tsne(X_test, y_test, y_pred_if, y_pred_svm, y_pred_lof)

    # Step 6.5: è®­ç»ƒæ¨¡å¼å¯¹æ¯”å®éªŒï¼ˆåŠç›‘ç£ vs æ— ç›‘ç£ï¼‰
    # è¿™ä¸ªå®éªŒå¸®åŠ©å­¦ä¹ è€…ç†è§£å¼‚å¸¸æ£€æµ‹çš„ä¸¤ç§è®­ç»ƒç­–ç•¥
    comparison_results = compare_training_modes(
        X_train, X_train_normal, X_test, y_train, y_test, contamination
    )

    # Step 7: ä¿å­˜æ¨¡å‹
    models = {
        'Isolation Forest': model_if,
        'One-Class SVM': model_svm,
        'Local Outlier Factor': model_lof
    }
    save_models_and_metrics(models, all_metrics, scaler)

    # Step 8: ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“Š é¡¹ç›®æ€»ç»“æŠ¥å‘Š")
    print("=" * 80)

    print("\nã€æ¨¡å‹æ€§èƒ½å¯¹æ¯”ã€‘")
    print(f"{'æ¨¡å‹':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'PR-AUC':<12}")
    print("-" * 68)
    for m in all_metrics:
        print(f"{m['model_name']:<20} {m['precision']:<12.4f} {m['recall']:<12.4f} "
              f"{m['f1_score']:<12.4f} {m['pr_auc']:<12.4f}")

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_f1_model = max(all_metrics, key=lambda x: x['f1_score'])
    best_recall_model = max(all_metrics, key=lambda x: x['recall'])
    best_precision_model = max(all_metrics, key=lambda x: x['precision'])

    print("\nã€æœ€ä½³æ¨¡å‹ã€‘")
    print(f"   - æœ€é«˜F1-Score: {best_f1_model['model_name']} ({best_f1_model['f1_score']:.4f})")
    print(f"   - æœ€é«˜Recall: {best_recall_model['model_name']} ({best_recall_model['recall']:.4f})")
    print(f"   - æœ€é«˜Precision: {best_precision_model['model_name']} ({best_precision_model['precision']:.4f})")

    print("\nã€ä¸šåŠ¡å»ºè®®ã€‘")
    print("   1. æ¨¡å‹é€‰æ‹©:")
    print(f"      - å¦‚æœå…³æ³¨å‡å°‘æ¼æŠ¥ï¼ˆæŠ“ä½æ›´å¤šæ¬ºè¯ˆï¼‰:")
    print(f"        æ¨èä½¿ç”¨ {best_recall_model['model_name']}")
    print(f"      - å¦‚æœå…³æ³¨å‡å°‘è¯¯æŠ¥ï¼ˆé¿å…è¯¯ä¼¤æ­£å¸¸ç”¨æˆ·ï¼‰:")
    print(f"        æ¨èä½¿ç”¨ {best_precision_model['model_name']}")
    print(f"      - ç»¼åˆè€ƒè™‘:")
    print(f"        æ¨èä½¿ç”¨ {best_f1_model['model_name']}")

    print("\n   2. é˜ˆå€¼è°ƒæ•´:")
    print("      - å¯ä»¥é€šè¿‡è°ƒæ•´decision_functionçš„é˜ˆå€¼æ¥æƒè¡¡Precisionå’ŒRecall")
    print("      - é™ä½é˜ˆå€¼ï¼šæé«˜Recallï¼Œé™ä½Precisionï¼ˆæŠ“æ›´å¤šæ¬ºè¯ˆï¼Œä½†è¯¯æŠ¥å¢åŠ ï¼‰")
    print("      - æé«˜é˜ˆå€¼ï¼šæé«˜Precisionï¼Œé™ä½Recallï¼ˆå‡å°‘è¯¯æŠ¥ï¼Œä½†æ¼æŠ¥å¢åŠ ï¼‰")

    print("\n   3. å®é™…éƒ¨ç½²:")
    print("      - Isolation Forest: è®­ç»ƒå¿«ï¼Œæ¨ç†å¿«ï¼Œé€‚åˆå¤§è§„æ¨¡å®æ—¶æ£€æµ‹")
    print("      - One-Class SVM: å‡†ç¡®åº¦é«˜ï¼Œä½†è®¡ç®—è¾ƒæ…¢ï¼Œé€‚åˆæ‰¹é‡æ£€æµ‹")
    print("      - LOF: é€‚åˆæ£€æµ‹å±€éƒ¨å¼‚å¸¸ï¼Œä½†ä¸é€‚åˆå¤§è§„æ¨¡æ•°æ®")

    print("\n   4. æ”¹è¿›æ–¹å‘:")
    print("      - ç‰¹å¾å·¥ç¨‹ï¼šä»Amountã€Timeæå–æ›´å¤šç‰¹å¾")
    print("      - é›†æˆæ–¹æ³•ï¼šç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ")
    print("      - åŠç›‘ç£å­¦ä¹ ï¼šåˆ©ç”¨å°‘é‡æ ‡æ³¨çš„æ¬ºè¯ˆæ ·æœ¬")
    print("      - åœ¨çº¿å­¦ä¹ ï¼šéšç€æ–°æ•°æ®ä¸æ–­æ›´æ–°æ¨¡å‹")

    print("\n" + "=" * 80)
    print("âœ… ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹é¡¹ç›®å®Œæˆï¼")
    print("=" * 80)

    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("   - outputs/01_class_distribution.png       # ç±»åˆ«åˆ†å¸ƒ")
    print("   - outputs/02_amount_distribution.png      # é‡‘é¢åˆ†å¸ƒ")
    print("   - outputs/03_time_distribution.png        # æ—¶é—´åˆ†å¸ƒ")
    print("   - outputs/04_confusion_matrices.png       # æ··æ·†çŸ©é˜µ")
    print("   - outputs/05_roc_curves.png              # ROCæ›²çº¿")
    print("   - outputs/06_pr_curves.png               # PRæ›²çº¿")
    print("   - outputs/07_metrics_comparison.png       # æŒ‡æ ‡å¯¹æ¯”")
    print("   - outputs/08_tsne_visualization.png       # t-SNEå¯è§†åŒ–")
    print("   - models/isolation_forest_model.pkl       # IFæ¨¡å‹")
    print("   - models/one-class_svm_model.pkl          # SVMæ¨¡å‹")
    print("   - models/local_outlier_factor_model.pkl   # LOFæ¨¡å‹")
    print("   - models/scaler.pkl                       # æ ‡å‡†åŒ–å™¨")
    print("   - models/evaluation_metrics.json          # è¯„ä¼°æŒ‡æ ‡")
    print("\n" + "=" * 80)


# ============================================================================
# ç¨‹åºå…¥å£
# ============================================================================

if __name__ == "__main__":
    main()
