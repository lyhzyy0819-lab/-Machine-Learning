"""
ğŸ¯ ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹é¡¹ç›® (Credit Card Fraud Detection)
===================================================

é¡¹ç›®ç›®æ ‡ï¼š
    ä½¿ç”¨å¼‚å¸¸æ£€æµ‹ç®—æ³•è¯†åˆ«ä¿¡ç”¨å¡æ¬ºè¯ˆäº¤æ˜“ï¼Œå¤„ç†é«˜åº¦ä¸å¹³è¡¡çš„æ•°æ®ï¼Œ
    å¯¹æ¯”Isolation Forestã€One-Class SVMã€LOFä¸‰ç§ç®—æ³•çš„æ€§èƒ½ã€‚

ä¸šåŠ¡åœºæ™¯ï¼š
    ä¿¡ç”¨å¡æ¬ºè¯ˆæ¯å¹´é€ æˆæ•°åäº¿ç¾å…ƒæŸå¤±ã€‚é€šè¿‡æœºå™¨å­¦ä¹ å®æ—¶æ£€æµ‹å¼‚å¸¸äº¤æ˜“ï¼Œ
    å¯ä»¥å¸®åŠ©é“¶è¡ŒåŠæ—¶å‘ç°æ¬ºè¯ˆè¡Œä¸ºï¼Œä¿æŠ¤å®¢æˆ·èµ„é‡‘å®‰å…¨ã€‚

æ•°æ®ç‰¹ç‚¹ï¼š
    - é«˜åº¦ä¸å¹³è¡¡ï¼šæ¬ºè¯ˆäº¤æ˜“ä»…å  ~0.2%
    - ç‰¹å¾å·²è„±æ•ï¼šä½¿ç”¨PCAé™ç»´ä¿æŠ¤éšç§
    - çœŸå®åœºæ™¯ï¼šéœ€è¦æƒè¡¡è¯¯æŠ¥å’Œæ¼æŠ¥

æ ¸å¿ƒç®—æ³•ï¼š
    - Isolation Forest (éš”ç¦»æ£®æ—)ï¼šåŸºäºå†³ç­–æ ‘çš„å¿«é€Ÿå¼‚å¸¸æ£€æµ‹
    - One-Class SVM (å•ç±»SVM)ï¼šå­¦ä¹ æ­£å¸¸æ ·æœ¬çš„è¾¹ç•Œ
    - LOF (å±€éƒ¨ç¦»ç¾¤å› å­)ï¼šåŸºäºå¯†åº¦çš„å¼‚å¸¸æ£€æµ‹

ä½œè€…: Machine Learning å­¦ä¹ é¡¹ç›®
æ—¥æœŸ: 2024å¹´11æœˆ
"""

# ============================================================================
# ç¬¬1éƒ¨åˆ†ï¼šå¯¼å…¥å¿…è¦çš„åº“
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time

# æ•°æ®ç”Ÿæˆå’Œé¢„å¤„ç†
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# å¼‚å¸¸æ£€æµ‹ç®—æ³•
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor

# é™ç»´å¯è§†åŒ–
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    precision_recall_curve,
    average_precision_score,
    precision_score,
    recall_score,
    f1_score
)

# æ¨¡å‹ä¿å­˜
import joblib
import json

# å¿½ç•¥è­¦å‘Š
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['font.sans-serif'] = [
    'Arial Unicode MS',  # macOSé€šç”¨
    'PingFang SC',       # macOSç³»ç»Ÿå­—ä½“
    'STHeiti',           # åæ–‡é»‘ä½“
    'SimHei',            # Windowsé»‘ä½“
]
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 11

# è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
np.random.seed(42)

print("âœ… åº“å¯¼å…¥å®Œæˆï¼")
print("=" * 80)


# ============================================================================
# ç¬¬2éƒ¨åˆ†ï¼šæ•°æ®ç”Ÿæˆï¼ˆæ¨¡æ‹ŸçœŸå®æ¬ºè¯ˆæ£€æµ‹åœºæ™¯ï¼‰
# ============================================================================

def generate_fraud_data(n_samples=50000, fraud_ratio=0.002, n_features=30):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„ä¿¡ç”¨å¡äº¤æ˜“æ•°æ®

    è¯´æ˜ï¼š
        ç”±äºKaggleæ•°æ®é›†éœ€è¦æ‰‹åŠ¨ä¸‹è½½ï¼Œè¿™é‡Œä½¿ç”¨sklearnç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ã€‚
        æ¨¡æ‹ŸçœŸå®åœºæ™¯ç‰¹ç‚¹ï¼š
        - é«˜åº¦ä¸å¹³è¡¡ï¼ˆæ¬ºè¯ˆç‡ ~0.2%ï¼‰
        - ç‰¹å¾å·²PCAé™ç»´ï¼ˆæ¨¡æ‹Ÿéšç§ä¿æŠ¤ï¼‰
        - åŒ…å«äº¤æ˜“é‡‘é¢ç‰¹å¾

    Parameters:
    -----------
    n_samples : int
        æ€»æ ·æœ¬æ•°é‡
    fraud_ratio : float
        æ¬ºè¯ˆäº¤æ˜“æ¯”ä¾‹ï¼ˆé»˜è®¤0.2%ï¼‰
    n_features : int
        ç‰¹å¾æ•°é‡ï¼ˆæ¨¡æ‹ŸV1-V28 + Time + Amountï¼‰

    Returns:
    --------
    df : DataFrame
        åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„æ•°æ®æ¡†
    """
    print("\n" + "=" * 80)
    print("ğŸ”§ ç”Ÿæˆæ¨¡æ‹Ÿä¿¡ç”¨å¡äº¤æ˜“æ•°æ®")
    print("=" * 80)

    # è®¡ç®—æ¬ºè¯ˆæ ·æœ¬æ•°é‡
    n_fraud = int(n_samples * fraud_ratio)
    n_normal = n_samples - n_fraud

    print(f"   - æ€»äº¤æ˜“æ•°: {n_samples:,}")
    print(f"   - æ­£å¸¸äº¤æ˜“: {n_normal:,} ({(1-fraud_ratio)*100:.2f}%)")
    print(f"   - æ¬ºè¯ˆäº¤æ˜“: {n_fraud:,} ({fraud_ratio*100:.3f}%)")

    # ä½¿ç”¨make_classificationç”Ÿæˆä¸å¹³è¡¡æ•°æ®
    # weights: æ§åˆ¶ç±»åˆ«æ¯”ä¾‹
    # n_informative: æœ‰ä¿¡æ¯çš„ç‰¹å¾æ•°é‡
    # n_redundant: å†—ä½™ç‰¹å¾æ•°é‡
    # n_clusters_per_class: æ¯ä¸ªç±»çš„ç°‡æ•°é‡
    # class_sep: ç±»åˆ«åˆ†ç¦»åº¦ï¼ˆè¾ƒå¤§è¡¨ç¤ºæ›´å®¹æ˜“åŒºåˆ†ï¼‰
    X, y = make_classification(
        n_samples=n_samples,
        n_features=n_features - 2,  # é¢„ç•™Timeå’ŒAmountç‰¹å¾
        n_informative=20,
        n_redundant=5,
        n_clusters_per_class=2,
        weights=[1 - fraud_ratio, fraud_ratio],
        flip_y=0.01,  # æ·»åŠ å°‘é‡å™ªå£°
        random_state=42,
        class_sep=0.8  # ç±»åˆ«åˆ†ç¦»åº¦ï¼ˆè¾ƒéš¾åŒºåˆ†ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰
    )

    # åˆ›å»ºDataFrame
    # ç‰¹å¾V1-V28æ¨¡æ‹ŸPCAé™ç»´åçš„ç‰¹å¾ï¼ˆéšç§ä¿æŠ¤ï¼‰
    feature_names = [f'V{i}' for i in range(1, n_features - 1)]
    df = pd.DataFrame(X, columns=feature_names)

    # æ·»åŠ Timeç‰¹å¾ï¼ˆè·ç¦»ç¬¬ä¸€ç¬”äº¤æ˜“çš„ç§’æ•°ï¼‰
    # æ¨¡æ‹Ÿ2å¤©çš„äº¤æ˜“æ•°æ®
    df['Time'] = np.random.randint(0, 172800, size=n_samples)  # 172800 = 48å°æ—¶

    # æ·»åŠ Amountç‰¹å¾ï¼ˆäº¤æ˜“é‡‘é¢ï¼‰
    # æ­£å¸¸äº¤æ˜“ï¼šå¹³å‡88ç¾å…ƒï¼Œæ ‡å‡†å·®250
    # æ¬ºè¯ˆäº¤æ˜“ï¼šé‡‘é¢åˆ†å¸ƒç•¥æœ‰ä¸åŒï¼ˆé€šå¸¸è¾ƒå°æˆ–è¾ƒå¤§ï¼‰
    normal_amounts = np.random.gamma(shape=2, scale=44, size=n_normal)
    fraud_amounts = np.concatenate([
        np.random.gamma(shape=1, scale=30, size=n_fraud // 2),  # å°é¢æ¬ºè¯ˆ
        np.random.gamma(shape=3, scale=100, size=n_fraud - n_fraud // 2)  # å¤§é¢æ¬ºè¯ˆ
    ])

    amounts = np.zeros(n_samples)
    amounts[y == 0] = normal_amounts
    amounts[y == 1] = np.random.permutation(fraud_amounts)
    df['Amount'] = amounts

    # æ·»åŠ æ ‡ç­¾
    df['Class'] = y

    print(f"\n   âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print(f"      - ç‰¹å¾æ•°é‡: {n_features}")
    print(f"      - æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"      - æ¬ºè¯ˆæ¯”ä¾‹: {y.mean():.4f}")

    return df


# ============================================================================
# ç¬¬3éƒ¨åˆ†ï¼šæ•°æ®æ¢ç´¢åˆ†æ (EDA)
# ============================================================================

def explore_data(df):
    """
    æ¢ç´¢æ€§æ•°æ®åˆ†æ

    ç›®çš„ï¼š
        äº†è§£æ•°æ®çš„åŸºæœ¬æƒ…å†µã€ç±»åˆ«åˆ†å¸ƒã€ç‰¹å¾å·®å¼‚ç­‰

    Parameters:
    -----------
    df : DataFrame
        äº¤æ˜“æ•°æ®
    """
    print("\n" + "=" * 80)
    print("ğŸ” æ•°æ®æ¢ç´¢åˆ†æ (EDA)")
    print("=" * 80)

    # 1. åŸºæœ¬ä¿¡æ¯
    print("\nã€æ•°æ®åŸºæœ¬ä¿¡æ¯ã€‘")
    print(f"   - æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"   - ç‰¹å¾æ•°é‡: {df.shape[1] - 1}")
    print(f"   - æ ·æœ¬æ•°é‡: {df.shape[0]:,}")
    print(f"   - ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
    print(f"\n   å‰5è¡Œæ•°æ®:")
    print(df.head())

    # 2. ç±»åˆ«åˆ†å¸ƒ
    print("\nã€ç±»åˆ«åˆ†å¸ƒã€‘")
    class_counts = df['Class'].value_counts()
    print(f"   - æ­£å¸¸äº¤æ˜“ (Class=0): {class_counts[0]:,} ({class_counts[0]/len(df)*100:.3f}%)")
    print(f"   - æ¬ºè¯ˆäº¤æ˜“ (Class=1): {class_counts[1]:,} ({class_counts[1]/len(df)*100:.3f}%)")
    print(f"   - ä¸å¹³è¡¡æ¯”ä¾‹: 1:{class_counts[0]//class_counts[1]}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path('outputs')
    output_dir.mkdir(exist_ok=True)

    # 3. å¯è§†åŒ–ç±»åˆ«åˆ†å¸ƒ
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # 3.1 æŸ±çŠ¶å›¾
    class_counts.plot(kind='bar', ax=axes[0], color=['steelblue', 'coral'], alpha=0.8, edgecolor='black')
    axes[0].set_xlabel('äº¤æ˜“ç±»åˆ«', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('äº¤æ˜“æ•°é‡', fontsize=12, fontweight='bold')
    axes[0].set_title('äº¤æ˜“ç±»åˆ«åˆ†å¸ƒï¼ˆæŸ±çŠ¶å›¾ï¼‰', fontsize=13, fontweight='bold')
    axes[0].set_xticklabels(['æ­£å¸¸ (0)', 'æ¬ºè¯ˆ (1)'], rotation=0)
    axes[0].grid(True, alpha=0.3, axis='y')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(class_counts):
        axes[0].text(i, v + max(class_counts)*0.02, f'{v:,}\n({v/len(df)*100:.3f}%)',
                    ha='center', fontsize=10, fontweight='bold')

    # 3.2 é¥¼å›¾
    colors = ['steelblue', 'coral']
    explode = (0, 0.1)  # çªå‡ºæ˜¾ç¤ºæ¬ºè¯ˆéƒ¨åˆ†
    axes[1].pie(class_counts, labels=['æ­£å¸¸äº¤æ˜“', 'æ¬ºè¯ˆäº¤æ˜“'], autopct='%1.3f%%',
               colors=colors, explode=explode, startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})
    axes[1].set_title('äº¤æ˜“ç±»åˆ«åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰', fontsize=13, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_dir / '01_class_distribution.png', dpi=150, bbox_inches='tight')
    print("\n   âœ… ç±»åˆ«åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: outputs/01_class_distribution.png")
    plt.show()

    # 4. äº¤æ˜“é‡‘é¢åˆ†æ
    print("\nã€äº¤æ˜“é‡‘é¢åˆ†æã€‘")
    normal_amounts = df[df['Class'] == 0]['Amount']
    fraud_amounts = df[df['Class'] == 1]['Amount']

    print(f"   æ­£å¸¸äº¤æ˜“é‡‘é¢:")
    print(f"      - å¹³å‡å€¼: ${normal_amounts.mean():.2f}")
    print(f"      - ä¸­ä½æ•°: ${normal_amounts.median():.2f}")
    print(f"      - æ ‡å‡†å·®: ${normal_amounts.std():.2f}")

    print(f"\n   æ¬ºè¯ˆäº¤æ˜“é‡‘é¢:")
    print(f"      - å¹³å‡å€¼: ${fraud_amounts.mean():.2f}")
    print(f"      - ä¸­ä½æ•°: ${fraud_amounts.median():.2f}")
    print(f"      - æ ‡å‡†å·®: ${fraud_amounts.std():.2f}")

    # å¯è§†åŒ–é‡‘é¢åˆ†å¸ƒ
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('äº¤æ˜“é‡‘é¢åˆ†å¸ƒåˆ†æ', fontsize=16, fontweight='bold', y=0.995)

    # 4.1 æ­£å¸¸äº¤æ˜“é‡‘é¢ç›´æ–¹å›¾
    axes[0, 0].hist(normal_amounts, bins=50, color='steelblue', alpha=0.7, edgecolor='black')
    axes[0, 0].set_xlabel('äº¤æ˜“é‡‘é¢ ($)', fontsize=11, fontweight='bold')
    axes[0, 0].set_ylabel('é¢‘æ•°', fontsize=11, fontweight='bold')
    axes[0, 0].set_title('æ­£å¸¸äº¤æ˜“é‡‘é¢åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[0, 0].axvline(normal_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'å‡å€¼: ${normal_amounts.mean():.2f}')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # 4.2 æ¬ºè¯ˆäº¤æ˜“é‡‘é¢ç›´æ–¹å›¾
    axes[0, 1].hist(fraud_amounts, bins=30, color='coral', alpha=0.7, edgecolor='black')
    axes[0, 1].set_xlabel('äº¤æ˜“é‡‘é¢ ($)', fontsize=11, fontweight='bold')
    axes[0, 1].set_ylabel('é¢‘æ•°', fontsize=11, fontweight='bold')
    axes[0, 1].set_title('æ¬ºè¯ˆäº¤æ˜“é‡‘é¢åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[0, 1].axvline(fraud_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'å‡å€¼: ${fraud_amounts.mean():.2f}')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # 4.3 ç®±çº¿å›¾å¯¹æ¯”
    data_to_plot = [normal_amounts, fraud_amounts]
    bp = axes[1, 0].boxplot(data_to_plot, labels=['æ­£å¸¸äº¤æ˜“', 'æ¬ºè¯ˆäº¤æ˜“'],
                           patch_artist=True, showmeans=True)
    for patch, color in zip(bp['boxes'], ['steelblue', 'coral']):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    axes[1, 0].set_ylabel('äº¤æ˜“é‡‘é¢ ($)', fontsize=11, fontweight='bold')
    axes[1, 0].set_title('äº¤æ˜“é‡‘é¢ç®±çº¿å›¾å¯¹æ¯”', fontsize=12, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3, axis='y')

    # 4.4 å¯¹æ•°å°ºåº¦å¯¹æ¯”
    axes[1, 1].hist([normal_amounts, fraud_amounts], bins=50, label=['æ­£å¸¸äº¤æ˜“', 'æ¬ºè¯ˆäº¤æ˜“'],
                   color=['steelblue', 'coral'], alpha=0.6, edgecolor='black')
    axes[1, 1].set_xlabel('äº¤æ˜“é‡‘é¢ ($)', fontsize=11, fontweight='bold')
    axes[1, 1].set_ylabel('é¢‘æ•° (å¯¹æ•°å°ºåº¦)', fontsize=11, fontweight='bold')
    axes[1, 1].set_title('äº¤æ˜“é‡‘é¢åˆ†å¸ƒå¯¹æ¯”ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰', fontsize=12, fontweight='bold')
    axes[1, 1].set_yscale('log')
    axes[1, 1].legend(fontsize=10)
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / '02_amount_distribution.png', dpi=150, bbox_inches='tight')
    print("   âœ… é‡‘é¢åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: outputs/02_amount_distribution.png")
    plt.show()

    # 5. æ—¶é—´åˆ†å¸ƒåˆ†æ
    print("\nã€æ—¶é—´åˆ†å¸ƒåˆ†æã€‘")
    normal_times = df[df['Class'] == 0]['Time']
    fraud_times = df[df['Class'] == 1]['Time']

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # 5.1 æ­£å¸¸äº¤æ˜“æ—¶é—´åˆ†å¸ƒ
    axes[0].hist(normal_times / 3600, bins=48, color='steelblue', alpha=0.7, edgecolor='black')
    axes[0].set_xlabel('æ—¶é—´ï¼ˆå°æ—¶ï¼‰', fontsize=11, fontweight='bold')
    axes[0].set_ylabel('äº¤æ˜“æ•°é‡', fontsize=11, fontweight='bold')
    axes[0].set_title('æ­£å¸¸äº¤æ˜“æ—¶é—´åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[0].grid(True, alpha=0.3)

    # 5.2 æ¬ºè¯ˆäº¤æ˜“æ—¶é—´åˆ†å¸ƒ
    axes[1].hist(fraud_times / 3600, bins=48, color='coral', alpha=0.7, edgecolor='black')
    axes[1].set_xlabel('æ—¶é—´ï¼ˆå°æ—¶ï¼‰', fontsize=11, fontweight='bold')
    axes[1].set_ylabel('äº¤æ˜“æ•°é‡', fontsize=11, fontweight='bold')
    axes[1].set_title('æ¬ºè¯ˆäº¤æ˜“æ—¶é—´åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / '03_time_distribution.png', dpi=150, bbox_inches='tight')
    print("   âœ… æ—¶é—´åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: outputs/03_time_distribution.png")
    plt.show()


# ============================================================================
# ç¬¬4éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†
# ============================================================================

def preprocess_data(df):
    """
    æ•°æ®é¢„å¤„ç†

    æ­¥éª¤ï¼š
        1. åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        2. æ ‡å‡†åŒ–å¤„ç†ï¼ˆAmountç‰¹å¾é€šå¸¸éœ€è¦æ ‡å‡†åŒ–ï¼‰
        3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        4. æå–æ­£å¸¸äº¤æ˜“ï¼ˆç”¨äºOne-Classè®­ç»ƒï¼‰

    Parameters:
    -----------
    df : DataFrame
        åŸå§‹æ•°æ®

    Returns:
    --------
    X_train : ndarray
        è®­ç»ƒé›†ç‰¹å¾
    X_test : ndarray
        æµ‹è¯•é›†ç‰¹å¾
    y_train : ndarray
        è®­ç»ƒé›†æ ‡ç­¾
    y_test : ndarray
        æµ‹è¯•é›†æ ‡ç­¾
    X_train_normal : ndarray
        è®­ç»ƒé›†ä¸­çš„æ­£å¸¸äº¤æ˜“ï¼ˆç”¨äºOne-Classæ–¹æ³•ï¼‰
    scaler : StandardScaler
        æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºåç»­æ–°æ•°æ®é¢„å¤„ç†ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†")
    print("=" * 80)

    # 1. åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop('Class', axis=1).values
    y = df['Class'].values

    print(f"\n   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    print(f"   æ ‡ç­¾å‘é‡å½¢çŠ¶: {y.shape}")

    # 2. æ ‡å‡†åŒ–å¤„ç†
    # ä¸ºä»€ä¹ˆéœ€è¦æ ‡å‡†åŒ–ï¼Ÿ
    # - Amountç‰¹å¾çš„æ•°å€¼èŒƒå›´ä¸V1-V28ä¸åŒ
    # - SVMç­‰ç®—æ³•å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿ
    # - æ ‡å‡†åŒ–å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½
    print("\n   æ­£åœ¨è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    print(f"   âœ… æ ‡å‡†åŒ–å®Œæˆï¼")
    print(f"      - ç‰¹å¾å‡å€¼: {X_scaled.mean(axis=0)[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
    print(f"      - ç‰¹å¾æ ‡å‡†å·®: {X_scaled.std(axis=0)[:5]}...")

    # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # stratify=y: ä¿æŒè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç±»åˆ«æ¯”ä¾‹ä¸€è‡´
    # test_size=0.3: 30%ä½œä¸ºæµ‹è¯•é›†
    print("\n   æ­£åœ¨åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42, stratify=y
    )

    print(f"   âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼")
    print(f"      - è®­ç»ƒé›†å¤§å°: {X_train.shape[0]:,}")
    print(f"      - æµ‹è¯•é›†å¤§å°: {X_test.shape[0]:,}")
    print(f"      - è®­ç»ƒé›†æ¬ºè¯ˆæ¯”ä¾‹: {y_train.mean():.4f}")
    print(f"      - æµ‹è¯•é›†æ¬ºè¯ˆæ¯”ä¾‹: {y_test.mean():.4f}")

    # 4. æå–æ­£å¸¸äº¤æ˜“ï¼ˆç”¨äºOne-Classæ–¹æ³•ï¼‰
    # One-Classæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼š
    # - åªä½¿ç”¨æ­£å¸¸æ ·æœ¬å­¦ä¹ "æ­£å¸¸"çš„æ¨¡å¼
    # - æµ‹è¯•æ—¶ï¼Œä¸ç¬¦åˆæ­£å¸¸æ¨¡å¼çš„è§†ä¸ºå¼‚å¸¸
    # - é€‚åˆæåº¦ä¸å¹³è¡¡çš„æ•°æ®
    X_train_normal = X_train[y_train == 0]

    print(f"\n   æå–æ­£å¸¸äº¤æ˜“ç”¨äºOne-Classè®­ç»ƒ:")
    print(f"      - æ­£å¸¸äº¤æ˜“æ•°é‡: {X_train_normal.shape[0]:,}")
    print(f"      - å è®­ç»ƒé›†æ¯”ä¾‹: {X_train_normal.shape[0]/X_train.shape[0]*100:.2f}%")

    return X_train, X_test, y_train, y_test, X_train_normal, scaler


# ============================================================================
# ç¬¬5éƒ¨åˆ†ï¼šå¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ
# ============================================================================

def train_isolation_forest(X_train_normal, contamination=0.002):
    """
    è®­ç»ƒIsolation Forestæ¨¡å‹

    ç®—æ³•åŸç†ï¼š
        Isolation Forestï¼ˆéš”ç¦»æ£®æ—ï¼‰åŸºäºä»¥ä¸‹ç›´è§‰ï¼š
        - å¼‚å¸¸ç‚¹æ›´å®¹æ˜“è¢«"éš”ç¦»"ï¼ˆä¸å…¶ä»–ç‚¹åˆ†ç¦»ï¼‰
        - ä½¿ç”¨éšæœºå†³ç­–æ ‘ï¼Œå¼‚å¸¸ç‚¹é€šå¸¸åœ¨æ ‘çš„è¾ƒæµ…å±‚å°±è¢«éš”ç¦»
        - æ­£å¸¸ç‚¹éœ€è¦æ›´æ·±çš„è·¯å¾„æ‰èƒ½è¢«éš”ç¦»

        ç®—æ³•æ­¥éª¤ï¼š
        1. éšæœºé€‰æ‹©ç‰¹å¾å’Œåˆ†å‰²ç‚¹æ„å»ºå†³ç­–æ ‘
        2. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡è·¯å¾„é•¿åº¦ï¼ˆä»æ ¹åˆ°å¶ï¼‰
        3. è·¯å¾„é•¿åº¦çŸ­çš„æ ·æœ¬ä¸ºå¼‚å¸¸ï¼ˆå®¹æ˜“è¢«éš”ç¦»ï¼‰

    å‚æ•°è¯´æ˜ï¼š
        - contamination: è®­ç»ƒæ•°æ®ä¸­å¼‚å¸¸çš„æ¯”ä¾‹ï¼ˆé¢„æœŸï¼‰
          è®¾ç½®ä¸º0.002è¡¨ç¤ºé¢„æœŸ0.2%çš„æ•°æ®ä¸ºå¼‚å¸¸
        - n_estimators: å†³ç­–æ ‘æ•°é‡ï¼Œé»˜è®¤100
        - max_samples: æ„å»ºæ¯æ£µæ ‘ä½¿ç”¨çš„æ ·æœ¬æ•°ï¼Œ'auto'è¡¨ç¤ºmin(256, n_samples)
        - random_state: éšæœºç§å­

    Parameters:
    -----------
    X_train_normal : ndarray
        æ­£å¸¸äº¤æ˜“æ•°æ®
    contamination : float
        é¢„æœŸçš„å¼‚å¸¸æ¯”ä¾‹

    Returns:
    --------
    model : IsolationForest
        è®­ç»ƒå¥½çš„æ¨¡å‹
    train_time : float
        è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸŒ² è®­ç»ƒ Isolation Forest æ¨¡å‹")
    print("=" * 80)

    print(f"\n   æ¨¡å‹å‚æ•°:")
    print(f"      - contamination: {contamination} (é¢„æœŸå¼‚å¸¸æ¯”ä¾‹)")
    print(f"      - n_estimators: 100 (å†³ç­–æ ‘æ•°é‡)")
    print(f"      - max_samples: 'auto'")

    # åˆ›å»ºæ¨¡å‹
    model = IsolationForest(
        contamination=contamination,
        n_estimators=100,
        max_samples='auto',
        random_state=42,
        n_jobs=-1  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    )

    # è®­ç»ƒæ¨¡å‹
    print("\n   æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
    start_time = time.time()
    model.fit(X_train_normal)
    train_time = time.time() - start_time

    print(f"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"      - è®­ç»ƒæ ·æœ¬æ•°: {X_train_normal.shape[0]:,}")
    print(f"      - è®­ç»ƒæ—¶é—´: {train_time:.3f} ç§’")

    return model, train_time


def train_one_class_svm(X_train_normal, nu=0.002):
    """
    è®­ç»ƒOne-Class SVMæ¨¡å‹

    ç®—æ³•åŸç†ï¼š
        One-Class SVMï¼ˆå•ç±»æ”¯æŒå‘é‡æœºï¼‰é€šè¿‡ä»¥ä¸‹æ–¹å¼æ£€æµ‹å¼‚å¸¸ï¼š
        - åœ¨ç‰¹å¾ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªæœ€å°çš„è¶…çƒé¢ï¼ˆæˆ–è¶…å¹³é¢ï¼‰åŒ…å›´æ­£å¸¸æ•°æ®
        - ä½¿ç”¨æ ¸æŠ€å·§å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´
        - åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ­£å¸¸æ•°æ®æ›´å®¹æ˜“è¢«åˆ†ç¦»

        å…³é”®æ¦‚å¿µï¼š
        - nuå‚æ•°ï¼šæ§åˆ¶å¼‚å¸¸çš„ä¸Šç•Œå’Œæ”¯æŒå‘é‡çš„ä¸‹ç•Œ
          è®¾ç½®ä¸º0.002è¡¨ç¤ºå…è®¸æœ€å¤š0.2%çš„è®­ç»ƒæ•°æ®ä¸ºå¼‚å¸¸
        - RBFæ ¸ï¼šå¾„å‘åŸºå‡½æ•°æ ¸ï¼Œé€‚åˆéçº¿æ€§æ•°æ®

    å‚æ•°è¯´æ˜ï¼š
        - nu: å¼‚å¸¸å€¼çš„ä¸Šç•Œï¼Œå–å€¼èŒƒå›´(0, 1]
        - kernel: æ ¸å‡½æ•°ç±»å‹ï¼Œ'rbf'æ˜¯å¸¸ç”¨é€‰æ‹©
        - gamma: RBFæ ¸çš„å‚æ•°ï¼Œ'scale'è¡¨ç¤º1/(n_features * X.var())

    æ³¨æ„ï¼š
        One-Class SVMè®­ç»ƒè¾ƒæ…¢ï¼Œç‰¹åˆ«æ˜¯å¤§æ•°æ®é›†
        å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´

    Parameters:
    -----------
    X_train_normal : ndarray
        æ­£å¸¸äº¤æ˜“æ•°æ®
    nu : float
        å¼‚å¸¸å€¼ä¸Šç•Œ

    Returns:
    --------
    model : OneClassSVM
        è®­ç»ƒå¥½çš„æ¨¡å‹
    train_time : float
        è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸ”µ è®­ç»ƒ One-Class SVM æ¨¡å‹")
    print("=" * 80)

    print(f"\n   æ¨¡å‹å‚æ•°:")
    print(f"      - nu: {nu} (å¼‚å¸¸å€¼ä¸Šç•Œ)")
    print(f"      - kernel: 'rbf' (å¾„å‘åŸºå‡½æ•°æ ¸)")
    print(f"      - gamma: 'scale'")

    # ç”±äºOne-Class SVMè®­ç»ƒè¾ƒæ…¢ï¼Œå¯¹å¤§æ•°æ®é›†è¿›è¡Œé‡‡æ ·
    max_samples = 10000
    if X_train_normal.shape[0] > max_samples:
        print(f"\n   âš ï¸  æ•°æ®é‡è¾ƒå¤§ï¼Œé‡‡æ · {max_samples:,} ä¸ªæ ·æœ¬ä»¥åŠ é€Ÿè®­ç»ƒ")
        indices = np.random.choice(X_train_normal.shape[0], max_samples, replace=False)
        X_train_sample = X_train_normal[indices]
    else:
        X_train_sample = X_train_normal

    # åˆ›å»ºæ¨¡å‹
    model = OneClassSVM(
        nu=nu,
        kernel='rbf',
        gamma='scale'
    )

    # è®­ç»ƒæ¨¡å‹
    print("\n   æ­£åœ¨è®­ç»ƒæ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
    start_time = time.time()
    model.fit(X_train_sample)
    train_time = time.time() - start_time

    print(f"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"      - è®­ç»ƒæ ·æœ¬æ•°: {X_train_sample.shape[0]:,}")
    print(f"      - è®­ç»ƒæ—¶é—´: {train_time:.3f} ç§’")

    return model, train_time


def train_local_outlier_factor(X_train_normal, contamination=0.002):
    """
    è®­ç»ƒLocal Outlier Factoræ¨¡å‹

    ç®—æ³•åŸç†ï¼š
        LOFï¼ˆå±€éƒ¨ç¦»ç¾¤å› å­ï¼‰é€šè¿‡æ¯”è¾ƒæ ·æœ¬çš„å±€éƒ¨å¯†åº¦æ¥æ£€æµ‹å¼‚å¸¸ï¼š
        - è®¡ç®—æ¯ä¸ªç‚¹ä¸å…¶kè¿‘é‚»çš„å¯†åº¦
        - æ¯”è¾ƒè¯¥ç‚¹çš„å¯†åº¦ä¸å…¶é‚»å±…çš„å¯†åº¦
        - å¯†åº¦æ˜æ˜¾ä½äºé‚»å±…çš„ç‚¹è¢«è®¤ä¸ºæ˜¯å¼‚å¸¸

        æ ¸å¿ƒæ¦‚å¿µï¼š
        - å±€éƒ¨å¯†åº¦ï¼šç‚¹åˆ°å…¶kè¿‘é‚»çš„å¹³å‡è·ç¦»çš„å€’æ•°
        - LOFå€¼ï¼šç‚¹çš„å±€éƒ¨å¯†åº¦ä¸å…¶é‚»å±…å±€éƒ¨å¯†åº¦çš„æ¯”å€¼
        - LOF >> 1: å¼‚å¸¸ç‚¹ï¼ˆå¯†åº¦è¿œä½äºé‚»å±…ï¼‰
        - LOF â‰ˆ 1: æ­£å¸¸ç‚¹ï¼ˆå¯†åº¦ä¸é‚»å±…ç›¸ä¼¼ï¼‰

    å‚æ•°è¯´æ˜ï¼š
        - n_neighbors: è€ƒè™‘çš„é‚»å±…æ•°é‡ï¼Œé»˜è®¤20
        - contamination: æ•°æ®é›†ä¸­å¼‚å¸¸çš„æ¯”ä¾‹
        - novelty: Trueè¡¨ç¤ºç”¨äºæ–°æ•°æ®æ£€æµ‹ï¼ŒFalseè¡¨ç¤ºç”¨äºå·²æœ‰æ•°æ®

    æ³¨æ„ï¼š
        LOFæœ‰ä¸¤ç§æ¨¡å¼ï¼š
        - novelty=False: åªèƒ½å¯¹è®­ç»ƒæ•°æ®æ‰“åˆ†ï¼Œä¸èƒ½é¢„æµ‹æ–°æ•°æ®
        - novelty=True: å¯ä»¥é¢„æµ‹æ–°æ•°æ®ï¼ˆæˆ‘ä»¬ä½¿ç”¨è¿™ç§æ¨¡å¼ï¼‰

    Parameters:
    -----------
    X_train_normal : ndarray
        æ­£å¸¸äº¤æ˜“æ•°æ®
    contamination : float
        é¢„æœŸçš„å¼‚å¸¸æ¯”ä¾‹

    Returns:
    --------
    model : LocalOutlierFactor
        è®­ç»ƒå¥½çš„æ¨¡å‹
    train_time : float
        è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸ“ è®­ç»ƒ Local Outlier Factor (LOF) æ¨¡å‹")
    print("=" * 80)

    print(f"\n   æ¨¡å‹å‚æ•°:")
    print(f"      - n_neighbors: 20 (é‚»å±…æ•°é‡)")
    print(f"      - contamination: {contamination} (é¢„æœŸå¼‚å¸¸æ¯”ä¾‹)")
    print(f"      - novelty: True (ç”¨äºæ–°æ•°æ®æ£€æµ‹)")

    # åˆ›å»ºæ¨¡å‹
    # novelty=True å…è®¸æ¨¡å‹é¢„æµ‹æ–°æ•°æ®
    model = LocalOutlierFactor(
        n_neighbors=20,
        contamination=contamination,
        novelty=True,  # é‡è¦ï¼šå…è®¸é¢„æµ‹æ–°æ•°æ®
        n_jobs=-1
    )

    # è®­ç»ƒæ¨¡å‹
    print("\n   æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
    start_time = time.time()
    model.fit(X_train_normal)
    train_time = time.time() - start_time

    print(f"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"      - è®­ç»ƒæ ·æœ¬æ•°: {X_train_normal.shape[0]:,}")
    print(f"      - è®­ç»ƒæ—¶é—´: {train_time:.3f} ç§’")

    return model, train_time


# ============================================================================
# ç¬¬6éƒ¨åˆ†ï¼šæ¨¡å‹è¯„ä¼°
# ============================================================================

def evaluate_model(model, model_name, X_test, y_test):
    """
    è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ¨¡å‹

    è¯„ä¼°æŒ‡æ ‡è¯´æ˜ï¼š
        - Precision (ç²¾ç¡®ç‡): é¢„æµ‹ä¸ºæ¬ºè¯ˆçš„äº¤æ˜“ä¸­ï¼ŒçœŸæ­£æ¬ºè¯ˆçš„æ¯”ä¾‹
          å…¬å¼: TP / (TP + FP)
          é‡è¦æ€§: é«˜ç²¾ç¡®ç‡å‡å°‘è¯¯æŠ¥ï¼Œé¿å…é”™è¯¯å†»ç»“æ­£å¸¸äº¤æ˜“

        - Recall (å¬å›ç‡): çœŸå®æ¬ºè¯ˆäº¤æ˜“ä¸­ï¼Œè¢«æ£€æµ‹å‡ºçš„æ¯”ä¾‹
          å…¬å¼: TP / (TP + FN)
          é‡è¦æ€§: é«˜å¬å›ç‡å‡å°‘æ¼æŠ¥ï¼Œé™ä½ç»æµæŸå¤±

        - F1-Score: Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡
          å…¬å¼: 2 * (Precision * Recall) / (Precision + Recall)
          ç”¨é€”: ç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½

        - ROC-AUC: ROCæ›²çº¿ä¸‹é¢ç§¯
          ç”¨é€”: è¡¡é‡æ¨¡å‹åŒºåˆ†æ­£è´Ÿæ ·æœ¬çš„èƒ½åŠ›
          æ³¨æ„: å¯¹ä¸å¹³è¡¡æ•°æ®å¯èƒ½ä¸å¤Ÿæ•æ„Ÿ

        - PR-AUC: Precision-Recallæ›²çº¿ä¸‹é¢ç§¯
          ç”¨é€”: æ›´é€‚åˆä¸å¹³è¡¡æ•°æ®çš„è¯„ä¼°æŒ‡æ ‡

    Parameters:
    -----------
    model : å¼‚å¸¸æ£€æµ‹æ¨¡å‹
        å·²è®­ç»ƒçš„æ¨¡å‹
    model_name : str
        æ¨¡å‹åç§°
    X_test : ndarray
        æµ‹è¯•é›†ç‰¹å¾
    y_test : ndarray
        æµ‹è¯•é›†æ ‡ç­¾

    Returns:
    --------
    metrics : dict
        åŒ…å«å„é¡¹è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    y_pred : ndarray
        é¢„æµ‹ç»“æœ (0æˆ–1)
    y_scores : ndarray
        å¼‚å¸¸åˆ†æ•°
    """
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š è¯„ä¼° {model_name} æ¨¡å‹")
    print("=" * 80)

    # 1. é¢„æµ‹
    print("\n   æ­£åœ¨è¿›è¡Œé¢„æµ‹...")
    start_time = time.time()

    # predictè¿”å›ï¼š1è¡¨ç¤ºæ­£å¸¸ï¼Œ-1è¡¨ç¤ºå¼‚å¸¸
    y_pred_raw = model.predict(X_test)

    # è½¬æ¢ä¸º0/1æ ‡ç­¾ï¼š0=æ­£å¸¸ï¼Œ1=å¼‚å¸¸
    y_pred = np.where(y_pred_raw == -1, 1, 0)

    # è·å–å¼‚å¸¸åˆ†æ•°
    # decision_functionè¿”å›ï¼šå€¼è¶Šå°è¶Šå¯èƒ½æ˜¯å¼‚å¸¸
    y_scores = -model.decision_function(X_test)  # å–è´Ÿæ•°ï¼Œä½¿å¾—åˆ†æ•°è¶Šå¤§è¶Šå¼‚å¸¸

    predict_time = time.time() - start_time
    print(f"   âœ… é¢„æµ‹å®Œæˆï¼ç”¨æ—¶: {predict_time:.3f} ç§’")

    # 2. è®¡ç®—åŸºæœ¬æŒ‡æ ‡
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    print(f"\n   ã€åˆ†ç±»æŒ‡æ ‡ã€‘")
    print(f"      - Precision (ç²¾ç¡®ç‡): {precision:.4f}")
    print(f"      - Recall (å¬å›ç‡):    {recall:.4f}")
    print(f"      - F1-Score:          {f1:.4f}")

    # 3. è®¡ç®—ROC-AUC
    fpr, tpr, roc_thresholds = roc_curve(y_test, y_scores)
    roc_auc = auc(fpr, tpr)

    print(f"\n   ã€ROCæŒ‡æ ‡ã€‘")
    print(f"      - ROC-AUC: {roc_auc:.4f}")

    # 4. è®¡ç®—PR-AUC
    pr_precision, pr_recall, pr_thresholds = precision_recall_curve(y_test, y_scores)
    pr_auc = average_precision_score(y_test, y_scores)

    print(f"\n   ã€PRæŒ‡æ ‡ã€‘")
    print(f"      - PR-AUC (Average Precision): {pr_auc:.4f}")

    # 5. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    print(f"\n   ã€æ··æ·†çŸ©é˜µã€‘")
    print(f"      çœŸè´Ÿä¾‹ (TN): {tn:,}  |  å‡æ­£ä¾‹ (FP): {fp:,}")
    print(f"      å‡è´Ÿä¾‹ (FN): {fn:,}  |  çœŸæ­£ä¾‹ (TP): {tp:,}")

    # 6. æ£€æµ‹ç»Ÿè®¡
    n_detected = (y_pred == 1).sum()
    n_actual = (y_test == 1).sum()

    print(f"\n   ã€æ£€æµ‹ç»Ÿè®¡ã€‘")
    print(f"      - å®é™…æ¬ºè¯ˆæ•°: {n_actual:,}")
    print(f"      - æ£€æµ‹æ¬ºè¯ˆæ•°: {n_detected:,}")
    print(f"      - æ£€æµ‹ç‡: {tp/n_actual*100:.2f}% (å¬å›ç‡)")
    print(f"      - è¯¯æŠ¥æ•°: {fp:,}")
    print(f"      - æ¼æŠ¥æ•°: {fn:,}")

    # 7. ç»„ç»‡è¿”å›ç»“æœ
    metrics = {
        'model_name': model_name,
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'roc_auc': float(roc_auc),
        'pr_auc': float(pr_auc),
        'confusion_matrix': cm.tolist(),
        'true_positives': int(tp),
        'false_positives': int(fp),
        'true_negatives': int(tn),
        'false_negatives': int(fn),
        'predict_time': float(predict_time),
        'fpr': fpr.tolist(),
        'tpr': tpr.tolist(),
        'pr_precision': pr_precision.tolist(),
        'pr_recall': pr_recall.tolist()
    }

    return metrics, y_pred, y_scores


# ============================================================================
# ç¬¬7éƒ¨åˆ†ï¼šå¯è§†åŒ–
# ============================================================================

def plot_confusion_matrices(all_metrics):
    """
    ç»˜åˆ¶ä¸‰ä¸ªæ¨¡å‹çš„æ··æ·†çŸ©é˜µå¯¹æ¯”

    Parameters:
    -----------
    all_metrics : list of dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„åˆ—è¡¨
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š ç»˜åˆ¶æ··æ·†çŸ©é˜µå¯¹æ¯”")
    print("=" * 80)

    output_dir = Path('outputs')

    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    fig.suptitle('ä¸‰ç§å¼‚å¸¸æ£€æµ‹ç®—æ³•çš„æ··æ·†çŸ©é˜µå¯¹æ¯”', fontsize=16, fontweight='bold', y=1.02)

    for i, metrics in enumerate(all_metrics):
        cm = np.array(metrics['confusion_matrix'])
        model_name = metrics['model_name']

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],
                   cbar=True, square=True, linewidths=2, linecolor='black',
                   annot_kws={'fontsize': 14, 'fontweight': 'bold'})

        axes[i].set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=11, fontweight='bold')
        axes[i].set_ylabel('çœŸå®æ ‡ç­¾', fontsize=11, fontweight='bold')
        axes[i].set_title(f'{model_name}\n(F1={metrics["f1_score"]:.4f})',
                         fontsize=12, fontweight='bold')
        axes[i].set_xticklabels(['æ­£å¸¸', 'æ¬ºè¯ˆ'])
        axes[i].set_yticklabels(['æ­£å¸¸', 'æ¬ºè¯ˆ'])

    plt.tight_layout()
    plt.savefig(output_dir / '04_confusion_matrices.png', dpi=150, bbox_inches='tight')
    print("   âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: outputs/04_confusion_matrices.png")
    plt.show()


def plot_roc_curves(all_metrics):
    """
    ç»˜åˆ¶ROCæ›²çº¿å¯¹æ¯”

    ROCæ›²çº¿è¯´æ˜ï¼š
        - æ¨ªè½´ï¼šå‡æ­£ä¾‹ç‡ (FPR) = FP / (FP + TN)
        - çºµè½´ï¼šçœŸæ­£ä¾‹ç‡ (TPR) = TP / (TP + FN) = Recall
        - å¯¹è§’çº¿ï¼šéšæœºçŒœæµ‹çš„æ€§èƒ½
        - æ›²çº¿è¶Šé è¿‘å·¦ä¸Šè§’ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå¥½
        - AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰ï¼š0.5=éšæœºï¼Œ1.0=å®Œç¾

    Parameters:
    -----------
    all_metrics : list of dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„åˆ—è¡¨
    """
    print("\nã€ç»˜åˆ¶ROCæ›²çº¿ã€‘")

    output_dir = Path('outputs')

    plt.figure(figsize=(10, 8))

    colors = ['steelblue', 'coral', 'green']

    for i, metrics in enumerate(all_metrics):
        fpr = np.array(metrics['fpr'])
        tpr = np.array(metrics['tpr'])
        roc_auc = metrics['roc_auc']
        model_name = metrics['model_name']

        plt.plot(fpr, tpr, color=colors[i], linewidth=2.5,
                label=f'{model_name} (AUC = {roc_auc:.4f})')

    # ç»˜åˆ¶å¯¹è§’çº¿ï¼ˆéšæœºçŒœæµ‹ï¼‰
    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='éšæœºçŒœæµ‹ (AUC = 0.5000)')

    plt.xlabel('å‡æ­£ä¾‹ç‡ (FPR)', fontsize=12, fontweight='bold')
    plt.ylabel('çœŸæ­£ä¾‹ç‡ (TPR / Recall)', fontsize=12, fontweight='bold')
    plt.title('ROCæ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold', pad=15)
    plt.legend(loc='lower right', fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])

    plt.tight_layout()
    plt.savefig(output_dir / '05_roc_curves.png', dpi=150, bbox_inches='tight')
    print("   âœ… ROCæ›²çº¿å·²ä¿å­˜åˆ°: outputs/05_roc_curves.png")
    plt.show()


def plot_pr_curves(all_metrics):
    """
    ç»˜åˆ¶Precision-Recallæ›²çº¿å¯¹æ¯”

    PRæ›²çº¿è¯´æ˜ï¼š
        - æ¨ªè½´ï¼šRecall (å¬å›ç‡) = TP / (TP + FN)
        - çºµè½´ï¼šPrecision (ç²¾ç¡®ç‡) = TP / (TP + FP)
        - å¯¹äºä¸å¹³è¡¡æ•°æ®ï¼ŒPRæ›²çº¿æ¯”ROCæ›²çº¿æ›´æœ‰æ„ä¹‰
        - æ›²çº¿è¶Šé è¿‘å³ä¸Šè§’ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå¥½
        - PR-AUCï¼ˆå¹³å‡ç²¾ç¡®ç‡ï¼‰ï¼šè¶Šæ¥è¿‘1è¶Šå¥½

    ä¸ºä»€ä¹ˆPRæ›²çº¿æ›´é€‚åˆä¸å¹³è¡¡æ•°æ®ï¼Ÿ
        - ROCæ›²çº¿ä¸­çš„FPRåœ¨è´Ÿæ ·æœ¬æå¤šæ—¶å˜åŒ–ä¸æ•æ„Ÿ
        - PRæ›²çº¿ç›´æ¥å…³æ³¨æ­£æ ·æœ¬çš„æ£€æµ‹è´¨é‡
        - æ›´ç¬¦åˆæ¬ºè¯ˆæ£€æµ‹çš„ä¸šåŠ¡éœ€æ±‚

    Parameters:
    -----------
    all_metrics : list of dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„åˆ—è¡¨
    """
    print("\nã€ç»˜åˆ¶PRæ›²çº¿ã€‘")

    output_dir = Path('outputs')

    plt.figure(figsize=(10, 8))

    colors = ['steelblue', 'coral', 'green']

    for i, metrics in enumerate(all_metrics):
        pr_recall = np.array(metrics['pr_recall'])
        pr_precision = np.array(metrics['pr_precision'])
        pr_auc = metrics['pr_auc']
        model_name = metrics['model_name']

        plt.plot(pr_recall, pr_precision, color=colors[i], linewidth=2.5,
                label=f'{model_name} (AP = {pr_auc:.4f})')

    # ç»˜åˆ¶åŸºå‡†çº¿ï¼ˆéšæœºçŒœæµ‹ï¼‰
    # å¯¹äºä¸å¹³è¡¡æ•°æ®ï¼ŒéšæœºçŒœæµ‹çš„PR = æ­£æ ·æœ¬æ¯”ä¾‹
    baseline = all_metrics[0]['true_positives'] + all_metrics[0]['false_negatives']
    total = baseline + all_metrics[0]['true_negatives'] + all_metrics[0]['false_positives']
    baseline_precision = baseline / total

    plt.axhline(y=baseline_precision, color='k', linestyle='--', linewidth=2,
               label=f'éšæœºçŒœæµ‹ (AP = {baseline_precision:.4f})')

    plt.xlabel('å¬å›ç‡ (Recall)', fontsize=12, fontweight='bold')
    plt.ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12, fontweight='bold')
    plt.title('Precision-Recallæ›²çº¿å¯¹æ¯”ï¼ˆæ›´é€‚åˆä¸å¹³è¡¡æ•°æ®ï¼‰', fontsize=14, fontweight='bold', pad=15)
    plt.legend(loc='best', fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])

    plt.tight_layout()
    plt.savefig(output_dir / '06_pr_curves.png', dpi=150, bbox_inches='tight')
    print("   âœ… PRæ›²çº¿å·²ä¿å­˜åˆ°: outputs/06_pr_curves.png")
    plt.show()


def plot_metrics_comparison(all_metrics):
    """
    ç»˜åˆ¶æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾

    Parameters:
    -----------
    all_metrics : list of dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„åˆ—è¡¨
    """
    print("\nã€ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”ã€‘")

    output_dir = Path('outputs')

    # æå–æŒ‡æ ‡
    model_names = [m['model_name'] for m in all_metrics]
    precisions = [m['precision'] for m in all_metrics]
    recalls = [m['recall'] for m in all_metrics]
    f1_scores = [m['f1_score'] for m in all_metrics]
    pr_aucs = [m['pr_auc'] for m in all_metrics]

    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('å¼‚å¸¸æ£€æµ‹æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold', y=0.995)

    x = np.arange(len(model_names))
    width = 0.6

    # 1. Precisionå¯¹æ¯”
    bars1 = axes[0, 0].bar(x, precisions, width, color='steelblue', alpha=0.8, edgecolor='black')
    axes[0, 0].set_ylabel('Precision (ç²¾ç¡®ç‡)', fontsize=11, fontweight='bold')
    axes[0, 0].set_title('ç²¾ç¡®ç‡å¯¹æ¯”ï¼ˆå‡å°‘è¯¯æŠ¥ï¼‰', fontsize=12, fontweight='bold')
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(model_names, rotation=15, ha='right')
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    axes[0, 0].set_ylim([0, 1])
    for i, v in enumerate(precisions):
        axes[0, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')

    # 2. Recallå¯¹æ¯”
    bars2 = axes[0, 1].bar(x, recalls, width, color='coral', alpha=0.8, edgecolor='black')
    axes[0, 1].set_ylabel('Recall (å¬å›ç‡)', fontsize=11, fontweight='bold')
    axes[0, 1].set_title('å¬å›ç‡å¯¹æ¯”ï¼ˆå‡å°‘æ¼æŠ¥ï¼‰', fontsize=12, fontweight='bold')
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(model_names, rotation=15, ha='right')
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    axes[0, 1].set_ylim([0, 1])
    for i, v in enumerate(recalls):
        axes[0, 1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')

    # 3. F1-Scoreå¯¹æ¯”
    bars3 = axes[1, 0].bar(x, f1_scores, width, color='green', alpha=0.8, edgecolor='black')
    axes[1, 0].set_ylabel('F1-Score', fontsize=11, fontweight='bold')
    axes[1, 0].set_title('F1-Scoreå¯¹æ¯”ï¼ˆç»¼åˆæŒ‡æ ‡ï¼‰', fontsize=12, fontweight='bold')
    axes[1, 0].set_xticks(x)
    axes[1, 0].set_xticklabels(model_names, rotation=15, ha='right')
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    axes[1, 0].set_ylim([0, 1])
    for i, v in enumerate(f1_scores):
        axes[1, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')

    # 4. PR-AUCå¯¹æ¯”
    bars4 = axes[1, 1].bar(x, pr_aucs, width, color='purple', alpha=0.8, edgecolor='black')
    axes[1, 1].set_ylabel('PR-AUC', fontsize=11, fontweight='bold')
    axes[1, 1].set_title('PR-AUCå¯¹æ¯”ï¼ˆä¸å¹³è¡¡æ•°æ®æ¨èæŒ‡æ ‡ï¼‰', fontsize=12, fontweight='bold')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels(model_names, rotation=15, ha='right')
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    axes[1, 1].set_ylim([0, 1])
    for i, v in enumerate(pr_aucs):
        axes[1, 1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_dir / '07_metrics_comparison.png', dpi=150, bbox_inches='tight')
    print("   âœ… æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: outputs/07_metrics_comparison.png")
    plt.show()


def visualize_anomalies_tsne(X_test, y_test, y_pred_if, y_pred_svm, y_pred_lof):
    """
    ä½¿ç”¨t-SNEé™ç»´å¯è§†åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœ

    t-SNEè¯´æ˜ï¼š
        - t-SNE (t-Distributed Stochastic Neighbor Embedding)
        - å°†é«˜ç»´æ•°æ®é™ç»´åˆ°2D/3Dç”¨äºå¯è§†åŒ–
        - ä¿æŒæ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§å…³ç³»
        - ç›¸ä¼¼çš„ç‚¹åœ¨ä½ç»´ç©ºé—´ä¸­ä¹Ÿä¼šé è¿‘

    Parameters:
    -----------
    X_test : ndarray
        æµ‹è¯•é›†ç‰¹å¾
    y_test : ndarray
        çœŸå®æ ‡ç­¾
    y_pred_if : ndarray
        Isolation Foresté¢„æµ‹ç»“æœ
    y_pred_svm : ndarray
        One-Class SVMé¢„æµ‹ç»“æœ
    y_pred_lof : ndarray
        LOFé¢„æµ‹ç»“æœ
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š ä½¿ç”¨t-SNEå¯è§†åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœ")
    print("=" * 80)

    output_dir = Path('outputs')

    # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œt-SNEï¼ˆt-SNEè®¡ç®—è¾ƒæ…¢ï¼‰
    max_samples = 5000
    if X_test.shape[0] > max_samples:
        print(f"   é‡‡æ · {max_samples:,} ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–...")
        indices = np.random.choice(X_test.shape[0], max_samples, replace=False)
        X_sample = X_test[indices]
        y_sample = y_test[indices]
        y_if_sample = y_pred_if[indices]
        y_svm_sample = y_pred_svm[indices]
        y_lof_sample = y_pred_lof[indices]
    else:
        X_sample = X_test
        y_sample = y_test
        y_if_sample = y_pred_if
        y_svm_sample = y_pred_svm
        y_lof_sample = y_pred_lof

    # åº”ç”¨t-SNEé™ç»´
    print("\n   æ­£åœ¨è¿›è¡Œt-SNEé™ç»´ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
    X_tsne = tsne.fit_transform(X_sample)

    print("   âœ… t-SNEé™ç»´å®Œæˆï¼")

    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(16, 14))
    fig.suptitle('å¼‚å¸¸æ£€æµ‹ç»“æœå¯è§†åŒ– (t-SNEé™ç»´)', fontsize=16, fontweight='bold', y=0.995)

    # 1. çœŸå®æ ‡ç­¾
    scatter1 = axes[0, 0].scatter(X_tsne[y_sample == 0, 0], X_tsne[y_sample == 0, 1],
                                  c='steelblue', s=20, alpha=0.5, label='æ­£å¸¸äº¤æ˜“', edgecolors='none')
    scatter2 = axes[0, 0].scatter(X_tsne[y_sample == 1, 0], X_tsne[y_sample == 1, 1],
                                  c='red', s=50, alpha=0.8, label='æ¬ºè¯ˆäº¤æ˜“', marker='X', edgecolors='black')
    axes[0, 0].set_title('çœŸå®æ ‡ç­¾', fontsize=13, fontweight='bold')
    axes[0, 0].set_xlabel('t-SNE ç»´åº¦ 1', fontsize=11)
    axes[0, 0].set_ylabel('t-SNE ç»´åº¦ 2', fontsize=11)
    axes[0, 0].legend(fontsize=10)
    axes[0, 0].grid(True, alpha=0.3)

    # 2. Isolation Foresté¢„æµ‹
    axes[0, 1].scatter(X_tsne[y_if_sample == 0, 0], X_tsne[y_if_sample == 0, 1],
                      c='lightblue', s=20, alpha=0.5, label='é¢„æµ‹æ­£å¸¸', edgecolors='none')
    axes[0, 1].scatter(X_tsne[y_if_sample == 1, 0], X_tsne[y_if_sample == 1, 1],
                      c='orange', s=50, alpha=0.8, label='é¢„æµ‹æ¬ºè¯ˆ', marker='X', edgecolors='black')
    # æ ‡è®°çœŸå®æ¬ºè¯ˆ
    axes[0, 1].scatter(X_tsne[y_sample == 1, 0], X_tsne[y_sample == 1, 1],
                      c='none', s=80, marker='o', edgecolors='red', linewidths=2, label='çœŸå®æ¬ºè¯ˆ')
    axes[0, 1].set_title('Isolation Forest é¢„æµ‹', fontsize=13, fontweight='bold')
    axes[0, 1].set_xlabel('t-SNE ç»´åº¦ 1', fontsize=11)
    axes[0, 1].set_ylabel('t-SNE ç»´åº¦ 2', fontsize=11)
    axes[0, 1].legend(fontsize=9)
    axes[0, 1].grid(True, alpha=0.3)

    # 3. One-Class SVMé¢„æµ‹
    axes[1, 0].scatter(X_tsne[y_svm_sample == 0, 0], X_tsne[y_svm_sample == 0, 1],
                      c='lightblue', s=20, alpha=0.5, label='é¢„æµ‹æ­£å¸¸', edgecolors='none')
    axes[1, 0].scatter(X_tsne[y_svm_sample == 1, 0], X_tsne[y_svm_sample == 1, 1],
                      c='orange', s=50, alpha=0.8, label='é¢„æµ‹æ¬ºè¯ˆ', marker='X', edgecolors='black')
    axes[1, 0].scatter(X_tsne[y_sample == 1, 0], X_tsne[y_sample == 1, 1],
                      c='none', s=80, marker='o', edgecolors='red', linewidths=2, label='çœŸå®æ¬ºè¯ˆ')
    axes[1, 0].set_title('One-Class SVM é¢„æµ‹', fontsize=13, fontweight='bold')
    axes[1, 0].set_xlabel('t-SNE ç»´åº¦ 1', fontsize=11)
    axes[1, 0].set_ylabel('t-SNE ç»´åº¦ 2', fontsize=11)
    axes[1, 0].legend(fontsize=9)
    axes[1, 0].grid(True, alpha=0.3)

    # 4. LOFé¢„æµ‹
    axes[1, 1].scatter(X_tsne[y_lof_sample == 0, 0], X_tsne[y_lof_sample == 0, 1],
                      c='lightblue', s=20, alpha=0.5, label='é¢„æµ‹æ­£å¸¸', edgecolors='none')
    axes[1, 1].scatter(X_tsne[y_lof_sample == 1, 0], X_tsne[y_lof_sample == 1, 1],
                      c='orange', s=50, alpha=0.8, label='é¢„æµ‹æ¬ºè¯ˆ', marker='X', edgecolors='black')
    axes[1, 1].scatter(X_tsne[y_sample == 1, 0], X_tsne[y_sample == 1, 1],
                      c='none', s=80, marker='o', edgecolors='red', linewidths=2, label='çœŸå®æ¬ºè¯ˆ')
    axes[1, 1].set_title('Local Outlier Factor é¢„æµ‹', fontsize=13, fontweight='bold')
    axes[1, 1].set_xlabel('t-SNE ç»´åº¦ 1', fontsize=11)
    axes[1, 1].set_ylabel('t-SNE ç»´åº¦ 2', fontsize=11)
    axes[1, 1].legend(fontsize=9)
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / '08_tsne_visualization.png', dpi=150, bbox_inches='tight')
    print("   âœ… t-SNEå¯è§†åŒ–å·²ä¿å­˜åˆ°: outputs/08_tsne_visualization.png")
    plt.show()


# ============================================================================
# ç¬¬8éƒ¨åˆ†ï¼šæ¨¡å‹ä¿å­˜
# ============================================================================

def save_models_and_metrics(models, all_metrics, scaler):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡

    Parameters:
    -----------
    models : dict
        åŒ…å«æ‰€æœ‰æ¨¡å‹çš„å­—å…¸
    all_metrics : list of dict
        æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡
    scaler : StandardScaler
        æ•°æ®æ ‡å‡†åŒ–å™¨
    """
    print("\n" + "=" * 80)
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡")
    print("=" * 80)

    # åˆ›å»ºæ¨¡å‹ç›®å½•
    models_dir = Path('models')
    models_dir.mkdir(exist_ok=True)

    # ä¿å­˜æ¨¡å‹
    for name, model in models.items():
        model_path = models_dir / f'{name.lower().replace(" ", "_")}_model.pkl'
        joblib.dump(model, model_path)
        print(f"   âœ… å·²ä¿å­˜æ¨¡å‹: {model_path}")

    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    scaler_path = models_dir / 'scaler.pkl'
    joblib.dump(scaler, scaler_path)
    print(f"   âœ… å·²ä¿å­˜æ ‡å‡†åŒ–å™¨: {scaler_path}")

    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼Œå»é™¤å¤§æ•°ç»„ï¼‰
    metrics_simplified = []
    for m in all_metrics:
        metrics_simple = {
            'model_name': m['model_name'],
            'precision': m['precision'],
            'recall': m['recall'],
            'f1_score': m['f1_score'],
            'roc_auc': m['roc_auc'],
            'pr_auc': m['pr_auc'],
            'confusion_matrix': m['confusion_matrix'],
            'true_positives': m['true_positives'],
            'false_positives': m['false_positives'],
            'true_negatives': m['true_negatives'],
            'false_negatives': m['false_negatives'],
            'predict_time': m['predict_time']
        }
        metrics_simplified.append(metrics_simple)

    metrics_path = models_dir / 'evaluation_metrics.json'
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_simplified, f, indent=4, ensure_ascii=False)

    print(f"   âœ… å·²ä¿å­˜è¯„ä¼°æŒ‡æ ‡: {metrics_path}")
    print("\n" + "=" * 80)


# ============================================================================
# ç¬¬9éƒ¨åˆ†ï¼šä¸»å‡½æ•°
# ============================================================================

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æ¬ºè¯ˆæ£€æµ‹æµç¨‹
    """
    print("\n" + "=" * 80)
    print("ğŸš€ ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹é¡¹ç›®å¼€å§‹")
    print("=" * 80)

    # Step 1: ç”Ÿæˆæ•°æ®
    df = generate_fraud_data(n_samples=50000, fraud_ratio=0.002, n_features=30)

    # Step 2: æ•°æ®æ¢ç´¢
    explore_data(df)

    # Step 3: æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test, X_train_normal, scaler = preprocess_data(df)

    # Step 4: è®­ç»ƒä¸‰ç§å¼‚å¸¸æ£€æµ‹æ¨¡å‹
    contamination = 0.002  # é¢„æœŸå¼‚å¸¸æ¯”ä¾‹

    # 4.1 Isolation Forest
    model_if, train_time_if = train_isolation_forest(X_train_normal, contamination)

    # 4.2 One-Class SVM
    model_svm, train_time_svm = train_one_class_svm(X_train_normal, nu=contamination)

    # 4.3 Local Outlier Factor
    model_lof, train_time_lof = train_local_outlier_factor(X_train_normal, contamination)

    # Step 5: è¯„ä¼°æ¨¡å‹
    metrics_if, y_pred_if, y_scores_if = evaluate_model(model_if, "Isolation Forest", X_test, y_test)
    metrics_svm, y_pred_svm, y_scores_svm = evaluate_model(model_svm, "One-Class SVM", X_test, y_test)
    metrics_lof, y_pred_lof, y_scores_lof = evaluate_model(model_lof, "Local Outlier Factor", X_test, y_test)

    all_metrics = [metrics_if, metrics_svm, metrics_lof]

    # Step 6: å¯è§†åŒ–å¯¹æ¯”
    print("\n" + "=" * 80)
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾")
    print("=" * 80)

    plot_confusion_matrices(all_metrics)
    plot_roc_curves(all_metrics)
    plot_pr_curves(all_metrics)
    plot_metrics_comparison(all_metrics)
    visualize_anomalies_tsne(X_test, y_test, y_pred_if, y_pred_svm, y_pred_lof)

    # Step 7: ä¿å­˜æ¨¡å‹
    models = {
        'Isolation Forest': model_if,
        'One-Class SVM': model_svm,
        'Local Outlier Factor': model_lof
    }
    save_models_and_metrics(models, all_metrics, scaler)

    # Step 8: ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“Š é¡¹ç›®æ€»ç»“æŠ¥å‘Š")
    print("=" * 80)

    print("\nã€æ¨¡å‹æ€§èƒ½å¯¹æ¯”ã€‘")
    print(f"{'æ¨¡å‹':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'PR-AUC':<12}")
    print("-" * 68)
    for m in all_metrics:
        print(f"{m['model_name']:<20} {m['precision']:<12.4f} {m['recall']:<12.4f} "
              f"{m['f1_score']:<12.4f} {m['pr_auc']:<12.4f}")

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_f1_model = max(all_metrics, key=lambda x: x['f1_score'])
    best_recall_model = max(all_metrics, key=lambda x: x['recall'])
    best_precision_model = max(all_metrics, key=lambda x: x['precision'])

    print("\nã€æœ€ä½³æ¨¡å‹ã€‘")
    print(f"   - æœ€é«˜F1-Score: {best_f1_model['model_name']} ({best_f1_model['f1_score']:.4f})")
    print(f"   - æœ€é«˜Recall: {best_recall_model['model_name']} ({best_recall_model['recall']:.4f})")
    print(f"   - æœ€é«˜Precision: {best_precision_model['model_name']} ({best_precision_model['precision']:.4f})")

    print("\nã€ä¸šåŠ¡å»ºè®®ã€‘")
    print("   1. æ¨¡å‹é€‰æ‹©:")
    print(f"      - å¦‚æœå…³æ³¨å‡å°‘æ¼æŠ¥ï¼ˆæŠ“ä½æ›´å¤šæ¬ºè¯ˆï¼‰:")
    print(f"        æ¨èä½¿ç”¨ {best_recall_model['model_name']}")
    print(f"      - å¦‚æœå…³æ³¨å‡å°‘è¯¯æŠ¥ï¼ˆé¿å…è¯¯ä¼¤æ­£å¸¸ç”¨æˆ·ï¼‰:")
    print(f"        æ¨èä½¿ç”¨ {best_precision_model['model_name']}")
    print(f"      - ç»¼åˆè€ƒè™‘:")
    print(f"        æ¨èä½¿ç”¨ {best_f1_model['model_name']}")

    print("\n   2. é˜ˆå€¼è°ƒæ•´:")
    print("      - å¯ä»¥é€šè¿‡è°ƒæ•´decision_functionçš„é˜ˆå€¼æ¥æƒè¡¡Precisionå’ŒRecall")
    print("      - é™ä½é˜ˆå€¼ï¼šæé«˜Recallï¼Œé™ä½Precisionï¼ˆæŠ“æ›´å¤šæ¬ºè¯ˆï¼Œä½†è¯¯æŠ¥å¢åŠ ï¼‰")
    print("      - æé«˜é˜ˆå€¼ï¼šæé«˜Precisionï¼Œé™ä½Recallï¼ˆå‡å°‘è¯¯æŠ¥ï¼Œä½†æ¼æŠ¥å¢åŠ ï¼‰")

    print("\n   3. å®é™…éƒ¨ç½²:")
    print("      - Isolation Forest: è®­ç»ƒå¿«ï¼Œæ¨ç†å¿«ï¼Œé€‚åˆå¤§è§„æ¨¡å®æ—¶æ£€æµ‹")
    print("      - One-Class SVM: å‡†ç¡®åº¦é«˜ï¼Œä½†è®¡ç®—è¾ƒæ…¢ï¼Œé€‚åˆæ‰¹é‡æ£€æµ‹")
    print("      - LOF: é€‚åˆæ£€æµ‹å±€éƒ¨å¼‚å¸¸ï¼Œä½†ä¸é€‚åˆå¤§è§„æ¨¡æ•°æ®")

    print("\n   4. æ”¹è¿›æ–¹å‘:")
    print("      - ç‰¹å¾å·¥ç¨‹ï¼šä»Amountã€Timeæå–æ›´å¤šç‰¹å¾")
    print("      - é›†æˆæ–¹æ³•ï¼šç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ")
    print("      - åŠç›‘ç£å­¦ä¹ ï¼šåˆ©ç”¨å°‘é‡æ ‡æ³¨çš„æ¬ºè¯ˆæ ·æœ¬")
    print("      - åœ¨çº¿å­¦ä¹ ï¼šéšç€æ–°æ•°æ®ä¸æ–­æ›´æ–°æ¨¡å‹")

    print("\n" + "=" * 80)
    print("âœ… ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹é¡¹ç›®å®Œæˆï¼")
    print("=" * 80)

    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("   - outputs/01_class_distribution.png       # ç±»åˆ«åˆ†å¸ƒ")
    print("   - outputs/02_amount_distribution.png      # é‡‘é¢åˆ†å¸ƒ")
    print("   - outputs/03_time_distribution.png        # æ—¶é—´åˆ†å¸ƒ")
    print("   - outputs/04_confusion_matrices.png       # æ··æ·†çŸ©é˜µ")
    print("   - outputs/05_roc_curves.png              # ROCæ›²çº¿")
    print("   - outputs/06_pr_curves.png               # PRæ›²çº¿")
    print("   - outputs/07_metrics_comparison.png       # æŒ‡æ ‡å¯¹æ¯”")
    print("   - outputs/08_tsne_visualization.png       # t-SNEå¯è§†åŒ–")
    print("   - models/isolation_forest_model.pkl       # IFæ¨¡å‹")
    print("   - models/one-class_svm_model.pkl          # SVMæ¨¡å‹")
    print("   - models/local_outlier_factor_model.pkl   # LOFæ¨¡å‹")
    print("   - models/scaler.pkl                       # æ ‡å‡†åŒ–å™¨")
    print("   - models/evaluation_metrics.json          # è¯„ä¼°æŒ‡æ ‡")
    print("\n" + "=" * 80)


# ============================================================================
# ç¨‹åºå…¥å£
# ============================================================================

if __name__ == "__main__":
    main()
