# 🚀 AI完整学习路线图

> 从机器学习到深度学习及高级方向的完整导航

---

## 📊 AI/ML 学习路径树状图

> 🎯 **目标**: 生成式AI + CV + NLP 三向并进
> 📍 **当前**: 阶段1 ✅ → 阶段2 🚀

```
AI/ML 完整学习路径
│
├── 【阶段0】数学与工具基础 (可选/按需学习)
│   ├── 线性代数 (矩阵运算、特征值)
│   ├── 概率统计 (贝叶斯、分布)
│   ├── 微积分 (梯度、优化)
│   └── Python编程 (NumPy/Pandas)
│
├── 【阶段1】传统机器学习基础 ✅ 已完成
│   │
│   ├── 1.1 监督学习 ✅
│   │   ├── 回归算法
│   │   │   ├── 线性回归
│   │   │   ├── 多项式回归
│   │   │   └── 正则化 (Ridge/Lasso/ElasticNet)
│   │   │
│   │   └── 分类算法 ✅
│   │       ├── 逻辑回归
│   │       ├── 支持向量机 (SVM)
│   │       ├── 决策树
│   │       └── 集成学习 (随机森林/XGBoost/LightGBM)
│   │
│   ├── 1.2 模型评估与选择 ✅
│   │   ├── 交叉验证
│   │   ├── 评估指标 (准确率/精确率/召回率/F1/AUC)
│   │   └── 超参数调优 (GridSearchCV)
│   │
│   └── 1.3 实战项目 ✅
│       ├── 房价预测
│       ├── 客户流失预测
│       └── 出租车时长预测
│
├── 【阶段2】无监督学习 ✅ 已完成（必学）
│   │
│   ├── 2.1 聚类算法 ✅ 已完成
│   │   ├── K-Means ⭐ 重点掌握
│   │   ├── DBSCAN
│   │   ├── 层次聚类 (Hierarchical)
│   │   └── 高斯混合模型 (GMM)
│   │
│   ├── 2.2 降维技术 ⭐⭐⭐ 重要（深度学习基础）
│   │   ├── PCA (主成分分析) ⭐ 必须深入理解
│   │   ├── t-SNE (可视化)
│   │   ├── UMAP
│   │   └── LDA (线性判别分析)
│   │
│   ├── 2.3 异常检测 ✅ 已完成
│   │   ├── Isolation Forest
│   │   ├── One-Class SVM
│   │   └── Local Outlier Factor (LOF)
│   │
│   ├── 2.4 AutoML工具链 ⭐⭐ 独立模块 ✅ 已创建
│   │   │
│   │   ├── 核心问题：算法太多，是否都要逐个调试？
│   │   │   ❌ 传统方法：手动尝试几十个模型+调参（耗时且低效）
│   │   │   ✅ AutoML方案：自动模型选择+超参数优化（3行代码）
│   │   │
│   │   ├── 主流AutoML框架
│   │   │   ├── PyCaret ⭐⭐⭐ 最易用（强烈推荐初学者）
│   │   │   │   ├── 一键对比15+个模型
│   │   │   │   ├── 自动特征工程
│   │   │   │   ├── 自动调优和融合
│   │   │   │   └── 支持监督/无监督/时序
│   │   │   │
│   │   │   ├── Auto-sklearn ⭐⭐⭐ 基于sklearn
│   │   │   │   ├── 自动算法选择
│   │   │   │   ├── 自动预处理
│   │   │   │   └── 自动集成学习
│   │   │   │
│   │   │   ├── FLAML (微软) ⭐⭐ 速度最快
│   │   │   │   ├── 低计算成本
│   │   │   │   ├── 大数据集友好
│   │   │   │   └── 工业级性能
│   │   │   │
│   │   │   ├── H2O AutoML ⭐⭐ 企业级
│   │   │   │   ├── 分布式训练
│   │   │   │   ├── 可解释性强
│   │   │   │   └── 生产环境部署
│   │   │   │
│   │   │   └── TPOT ⭐ 遗传算法优化
│   │   │       ├── 优化整个Pipeline
│   │   │       ├── 自动生成代码
│   │   │       └── 研究导向
│   │   │
│   │   ├── AutoML使用场景
│   │   │   ✅ 快速建立baseline（几分钟获得80%最佳性能）
│   │   │   ✅ 数据探索阶段（快速验证数据价值）
│   │   │   ✅ 时间紧迫的项目
│   │   │   ✅ 非ML专家的应用开发
│   │   │   ❌ 需要完全自定义模型架构
│   │   │   ❌ 需要深入理解模型行为
│   │   │   ❌ 超大规模数据集（部分工具）
│   │   │
│   │   └── AutoML实践建议
│   │       1. 先用AutoML快速建立baseline
│   │       2. 理解AutoML选择的最佳模型
│   │       3. 在此基础上手动精细调优
│   │       4. 学习AutoML的特征工程思路
│   │
│   └── 2.5 实战项目 📝 待完成
│       ├── 客户分群 (K-Means/GMM)
│       ├── PCA图像压缩
│       └── 欺诈检测 (Isolation Forest)
│
├── 【工具模块】AutoML 工具链 ⭐⭐ 独立模块 ✅ 已创建
│   │   📁 位置: /automl_toolkit/
│   │
│   ├── 核心工具（已完成5个notebook + 完整README）
│   │   ├── 01_pycaret_basics.ipynb ✅
│   │   ├── 02_autosklearn_intro.ipynb ✅
│   │   ├── 03_flaml_optimization.ipynb ✅
│   │   ├── 04_h2o_automl.ipynb ✅
│   │   └── 05_automl_comparison.ipynb ✅
│   │
│   ├── 实战项目（待完成）
│   │   ├── 01_baseline_comparison/ - 传统方法 vs AutoML
│   │   ├── 02_kaggle_competition/ - Kaggle快速上手
│   │   └── 03_feature_engineering/ - 学习AutoML特征工程
│   │
│   └── 使用建议
│       ✅ 快速建立baseline（几分钟获得80%最佳性能）
│       ✅ 数据探索阶段（快速验证数据价值）
│       ✅ 学习AutoML的特征工程和模型选择思路
│       💡 在任何机器学习阶段都可以使用AutoML作为辅助工具
│
├── 【阶段3】神经网络与深度学习基础 ⭐⭐⭐ 从原理到实现
│   │
│   ├── 3.1 神经网络数学基础 ⭐⭐⭐ 必备知识
│   │   │
│   │   ├── 线性代数核心
│   │   │   ├── 向量与矩阵运算
│   │   │   ├── 特征值与特征向量
│   │   │   ├── 矩阵微分
│   │   │   └── 向量化编程技巧
│   │   │
│   │   ├── 微积分基础
│   │   │   ├── 偏导数与方向导数
│   │   │   ├── 链式法则深入理解
│   │   │   ├── 泰勒展开与近似
│   │   │   └── 梯度的几何意义
│   │   │
│   │   ├── 概率论基础
│   │   │   ├── 期望、方差、协方差
│   │   │   ├── 常见分布（高斯、伯努利）
│   │   │   ├── 最大似然估计
│   │   │   └── 贝叶斯定理应用
│   │   │
│   │   └── 最优化理论
│   │       ├── 凸函数与凸优化
│   │       ├── 拉格朗日乘数法
│   │       ├── KKT条件
│   │       └── 数值优化基础
│   │
│   ├── 3.2 从感知器到神经网络 ⭐⭐⭐ 历史演进
│   │   │
│   │   ├── 生物神经元启发
│   │   │   ├── 神经元结构（树突、轴突、突触）
│   │   │   ├── 动作电位与信号传递
│   │   │   └── 从生物到人工的简化
│   │   │
│   │   ├── 单层感知器详解
│   │   │   ├── McCulloch-Pitts神经元
│   │   │   ├── Rosenblatt感知器
│   │   │   ├── 感知器学习规则推导
│   │   │   ├── 线性可分性质
│   │   │   └── 感知器收敛定理
│   │   │
│   │   ├── XOR问题与局限性
│   │   │   ├── Minsky的批判
│   │   │   ├── 线性不可分问题
│   │   │   ├── 特征空间变换思想
│   │   │   └── 多层网络的必要性
│   │   │
│   │   └── 实践项目
│   │       ├── NumPy实现单层感知器
│   │       ├── 可视化决策边界
│   │       ├── AND/OR/XOR门实验
│   │       └── 感知器学习过程动画
│   │
│   ├── 3.3 前馈神经网络详解 ⭐⭐⭐ 核心架构
│   │   │
│   │   ├── 网络架构设计
│   │   │   ├── 层次结构（输入层、隐藏层、输出层）
│   │   │   ├── 神经元连接模式
│   │   │   ├── 权重矩阵组织
│   │   │   ├── 偏置项的作用
│   │   │   └── 网络容量与表达能力
│   │   │
│   │   ├── 前向传播详解
│   │   │   ├── 单个神经元计算
│   │   │   ├── 层级计算的矩阵表示
│   │   │   ├── 批处理的向量化实现
│   │   │   ├── 计算图构建
│   │   │   └── 内存布局优化
│   │   │
│   │   ├── 激活函数深度分析
│   │   │   ├── 为什么需要非线性
│   │   │   ├── Sigmoid的饱和问题
│   │   │   ├── ReLU的稀疏性优势
│   │   │   ├── 死亡ReLU问题
│   │   │   └── 新型激活函数（Swish/GELU/Mish）
│   │   │
│   │   ├── 万能近似定理
│   │   │   ├── 定理表述与直觉
│   │   │   ├── Cybenko定理证明思路
│   │   │   ├── 宽度vs深度的权衡
│   │   │   └── 实际意义与局限
│   │   │
│   │   └── 实践项目
│   │       ├── NumPy实现前馈网络类
│   │       ├── 函数拟合实验
│   │       ├── 决策边界可视化
│   │       └── 网络深度影响分析
│   │
│   ├── 3.4 反向传播算法深入 ⭐⭐⭐⭐⭐ 最重要
│   │   │
│   │   ├── 计算图与自动微分
│   │   │   ├── 计算图的构建
│   │   │   ├── 前向模式vs反向模式
│   │   │   ├── 雅可比矩阵与向量积
│   │   │   └── 动态图vs静态图
│   │   │
│   │   ├── 反向传播推导
│   │   │   ├── 标量链式法则回顾
│   │   │   ├── 向量链式法则
│   │   │   ├── 逐层误差传递
│   │   │   ├── 权重梯度计算
│   │   │   └── 完整数学推导
│   │   │
│   │   ├── 矩阵形式优化
│   │   │   ├── 批量样本的梯度
│   │   │   ├── 向量化实现技巧
│   │   │   ├── 内存效率优化
│   │   │   └── 数值稳定性技巧
│   │   │
│   │   ├── 梯度问题分析
│   │   │   ├── 梯度消失原因与对策
│   │   │   ├── 梯度爆炸检测与处理
│   │   │   ├── 梯度裁剪技术
│   │   │   └── 残差连接原理
│   │   │
│   │   └── 实践项目
│   │       ├── 手动推导三层网络反向传播
│   │       ├── NumPy实现自动微分引擎
│   │       ├── 梯度检查（数值梯度验证）
│   │       └── 可视化梯度流
│   │
│   ├── 3.5 损失函数与优化 ⭐⭐⭐ 训练核心
│   │   │
│   │   ├── 损失函数设计
│   │   │   ├── MSE推导与性质
│   │   │   ├── 交叉熵的信息论解释
│   │   │   ├── KL散度与JS散度
│   │   │   ├── Focal Loss（类别不平衡）
│   │   │   └── 自定义损失函数设计
│   │   │
│   │   ├── 梯度下降详解
│   │   │   ├── 批量梯度下降（BGD）
│   │   │   ├── 随机梯度下降（SGD）
│   │   │   ├── 小批量梯度下降
│   │   │   ├── 收敛性分析
│   │   │   └── 学习率选择策略
│   │   │
│   │   ├── 动量优化方法
│   │   │   ├── 动量法的物理直觉
│   │   │   ├── Nesterov加速梯度
│   │   │   ├── 动量参数调优
│   │   │   └── 振荡问题处理
│   │   │
│   │   ├── 自适应学习率
│   │   │   ├── AdaGrad原理
│   │   │   ├── RMSprop改进
│   │   │   ├── Adam算法详解
│   │   │   ├── AdamW与权重衰减
│   │   │   └── 新型优化器（RAdam/Lookahead）
│   │   │
│   │   └── 实践项目
│   │       ├── 优化器对比实验
│   │       ├── 学习率调度实现
│   │       ├── 损失函数景观可视化
│   │       └── 自适应优化器实现
│   │
│   ├── 3.6 正则化与泛化 ⭐⭐⭐ 提升泛化
│   │   │
│   │   ├── 过拟合本质分析
│   │   │   ├── 偏差-方差分解
│   │   │   ├── VC维理论
│   │   │   ├── Rademacher复杂度
│   │   │   └── 泛化界理论
│   │   │
│   │   ├── 经典正则化技术
│   │   │   ├── L1正则化（稀疏性）
│   │   │   ├── L2正则化（权重衰减）
│   │   │   ├── 弹性网络（Elastic Net）
│   │   │   └── 贝叶斯解释
│   │   │
│   │   ├── Dropout技术
│   │   │   ├── Dropout原理与实现
│   │   │   ├── 训练vs推理的差异
│   │   │   ├── DropConnect变种
│   │   │   ├── Dropout的集成解释
│   │   │   └── 自适应Dropout
│   │   │
│   │   ├── 数据增强策略
│   │   │   ├── 图像增强技术
│   │   │   ├── 文本增强方法
│   │   │   ├── Mixup与CutMix
│   │   │   └── 自动数据增强
│   │   │
│   │   └── 实践项目
│   │       ├── 过拟合实验与对策
│   │       ├── Dropout效果分析
│   │       ├── 正则化强度调优
│   │       └── 集成方法实现
│   │
│   ├── 3.7 深度网络训练技巧 ⭐⭐⭐ 实战经验
│   │   │
│   │   ├── 权重初始化策略
│   │   │   ├── 随机初始化问题
│   │   │   ├── Xavier初始化推导
│   │   │   ├── He初始化（ReLU网络）
│   │   │   ├── LSUV初始化
│   │   │   └── 预训练初始化
│   │   │
│   │   ├── Batch Normalization
│   │   │   ├── Internal Covariate Shift
│   │   │   ├── BN的前向与反向传播
│   │   │   ├── 训练与推理的差异
│   │   │   ├── BN的正则化效果
│   │   │   └── Layer Norm等变种
│   │   │
│   │   ├── 残差连接深入
│   │   │   ├── ResNet的成功原因
│   │   │   ├── 恒等映射的重要性
│   │   │   ├── 残差块设计变种
│   │   │   └── DenseNet密集连接
│   │   │
│   │   ├── 学习率调度
│   │   │   ├── Step Decay
│   │   │   ├── Exponential Decay
│   │   │   ├── Cosine Annealing
│   │   │   ├── Warm Restart
│   │   │   └── One Cycle Policy
│   │   │
│   │   └── 实践项目
│   │       ├── 初始化方法对比
│   │       ├── BN实现与效果
│   │       ├── 残差网络构建
│   │       └── 训练监控系统
│   │
│   ├── 3.8 实战项目：深度学习框架 ⭐⭐⭐⭐ 综合应用
│   │   │
│   │   ├── 框架设计
│   │   │   ├── 张量类设计
│   │   │   ├── 自动微分系统
│   │   │   ├── 层抽象设计
│   │   │   └── 优化器接口
│   │   │
│   │   ├── 核心功能实现
│   │   │   ├── 前向传播引擎
│   │   │   ├── 反向传播引擎
│   │   │   ├── 梯度累积与清零
│   │   │   └── 参数更新机制
│   │   │
│   │   ├── 实用模块
│   │   │   ├── 数据加载器
│   │   │   ├── 模型保存与加载
│   │   │   ├── 训练循环封装
│   │   │   └── 评估指标计算
│   │   │
│   │   └── 应用示例
│   │       ├── MNIST手写识别
│   │       ├── CIFAR-10分类
│   │       ├── 回归问题求解
│   │       └── 自编码器实现
│   │
│   ├── 3.9 PyTorch基础与实践 ⭐⭐⭐
│   │   ├── 张量操作与自动微分
│   │   ├── nn.Module设计模式
│   │   ├── 数据加载与预处理
│   │   ├── 模型训练流程
│   │   └── 保存与加载模型
│   │
│   └── 3.10 理解深度学习的本质 💡
│       ├── 为什么深度网络更强大
│       ├── 表示学习的力量
│       ├── 端到端学习的优势
│       └── 深度学习的局限性
│
├── 【阶段4】深度学习核心架构 ⭐⭐⭐⭐ 掌握主流模型
│   │
│   ├── 4.1 卷积神经网络 (CNN) ⭐⭐⭐
│   │   │
│   │   ├── CNN基础原理
│   │   │   ├── 卷积操作与特征提取
│   │   │   ├── 池化层与下采样
│   │   │   ├── 感受野概念
│   │   │   └── 参数共享与平移不变性
│   │   │
│   │   ├── 经典CNN架构
│   │   │   ├── LeNet-5 (手写识别)
│   │   │   ├── AlexNet (ImageNet突破)
│   │   │   ├── VGGNet (深度探索)
│   │   │   ├── GoogLeNet/Inception (多尺度)
│   │   │   ├── ResNet (残差学习) ⭐
│   │   │   ├── DenseNet (密集连接)
│   │   │   └── EfficientNet (复合缩放)
│   │   │
│   │   └── CNN实践
│   │       ├── 图像分类任务
│   │       ├── 迁移学习应用
│   │       ├── 数据增强技术
│   │       └── 特征可视化
│   │
│   ├── 4.2 循环神经网络 (RNN/LSTM/GRU) ⭐⭐⭐
│   │   │
│   │   ├── RNN基础
│   │   │   ├── 序列建模原理
│   │   │   ├── 隐状态与记忆
│   │   │   ├── BPTT算法
│   │   │   └── 梯度消失/爆炸问题
│   │   │
│   │   ├── LSTM与GRU
│   │   │   ├── LSTM门控机制
│   │   │   ├── GRU简化设计
│   │   │   ├── 双向RNN
│   │   │   └── 多层RNN架构
│   │   │
│   │   └── 序列建模应用
│   │       ├── 语言模型
│   │       ├── Seq2Seq架构
│   │       ├── 注意力机制引入
│   │       └── 时间序列预测
│   │
│   ├── 4.3 Transformer架构 ⭐⭐⭐⭐⭐
│   │   │
│   │   ├── 自注意力机制
│   │   │   ├── Scaled Dot-Product Attention
│   │   │   ├── Multi-Head Attention
│   │   │   ├── 位置编码设计
│   │   │   └── 层归一化与残差连接
│   │   │
│   │   ├── Transformer架构
│   │   │   ├── Encoder-Decoder结构
│   │   │   ├── 仅编码器模型 (BERT系)
│   │   │   ├── 仅解码器模型 (GPT系)
│   │   │   └── 编码器-解码器模型 (T5/BART)
│   │   │
│   │   ├── 预训练范式
│   │   │   ├── 掩码语言模型 (MLM)
│   │   │   ├── 自回归语言模型
│   │   │   ├── 去噪自编码
│   │   │   └── 对比学习预训练
│   │   │
│   │   └── Transformer实践
│   │       ├── 从零实现Transformer
│   │       ├── Hugging Face使用
│   │       ├── 微调技术 (Full/LoRA/Adapter)
│   │       └── 推理优化
│   │
│   ├── 4.4 生成模型基础 ⭐⭐⭐⭐
│   │   │
│   │   ├── 自编码器 (AE)
│   │   │   ├── 基础自编码器
│   │   │   ├── 去噪自编码器
│   │   │   ├── 稀疏自编码器
│   │   │   └── 收缩自编码器
│   │   │
│   │   ├── 变分自编码器 (VAE)
│   │   │   ├── VAE原理与推导
│   │   │   ├── 重参数化技巧
│   │   │   ├── ELBO优化目标
│   │   │   ├── β-VAE与解耦表示
│   │   │   └── VQ-VAE量化表示
│   │   │
│   │   ├── 生成对抗网络 (GAN)
│   │   │   ├── GAN基本原理
│   │   │   ├── 训练不稳定性与技巧
│   │   │   ├── DCGAN (深度卷积GAN)
│   │   │   ├── WGAN (Wasserstein GAN)
│   │   │   ├── StyleGAN系列
│   │   │   └── 条件GAN变种
│   │   │
│   │   └── 扩散模型 (Diffusion)
│   │       ├── DDPM原理
│   │       ├── DDIM加速采样
│   │       ├── Score-based模型
│   │       └── Stable Diffusion架构
│   │
│   ├── 4.5 图神经网络 (GNN) ⭐⭐
│   │   │
│   │   ├── 图基础概念
│   │   │   ├── 图的表示方法
│   │   │   ├── 邻接矩阵与度矩阵
│   │   │   └── 图上的信号处理
│   │   │
│   │   ├── 主流GNN模型
│   │   │   ├── GCN (图卷积网络)
│   │   │   ├── GraphSAGE (采样聚合)
│   │   │   ├── GAT (图注意力网络)
│   │   │   └── GIN (图同构网络)
│   │   │
│   │   └── GNN应用
│   │       ├── 节点分类
│   │       ├── 链接预测
│   │       ├── 图分类
│   │       └── 分子性质预测
│   │
│   └── 4.6 深度强化学习基础 ⭐⭐
│       │
│       ├── 强化学习基础
│       │   ├── MDP框架
│       │   ├── 价值函数与策略
│       │   ├── Q-Learning
│       │   └── Policy Gradient
│       │
│       ├── 深度强化学习
│       │   ├── DQN及其变种
│       │   ├── A3C/A2C
│       │   ├── PPO算法 ⭐
│       │   └── SAC算法
│       │
│       └── RL应用场景
│           ├── 游戏AI
│           ├── 机器人控制
│           ├── RLHF (LLM对齐)
│           └── 推荐系统优化
│├── 【阶段4.5】深度学习高级技巧 ⭐⭐ 提升性能
│   │
│   ├── 训练技巧
│   │   ├── 学习率调度 (Warmup/Cosine/OneCycle)
│   │   ├── 数据增强 (Mixup/CutMix/AutoAugment)
│   │   ├── 正则化技术 (Dropout/DropConnect/Stochastic Depth)
│   │   └── 梯度技巧 (Gradient Clipping/Accumulation)
│   │
│   ├── 模型优化
│   │   ├── 知识蒸馏
│   │   ├── 模型剪枝
│   │   ├── 量化技术 (INT8/INT4)
│   │   └── 神经架构搜索 (NAS)
│   │
│   └── 工程实践
│       ├── 混合精度训练
│       ├── 分布式训练
│       ├── 模型部署优化
│       └── 推理加速技术
│├── 【阶段5】深度学习三大应用方向 ⭐⭐⭐⭐⭐ 专业深化
│   │
│   ├── 5.1 计算机视觉 (Computer Vision) ⭐⭐⭐⭐
│   │   │
│   │   ├── 图像分类
│   │   │   ├── 经典CNN应用
│   │   │   ├── Vision Transformer (ViT)
│   │   │   ├── Swin Transformer
│   │   │   └── ConvNeXt (现代CNN)
│   │   │
│   │   ├── 目标检测
│   │   │   ├── 两阶段检测器 (R-CNN系列)
│   │   │   ├── 单阶段检测器 (YOLO/SSD)
│   │   │   ├── DETR (Transformer检测)
│   │   │   └── 实时检测优化
│   │   │
│   │   ├── 图像分割
│   │   │   ├── 语义分割 (FCN/U-Net/DeepLab)
│   │   │   ├── 实例分割 (Mask R-CNN)
│   │   │   ├── 全景分割
│   │   │   └── SAM (Segment Anything)
│   │   │
│   │   ├── 3D视觉
│   │   │   ├── 深度估计
│   │   │   ├── 3D重建
│   │   │   ├── NeRF (神经辐射场)
│   │   │   └── 3D Gaussian Splatting
│   │   │
│   │   └── CV中的强化学习
│   │       ├── 视觉导航
│   │       ├── 主动视觉
│   │       └── 机器人视觉控制
│   │
│   ├── 5.2 自然语言处理 (NLP) ⭐⭐⭐⭐
│   │   │
│   │   ├── NLP基础任务
│   │   │   ├── 文本分类
│   │   │   ├── 命名实体识别 (NER)
│   │   │   ├── 关系抽取
│   │   │   └── 情感分析
│   │   │
│   │   ├── 预训练语言模型
│   │   │   ├── BERT及其变种
│   │   │   ├── GPT系列演进
│   │   │   ├── T5/BART
│   │   │   └── 多语言模型 (mBERT/XLM-R)
│   │   │
│   │   ├── 大语言模型 (LLM)
│   │   │   ├── LLM原理与架构
│   │   │   ├── 指令微调 (Instruction Tuning)
│   │   │   ├── 上下文学习 (In-Context Learning)
│   │   │   ├── 思维链 (Chain-of-Thought)
│   │   │   └── 提示工程 (Prompt Engineering)
│   │   │
│   │   ├── LLM对齐技术
│   │   │   ├── RLHF (人类反馈强化学习) ⭐
│   │   │   ├── DPO (直接偏好优化)
│   │   │   ├── Constitutional AI
│   │   │   └── RLAIF (AI反馈强化学习)
│   │   │
│   │   └── RAG与Agent系统
│   │       ├── 检索增强生成 (RAG)
│   │       ├── 向量数据库应用
│   │       ├── LangChain框架
│   │       ├── Function Calling
│   │       └── Multi-Agent系统
│   │
│   ├── 5.3 生成式AI ⭐⭐⭐⭐
│   │   │
│   │   ├── 文本生成
│   │   │   ├── 语言模型微调
│   │   │   ├── 可控文本生成
│   │   │   ├── 对话系统
│   │   │   └── 创意写作应用
│   │   │
│   │   ├── 图像生成
│   │   │   ├── GAN应用 (StyleGAN)
│   │   │   ├── 扩散模型 (Stable Diffusion)
│   │   │   ├── DALL-E系列
│   │   │   └── Midjourney原理
│   │   │
│   │   ├── 可控生成技术
│   │   │   ├── ControlNet
│   │   │   ├── IP-Adapter
│   │   │   ├── LoRA for SD
│   │   │   ├── DreamBooth
│   │   │   └── Textual Inversion
│   │   │
│   │   ├── 视频生成
│   │   │   ├── 视频扩散模型
│   │   │   ├── Sora技术原理
│   │   │   ├── Runway Gen系列
│   │   │   └── 动画生成
│   │   │
│   │   ├── 音频生成
│   │   │   ├── 语音合成 (TTS)
│   │   │   ├── 音乐生成 (MusicGen)
│   │   │   ├── 语音克隆
│   │   │   └── 音频编辑
│   │   │
│   │   └── 生成模型中的强化学习
│   │       ├── GAN训练稳定性
│   │       ├── 奖励模型设计
│   │       └── 人类偏好学习
│   │
│   └── 5.4 实践项目库
│       ├── CV项目：智能相册系统
│       ├── NLP项目：智能问答助手
│       ├── 生成项目：AI创作平台
│       └── 综合项目：多模态内容理解
│├── 【阶段6】多模态融合与专精深化 ⭐⭐⭐ 三向汇聚
│   │
│   ├── 6.1 CLIP深入 (对比学习统一图文) ⭐⭐⭐
│   │   ├── CLIP原理详解
│   │   ├── 对比学习 (Contrastive Learning)
│   │   ├── 图文相似度计算
│   │   ├── Zero-Shot图像分类
│   │   ├── 图像检索系统
│   │   ├── CLIP微调与领域适配
│   │   └── 应用: 以图搜图、图文匹配
│   │
│   ├── 6.2 视觉语言模型 (VLM)
│   │   ├── Image Captioning (图像描述)
│   │   │   ├── BLIP/BLIP-2架构
│   │   │   ├── 自动图像描述生成
│   │   │   └── 视频描述生成
│   │   │
│   │   ├── Visual Question Answering (VQA)
│   │   │   ├── VQA任务理解
│   │   │   ├── Flamingo架构
│   │   │   ├── 实现VQA系统
│   │   │   └── 多模态推理
│   │   │
│   │   └── GPT-4V级应用
│   │       ├── LLaVA (Large Language and Vision Assistant)
│   │       ├── 视觉指令微调
│   │       ├── 多模态对话系统
│   │       └── OCR + 理解
│   │
│   ├── 6.3 多模态生成
│   │   ├── 文生图 (Text-to-Image)
│   │   │   └── Stable Diffusion/DALL-E应用
│   │   ├── 图生文 (Image-to-Text)
│   │   │   └── BLIP/Image Captioning
│   │   ├── 图像编辑
│   │   │   ├── InstructPix2Pix
│   │   │   ├── ControlNet应用
│   │   │   └── IP-Adapter
│   │   └── 视频理解与生成
│   │       ├── Video Transformers
│   │       ├── Runway/Pika原理
│   │       └── 视频编辑
│   │
│   ├── 6.4 综合项目
│   │   │
│   │   ├── 项目1: 智能内容创作平台
│   │   │   ├── 文本生成 (GPT)
│   │   │   ├── 图像生成 (Stable Diffusion)
│   │   │   ├── 图文匹配 (CLIP)
│   │   │   ├── 图像编辑 (ControlNet)
│   │   │   └── Web界面 (Gradio/Streamlit)
│   │   │
│   │   ├── 项目2: 多模态AI助手
│   │   │   ├── 视觉理解 (ViT/CLIP)
│   │   │   ├── 文本对话 (LLM + RAG)
│   │   │   ├── 图像生成/编辑
│   │   │   ├── 视频理解
│   │   │   └── 多轮对话管理
│   │   │
│   │   └── 项目3: 垂直领域应用
│   │       ├── 选择领域 (医疗/教育/电商/设计)
│   │       ├── 整合NLP+CV+生成式AI
│   │       ├── 端到端解决方案
│   │       └── 生产级部署
│   │
│   └── 6.5 选择1-2个方向专精深化
│       │
│       ├── NLP专精路线（7-9个月）
│       │   │
│       │   ├── 1. 大模型从零预训练（3-4个月）
│       │   │   ├── 理论基础
│       │   │   │   ├── Scaling Laws（模型规模定律）
│       │   │   │   ├── 分词器设计（BPE、WordPiece、SentencePiece）
│       │   │   │   ├── 预训练目标设计（MLM、CLM、Span Corruption）
│       │   │   │   └── 数据处理pipeline（Common Crawl、数据清洗）
│       │   │   │
│       │   │   ├── 实践项目
│       │   │   │   ├── 从零训练一个1B参数的语言模型
│       │   │   │   ├── 构建专属领域的预训练模型
│       │   │   │   ├── 多语言模型训练
│       │   │   │   └── 持续预训练技术
│       │   │   │
│       │   │   └── 工具与框架
│       │   │       ├── Megatron-LM
│       │   │       ├── DeepSpeed
│       │   │       ├── FairScale
│       │   │       └── Flash Attention实现
│       │   │
│       │   ├── 2. RLHF/DPO对齐技术（2-3个月）
│       │   │   ├── 深入理解
│       │   │   │   ├── 奖励模型训练
│       │   │   │   ├── PPO在LLM中的应用
│       │   │   │   ├── Constitutional AI原理
│       │   │   │   └── DPO vs RLHF对比
│       │   │   │
│       │   │   └── 实战项目
│       │   │       ├── 实现完整RLHF pipeline
│       │   │       ├── 构建人类偏好数据集
│       │   │       ├── 对比不同对齐方法效果
│       │   │       └── 安全性与有用性平衡
│       │   │
│       │   └── 3. 长文本处理（2个月）
│       │       └── 技术方案
│       │           ├── Longformer的局部+全局注意力
│       │           ├── BigBird的稀疏注意力
│       │           ├── Flash Attention优化
│       │           ├── RoPE位置编码扩展
│       │           └── 流式处理技术
│       │
│       ├── CV专精路线（8-10个月）
│       │   │
│       │   ├── 1. 视频理解深入（3个月）
│       │   │   ├── 视频建模
│       │   │   │   ├── 3D CNN（C3D、I3D）
│       │   │   │   ├── Video Transformer（TimeSformer、ViViT）
│       │   │   │   ├── 时空特征提取
│       │   │   │   └── 动作识别与检测
│       │   │   │
│       │   │   └── 实践应用
│       │   │       ├── 视频分类与标注
│       │   │       ├── 视频摘要生成
│       │   │       ├── 异常行为检测
│       │   │       └── 实时视频分析
│       │   │
│       │   ├── 2. 3D视觉（3-4个月）
│       │   │   ├── NeRF系列
│       │   │   │   ├── 原始NeRF原理与实现
│       │   │   │   ├── Instant-NGP（加速版）
│       │   │   │   ├── Mip-NeRF（多尺度）
│       │   │   │   └── NeRF in the Wild
│       │   │   │
│       │   │   ├── 3D Gaussian Splatting
│       │   │   │   ├── 点云表示与渲染
│       │   │   │   ├── 实时渲染优化
│       │   │   │   ├── 场景编辑技术
│       │   │   │   └── 与NeRF对比
│       │   │   │
│       │   │   └── 应用项目
│       │   │       ├── 3D场景重建
│       │   │       ├── 新视角合成
│       │   │       ├── 3D物体生成
│       │   │       └── AR/VR应用
│       │   │
│       │   └── 3. 医疗影像AI（2-3个月）
│       │       ├── 医学图像处理
│       │       │   ├── CT/MRI/X-ray特点
│       │       │   ├── 医学图像分割（U-Net++、nnU-Net）
│       │       │   ├── 病变检测与定位
│       │       │   └── 3D医学图像处理
│       │       │
│       │       └── 实际应用
│       │           ├── 肿瘤检测系统
│       │           ├── 器官分割
│       │           ├── 疾病分类诊断
│       │           └── 影像报告生成
│       │
│       └── 生成式AI专精路线（10-13个月）
│           │
│           ├── 1. 视频生成（Sora级别）（4-5个月）
│           │   ├── 核心技术
│           │   │   ├── Video Diffusion Models
│           │   │   ├── 时序一致性保证
│           │   │   ├── 长视频生成策略
│           │   │   └── 条件控制机制
│           │   │
│           │   ├── 前沿模型研究
│           │   │   ├── Sora架构分析
│           │   │   ├── Runway Gen-2/Gen-3
│           │   │   ├── Pika Labs技术
│           │   │   └── Stable Video Diffusion
│           │   │
│           │   └── 实战项目
│           │       ├── 文本到视频生成
│           │       ├── 图像到视频动画
│           │       ├── 视频编辑与修复
│           │       └── 风格化视频生成
│           │
│           ├── 2. 3D生成（3个月）
│           │   ├── 3D生成模型
│           │   │   ├── Point-E（点云生成）
│           │   │   ├── Shap-E（mesh生成）
│           │   │   ├── DreamFusion（文本到3D）
│           │   │   └── Magic3D、ProlificDreamer
│           │   │
│           │   └── 技术实现
│           │       ├── 3D表示学习（点云、mesh、体素）
│           │       ├── 多视角一致性
│           │       ├── 纹理生成
│           │       └── 3D编辑技术
│           │
│           ├── 3. 音频生成（2-3个月）
│           │   ├── 核心技术
│           │   │   ├── MusicGen架构
│           │   │   ├── AudioLDM原理
│           │   │   ├── Bark（语音生成）
│           │   │   └── MusicLM技术
│           │   │
│           │   └── 应用开发
│           │       ├── 音乐创作助手
│           │       ├── 语音克隆系统
│           │       ├── 音效生成
│           │       └── 音频修复增强
│           │
│           └── 4. 可控生成技术深化（3个月）
│               ├── 精确控制方法
│               │   ├── ControlNet深入
│               │   ├── IP-Adapter进阶
│               │   ├── 多条件融合
│               │   └── 区域控制技术
│               │
│               └── 高级应用
│                   ├── 细粒度属性控制
│                   ├── 风格迁移精确控制
│                   ├── 组合式生成
│                   └── 交互式编辑系统
│
└── 【阶段7】工程化与持续提升 (可选)
    │
    ├── 7.1 MLOps与部署
    │   ├── 模型部署 (Flask/FastAPI)
    │   ├── Docker容器化
    │   ├── ONNX/TensorRT优化
    │   ├── 模型监控与A/B测试
    │   └── 生产环境最佳实践
    │
    ├── 7.2 分布式训练
    │   ├── PyTorch DDP
    │   ├── DeepSpeed
    │   ├── FSDP (Fully Sharded Data Parallel)
    │   └── 多GPU/多节点训练
    │
    ├── 7.3 前沿技术跟进
    │   ├── 阅读最新论文 (arXiv)
    │   ├── 复现SOTA模型
    │   ├── 参加Kaggle竞赛
    │   └── 开源项目贡献
    │
    └── 7.4 持续深化
        ├── 选定领域成为专家
        ├── 技术博客分享
        ├── 学术会议参与
        └── 大型项目主导
```

---

## 📍 如何使用本路线图

### 学习阶段概览
- **阶段0**: 数学基础 (可选)
- **阶段1**: 传统ML ✅ 已完成
- **阶段2**: 无监督学习 🚀 当前
- **阶段3**: 神经网络与深度学习基础
- **阶段4**: 深度学习核心架构 (CNN/RNN/Transformer/生成模型)
- **阶段4.5**: 深度学习高级技巧 (穿插学习)
- **阶段5**: 深度学习三大应用方向 (CV/NLP/生成式AI)
- **阶段6**: 多模态融合与专精深化
- **阶段7**: 工程化与持续提升

### 8条学习路径快速导航
根据你的目标，选择最适合的路径：
- **路径A**：生成式AI方向 → 适合对ChatGPT/Midjourney感兴趣
- **路径B**：计算机视觉方向 → 适合对自动驾驶/医疗影像感兴趣
- **路径C**：NLP方向 → 适合对文本分析/聊天机器人感兴趣
- **路径D**：全栈AI工程师 → 适合对模型部署/MLOps感兴趣
- **路径E**：学术研究方向 → 适合准备读博/发论文
- **路径F**：强化学习方向 → 适合对游戏AI/机器人感兴趣
- **路径G**：AutoML快速应用 → 适合急需上手/业务导向
- **路径H**：三向并进（您的目标）⭐ → NLP+CV+生成式AI全面发展

### 使用建议
1. **先看树状图**：了解当前阶段在整体技术栈中的位置
2. **再看时间线**：制定具体的学习计划和时间安排
3. **选择路径**：根据目标从8条路径中选择重点方向
4. **定期复盘**：每月回顾进度，调整学习策略

---
