{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# æœ´ç´ è´å¶æ–¯ (Naive Bayes)\n",
    "\n",
    "## ðŸ“š æœ¬èŠ‚ç›®æ ‡\n",
    "- ç†è§£è´å¶æ–¯å®šç†å’Œæœ´ç´ è´å¶æ–¯å‡è®¾\n",
    "- æŽŒæ¡ä¸‰ç§å¸¸è§çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨\n",
    "- å­¦ä¹ æ–‡æœ¬åˆ†ç±»åº”ç”¨\n",
    "- ä»Žé›¶å®žçŽ°æœ´ç´ è´å¶æ–¯\n",
    "- ä½¿ç”¨Scikit-learnçš„æœ´ç´ è´å¶æ–¯æ¨¡åž‹\n",
    "- ç†è§£æœ´ç´ è´å¶æ–¯çš„ä¼˜ç¼ºç‚¹\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ è´å¶æ–¯å®šç†\n",
    "\n",
    "### è´å¶æ–¯å…¬å¼\n",
    "$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $P(y|X)$: **åŽéªŒæ¦‚çŽ‡** - ç»™å®šç‰¹å¾Xï¼Œæ ·æœ¬å±žäºŽç±»åˆ«yçš„æ¦‚çŽ‡\n",
    "- $P(X|y)$: **ä¼¼ç„¶** - ç±»åˆ«yä¸‹è§‚å¯Ÿåˆ°ç‰¹å¾Xçš„æ¦‚çŽ‡\n",
    "- $P(y)$: **å…ˆéªŒæ¦‚çŽ‡** - ç±»åˆ«yçš„æ¦‚çŽ‡\n",
    "- $P(X)$: **è¯æ®** - è§‚å¯Ÿåˆ°ç‰¹å¾Xçš„æ¦‚çŽ‡\n",
    "\n",
    "### æœ´ç´ è´å¶æ–¯å‡è®¾\n",
    "**\"æœ´ç´ \"**ï¼šå‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹\n",
    "\n",
    "$$P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y)$$\n",
    "\n",
    "è™½ç„¶è¿™ä¸ªå‡è®¾åœ¨çŽ°å®žä¸­å¾ˆå°‘æˆç«‹ï¼Œä½†æœ´ç´ è´å¶æ–¯åœ¨å®žè·µä¸­æ•ˆæžœå‡ºå¥‡åœ°å¥½ï¼\n",
    "\n",
    "### åˆ†ç±»å†³ç­–\n",
    "é€‰æ‹©åŽéªŒæ¦‚çŽ‡æœ€å¤§çš„ç±»åˆ«ï¼š\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i|y)$$\n",
    "\n",
    "### ä¸‰ç§å¸¸è§çš„æœ´ç´ è´å¶æ–¯\n",
    "1. **é«˜æ–¯æœ´ç´ è´å¶æ–¯** (GaussianNB)\n",
    "   - ç‰¹å¾æœä»Žæ­£æ€åˆ†å¸ƒ\n",
    "   - é€‚ç”¨äºŽè¿žç»­ç‰¹å¾\n",
    "\n",
    "2. **å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯** (MultinomialNB)\n",
    "   - ç‰¹å¾ä¸ºç¦»æ•£è®¡æ•°\n",
    "   - é€‚ç”¨äºŽæ–‡æœ¬åˆ†ç±»ï¼ˆè¯é¢‘ï¼‰\n",
    "\n",
    "3. **ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯** (BernoulliNB)\n",
    "   - ç‰¹å¾ä¸ºäºŒå…ƒï¼ˆ0/1ï¼‰\n",
    "   - é€‚ç”¨äºŽæ–‡æœ¬åˆ†ç±»ï¼ˆè¯æ˜¯å¦å‡ºçŽ°ï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## ðŸ“¦ å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# sklearnç›¸å…³\n",
    "from sklearn.datasets import load_iris, make_classification, fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# æ–‡æœ¬å¤„ç†\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# å¯è§†åŒ–è®¾ç½®\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "from-scratch",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ä»Žé›¶å®žçŽ°é«˜æ–¯æœ´ç´ è´å¶æ–¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gnb-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    \"\"\"ä»Žé›¶å®žçŽ°çš„é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.priors = {}  # å…ˆéªŒæ¦‚çŽ‡ P(y)\n",
    "        self.means = {}   # æ¯ä¸ªç±»åˆ«æ¯ä¸ªç‰¹å¾çš„å‡å€¼\n",
    "        self.stds = {}    # æ¯ä¸ªç±»åˆ«æ¯ä¸ªç‰¹å¾çš„æ ‡å‡†å·®\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"è®­ç»ƒæ¨¡åž‹\"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        for c in self.classes:\n",
    "            # èŽ·å–ç±»åˆ«cçš„æ‰€æœ‰æ ·æœ¬\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # è®¡ç®—å…ˆéªŒæ¦‚çŽ‡ P(y=c)\n",
    "            self.priors[c] = len(X_c) / n_samples\n",
    "            \n",
    "            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®\n",
    "            self.means[c] = X_c.mean(axis=0)\n",
    "            self.stds[c] = X_c.std(axis=0) + 1e-9  # é¿å…é™¤é›¶\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _gaussian_probability(self, x, mean, std):\n",
    "        \"\"\"è®¡ç®—é«˜æ–¯æ¦‚çŽ‡å¯†åº¦\"\"\"\n",
    "        exponent = np.exp(-((x - mean) ** 2) / (2 * std ** 2))\n",
    "        return exponent / (np.sqrt(2 * np.pi) * std)\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"é¢„æµ‹å•ä¸ªæ ·æœ¬\"\"\"\n",
    "        posteriors = []\n",
    "        \n",
    "        for c in self.classes:\n",
    "            # å…ˆéªŒæ¦‚çŽ‡ P(y=c)\n",
    "            prior = np.log(self.priors[c])\n",
    "            \n",
    "            # ä¼¼ç„¶ P(X|y=c) = P(x1|y=c) * P(x2|y=c) * ...\n",
    "            # ä½¿ç”¨logé¿å…æ•°å€¼ä¸‹æº¢\n",
    "            likelihood = np.sum(\n",
    "                np.log(self._gaussian_probability(x, self.means[c], self.stds[c]))\n",
    "            )\n",
    "            \n",
    "            # åŽéªŒæ¦‚çŽ‡ P(y=c|X) âˆ P(X|y=c) * P(y=c)\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        # è¿”å›žåŽéªŒæ¦‚çŽ‡æœ€å¤§çš„ç±»åˆ«\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"é¢„æµ‹å¤šä¸ªæ ·æœ¬\"\"\"\n",
    "        return np.array([self._predict_single(x) for x in X])\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"è®¡ç®—å‡†ç¡®çŽ‡\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-custom",
   "metadata": {},
   "source": [
    "### æµ‹è¯•è‡ªå®šä¹‰é«˜æ–¯æœ´ç´ è´å¶æ–¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-custom-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½irisæ•°æ®é›†\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# åˆ’åˆ†æ•°æ®é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# è®­ç»ƒè‡ªå®šä¹‰æ¨¡åž‹\n",
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# è¯„ä¼°\n",
    "train_score = gnb.score(X_train, y_train)\n",
    "test_score = gnb.score(X_test, y_test)\n",
    "\n",
    "print(\"è‡ªå®šä¹‰é«˜æ–¯æœ´ç´ è´å¶æ–¯ï¼š\")\n",
    "print(f\"è®­ç»ƒé›†å‡†ç¡®çŽ‡: {train_score:.4f}\")\n",
    "print(f\"æµ‹è¯•é›†å‡†ç¡®çŽ‡: {test_score:.4f}\")\n",
    "\n",
    "# å¯¹æ¯”sklearnçš„å®žçŽ°\n",
    "sklearn_gnb = GaussianNB()\n",
    "sklearn_gnb.fit(X_train, y_train)\n",
    "sklearn_score = sklearn_gnb.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nSklearné«˜æ–¯æœ´ç´ è´å¶æ–¯å‡†ç¡®çŽ‡: {sklearn_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sklearn-gnb",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ é«˜æ–¯æœ´ç´ è´å¶æ–¯è¯¦è§£\n",
    "\n",
    "### 3.1 åŸºæœ¬ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gnb-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”ŸæˆäºŒåˆ†ç±»æ•°æ®\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# è®­ç»ƒæ¨¡åž‹\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# é¢„æµ‹\n",
    "y_pred = gnb.predict(X_test)\n",
    "y_proba = gnb.predict_proba(X_test)  # é¢„æµ‹æ¦‚çŽ‡\n",
    "\n",
    "# è¯„ä¼°\n",
    "print(\"é«˜æ–¯æœ´ç´ è´å¶æ–¯æ€§èƒ½ï¼š\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"å‡†ç¡®çŽ‡: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_proba[:, 1]):.4f}\")\n",
    "print(\"\\nåˆ†ç±»æŠ¥å‘Šï¼š\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# æŸ¥çœ‹å­¦åˆ°çš„å‚æ•°\n",
    "print(\"\\nå­¦åˆ°çš„å‚æ•°ï¼š\")\n",
    "print(f\"ç±»å…ˆéªŒæ¦‚çŽ‡: {gnb.class_prior_}\")\n",
    "print(f\"\\nç±»åˆ«0çš„ç‰¹å¾å‡å€¼: {gnb.theta_[0]}\")\n",
    "print(f\"ç±»åˆ«1çš„ç‰¹å¾å‡å€¼: {gnb.theta_[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decision-boundary",
   "metadata": {},
   "source": [
    "### 3.2 å†³ç­–è¾¹ç•Œå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"å†³ç­–è¾¹ç•Œ\"):\n",
    "    \"\"\"ç»˜åˆ¶å†³ç­–è¾¹ç•Œå’Œæ¦‚çŽ‡\"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, levels=20, alpha=0.6, cmap='RdYlBu_r')\n",
    "    plt.colorbar(label='ç±»åˆ«1çš„æ¦‚çŽ‡')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu_r', \n",
    "                edgecolors='black', s=50, alpha=0.8)\n",
    "    plt.xlabel('ç‰¹å¾ 1')\n",
    "    plt.ylabel('ç‰¹å¾ 2')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(gnb, X_train, y_train, \"é«˜æ–¯æœ´ç´ è´å¶æ–¯å†³ç­–è¾¹ç•Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-classification",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ æ–‡æœ¬åˆ†ç±»ï¼šå¤šé¡¹å¼å’Œä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯\n",
    "\n",
    "### 4.1 ç®€å•æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºä¾‹æ–‡æœ¬æ•°æ®\n",
    "texts = [\n",
    "    \"æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£\",\n",
    "    \"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€éƒ¨åˆ†\",\n",
    "    \"æˆ‘å–œæ¬¢å­¦ä¹ Python\",\n",
    "    \"ä»Šå¤©å¤©æ°”å¾ˆå¥½\",\n",
    "    \"æˆ‘å–œæ¬¢æ™´å¤©\",\n",
    "    \"æ˜Žå¤©ä¼šä¸‹é›¨å—\",\n",
    "    \"ç¥žç»ç½‘ç»œå¾ˆå¼ºå¤§\",\n",
    "    \"å·ç§¯ç¥žç»ç½‘ç»œç”¨äºŽå›¾åƒå¤„ç†\",\n",
    "]\n",
    "\n",
    "# æ ‡ç­¾ï¼š0è¡¨ç¤ºæŠ€æœ¯ç±»ï¼Œ1è¡¨ç¤ºå¤©æ°”ç±»\n",
    "labels = [0, 0, 0, 1, 1, 1, 0, 0]\n",
    "\n",
    "# æ–‡æœ¬å‘é‡åŒ–\n",
    "vectorizer = CountVectorizer()\n",
    "X_text = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(\"è¯æ±‡è¡¨ï¼š\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(f\"\\nç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_text.shape}\")\n",
    "print(\"\\nç‰¹å¾çŸ©é˜µï¼ˆè¯é¢‘ï¼‰ï¼š\")\n",
    "print(X_text.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multinomial",
   "metadata": {},
   "source": [
    "### 4.2 å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multinomial-nb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_text, labels)\n",
    "\n",
    "# æµ‹è¯•\n",
    "test_texts = [\n",
    "    \"å­¦ä¹ æœºå™¨å­¦ä¹ \",\n",
    "    \"å¤©æ°”é¢„æŠ¥\",\n",
    "    \"ç¥žç»ç½‘ç»œå­¦ä¹ \"\n",
    "]\n",
    "\n",
    "X_test_text = vectorizer.transform(test_texts)\n",
    "predictions = mnb.predict(X_test_text)\n",
    "probabilities = mnb.predict_proba(X_test_text)\n",
    "\n",
    "print(\"å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯é¢„æµ‹ï¼š\")\n",
    "print(\"=\" * 60)\n",
    "for text, pred, proba in zip(test_texts, predictions, probabilities):\n",
    "    category = \"æŠ€æœ¯\" if pred == 0 else \"å¤©æ°”\"\n",
    "    print(f\"æ–‡æœ¬: '{text}'\")\n",
    "    print(f\"é¢„æµ‹ç±»åˆ«: {category}\")\n",
    "    print(f\"æ¦‚çŽ‡: æŠ€æœ¯={proba[0]:.3f}, å¤©æ°”={proba[1]:.3f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bernoulli",
   "metadata": {},
   "source": [
    "### 4.3 ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bernoulli-nb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ï¼ˆå…³æ³¨è¯æ˜¯å¦å‡ºçŽ°ï¼Œè€Œéžé¢‘çŽ‡ï¼‰\n",
    "bnb = BernoulliNB()\n",
    "# éœ€è¦å°†è®¡æ•°è½¬ä¸ºäºŒå€¼\n",
    "X_binary = (X_text > 0).astype(int)\n",
    "bnb.fit(X_binary, labels)\n",
    "\n",
    "# é¢„æµ‹\n",
    "X_test_binary = (X_test_text > 0).astype(int)\n",
    "predictions_bnb = bnb.predict(X_test_binary)\n",
    "probabilities_bnb = bnb.predict_proba(X_test_binary)\n",
    "\n",
    "print(\"ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯é¢„æµ‹ï¼š\")\n",
    "print(\"=\" * 60)\n",
    "for text, pred, proba in zip(test_texts, predictions_bnb, probabilities_bnb):\n",
    "    category = \"æŠ€æœ¯\" if pred == 0 else \"å¤©æ°”\"\n",
    "    print(f\"æ–‡æœ¬: '{text}'\")\n",
    "    print(f\"é¢„æµ‹ç±»åˆ«: {category}\")\n",
    "    print(f\"æ¦‚çŽ‡: æŠ€æœ¯={proba[0]:.3f}, å¤©æ°”={proba[1]:.3f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "newsgroups",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ å®žæˆ˜ï¼šæ–°é—»åˆ†ç±»\n",
    "\n",
    "ä½¿ç”¨20 Newsgroupsæ•°æ®é›†è¿›è¡Œå¤šç±»åˆ«æ–‡æœ¬åˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-news",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½éƒ¨åˆ†ç±»åˆ«çš„æ–°é—»æ•°æ®\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# åŠ è½½è®­ç»ƒé›†\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# åŠ è½½æµ‹è¯•é›†\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"è®­ç»ƒæ ·æœ¬æ•°: {len(newsgroups_train.data)}\")\n",
    "print(f\"æµ‹è¯•æ ·æœ¬æ•°: {len(newsgroups_test.data)}\")\n",
    "print(f\"ç±»åˆ«: {newsgroups_train.target_names}\")\n",
    "\n",
    "# æŸ¥çœ‹ç¤ºä¾‹\n",
    "print(\"\\nç¤ºä¾‹æ–‡æœ¬ï¼š\")\n",
    "print(newsgroups_train.data[0][:200] + \"...\")\n",
    "print(f\"ç±»åˆ«: {newsgroups_train.target_names[newsgroups_train.target[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "news-vectorize",
   "metadata": {},
   "source": [
    "### æ–‡æœ¬å‘é‡åŒ–å’Œæ¨¡åž‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "news-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDFå‘é‡åŒ–\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "print(f\"TF-IDFç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_train_tfidf.shape}\")\n",
    "\n",
    "# è®­ç»ƒå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯\n",
    "mnb_news = MultinomialNB(alpha=0.1)  # alphaæ˜¯å¹³æ»‘å‚æ•°\n",
    "mnb_news.fit(X_train_tfidf, newsgroups_train.target)\n",
    "\n",
    "# é¢„æµ‹\n",
    "y_pred = mnb_news.predict(X_test_tfidf)\n",
    "\n",
    "# è¯„ä¼°\n",
    "print(\"\\næ–°é—»åˆ†ç±»æ€§èƒ½ï¼š\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"å‡†ç¡®çŽ‡: {accuracy_score(newsgroups_test.target, y_pred):.4f}\")\n",
    "print(\"\\nåˆ†ç±»æŠ¥å‘Šï¼š\")\n",
    "print(classification_report(\n",
    "    newsgroups_test.target, \n",
    "    y_pred, \n",
    "    target_names=newsgroups_test.target_names\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix",
   "metadata": {},
   "source": [
    "### æ··æ·†çŸ©é˜µå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··æ·†çŸ©é˜µ\n",
    "cm = confusion_matrix(newsgroups_test.target, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=newsgroups_test.target_names,\n",
    "            yticklabels=newsgroups_test.target_names)\n",
    "plt.xlabel('é¢„æµ‹ç±»åˆ«')\n",
    "plt.ylabel('çœŸå®žç±»åˆ«')\n",
    "plt.title('æ–°é—»åˆ†ç±»æ··æ·†çŸ©é˜µ')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance",
   "metadata": {},
   "source": [
    "### ç‰¹å¾é‡è¦æ€§åˆ†æž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# èŽ·å–æ¯ä¸ªç±»åˆ«æœ€é‡è¦çš„è¯\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "n_top_words = 10\n",
    "\n",
    "print(\"æ¯ä¸ªç±»åˆ«çš„Topå…³é”®è¯ï¼š\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, category in enumerate(newsgroups_test.target_names):\n",
    "    # èŽ·å–è¯¥ç±»åˆ«çš„logæ¦‚çŽ‡\n",
    "    top_indices = mnb_news.feature_log_prob_[i].argsort()[-n_top_words:][::-1]\n",
    "    top_words = [feature_names[idx] for idx in top_indices]\n",
    "    \n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\", \".join(top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ ä¸‰ç§æœ´ç´ è´å¶æ–¯å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-nb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨CountVectorizerï¼ˆè¯é¢‘ï¼‰\n",
    "count_vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_count = count_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_count = count_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# äºŒå€¼åŒ–\n",
    "X_train_binary = (X_train_count > 0).astype(int)\n",
    "X_test_binary = (X_test_count > 0).astype(int)\n",
    "\n",
    "# è®­ç»ƒä¸‰ç§æ¨¡åž‹\n",
    "models = {\n",
    "    'å¤šé¡¹å¼NB (è¯é¢‘)': MultinomialNB().fit(X_train_count, newsgroups_train.target),\n",
    "    'å¤šé¡¹å¼NB (TF-IDF)': MultinomialNB().fit(X_train_tfidf, newsgroups_train.target),\n",
    "    'ä¼¯åŠªåˆ©NB': BernoulliNB().fit(X_train_binary, newsgroups_train.target),\n",
    "}\n",
    "\n",
    "# è¯„ä¼°\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    if 'TF-IDF' in name:\n",
    "        X_test_use = X_test_tfidf\n",
    "    elif 'ä¼¯åŠªåˆ©' in name:\n",
    "        X_test_use = X_test_binary\n",
    "    else:\n",
    "        X_test_use = X_test_count\n",
    "    \n",
    "    y_pred = model.predict(X_test_use)\n",
    "    accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'æ¨¡åž‹': name,\n",
    "        'å‡†ç¡®çŽ‡': accuracy\n",
    "    })\n",
    "\n",
    "# æ˜¾ç¤ºç»“æžœ\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nä¸‰ç§æœ´ç´ è´å¶æ–¯å¯¹æ¯”ï¼š\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(results_df['æ¨¡åž‹'], results_df['å‡†ç¡®çŽ‡'])\n",
    "plt.xlabel('å‡†ç¡®çŽ‡')\n",
    "plt.title('ä¸‰ç§æœ´ç´ è´å¶æ–¯æ¨¡åž‹æ€§èƒ½å¯¹æ¯”')\n",
    "plt.xlim(0.7, 1.0)\n",
    "for i, v in enumerate(results_df['å‡†ç¡®çŽ‡']):\n",
    "    plt.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoothing",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ å¹³æ»‘å‚æ•°è°ƒä¼˜\n",
    "\n",
    "å¹³æ»‘å‚æ•°(alpha)ç”¨äºŽå¤„ç†è®­ç»ƒé›†ä¸­æœªå‡ºçŽ°çš„ç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä¸åŒçš„alphaå€¼\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    mnb = MultinomialNB(alpha=alpha)\n",
    "    mnb.fit(X_train_tfidf, newsgroups_train.target)\n",
    "    \n",
    "    train_scores.append(mnb.score(X_train_tfidf, newsgroups_train.target))\n",
    "    test_scores.append(mnb.score(X_test_tfidf, newsgroups_test.target))\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(alphas, train_scores, marker='o', label='è®­ç»ƒé›†')\n",
    "plt.plot(alphas, test_scores, marker='s', label='æµ‹è¯•é›†')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (å¹³æ»‘å‚æ•°)')\n",
    "plt.ylabel('å‡†ç¡®çŽ‡')\n",
    "plt.title('å¹³æ»‘å‚æ•°å¯¹æ¨¡åž‹æ€§èƒ½çš„å½±å“')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_alpha = alphas[np.argmax(test_scores)]\n",
    "print(f\"\\næœ€ä¼˜alpha: {best_alpha}\")\n",
    "print(f\"æœ€é«˜æµ‹è¯•å‡†ç¡®çŽ‡: {max(test_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grid-search",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ ç½‘æ ¼æœç´¢ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid-search-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‚æ•°ç½‘æ ¼\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0],\n",
    "    'fit_prior': [True, False]  # æ˜¯å¦å­¦ä¹ ç±»å…ˆéªŒæ¦‚çŽ‡\n",
    "}\n",
    "\n",
    "# ç½‘æ ¼æœç´¢\n",
    "grid_search = GridSearchCV(\n",
    "    MultinomialNB(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_tfidf, newsgroups_train.target)\n",
    "\n",
    "print(\"\\næœ€ä¼˜å‚æ•°ï¼š\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\næœ€ä¼˜äº¤å‰éªŒè¯å‡†ç¡®çŽ‡: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# æµ‹è¯•é›†è¯„ä¼°\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test_tfidf, newsgroups_test.target)\n",
    "print(f\"æµ‹è¯•é›†å‡†ç¡®çŽ‡: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pros-cons",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ æœ´ç´ è´å¶æ–¯çš„ä¼˜ç¼ºç‚¹\n",
    "\n",
    "### âœ… ä¼˜ç‚¹\n",
    "1. **ç®€å•é«˜æ•ˆ**\n",
    "   - è®­ç»ƒå’Œé¢„æµ‹é€Ÿåº¦å¿«\n",
    "   - å†…å­˜å ç”¨å°\n",
    "   \n",
    "2. **é€‚åˆé«˜ç»´æ•°æ®**\n",
    "   - ç‰¹åˆ«é€‚åˆæ–‡æœ¬åˆ†ç±»\n",
    "   - ä¸æ˜“è¿‡æ‹Ÿåˆ\n",
    "   \n",
    "3. **å¯è§£é‡Šæ€§å¼º**\n",
    "   - åŸºäºŽæ¦‚çŽ‡è®ºï¼Œç»“æžœæ˜“ç†è§£\n",
    "   - å¯ä»¥æŸ¥çœ‹ç‰¹å¾é‡è¦æ€§\n",
    "   \n",
    "4. **å°‘é‡æ•°æ®ä¹Ÿèƒ½å·¥ä½œ**\n",
    "   - å¯¹å°æ ·æœ¬å‹å¥½\n",
    "   \n",
    "5. **å¤šåˆ†ç±»å¤©ç„¶æ”¯æŒ**\n",
    "   - ä¸éœ€è¦one-vs-restç­–ç•¥\n",
    "\n",
    "### âŒ ç¼ºç‚¹\n",
    "1. **ç‰¹å¾ç‹¬ç«‹æ€§å‡è®¾**\n",
    "   - çŽ°å®žä¸­ç‰¹å¾å¾€å¾€ç›¸å…³\n",
    "   - ä½†å®žè·µä¸­å½±å“ä¸å¤§\n",
    "   \n",
    "2. **å¯¹ç‰¹å¾åˆ†å¸ƒæ•æ„Ÿ**\n",
    "   - éœ€è¦é€‰æ‹©åˆé€‚çš„æœ´ç´ è´å¶æ–¯ç±»åž‹\n",
    "   - é«˜æ–¯NBè¦æ±‚ç‰¹å¾è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ\n",
    "   \n",
    "3. **é›¶æ¦‚çŽ‡é—®é¢˜**\n",
    "   - éœ€è¦å¹³æ»‘æŠ€æœ¯ï¼ˆLaplaceå¹³æ»‘ï¼‰\n",
    "   \n",
    "4. **ä¸é€‚åˆå›žå½’é—®é¢˜**\n",
    "   - ä¸»è¦ç”¨äºŽåˆ†ç±»\n",
    "\n",
    "### ä½¿ç”¨åœºæ™¯\n",
    "âœ… **é€‚åˆï¼š**\n",
    "- æ–‡æœ¬åˆ†ç±»ï¼ˆåžƒåœ¾é‚®ä»¶è¿‡æ»¤ã€æƒ…æ„Ÿåˆ†æžï¼‰\n",
    "- å®žæ—¶é¢„æµ‹ï¼ˆé€Ÿåº¦è¦æ±‚é«˜ï¼‰\n",
    "- å¤šåˆ†ç±»é—®é¢˜\n",
    "- é«˜ç»´ç¨€ç–æ•°æ®\n",
    "- ä½œä¸ºbaselineæ¨¡åž‹\n",
    "\n",
    "âŒ **ä¸é€‚åˆï¼š**\n",
    "- ç‰¹å¾é«˜åº¦ç›¸å…³çš„æ•°æ®\n",
    "- éœ€è¦ç²¾ç¡®æ¦‚çŽ‡ä¼°è®¡çš„åœºæ™¯\n",
    "- å›žå½’é—®é¢˜\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## ðŸ“Š æ€»ç»“\n",
    "\n",
    "### å…³é”®è¦ç‚¹\n",
    "1. **è´å¶æ–¯å®šç†**ï¼šåŽéªŒ = (ä¼¼ç„¶ Ã— å…ˆéªŒ) / è¯æ®\n",
    "2. **æœ´ç´ å‡è®¾**ï¼šç‰¹å¾æ¡ä»¶ç‹¬ç«‹\n",
    "3. **ä¸‰ç§ç±»åž‹**ï¼š\n",
    "   - é«˜æ–¯NBï¼šè¿žç»­ç‰¹å¾\n",
    "   - å¤šé¡¹å¼NBï¼šç¦»æ•£è®¡æ•°ï¼ˆè¯é¢‘ï¼‰\n",
    "   - ä¼¯åŠªåˆ©NBï¼šäºŒå€¼ç‰¹å¾ï¼ˆè¯å‡ºçŽ°ï¼‰\n",
    "4. **å¹³æ»‘æŠ€æœ¯**ï¼šé˜²æ­¢é›¶æ¦‚çŽ‡\n",
    "5. **æ–‡æœ¬åˆ†ç±»çŽ‹è€…**ï¼šé€Ÿåº¦å¿«ï¼Œæ•ˆæžœå¥½\n",
    "\n",
    "### å®žè·µå»ºè®®\n",
    "1. **æ•°æ®ç±»åž‹å†³å®šNBç±»åž‹**\n",
    "   - è¿žç»­ â†’ é«˜æ–¯NB\n",
    "   - è®¡æ•° â†’ å¤šé¡¹å¼NB\n",
    "   - äºŒå€¼ â†’ ä¼¯åŠªåˆ©NB\n",
    "\n",
    "2. **è°ƒä¼˜é‡ç‚¹**\n",
    "   - å¹³æ»‘å‚æ•°alpha\n",
    "   - æ–‡æœ¬å‘é‡åŒ–æ–¹æ³•ï¼ˆè¯é¢‘ vs TF-IDFï¼‰\n",
    "   - ç‰¹å¾æ•°é‡\n",
    "\n",
    "3. **æ€§èƒ½ä¼˜åŒ–**\n",
    "   - ç‰¹å¾é€‰æ‹©\n",
    "   - åœç”¨è¯å¤„ç†\n",
    "   - N-gramç‰¹å¾\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ ç»ƒä¹ é¢˜\n",
    "\n",
    "1. **åŸºç¡€ç»ƒä¹ **ï¼šä½¿ç”¨é«˜æ–¯æœ´ç´ è´å¶æ–¯åœ¨irisæ•°æ®é›†ä¸Šå®žéªŒï¼Œå¯¹æ¯”ä¸åŒå…ˆéªŒè®¾ç½®çš„æ•ˆæžœ\n",
    "\n",
    "2. **æ–‡æœ¬åˆ†ç±»**ï¼šæž„å»ºä¸€ä¸ªåžƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ï¼Œå¯¹æ¯”ä¸‰ç§æœ´ç´ è´å¶æ–¯çš„æ€§èƒ½\n",
    "\n",
    "3. **è¿›é˜¶ç»ƒä¹ **ï¼šå®žçŽ°å¸¦æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘çš„å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯\n",
    "\n",
    "4. **æŒ‘æˆ˜ç»ƒä¹ **ï¼šåœ¨IMDBç”µå½±è¯„è®ºæ•°æ®é›†ä¸Šè¿›è¡Œæƒ…æ„Ÿåˆ†æžï¼Œä¼˜åŒ–è‡³å‡†ç¡®çŽ‡>85%\n",
    "\n",
    "ä¸‹ä¸€èŠ‚ï¼š[05_logistic_regression.ipynb](05_logistic_regression.ipynb) - é€»è¾‘å›žå½’"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
