{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ 8 ç« ï¼šè¯­è¨€æ¨¡å‹\n",
    "\n",
    "> ç”¨ RNN é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œç”Ÿæˆæ–‡æœ¬\n",
    "\n",
    "---\n",
    "\n",
    "## æœ¬ç« ç›®æ ‡\n",
    "\n",
    "- [ ] ç†è§£è¯­è¨€æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µ\n",
    "- [ ] ç”¨ RNN/LSTM å®ç°å­—ç¬¦çº§è¯­è¨€æ¨¡å‹\n",
    "- [ ] ç†è§£å›°æƒ‘åº¦ (Perplexity) è¯„ä¼°æŒ‡æ ‡\n",
    "- [ ] å®ç°æ–‡æœ¬ç”Ÿæˆï¼ˆé‡‡æ ·ç­–ç•¥ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ä»€ä¹ˆæ˜¯è¯­è¨€æ¨¡å‹ï¼Ÿ\n",
    "\n",
    "è¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ˜¯å­¦ä¹ **åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒ**ï¼š\n",
    "\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_1, ..., w_{t-1})$$\n",
    "\n",
    "ç®€å•è¯´ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯/å­—ç¬¦çš„æ¦‚ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡\n",
    "\n",
    "### 1.1 å­—ç¬¦çº§è¯­è¨€æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset:\n",
    "    \"\"\"\n",
    "    å­—ç¬¦çº§æ•°æ®é›†\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text, seq_len=50):\n",
    "        self.text = text\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # æ„å»ºå­—ç¬¦è¯è¡¨\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # å°†æ–‡æœ¬è½¬ä¸ºç´¢å¼•\n",
    "        self.data = [self.char_to_idx[ch] for ch in text]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + 1:idx + self.seq_len + 1]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æœ¬\n",
    "sample_text = \"\"\"\n",
    "To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles,\n",
    "And by opposing end them. To die: to sleep;\n",
    "No more; and by a sleep to say we end\n",
    "The heart-ache and the thousand natural shocks\n",
    "That flesh is heir to, 'tis a consummation\n",
    "Devoutly to be wish'd. To die, to sleep;\n",
    "To sleep: perchance to dream: ay, there's the rub;\n",
    "\"\"\"\n",
    "\n",
    "dataset = CharDataset(sample_text, seq_len=30)\n",
    "\n",
    "print(f\"æ–‡æœ¬é•¿åº¦: {len(sample_text)}\")\n",
    "print(f\"è¯è¡¨å¤§å°: {dataset.vocab_size}\")\n",
    "print(f\"å­—ç¬¦: {dataset.chars[:20]}...\")\n",
    "\n",
    "# æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬\n",
    "x, y = dataset[0]\n",
    "print(f\"\\nè¾“å…¥: {''.join([dataset.idx_to_char[i.item()] for i in x])}\")\n",
    "print(f\"ç›®æ ‡: {''.join([dataset.idx_to_char[i.item()] for i in y])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹å®ç°\n",
    "\n",
    "### 2.1 å­—ç¬¦çº§ LSTM è¯­è¨€æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    å­—ç¬¦çº§ LSTM è¯­è¨€æ¨¡å‹\n",
    "    \n",
    "    è¾“å…¥: å­—ç¬¦ç´¢å¼•åºåˆ—\n",
    "    è¾“å‡º: ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # å­—ç¬¦åµŒå…¥\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        x: (batch, seq_len) å­—ç¬¦ç´¢å¼•\n",
    "        \"\"\"\n",
    "        # åµŒå…¥\n",
    "        embedded = self.dropout(self.embedding(x))  # (batch, seq, embed)\n",
    "        \n",
    "        # LSTM\n",
    "        output, hidden = self.lstm(embedded, hidden)  # (batch, seq, hidden)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        output = self.dropout(output)\n",
    "        logits = self.fc(output)  # (batch, seq, vocab_size)\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"åˆå§‹åŒ–éšçŠ¶æ€\"\"\"\n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        c = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        return (h, c)\n",
    "\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "model = CharLSTM(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embed_dim=64,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\næ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_language_model(model, dataset, n_epochs=50, batch_size=32, lr=0.002):\n",
    "    \"\"\"\n",
    "    è®­ç»ƒè¯­è¨€æ¨¡å‹\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # å‰å‘ä¼ æ’­\n",
    "            logits, _ = model(x)\n",
    "            \n",
    "            # è®¡ç®—æŸå¤±\n",
    "            # logits: (batch, seq, vocab) â†’ (batch*seq, vocab)\n",
    "            # y: (batch, seq) â†’ (batch*seq,)\n",
    "            loss = criterion(logits.view(-1, dataset.vocab_size), y.view(-1))\n",
    "            \n",
    "            # åå‘ä¼ æ’­\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            perplexity = np.exp(avg_loss)\n",
    "            print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Perplexity = {perplexity:.2f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# è®­ç»ƒ\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "losses = train_language_model(model, dataset, n_epochs=50)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('è¯­è¨€æ¨¡å‹è®­ç»ƒæŸå¤±')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "### 3.1 é‡‡æ ·ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, dataset, start_text, length=100, temperature=1.0):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ–‡æœ¬\n",
    "    \n",
    "    å‚æ•°:\n",
    "        start_text: èµ·å§‹æ–‡æœ¬\n",
    "        length: ç”Ÿæˆé•¿åº¦\n",
    "        temperature: æ¸©åº¦å‚æ•° (è¶Šä½è¶Šç¡®å®šï¼Œè¶Šé«˜è¶Šéšæœº)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # å°†èµ·å§‹æ–‡æœ¬è½¬ä¸ºç´¢å¼•\n",
    "    chars = [dataset.char_to_idx.get(ch, 0) for ch in start_text]\n",
    "    x = torch.tensor([chars]).to(device)\n",
    "    \n",
    "    generated = list(start_text)\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # å…ˆå¤„ç†èµ·å§‹æ–‡æœ¬ï¼Œæ›´æ–°éšçŠ¶æ€\n",
    "        logits, hidden = model(x, hidden)\n",
    "        \n",
    "        # é€å­—ç¬¦ç”Ÿæˆ\n",
    "        for _ in range(length):\n",
    "            # è·å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º\n",
    "            probs = F.softmax(logits[0, -1] / temperature, dim=0)\n",
    "            \n",
    "            # é‡‡æ ·\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = dataset.idx_to_char[next_idx]\n",
    "            generated.append(next_char)\n",
    "            \n",
    "            # å‡†å¤‡ä¸‹ä¸€ä¸ªè¾“å…¥\n",
    "            x = torch.tensor([[next_idx]]).to(device)\n",
    "            logits, hidden = model(x, hidden)\n",
    "    \n",
    "    return ''.join(generated)\n",
    "\n",
    "\n",
    "# æµ‹è¯•ä¸åŒæ¸©åº¦\n",
    "print(\"=\" * 60)\n",
    "print(\"æ–‡æœ¬ç”Ÿæˆï¼ˆä¸åŒæ¸©åº¦ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(\"-\" * 40)\n",
    "    text = generate_text(model, dataset, \"To be\", length=100, temperature=temp)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬å››éƒ¨åˆ†ï¼šå›°æƒ‘åº¦ (Perplexity)\n",
    "\n",
    "### 4.1 å›°æƒ‘åº¦çš„å®šä¹‰\n",
    "\n",
    "å›°æƒ‘åº¦æ˜¯è¯­è¨€æ¨¡å‹çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼š\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log P(w_i|w_{<i})\\right)$$\n",
    "\n",
    "ç›´è§‰ï¼šæ¨¡å‹å¯¹æ¯ä¸ªè¯çš„\"å¹³å‡æƒŠè®¶ç¨‹åº¦\"\n",
    "- å›°æƒ‘åº¦ = 1ï¼šå®Œç¾é¢„æµ‹\n",
    "- å›°æƒ‘åº¦ = Vï¼ˆè¯è¡¨å¤§å°ï¼‰ï¼šéšæœºçŒœæµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataset):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å›°æƒ‘åº¦\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), 32):\n",
    "            batch = [dataset[j] for j in range(i, min(i+32, len(dataset)))]\n",
    "            x = torch.stack([b[0] for b in batch]).to(device)\n",
    "            y = torch.stack([b[1] for b in batch]).to(device)\n",
    "            \n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, dataset.vocab_size), y.view(-1))\n",
    "            \n",
    "            total_loss += loss.item() * x.size(0) * x.size(1)\n",
    "            total_count += x.size(0) * x.size(1)\n",
    "    \n",
    "    avg_loss = total_loss / total_count\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "ppl = compute_perplexity(model, dataset)\n",
    "print(f\"å›°æƒ‘åº¦: {ppl:.2f}\")\n",
    "print(f\"è§£é‡Š: æ¨¡å‹åœ¨é¢„æµ‹æ¯ä¸ªå­—ç¬¦æ—¶ï¼Œå¹³å‡æœ‰ {ppl:.1f} ä¸ªå¯èƒ½çš„é€‰æ‹©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æœ¬ç« æ€»ç»“\n",
    "\n",
    "1. **è¯­è¨€æ¨¡å‹**ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯/å­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ\n",
    "2. **è®­ç»ƒç›®æ ‡**ï¼šæœ€å¤§åŒ–ä¼¼ç„¶ï¼ˆæœ€å°åŒ–äº¤å‰ç†µï¼‰\n",
    "3. **è¯„ä¼°æŒ‡æ ‡**ï¼šå›°æƒ‘åº¦ $\\text{PPL} = \\exp(\\text{Loss})$\n",
    "4. **ç”Ÿæˆç­–ç•¥**ï¼šæ¸©åº¦é‡‡æ ·æ§åˆ¶éšæœºæ€§\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸‹ä¸€ç« é¢„å‘Š\n",
    "\n",
    "**ç¬¬ 9 ç« ï¼šSeq2Seq æ¶æ„**\n",
    "\n",
    "ğŸ‘‰ [09_seq2seq_architecture.ipynb](./09_seq2seq_architecture.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
