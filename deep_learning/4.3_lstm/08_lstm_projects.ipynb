{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ 12 ç« ï¼šRNN ç»¼åˆå®æˆ˜é¡¹ç›®\n",
    "\n",
    "> å°†æ‰€å­¦çŸ¥è¯†åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­\n",
    "\n",
    "---\n",
    "\n",
    "## é¡¹ç›®åˆ—è¡¨\n",
    "\n",
    "1. **æƒ…æ„Ÿåˆ†æ** - BiLSTM æ–‡æœ¬åˆ†ç±»\n",
    "2. **æ–‡æœ¬ç”Ÿæˆ** - å­—ç¬¦çº§è¯­è¨€æ¨¡å‹\n",
    "3. **åºåˆ—æ ‡æ³¨** - BiLSTM-CRF å‘½åå®ä½“è¯†åˆ«\n",
    "4. **æœºå™¨ç¿»è¯‘** - Seq2Seq + Attention\n",
    "5. **è‚¡ç¥¨é¢„æµ‹** - LSTM æ—¶é—´åºåˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## é¡¹ç›®ä¸€ï¼šæƒ…æ„Ÿåˆ†æ\n",
    "\n",
    "### 1.1 æ¨¡å‹å®šä¹‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    åŸºäº BiLSTM çš„æƒ…æ„Ÿåˆ†ç±»å™¨\n",
    "    \n",
    "    ç»“æ„: Embedding â†’ BiLSTM â†’ Attention â†’ FC â†’ Softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, \n",
    "                 num_layers=2, dropout=0.3, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # è¯åµŒå…¥\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        \n",
    "        # åŒå‘ LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_size, num_layers,\n",
    "            bidirectional=True, batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # æ³¨æ„åŠ›å±‚\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        \n",
    "        # åˆ†ç±»å±‚\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) è¯ç´¢å¼•\n",
    "        \"\"\"\n",
    "        # è¯åµŒå…¥\n",
    "        embedded = self.embedding(x)  # (batch, seq, embed)\n",
    "        \n",
    "        # BiLSTM\n",
    "        output, _ = self.lstm(embedded)  # (batch, seq, hidden*2)\n",
    "        \n",
    "        # æ³¨æ„åŠ›\n",
    "        attn_weights = F.softmax(self.attention(output).squeeze(-1), dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), output).squeeze(1)\n",
    "        \n",
    "        # åˆ†ç±»\n",
    "        logits = self.fc(context)\n",
    "        return logits, attn_weights\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "model = SentimentClassifier(\n",
    "    vocab_size=10000, embed_dim=128, hidden_size=256, num_classes=2\n",
    ")\n",
    "\n",
    "# æ¨¡æ‹Ÿè¾“å…¥\n",
    "x = torch.randint(0, 10000, (4, 50))\n",
    "logits, attn = model(x)\n",
    "\n",
    "print(f\"è¾“å‡ºå½¢çŠ¶: {logits.shape}\")\n",
    "print(f\"æ³¨æ„åŠ›å½¢çŠ¶: {attn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## é¡¹ç›®äºŒï¼šåå­—ç”Ÿæˆ\n",
    "\n",
    "### 2.1 å­—ç¬¦çº§ RNN ç”Ÿæˆåå­—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    å­—ç¬¦çº§åå­—ç”Ÿæˆå™¨\n",
    "    \n",
    "    å¯ä»¥æ ¹æ®ç±»åˆ«ï¼ˆå¦‚å›½å®¶ï¼‰ç”Ÿæˆå¯¹åº”é£æ ¼çš„åå­—\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_categories, vocab_size, embed_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # ç±»åˆ«åµŒå…¥\n",
    "        self.category_embedding = nn.Embedding(n_categories, embed_dim)\n",
    "        \n",
    "        # å­—ç¬¦åµŒå…¥\n",
    "        self.char_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # GRU (è¾“å…¥ = ç±»åˆ« + å­—ç¬¦)\n",
    "        self.gru = nn.GRU(embed_dim * 2, hidden_size, batch_first=True)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, category, chars, hidden=None):\n",
    "        \"\"\"\n",
    "        category: (batch,) ç±»åˆ«ç´¢å¼•\n",
    "        chars: (batch, seq_len) å­—ç¬¦ç´¢å¼•\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = chars.shape\n",
    "        \n",
    "        # ç±»åˆ«åµŒå…¥ï¼ˆæ‰©å±•åˆ°æ‰€æœ‰æ—¶é—´æ­¥ï¼‰\n",
    "        cat_emb = self.category_embedding(category)  # (batch, embed)\n",
    "        cat_emb = cat_emb.unsqueeze(1).expand(-1, seq_len, -1)  # (batch, seq, embed)\n",
    "        \n",
    "        # å­—ç¬¦åµŒå…¥\n",
    "        char_emb = self.char_embedding(chars)  # (batch, seq, embed)\n",
    "        \n",
    "        # æ‹¼æ¥\n",
    "        combined = torch.cat([cat_emb, char_emb], dim=2)  # (batch, seq, embed*2)\n",
    "        \n",
    "        # GRU\n",
    "        output, hidden = self.gru(combined, hidden)\n",
    "        \n",
    "        # é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦\n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def generate(self, category, start_char, max_len=20, temperature=1.0):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆåå­—\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        generated = [start_char]\n",
    "        hidden = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_len):\n",
    "                char_tensor = torch.tensor([[generated[-1]]])\n",
    "                cat_tensor = torch.tensor([category])\n",
    "                \n",
    "                logits, hidden = self.forward(cat_tensor, char_tensor, hidden)\n",
    "                probs = F.softmax(logits[0, 0] / temperature, dim=0)\n",
    "                next_char = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                if next_char == 0:  # EOS\n",
    "                    break\n",
    "                generated.append(next_char)\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "gen = NameGenerator(n_categories=5, vocab_size=30, embed_dim=64, hidden_size=128)\n",
    "print(f\"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in gen.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## é¡¹ç›®ä¸‰ï¼šç®€å•å¯¹è¯æœºå™¨äºº\n",
    "\n",
    "### 3.1 Seq2Seq å¯¹è¯æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot(nn.Module):\n",
    "    \"\"\"\n",
    "    ç®€å•çš„ Seq2Seq å¯¹è¯æœºå™¨äºº\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # å…±äº«è¯åµŒå…¥\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # ç¼–ç å™¨\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_size, num_layers,\n",
    "                              batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # è§£ç å™¨\n",
    "        self.decoder = nn.LSTM(embed_dim + hidden_size * 2, hidden_size, num_layers,\n",
    "                              batch_first=True)\n",
    "        \n",
    "        # æ³¨æ„åŠ›\n",
    "        self.attention = nn.Linear(hidden_size * 3, 1)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.fc = nn.Linear(hidden_size * 3, vocab_size)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ teacher forcingï¼‰\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        \n",
    "        # ç¼–ç \n",
    "        src_embedded = self.embedding(src)\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src_embedded)\n",
    "        \n",
    "        # è°ƒæ•´ hidden ç»´åº¦ï¼ˆåŒå‘ â†’ å•å‘ï¼‰\n",
    "        hidden = hidden.view(self.encoder.num_layers, 2, batch_size, -1)\n",
    "        hidden = hidden[:, 0, :, :] + hidden[:, 1, :, :]  # ç›¸åŠ \n",
    "        cell = cell.view(self.encoder.num_layers, 2, batch_size, -1)\n",
    "        cell = cell[:, 0, :, :] + cell[:, 1, :, :]\n",
    "        \n",
    "        # è§£ç \n",
    "        outputs = []\n",
    "        trg_embedded = self.embedding(trg)\n",
    "        \n",
    "        for t in range(trg_len):\n",
    "            # æ³¨æ„åŠ›\n",
    "            h_expanded = hidden[-1].unsqueeze(1).expand(-1, encoder_outputs.size(1), -1)\n",
    "            attn_input = torch.cat([h_expanded, encoder_outputs], dim=2)\n",
    "            attn_weights = F.softmax(self.attention(attn_input).squeeze(-1), dim=1)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            \n",
    "            # è§£ç å™¨è¾“å…¥\n",
    "            dec_input = torch.cat([trg_embedded[:, t:t+1, :], context.unsqueeze(1)], dim=2)\n",
    "            output, (hidden, cell) = self.decoder(dec_input, (hidden, cell))\n",
    "            \n",
    "            # é¢„æµ‹\n",
    "            pred = self.fc(torch.cat([output.squeeze(1), context], dim=1))\n",
    "            outputs.append(pred)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)  # (batch, trg_len, vocab)\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "chatbot = ChatBot(vocab_size=5000, embed_dim=128, hidden_size=256)\n",
    "print(f\"ChatBot å‚æ•°é‡: {sum(p.numel() for p in chatbot.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æœ¬ç« æ€»ç»“\n",
    "\n",
    "### é¡¹ç›®æ¸…å•\n",
    "\n",
    "| é¡¹ç›® | æŠ€æœ¯è¦ç‚¹ | éš¾åº¦ |\n",
    "|------|----------|------|\n",
    "| æƒ…æ„Ÿåˆ†æ | BiLSTM + Attention | â­â­ |\n",
    "| åå­—ç”Ÿæˆ | æ¡ä»¶è¯­è¨€æ¨¡å‹ | â­â­ |\n",
    "| å¯¹è¯æœºå™¨äºº | Seq2Seq + Attention | â­â­â­ |\n",
    "| æœºå™¨ç¿»è¯‘ | Seq2Seq + Beam Search | â­â­â­â­ |\n",
    "| è¯­éŸ³è¯†åˆ« | CTC + BiLSTM | â­â­â­â­ |\n",
    "\n",
    "### åç»­å­¦ä¹ \n",
    "\n",
    "- **Transformer**: ä¸‹ä¸€ä¸ªæ¨¡å— `4.3_transformer/`\n",
    "- **é¢„è®­ç»ƒæ¨¡å‹**: BERT, GPT ç­‰\n",
    "- **å¤§è¯­è¨€æ¨¡å‹**: LLM å¾®è°ƒå’Œåº”ç”¨\n",
    "\n",
    "---\n",
    "\n",
    "## æ­å–œå®Œæˆ RNN æ¨¡å—ï¼\n",
    "\n",
    "ä½ å·²ç»æŒæ¡äº†ï¼š\n",
    "- RNN/LSTM/GRU çš„åŸç†å’Œå®ç°\n",
    "- BPTT åå‘ä¼ æ’­ç®—æ³•\n",
    "- æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ\n",
    "- åŒå‘å’Œå¤šå±‚ RNN\n",
    "- è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬ç”Ÿæˆ\n",
    "- Seq2Seq å’Œæ³¨æ„åŠ›æœºåˆ¶\n",
    "- æ—¶é—´åºåˆ—é¢„æµ‹\n",
    "\n",
    "ä¸‹ä¸€æ­¥ï¼šå­¦ä¹  Transformerï¼Œè¿™æ˜¯ç°ä»£ NLP çš„åŸºç¡€ï¼\n",
    "\n",
    "ğŸ‘‰ [4.3_transformer/](../4.3_transformer/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
