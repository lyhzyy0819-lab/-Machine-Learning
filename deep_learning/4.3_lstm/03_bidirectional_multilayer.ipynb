{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ 7 ç« ï¼šåŒå‘ä¸å¤šå±‚ RNN\n",
    "\n",
    "> å¢å¼º RNN è¡¨è¾¾èƒ½åŠ›çš„ä¸¤ç§é‡è¦æŠ€æœ¯\n",
    "\n",
    "---\n",
    "\n",
    "## æœ¬ç« ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "- [ ] ç†è§£åŒå‘ RNN çš„åŸç†å’Œåº”ç”¨åœºæ™¯\n",
    "- [ ] æŒæ¡å¤šå±‚ RNN çš„å †å æ–¹å¼\n",
    "- [ ] å®ç° BiLSTM å’Œå¤šå±‚ GRU\n",
    "- [ ] åœ¨å®é™…ä»»åŠ¡ä¸­é€‰æ‹©åˆé€‚çš„æ¶æ„\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸ºä»€ä¹ˆéœ€è¦æ‰©å±• RNNï¼Ÿ\n",
    "\n",
    "**æ ‡å‡† RNN çš„å±€é™**ï¼š\n",
    "\n",
    "1. **å•å‘æ€§**ï¼šåªèƒ½åˆ©ç”¨è¿‡å»çš„ä¸Šä¸‹æ–‡ï¼Œæ— æ³•çœ‹åˆ°æœªæ¥\n",
    "2. **æ·±åº¦æœ‰é™**ï¼šå•å±‚ RNN çš„è¡¨è¾¾èƒ½åŠ›æœ‰é™\n",
    "\n",
    "**è§£å†³æ–¹æ¡ˆ**ï¼š\n",
    "\n",
    "| æ‰©å±•æ–¹å¼ | è§£å†³é—®é¢˜ | ä»£ä»· |\n",
    "|----------|----------|------|\n",
    "| åŒå‘ RNN | åˆ©ç”¨åŒå‘ä¸Šä¸‹æ–‡ | 2x å‚æ•°å’Œè®¡ç®—é‡ |\n",
    "| å¤šå±‚ RNN | å¢åŠ æ¨¡å‹æ·±åº¦ | N x å‚æ•°å’Œè®¡ç®—é‡ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šåŒå‘ RNN (Bidirectional RNN)\n",
    "\n",
    "### 1.1 åŒå‘ RNN çš„åŠ¨æœº\n",
    "\n",
    "è€ƒè™‘å¡«ç©ºä»»åŠ¡ï¼š\n",
    "\n",
    "```\n",
    "\"æˆ‘ å–œæ¬¢ åƒ ___ å’Œ è¥¿ç“œ\"\n",
    "```\n",
    "\n",
    "è¦é¢„æµ‹ç©ºæ ¼å¤„çš„è¯ï¼Œæˆ‘ä»¬éœ€è¦ï¼š\n",
    "- **å‰æ–‡**ï¼š\"æˆ‘ å–œæ¬¢ åƒ\" â†’ åé¢å¯èƒ½æ˜¯é£Ÿç‰©\n",
    "- **åæ–‡**ï¼š\"å’Œ è¥¿ç“œ\" â†’ å¯èƒ½æ˜¯å¦ä¸€ç§æ°´æœ\n",
    "\n",
    "å•å‘ RNN åªèƒ½çœ‹åˆ°å‰æ–‡ï¼ŒåŒå‘ RNN å¯ä»¥åŒæ—¶åˆ©ç”¨å‰åæ–‡ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ï¼šå•å‘ vs åŒå‘ RNN\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# å·¦å›¾ï¼šå•å‘ RNN\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 8)\n",
    "\n",
    "positions = [2, 4, 6, 8, 10]\n",
    "words = ['æˆ‘', 'å–œæ¬¢', 'åƒ', '___', 'è¥¿ç“œ']\n",
    "\n",
    "for i, (x, word) in enumerate(zip(positions, words)):\n",
    "    # éšçŠ¶æ€èŠ‚ç‚¹\n",
    "    circle = plt.Circle((x, 5), 0.5, fill=True, facecolor='lightblue', \n",
    "                         edgecolor='blue', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, 5, f'$h_{i+1}$', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # è¾“å…¥\n",
    "    ax.text(x, 2.5, word, ha='center', fontsize=12, \n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow' if word != '___' else 'lightcoral'))\n",
    "    ax.annotate('', xy=(x, 4.5), xytext=(x, 3.2),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "    \n",
    "    # æ°´å¹³è¿æ¥ï¼ˆåªæœ‰å‘å³ï¼‰\n",
    "    if i < len(positions) - 1:\n",
    "        ax.annotate('', xy=(positions[i+1]-0.5, 5), xytext=(x+0.5, 5),\n",
    "                    arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "ax.set_title('å•å‘ RNN: åªèƒ½çœ‹åˆ°è¿‡å»', fontsize=14, fontweight='bold')\n",
    "ax.text(6, 7, 'é—®é¢˜: é¢„æµ‹ \"___\" æ—¶çœ‹ä¸åˆ° \"è¥¿ç“œ\"', ha='center', fontsize=11, color='red')\n",
    "ax.axis('off')\n",
    "\n",
    "# å³å›¾ï¼šåŒå‘ RNN\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 8)\n",
    "\n",
    "for i, (x, word) in enumerate(zip(positions, words)):\n",
    "    # å‰å‘éšçŠ¶æ€ï¼ˆä¸Šå±‚ï¼‰\n",
    "    circle = plt.Circle((x, 5.8), 0.4, fill=True, facecolor='lightblue', \n",
    "                         edgecolor='blue', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, 5.8, f'$\\\\overrightarrow{{h}}_{i+1}$', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # åå‘éšçŠ¶æ€ï¼ˆä¸‹å±‚ï¼‰\n",
    "    circle = plt.Circle((x, 4.2), 0.4, fill=True, facecolor='lightcoral', \n",
    "                         edgecolor='red', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, 4.2, f'$\\\\overleftarrow{{h}}_{i+1}$', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # è¾“å…¥\n",
    "    ax.text(x, 2, word, ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow' if word != '___' else 'lightgreen'))\n",
    "    ax.annotate('', xy=(x, 3.8), xytext=(x, 2.7),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "    \n",
    "    # å‰å‘è¿æ¥ï¼ˆå‘å³ï¼‰\n",
    "    if i < len(positions) - 1:\n",
    "        ax.annotate('', xy=(positions[i+1]-0.4, 5.8), xytext=(x+0.4, 5.8),\n",
    "                    arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "    \n",
    "    # åå‘è¿æ¥ï¼ˆå‘å·¦ï¼‰\n",
    "    if i > 0:\n",
    "        ax.annotate('', xy=(positions[i-1]+0.4, 4.2), xytext=(x-0.4, 4.2),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "ax.set_title('åŒå‘ RNN: åŒæ—¶çœ‹è¿‡å»å’Œæœªæ¥', fontsize=14, fontweight='bold')\n",
    "ax.text(6, 7.2, 'ä¼˜åŠ¿: é¢„æµ‹ \"___\" æ—¶å¯ä»¥åˆ©ç”¨ \"åƒ\" å’Œ \"è¥¿ç“œ\"', ha='center', fontsize=11, color='green')\n",
    "ax.text(6, 0.8, '$h_t = [\\\\overrightarrow{h}_t ; \\\\overleftarrow{h}_t]$ (æ‹¼æ¥)', \n",
    "        ha='center', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 åŒå‘ RNN çš„æ•°å­¦è¡¨ç¤º\n",
    "\n",
    "**å‰å‘ RNN**ï¼ˆä»å·¦åˆ°å³ï¼‰ï¼š\n",
    "$$\\overrightarrow{h}_t = \\text{RNN}_{\\rightarrow}(x_t, \\overrightarrow{h}_{t-1})$$\n",
    "\n",
    "**åå‘ RNN**ï¼ˆä»å³åˆ°å·¦ï¼‰ï¼š\n",
    "$$\\overleftarrow{h}_t = \\text{RNN}_{\\leftarrow}(x_t, \\overleftarrow{h}_{t+1})$$\n",
    "\n",
    "**åˆå¹¶è¾“å‡º**ï¼š\n",
    "$$h_t = [\\overrightarrow{h}_t ; \\overleftarrow{h}_t] \\quad \\text{(æ‹¼æ¥)}$$\n",
    "\n",
    "æˆ–è€…ï¼š\n",
    "$$h_t = \\overrightarrow{h}_t + \\overleftarrow{h}_t \\quad \\text{(ç›¸åŠ )}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy å®ç°åŒå‘ RNN\n",
    "\n",
    "class BidirectionalRNN:\n",
    "    \"\"\"\n",
    "    åŒå‘ RNN çš„ NumPy å®ç°\n",
    "    \n",
    "    åŒ…å«ä¸¤ä¸ªç‹¬ç«‹çš„ RNNï¼š\n",
    "    - å‰å‘ RNN: ä» t=1 åˆ° t=T\n",
    "    - åå‘ RNN: ä» t=T åˆ° t=1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # å‰å‘ RNN å‚æ•°\n",
    "        scale = np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        self.W_xh_forward = np.random.randn(hidden_size, input_size) * scale\n",
    "        self.W_hh_forward = np.random.randn(hidden_size, hidden_size) * scale\n",
    "        self.b_h_forward = np.zeros(hidden_size)\n",
    "        \n",
    "        # åå‘ RNN å‚æ•°ï¼ˆç‹¬ç«‹çš„å‚æ•°ï¼‰\n",
    "        self.W_xh_backward = np.random.randn(hidden_size, input_size) * scale\n",
    "        self.W_hh_backward = np.random.randn(hidden_size, hidden_size) * scale\n",
    "        self.b_h_backward = np.zeros(hidden_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        åŒå‘å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            X: è¾“å…¥åºåˆ—, shape: (seq_len, input_size)\n",
    "        \n",
    "        è¿”å›:\n",
    "            h_concat: æ‹¼æ¥åçš„éšçŠ¶æ€, shape: (seq_len, 2 * hidden_size)\n",
    "        \"\"\"\n",
    "        seq_len = X.shape[0]\n",
    "        \n",
    "        # ========================================\n",
    "        # å‰å‘ä¼ æ’­ï¼ˆä»å·¦åˆ°å³ï¼‰\n",
    "        # ========================================\n",
    "        h_forward = np.zeros((seq_len, self.hidden_size))\n",
    "        h_t = np.zeros(self.hidden_size)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            h_t = np.tanh(self.W_xh_forward @ X[t] + \n",
    "                          self.W_hh_forward @ h_t + \n",
    "                          self.b_h_forward)\n",
    "            h_forward[t] = h_t\n",
    "        \n",
    "        # ========================================\n",
    "        # åå‘ä¼ æ’­ï¼ˆä»å³åˆ°å·¦ï¼‰\n",
    "        # ========================================\n",
    "        h_backward = np.zeros((seq_len, self.hidden_size))\n",
    "        h_t = np.zeros(self.hidden_size)\n",
    "        \n",
    "        for t in range(seq_len - 1, -1, -1):  # ä»åå¾€å‰\n",
    "            h_t = np.tanh(self.W_xh_backward @ X[t] + \n",
    "                          self.W_hh_backward @ h_t + \n",
    "                          self.b_h_backward)\n",
    "            h_backward[t] = h_t\n",
    "        \n",
    "        # ========================================\n",
    "        # æ‹¼æ¥å‰å‘å’Œåå‘çš„éšçŠ¶æ€\n",
    "        # ========================================\n",
    "        h_concat = np.concatenate([h_forward, h_backward], axis=1)\n",
    "        \n",
    "        return h_concat, h_forward, h_backward\n",
    "\n",
    "\n",
    "# æµ‹è¯•\n",
    "print(\"=\" * 60)\n",
    "print(\"åŒå‘ RNN æµ‹è¯•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 8\n",
    "seq_len = 10\n",
    "\n",
    "birnn = BidirectionalRNN(input_size, hidden_size)\n",
    "X = np.random.randn(seq_len, input_size)\n",
    "\n",
    "h_concat, h_forward, h_backward = birnn.forward(X)\n",
    "\n",
    "print(f\"\\nè¾“å…¥å½¢çŠ¶: {X.shape}\")\n",
    "print(f\"å‰å‘éšçŠ¶æ€å½¢çŠ¶: {h_forward.shape}\")\n",
    "print(f\"åå‘éšçŠ¶æ€å½¢çŠ¶: {h_backward.shape}\")\n",
    "print(f\"æ‹¼æ¥åå½¢çŠ¶: {h_concat.shape}  # hidden_size * 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 PyTorch åŒå‘ RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_bidirectional_demo():\n",
    "    \"\"\"\n",
    "    PyTorch åŒå‘ RNN ä½¿ç”¨æ¼”ç¤º\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PyTorch åŒå‘ RNN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "    input_size = 8\n",
    "    hidden_size = 16\n",
    "    \n",
    "    # å•å‘ LSTM\n",
    "    lstm_uni = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=False)\n",
    "    \n",
    "    # åŒå‘ LSTM\n",
    "    lstm_bi = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    # è¾“å…¥\n",
    "    x = torch.randn(batch_size, seq_len, input_size)\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    out_uni, (h_uni, c_uni) = lstm_uni(x)\n",
    "    out_bi, (h_bi, c_bi) = lstm_bi(x)\n",
    "    \n",
    "    print(\"\\nå•å‘ LSTM:\")\n",
    "    print(f\"  è¾“å‡º shape: {out_uni.shape}  # (batch, seq, hidden)\")\n",
    "    print(f\"  h_n shape: {h_uni.shape}     # (1, batch, hidden)\")\n",
    "    \n",
    "    print(\"\\nåŒå‘ LSTM:\")\n",
    "    print(f\"  è¾“å‡º shape: {out_bi.shape}  # (batch, seq, hidden*2)\")\n",
    "    print(f\"  h_n shape: {h_bi.shape}     # (2, batch, hidden) - å‰å‘å’Œåå‘\")\n",
    "    \n",
    "    # åˆ†ç¦»å‰å‘å’Œåå‘è¾“å‡º\n",
    "    print(\"\\nåˆ†ç¦»å‰å‘å’Œåå‘:\")\n",
    "    forward_out = out_bi[:, :, :hidden_size]\n",
    "    backward_out = out_bi[:, :, hidden_size:]\n",
    "    print(f\"  å‰å‘è¾“å‡º: {forward_out.shape}\")\n",
    "    print(f\"  åå‘è¾“å‡º: {backward_out.shape}\")\n",
    "    \n",
    "    # å‚æ•°é‡å¯¹æ¯”\n",
    "    params_uni = sum(p.numel() for p in lstm_uni.parameters())\n",
    "    params_bi = sum(p.numel() for p in lstm_bi.parameters())\n",
    "    \n",
    "    print(f\"\\nå‚æ•°é‡:\")\n",
    "    print(f\"  å•å‘: {params_uni:,}\")\n",
    "    print(f\"  åŒå‘: {params_bi:,}\")\n",
    "    print(f\"  æ¯”å€¼: {params_bi/params_uni:.1f}x\")\n",
    "\n",
    "\n",
    "pytorch_bidirectional_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 åŒå‘ RNN çš„åº”ç”¨åœºæ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_applications():\n",
    "    \"\"\"\n",
    "    åŒå‘ RNN çš„åº”ç”¨åœºæ™¯\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"åŒå‘ RNN é€‚ç”¨åœºæ™¯\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    applications = \"\"\"\n",
    "    \n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    âœ“ é€‚åˆåŒå‘ RNN                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  1. åºåˆ—æ ‡æ³¨ (Sequence Labeling)                            â”‚\n",
    "â”‚     - è¯æ€§æ ‡æ³¨ (POS Tagging)                                â”‚\n",
    "â”‚     - å‘½åå®ä½“è¯†åˆ« (NER)                                    â”‚\n",
    "â”‚     - è¯­ä¹‰è§’è‰²æ ‡æ³¨                                          â”‚\n",
    "â”‚     åŸå› : æ¯ä¸ªä½ç½®çš„æ ‡ç­¾ä¾èµ–å‰åæ–‡                          â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  2. æ–‡æœ¬åˆ†ç±» / æƒ…æ„Ÿåˆ†æ                                     â”‚\n",
    "â”‚     - æ•´ä¸ªå¥å­çš„åˆ†ç±»                                        â”‚\n",
    "â”‚     åŸå› : å¯ä»¥è·å¾—æ•´ä¸ªåºåˆ—çš„å®Œæ•´è¡¨ç¤º                        â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  3. æœºå™¨é˜…è¯»ç†è§£                                            â”‚\n",
    "â”‚     - é—®ç­”ç³»ç»Ÿä¸­çš„ä¸Šä¸‹æ–‡ç¼–ç                                 â”‚\n",
    "â”‚     åŸå› : ç­”æ¡ˆå¯èƒ½ä¾èµ–é—®é¢˜çš„ä»»ä½•éƒ¨åˆ†                        â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  4. è¯­éŸ³è¯†åˆ«çš„ç¼–ç å™¨                                        â”‚\n",
    "â”‚     - å¤„ç†å®Œæ•´çš„éŸ³é¢‘åå†è§£ç                                 â”‚\n",
    "â”‚     åŸå› : å¯ä»¥åˆ©ç”¨æ•´ä¸ªéŸ³é¢‘çš„ä¸Šä¸‹æ–‡                          â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    âœ— ä¸é€‚åˆåŒå‘ RNN                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  1. è¯­è¨€æ¨¡å‹ / æ–‡æœ¬ç”Ÿæˆ                                     â”‚\n",
    "â”‚     - é¢„æµ‹ä¸‹ä¸€ä¸ªè¯                                          â”‚\n",
    "â”‚     åŸå› : ç”Ÿæˆæ—¶çœ‹ä¸åˆ°æœªæ¥çš„è¯                              â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  2. å®æ—¶æµå¤„ç†                                              â”‚\n",
    "â”‚     - å®æ—¶è¯­éŸ³è¯†åˆ«                                          â”‚\n",
    "â”‚     - åœ¨çº¿ç¿»è¯‘                                              â”‚\n",
    "â”‚     åŸå› : å¿…é¡»ç­‰å¾…æ•´ä¸ªåºåˆ—å®Œæˆæ‰èƒ½å¤„ç†                      â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  3. è‡ªå›å½’è§£ç å™¨                                            â”‚\n",
    "â”‚     - Seq2Seq çš„è§£ç å™¨éƒ¨åˆ†                                  â”‚\n",
    "â”‚     åŸå› : è§£ç æ˜¯é€æ­¥è¿›è¡Œçš„                                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "    \n",
    "    print(applications)\n",
    "\n",
    "\n",
    "bidirectional_applications()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šå¤šå±‚ RNN (Stacked RNN)\n",
    "\n",
    "### 2.1 å¤šå±‚ RNN çš„åŠ¨æœº\n",
    "\n",
    "å°±åƒ CNN ä¸­å †å å¤šå±‚å·ç§¯å¯ä»¥å­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ï¼Œå †å å¤šå±‚ RNN å¯ä»¥å­¦ä¹ æ›´æŠ½è±¡çš„åºåˆ—è¡¨ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ï¼šå•å±‚ vs å¤šå±‚ RNN\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# å·¦å›¾ï¼šå•å±‚ RNN\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "positions = [2, 5, 8, 11]\n",
    "\n",
    "for i, x in enumerate(positions):\n",
    "    # å•å±‚èŠ‚ç‚¹\n",
    "    circle = plt.Circle((x, 5), 0.6, fill=True, facecolor='lightblue', \n",
    "                         edgecolor='blue', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, 5, f'$h_{{1,{i+1}}}$', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # è¾“å…¥\n",
    "    ax.text(x, 2, f'$x_{i+1}$', ha='center', fontsize=12)\n",
    "    ax.annotate('', xy=(x, 4.4), xytext=(x, 2.6),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "    \n",
    "    # è¾“å‡º\n",
    "    ax.annotate('', xy=(x, 7.5), xytext=(x, 5.6),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "    ax.text(x, 8, f'$y_{i+1}$', ha='center', fontsize=12)\n",
    "    \n",
    "    # æ°´å¹³è¿æ¥\n",
    "    if i < len(positions) - 1:\n",
    "        ax.annotate('', xy=(positions[i+1]-0.6, 5), xytext=(x+0.6, 5),\n",
    "                    arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "ax.set_title('å•å±‚ RNN', fontsize=14, fontweight='bold')\n",
    "ax.text(7, 9.2, 'ä¸€å±‚éšè—å±‚', ha='center', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "# å³å›¾ï¼šå¤šå±‚ RNN\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "layers = 3\n",
    "layer_colors = ['lightblue', 'lightgreen', 'lightyellow']\n",
    "layer_y = [3, 5, 7]\n",
    "\n",
    "for layer, (y, color) in enumerate(zip(layer_y, layer_colors)):\n",
    "    for i, x in enumerate(positions):\n",
    "        # èŠ‚ç‚¹\n",
    "        circle = plt.Circle((x, y), 0.5, fill=True, facecolor=color, \n",
    "                             edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, f'$h_{{L{layer+1}}}$', ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        # æ°´å¹³è¿æ¥ï¼ˆåŒå±‚ï¼‰\n",
    "        if i < len(positions) - 1:\n",
    "            ax.annotate('', xy=(positions[i+1]-0.5, y), xytext=(x+0.5, y),\n",
    "                        arrowprops=dict(arrowstyle='->', color='blue', lw=1.5))\n",
    "        \n",
    "        # å‚ç›´è¿æ¥ï¼ˆå±‚ä¸å±‚ä¹‹é—´ï¼‰\n",
    "        if layer < layers - 1:\n",
    "            ax.annotate('', xy=(x, layer_y[layer+1]-0.5), xytext=(x, y+0.5),\n",
    "                        arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "\n",
    "# è¾“å…¥\n",
    "for i, x in enumerate(positions):\n",
    "    ax.text(x, 1, f'$x_{i+1}$', ha='center', fontsize=10)\n",
    "    ax.annotate('', xy=(x, 2.5), xytext=(x, 1.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "\n",
    "# è¾“å‡º\n",
    "for i, x in enumerate(positions):\n",
    "    ax.text(x, 9, f'$y_{i+1}$', ha='center', fontsize=10)\n",
    "    ax.annotate('', xy=(x, 8.5), xytext=(x, 7.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "\n",
    "ax.set_title('å¤šå±‚ RNN (3 å±‚)', fontsize=14, fontweight='bold')\n",
    "ax.text(7, 9.5, 'æ¯å±‚å­¦ä¹ ä¸åŒå±‚æ¬¡çš„è¡¨ç¤º', ha='center', fontsize=12)\n",
    "\n",
    "# å±‚æ ‡ç­¾\n",
    "for layer, y in enumerate(layer_y):\n",
    "    ax.text(0.5, y, f'Layer {layer+1}', fontsize=10, va='center')\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 å¤šå±‚ RNN çš„æ•°å­¦è¡¨ç¤º\n",
    "\n",
    "å¯¹äº $L$ å±‚çš„ RNNï¼š\n",
    "\n",
    "**ç¬¬ 1 å±‚**ï¼š\n",
    "$$h_t^{(1)} = \\text{RNN}^{(1)}(x_t, h_{t-1}^{(1)})$$\n",
    "\n",
    "**ç¬¬ $l$ å±‚**ï¼ˆ$l > 1$ï¼‰ï¼š\n",
    "$$h_t^{(l)} = \\text{RNN}^{(l)}(h_t^{(l-1)}, h_{t-1}^{(l)})$$\n",
    "\n",
    "å³ï¼šä¸Šä¸€å±‚çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚\n",
    "\n",
    "**æœ€ç»ˆè¾“å‡º**ï¼š\n",
    "$$y_t = \\text{Linear}(h_t^{(L)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_rnn_demo():\n",
    "    \"\"\"\n",
    "    PyTorch å¤šå±‚ RNN æ¼”ç¤º\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"å¤šå±‚ RNN æ¼”ç¤º\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "    input_size = 8\n",
    "    hidden_size = 16\n",
    "    \n",
    "    # ä¸åŒå±‚æ•°çš„ LSTM\n",
    "    lstm_1 = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "    lstm_2 = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "    lstm_4 = nn.LSTM(input_size, hidden_size, num_layers=4, batch_first=True)\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, input_size)\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    out_1, (h_1, c_1) = lstm_1(x)\n",
    "    out_2, (h_2, c_2) = lstm_2(x)\n",
    "    out_4, (h_4, c_4) = lstm_4(x)\n",
    "    \n",
    "    print(\"\\nè¾“å‡ºå½¢çŠ¶ (num_layers ä¸å½±å“è¾“å‡ºç»´åº¦):\")\n",
    "    print(f\"  1 å±‚: output={out_1.shape}, h_n={h_1.shape}\")\n",
    "    print(f\"  2 å±‚: output={out_2.shape}, h_n={h_2.shape}\")\n",
    "    print(f\"  4 å±‚: output={out_4.shape}, h_n={h_4.shape}\")\n",
    "    \n",
    "    # å‚æ•°é‡å¯¹æ¯”\n",
    "    params = []\n",
    "    for lstm, name in [(lstm_1, '1å±‚'), (lstm_2, '2å±‚'), (lstm_4, '4å±‚')]:\n",
    "        p = sum(p.numel() for p in lstm.parameters())\n",
    "        params.append((name, p))\n",
    "        print(f\"\\n{name} LSTM å‚æ•°é‡: {p:,}\")\n",
    "    \n",
    "    # è®¿é—®æ¯å±‚çš„éšçŠ¶æ€\n",
    "    print(\"\\nå¤šå±‚ LSTM çš„éšçŠ¶æ€ç»“æ„:\")\n",
    "    print(f\"  h_n å½¢çŠ¶: (num_layers, batch, hidden)\")\n",
    "    print(f\"  h_2[0]: ç¬¬ 1 å±‚çš„æœ€ç»ˆéšçŠ¶æ€\")\n",
    "    print(f\"  h_2[1]: ç¬¬ 2 å±‚çš„æœ€ç»ˆéšçŠ¶æ€\")\n",
    "    \n",
    "    # å¯è§†åŒ–å‚æ•°é‡\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    names = [p[0] for p in params]\n",
    "    values = [p[1]/1000 for p in params]\n",
    "    \n",
    "    bars = ax.bar(names, values, color=['steelblue', 'coral', 'green'], edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel('å±‚æ•°', fontsize=12)\n",
    "    ax.set_ylabel('å‚æ•°é‡ (åƒ)', fontsize=12)\n",
    "    ax.set_title('å¤šå±‚ LSTM å‚æ•°é‡', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                f'{val:.0f}k', ha='center', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "multilayer_rnn_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dropout åœ¨å¤šå±‚ RNN ä¸­çš„åº”ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_in_rnn():\n",
    "    \"\"\"\n",
    "    RNN ä¸­çš„ Dropout\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RNN ä¸­çš„ Dropout\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    explanation = \"\"\"\n",
    "    PyTorch RNN çš„ dropout å‚æ•°:\n",
    "    \n",
    "    - åº”ç”¨åœ¨å±‚ä¸å±‚ä¹‹é—´ï¼ˆä¸æ˜¯æ—¶é—´æ­¥ä¹‹é—´ï¼‰\n",
    "    - åªæœ‰å¤šå±‚ RNN æ—¶æ‰æœ‰æ•ˆï¼ˆnum_layers > 1ï¼‰\n",
    "    - æœ€åä¸€å±‚æ²¡æœ‰ dropout\n",
    "    \n",
    "    ç¤ºä¾‹: 3 å±‚ LSTM, dropout=0.2\n",
    "    \n",
    "    Layer 3:  h3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ output (æ—  dropout)\n",
    "              â–²\n",
    "              â”‚ dropout (0.2)\n",
    "    Layer 2:  h2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶\n",
    "              â–²\n",
    "              â”‚ dropout (0.2)\n",
    "    Layer 1:  h1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶\n",
    "              â–²\n",
    "              â”‚ (æ—  dropout)\n",
    "    Input:    x\n",
    "    \"\"\"\n",
    "    \n",
    "    print(explanation)\n",
    "    \n",
    "    # ä»£ç ç¤ºä¾‹\n",
    "    print(\"\\nPyTorch ä»£ç :\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 3 å±‚ LSTM with dropout\n",
    "    lstm = nn.LSTM(input_size=32, hidden_size=64, \n",
    "                   num_layers=3, \n",
    "                   dropout=0.2,  # å±‚é—´ dropout\n",
    "                   batch_first=True)\n",
    "    \n",
    "    print(\"lstm = nn.LSTM(input_size=32, hidden_size=64,\")\n",
    "    print(\"               num_layers=3, dropout=0.2)\")\n",
    "    \n",
    "    # è®­ç»ƒ vs æ¨ç†\n",
    "    x = torch.randn(4, 10, 32)\n",
    "    \n",
    "    lstm.train()\n",
    "    out_train, _ = lstm(x)\n",
    "    \n",
    "    lstm.eval()\n",
    "    out_eval, _ = lstm(x)\n",
    "    \n",
    "    print(f\"\\nè®­ç»ƒæ¨¡å¼è¾“å‡º: {out_train.shape}\")\n",
    "    print(f\"è¯„ä¼°æ¨¡å¼è¾“å‡º: {out_eval.shape}\")\n",
    "    print(\"\\næ³¨æ„: è¯„ä¼°æ—¶ dropout è‡ªåŠ¨å…³é—­ï¼\")\n",
    "\n",
    "\n",
    "dropout_in_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šç»„åˆæ¶æ„\n",
    "\n",
    "### 3.1 åŒå‘å¤šå±‚ RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_multilayer():\n",
    "    \"\"\"\n",
    "    åŒå‘å¤šå±‚ RNN\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"åŒå‘å¤šå±‚ RNN (BiLSTM)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "    input_size = 8\n",
    "    hidden_size = 16\n",
    "    num_layers = 2\n",
    "    \n",
    "    # åŒå‘å¤šå±‚ LSTM\n",
    "    bilstm = nn.LSTM(input_size, hidden_size, \n",
    "                     num_layers=num_layers,\n",
    "                     bidirectional=True,\n",
    "                     batch_first=True)\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, input_size)\n",
    "    output, (h_n, c_n) = bilstm(x)\n",
    "    \n",
    "    print(f\"\\né…ç½®: {num_layers} å±‚, åŒå‘\")\n",
    "    print(f\"\\nè¾“å‡ºå½¢çŠ¶:\")\n",
    "    print(f\"  output: {output.shape}  # (batch, seq, hidden*2)\")\n",
    "    print(f\"  h_n: {h_n.shape}  # (num_layers*2, batch, hidden)\")\n",
    "    print(f\"  c_n: {c_n.shape}  # (num_layers*2, batch, hidden)\")\n",
    "    \n",
    "    # h_n çš„ç»“æ„\n",
    "    print(\"\\nh_n ç»“æ„è§£æ:\")\n",
    "    print(\"  h_n[0]: Layer 1 å‰å‘çš„æœ€ç»ˆçŠ¶æ€\")\n",
    "    print(\"  h_n[1]: Layer 1 åå‘çš„æœ€ç»ˆçŠ¶æ€\")\n",
    "    print(\"  h_n[2]: Layer 2 å‰å‘çš„æœ€ç»ˆçŠ¶æ€\")\n",
    "    print(\"  h_n[3]: Layer 2 åå‘çš„æœ€ç»ˆçŠ¶æ€\")\n",
    "    \n",
    "    # è·å–æœ€ç»ˆè¡¨ç¤ºçš„å¸¸ç”¨æ–¹æ³•\n",
    "    print(\"\\nè·å–æœ€ç»ˆè¡¨ç¤º:\")\n",
    "    \n",
    "    # æ–¹æ³• 1: å–æœ€åä¸€å±‚çš„å‰å‘å’Œåå‘æ‹¼æ¥\n",
    "    last_forward = h_n[-2]  # (batch, hidden)\n",
    "    last_backward = h_n[-1]  # (batch, hidden)\n",
    "    final_repr = torch.cat([last_forward, last_backward], dim=1)\n",
    "    print(f\"  æ–¹æ³•1 (æ‹¼æ¥æœ€åä¸€å±‚): {final_repr.shape}\")\n",
    "    \n",
    "    # æ–¹æ³• 2: å–è¾“å‡ºåºåˆ—çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥\n",
    "    last_output = output[:, -1, :]\n",
    "    print(f\"  æ–¹æ³•2 (è¾“å‡ºæœ€åæ—¶åˆ»): {last_output.shape}\")\n",
    "    \n",
    "    # æ–¹æ³• 3: å¯¹è¾“å‡ºåºåˆ—åšå¹³å‡æ± åŒ–\n",
    "    avg_pool = output.mean(dim=1)\n",
    "    print(f\"  æ–¹æ³•3 (å¹³å‡æ± åŒ–): {avg_pool.shape}\")\n",
    "\n",
    "\n",
    "bidirectional_multilayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 å®æˆ˜ï¼šBiLSTM æ–‡æœ¬åˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    åŒå‘ LSTM æ–‡æœ¬åˆ†ç±»å™¨\n",
    "    \n",
    "    ç»“æ„:\n",
    "    Embedding â†’ BiLSTM â†’ Dropout â†’ Linear â†’ Softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, \n",
    "                 num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # è¯åµŒå…¥å±‚\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # åŒå‘ LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # åˆ†ç±»å±‚ï¼ˆè¾“å…¥æ˜¯ hidden_size * 2 å› ä¸ºæ˜¯åŒå‘ï¼‰\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            x: è¯ç´¢å¼•åºåˆ—, shape: (batch, seq_len)\n",
    "        \n",
    "        è¿”å›:\n",
    "            logits: åˆ†ç±» logits, shape: (batch, num_classes)\n",
    "        \"\"\"\n",
    "        # è¯åµŒå…¥: (batch, seq_len) â†’ (batch, seq_len, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # BiLSTM\n",
    "        # output: (batch, seq_len, hidden*2)\n",
    "        # h_n: (num_layers*2, batch, hidden)\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "        \n",
    "        # å–æœ€åä¸€å±‚çš„å‰å‘å’Œåå‘çŠ¶æ€æ‹¼æ¥\n",
    "        # h_n[-2]: æœ€åä¸€å±‚å‰å‘, h_n[-1]: æœ€åä¸€å±‚åå‘\n",
    "        final_hidden = torch.cat([h_n[-2], h_n[-1]], dim=1)  # (batch, hidden*2)\n",
    "        \n",
    "        # Dropout + åˆ†ç±»\n",
    "        out = self.dropout(final_hidden)\n",
    "        logits = self.fc(out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# æµ‹è¯•æ¨¡å‹\n",
    "print(\"=\" * 60)\n",
    "print(\"BiLSTM æ–‡æœ¬åˆ†ç±»å™¨æµ‹è¯•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ¨¡å‹é…ç½®\n",
    "vocab_size = 10000\n",
    "embed_dim = 128\n",
    "hidden_size = 256\n",
    "num_classes = 5\n",
    "num_layers = 2\n",
    "\n",
    "model = BiLSTMClassifier(vocab_size, embed_dim, hidden_size, num_classes, num_layers)\n",
    "\n",
    "# æ¨¡æ‹Ÿè¾“å…¥\n",
    "batch_size = 8\n",
    "seq_len = 50\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "logits = model(x)\n",
    "\n",
    "print(f\"\\nè¾“å…¥å½¢çŠ¶: {x.shape}\")\n",
    "print(f\"è¾“å‡ºå½¢çŠ¶: {logits.shape}\")\n",
    "print(f\"\\næ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# æ‰“å°æ¨¡å‹ç»“æ„\n",
    "print(\"\\næ¨¡å‹ç»“æ„:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬å››éƒ¨åˆ†ï¼šå®è·µå»ºè®®\n",
    "\n",
    "### 4.1 æ¶æ„é€‰æ‹©æŒ‡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def architecture_guide():\n",
    "    \"\"\"\n",
    "    RNN æ¶æ„é€‰æ‹©æŒ‡å—\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RNN æ¶æ„é€‰æ‹©æŒ‡å—\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    guide = \"\"\"\n",
    "    \n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         å±‚æ•°é€‰æ‹©                                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  â€¢ 1-2 å±‚: å¤§å¤šæ•°ä»»åŠ¡çš„èµ·ç‚¹                                        â”‚\n",
    "â”‚  â€¢ 2-4 å±‚: å¤æ‚ä»»åŠ¡ï¼ˆæœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ï¼‰                          â”‚\n",
    "â”‚  â€¢ 4+ å±‚: éœ€è¦æ®‹å·®è¿æ¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±                                 â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  ç»éªŒæ³•åˆ™: ä» 2 å±‚å¼€å§‹ï¼Œå¦‚æœè¿‡æ‹Ÿåˆå‡å°‘ï¼Œæ¬ æ‹Ÿåˆå¢åŠ                  â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                       å•å‘ vs åŒå‘                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  å•å‘:                                                             â”‚\n",
    "â”‚    âœ“ è¯­è¨€æ¨¡å‹ã€æ–‡æœ¬ç”Ÿæˆ                                            â”‚\n",
    "â”‚    âœ“ å®æ—¶/æµå¼å¤„ç†                                                 â”‚\n",
    "â”‚    âœ“ è‡ªå›å½’è§£ç                                                     â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  åŒå‘:                                                             â”‚\n",
    "â”‚    âœ“ åºåˆ—æ ‡æ³¨ï¼ˆNERã€POSï¼‰                                          â”‚\n",
    "â”‚    âœ“ æ–‡æœ¬åˆ†ç±»                                                      â”‚\n",
    "â”‚    âœ“ ç¼–ç å™¨ï¼ˆSeq2Seq çš„ encoderï¼‰                                  â”‚\n",
    "â”‚    âœ“ æœºå™¨é˜…è¯»ç†è§£                                                  â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                       å¸¸è§é…ç½®                                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  æ–‡æœ¬åˆ†ç±»:                                                         â”‚\n",
    "â”‚    BiLSTM, 2 å±‚, hidden=256, dropout=0.3                           â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  åºåˆ—æ ‡æ³¨ (NER):                                                   â”‚\n",
    "â”‚    BiLSTM-CRF, 2 å±‚, hidden=256                                    â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  æœºå™¨ç¿»è¯‘ç¼–ç å™¨:                                                   â”‚\n",
    "â”‚    BiLSTM, 2-4 å±‚, hidden=512                                      â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  è¯­è¨€æ¨¡å‹:                                                         â”‚\n",
    "â”‚    å•å‘ LSTM, 2-3 å±‚, hidden=512-1024                              â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "    \n",
    "    print(guide)\n",
    "\n",
    "\n",
    "architecture_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æœ¬ç« æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒè¦ç‚¹\n",
    "\n",
    "1. **åŒå‘ RNN**ï¼š\n",
    "   - åŒæ—¶åˆ©ç”¨è¿‡å»å’Œæœªæ¥çš„ä¸Šä¸‹æ–‡\n",
    "   - è¾“å‡ºç»´åº¦ç¿»å€ï¼š`hidden_size * 2`\n",
    "   - ä¸é€‚ç”¨äºéœ€è¦å®æ—¶é¢„æµ‹çš„ä»»åŠ¡\n",
    "\n",
    "2. **å¤šå±‚ RNN**ï¼š\n",
    "   - å¢åŠ æ¨¡å‹æ·±åº¦ï¼Œå­¦ä¹ æ›´æŠ½è±¡çš„è¡¨ç¤º\n",
    "   - å±‚é—´å¯ä»¥æ·»åŠ  dropout\n",
    "   - é€šå¸¸ 2-4 å±‚å°±è¶³å¤Ÿ\n",
    "\n",
    "3. **ç»„åˆä½¿ç”¨**ï¼š\n",
    "   - åŒå‘ + å¤šå±‚ = BiLSTM/BiGRU\n",
    "   - `h_n` å½¢çŠ¶ä¸º `(num_layers * num_directions, batch, hidden)`\n",
    "\n",
    "### ä»£ç æ¨¡æ¿\n",
    "\n",
    "```python\n",
    "# åŒå‘å¤šå±‚ LSTM\n",
    "lstm = nn.LSTM(\n",
    "    input_size=embed_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    dropout=0.3,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "output, (h_n, c_n) = lstm(x)\n",
    "# output: (batch, seq, hidden*2)\n",
    "# h_n: (num_layers*2, batch, hidden)\n",
    "\n",
    "# è·å–æœ€ç»ˆè¡¨ç¤º\n",
    "final = torch.cat([h_n[-2], h_n[-1]], dim=1)  # (batch, hidden*2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸‹ä¸€ç« é¢„å‘Š\n",
    "\n",
    "**ç¬¬ 8 ç« ï¼šè¯­è¨€æ¨¡å‹**\n",
    "\n",
    "ç”¨ RNN æ„å»ºè¯­è¨€æ¨¡å‹ï¼š\n",
    "- é¢„æµ‹ä¸‹ä¸€ä¸ªè¯\n",
    "- è®¡ç®—å›°æƒ‘åº¦ (Perplexity)\n",
    "- æ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "ğŸ‘‰ [08_language_model.ipynb](./08_language_model.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
