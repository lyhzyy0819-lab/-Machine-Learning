{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ 9 ç« ï¼šSeq2Seq æ¶æ„\n",
    "\n",
    "> åºåˆ—åˆ°åºåˆ—æ¨¡å‹ - æœºå™¨ç¿»è¯‘çš„åŸºç¡€æ¶æ„\n",
    "\n",
    "---\n",
    "\n",
    "## æœ¬ç« ç›®æ ‡\n",
    "\n",
    "- [ ] ç†è§£ç¼–ç å™¨-è§£ç å™¨æ¶æ„\n",
    "- [ ] å®ç°åŸºç¡€ Seq2Seq æ¨¡å‹\n",
    "- [ ] ç†è§£ Teacher Forcing è®­ç»ƒç­–ç•¥\n",
    "- [ ] äº†è§£ Seq2Seq çš„å±€é™æ€§\n",
    "\n",
    "---\n",
    "\n",
    "## Seq2Seq æ¦‚è¿°\n",
    "\n",
    "**åº”ç”¨åœºæ™¯**ï¼š\n",
    "- æœºå™¨ç¿»è¯‘ï¼šè‹±æ–‡ â†’ ä¸­æ–‡\n",
    "- æ–‡æœ¬æ‘˜è¦ï¼šé•¿æ–‡æœ¬ â†’ çŸ­æ‘˜è¦\n",
    "- å¯¹è¯ç³»ç»Ÿï¼šé—®é¢˜ â†’ å›ç­”\n",
    "- è¯­éŸ³è¯†åˆ«ï¼šéŸ³é¢‘ â†’ æ–‡æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šSeq2Seq æ¶æ„\n",
    "\n",
    "### 1.1 ç¼–ç å™¨-è§£ç å™¨ç»“æ„\n",
    "\n",
    "```\n",
    "ç¼–ç å™¨ (Encoder):\n",
    "[x1] â†’ [x2] â†’ [x3] â†’ [x4] â†’ [context]\n",
    "  \"    I     love    you\"      â†“\n",
    "                               â†“\n",
    "è§£ç å™¨ (Decoder):              â†“\n",
    "         [context] â†’ [y1] â†’ [y2] â†’ [y3]\n",
    "                      \"æˆ‘\"   \"çˆ±\"   \"ä½ \"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ç¼–ç å™¨ï¼šå°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºä¸Šä¸‹æ–‡å‘é‡\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        src: (batch, src_len)\n",
    "        è¿”å›: hidden, cell (ç¼–ç åçš„ä¸Šä¸‹æ–‡)\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    è§£ç å™¨ï¼šæ ¹æ®ä¸Šä¸‹æ–‡å’Œä¹‹å‰çš„è¾“å‡ºç”Ÿæˆç›®æ ‡åºåˆ—\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers,\n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \"\"\"\n",
    "        input: (batch, 1) å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥\n",
    "        è¿”å›: output, hidden, cell\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„ Seq2Seq æ¨¡å‹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        src: (batch, src_len) æºåºåˆ—\n",
    "        trg: (batch, trg_len) ç›®æ ‡åºåˆ—\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        # å­˜å‚¨è¾“å‡º\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # ç¼–ç \n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯ <sos>\n",
    "        input = trg[:, 0:1]\n",
    "        \n",
    "        # é€æ­¥è§£ç \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            # Teacher Forcing\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input = trg[:, t:t+1] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "SRC_VOCAB = 1000\n",
    "TRG_VOCAB = 1000\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB, EMBED_DIM, HIDDEN_SIZE)\n",
    "decoder = Decoder(TRG_VOCAB, EMBED_DIM, HIDDEN_SIZE)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "print(f\"æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šä¿¡æ¯ç“¶é¢ˆé—®é¢˜\n",
    "\n",
    "### Seq2Seq çš„å±€é™æ€§\n",
    "\n",
    "æ‰€æœ‰æºåºåˆ—ä¿¡æ¯è¢«å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå¤§å°çš„ä¸Šä¸‹æ–‡å‘é‡ä¸­ï¼š\n",
    "\n",
    "```\n",
    "[å¾ˆé•¿çš„è¾“å…¥åºåˆ—...] â†’ [å•ä¸ªå‘é‡] â†’ [è¾“å‡ºåºåˆ—]\n",
    "                        â†‘\n",
    "                    ä¿¡æ¯ç“¶é¢ˆï¼\n",
    "```\n",
    "\n",
    "è¿™ä¸ªé—®é¢˜é€šè¿‡**æ³¨æ„åŠ›æœºåˆ¶**è§£å†³ï¼ˆä¸‹ä¸€ç« ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bottleneck():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–ä¿¡æ¯ç“¶é¢ˆé—®é¢˜\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    # ç¼–ç å™¨\n",
    "    for i in range(5):\n",
    "        rect = plt.Rectangle((i*1.5, 3), 1, 1, fill=True, \n",
    "                              facecolor='lightblue', edgecolor='blue')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(i*1.5 + 0.5, 3.5, f'$x_{i+1}$', ha='center', va='center')\n",
    "    \n",
    "    # ç“¶é¢ˆ\n",
    "    circle = plt.Circle((8, 3.5), 0.5, fill=True, facecolor='red', alpha=0.7)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(8, 3.5, 'ctx', ha='center', va='center', color='white', fontweight='bold')\n",
    "    \n",
    "    # è§£ç å™¨\n",
    "    for i in range(4):\n",
    "        rect = plt.Rectangle((9.5 + i*1.5, 3), 1, 1, fill=True,\n",
    "                              facecolor='lightgreen', edgecolor='green')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(9.5 + i*1.5 + 0.5, 3.5, f'$y_{i+1}$', ha='center', va='center')\n",
    "    \n",
    "    # ç®­å¤´\n",
    "    ax.annotate('', xy=(7.5, 3.5), xytext=(6.5, 3.5),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    ax.annotate('', xy=(9.5, 3.5), xytext=(8.5, 3.5),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    \n",
    "    ax.set_xlim(-1, 16)\n",
    "    ax.set_ylim(1, 6)\n",
    "    ax.set_title('Seq2Seq ä¿¡æ¯ç“¶é¢ˆï¼šæ‰€æœ‰ä¿¡æ¯å‹ç¼©åˆ°å•ä¸ªå‘é‡', fontsize=14)\n",
    "    ax.text(8, 2, 'ç“¶é¢ˆï¼', ha='center', fontsize=12, color='red')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_bottleneck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## æœ¬ç« æ€»ç»“\n",
    "\n",
    "1. **ç¼–ç å™¨**ï¼šå°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºä¸Šä¸‹æ–‡å‘é‡\n",
    "2. **è§£ç å™¨**ï¼šæ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆè¾“å‡ºåºåˆ—\n",
    "3. **Teacher Forcing**ï¼šè®­ç»ƒæ—¶ä½¿ç”¨çœŸå®ç›®æ ‡ä½œä¸ºè¾“å…¥\n",
    "4. **ä¿¡æ¯ç“¶é¢ˆ**ï¼šå›ºå®šå¤§å°çš„ä¸Šä¸‹æ–‡å‘é‡é™åˆ¶äº†æ€§èƒ½\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸‹ä¸€ç« é¢„å‘Š\n",
    "\n",
    "**ç¬¬ 10 ç« ï¼šæ³¨æ„åŠ›æœºåˆ¶**\n",
    "\n",
    "ğŸ‘‰ [10_attention_mechanism.ipynb](./10_attention_mechanism.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
