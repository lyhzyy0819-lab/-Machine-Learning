# 4.3 LSTM/GRU 及序列建模

> 长短期记忆网络、门控循环单元及其应用

---

## 学习目标

- [ ] 深入理解 LSTM 的门控机制
- [ ] 掌握 GRU 的简化设计
- [ ] 实现双向和多层 RNN
- [ ] 构建语言模型和 Seq2Seq 架构
- [ ] 理解注意力机制的原理
- [ ] 应用于时间序列预测

---

## 课程内容

### 核心理论

| 编号 | 文件 | 内容 |
|------|------|------|
| 01 | lstm_deep_dive | LSTM 深入：遗忘门、输入门、输出门 |
| 02 | gru_simplified | GRU：LSTM 的简化版本 |
| 03 | bidirectional_multilayer | 双向 RNN 和多层堆叠 |

### 应用架构

| 编号 | 文件 | 内容 |
|------|------|------|
| 04 | language_model | 字符级/词级语言模型 |
| 05 | seq2seq_architecture | 编码器-解码器架构 |
| 06 | attention_mechanism | 注意力机制 |
| 07 | time_series_forecasting | 时间序列预测 |
| 08 | lstm_projects | 综合项目 |

---

## 练习

| 文件 | 内容 |
|------|------|
| exercise_01 | LSTM 从零实现 |
| exercise_02 | 文本分类 |
| exercise_03 | Seq2Seq 模型 |
| exercise_04 | 时间序列预测 |

---

## 前置知识

- **4.2_rnn** - RNN 基础（必须先学）
- Python、PyTorch 基础
- 梯度下降和反向传播

---

## LSTM vs GRU 速查

| 特性 | LSTM | GRU |
|------|------|-----|
| 门数量 | 3（遗忘、输入、输出） | 2（重置、更新） |
| 隐状态 | h_t 和 c_t | 仅 h_t |
| 参数量 | 较多 | 较少 |
| 性能 | 长序列更好 | 短序列相当 |
| 训练速度 | 较慢 | 较快 |

---

## 学习路线建议

```
LSTM 深入 (01) → GRU (02) → 双向/多层 (03)
                     ↓
语言模型 (04) → Seq2Seq (05) → 注意力 (06)
                     ↓
            时间序列 (07) → 项目 (08)
```

---

*最后更新：2025-01*
