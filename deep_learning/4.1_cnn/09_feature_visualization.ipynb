{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 ç‰¹å¾å¯è§†åŒ–\n",
    "\n",
    "> ç†è§£ CNN \"çœ‹åˆ°äº†ä»€ä¹ˆ\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "- [ ] å¯è§†åŒ–å·ç§¯æ ¸\n",
    "- [ ] å¯è§†åŒ–ç‰¹å¾å›¾\n",
    "- [ ] ä½¿ç”¨ Grad-CAM è§£é‡Šæ¨¡å‹å†³ç­–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as transforms\n",
    "    from PIL import Image\n",
    "    HAS_TORCH = True\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\"è¯·å®‰è£… PyTorch\")\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’» 1. å¯è§†åŒ–å·ç§¯æ ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TORCH:\n",
    "    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "    model = models.vgg16(weights='DEFAULT')\n",
    "    \n",
    "    # è·å–ç¬¬ä¸€å±‚å·ç§¯æ ¸\n",
    "    # VGG16 çš„ç¬¬ä¸€å±‚: 3 è¾“å…¥é€šé“ â†’ 64 è¾“å‡ºé€šé“, 3Ã—3 æ ¸\n",
    "    first_conv = model.features[0]\n",
    "    kernels = first_conv.weight.data.cpu().numpy()\n",
    "    \n",
    "    print(f\"ç¬¬ä¸€å±‚å·ç§¯æ ¸å½¢çŠ¶: {kernels.shape}\")\n",
    "    print(f\"  = (64 ä¸ªæ ¸, 3 é€šé“, 3Ã—3)\")\n",
    "    \n",
    "    # å¯è§†åŒ–å‰ 16 ä¸ªæ ¸ï¼ˆå– RGB å¹³å‡ï¼‰\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < 16:\n",
    "            kernel = kernels[i]\n",
    "            # å½’ä¸€åŒ–åˆ° [0, 1]\n",
    "            kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())\n",
    "            # è½¬ç½®ä¸º (H, W, C)\n",
    "            kernel = np.transpose(kernel, (1, 2, 0))\n",
    "            ax.imshow(kernel)\n",
    "            ax.set_title(f'æ ¸ {i}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('VGG16 ç¬¬ä¸€å±‚å·ç§¯æ ¸å¯è§†åŒ–', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ç¬¬ä¸€å±‚æ ¸é€šå¸¸æ£€æµ‹è¾¹ç¼˜ã€é¢œè‰²ç­‰åŸºç¡€ç‰¹å¾\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’» 2. å¯è§†åŒ–ç‰¹å¾å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TORCH:\n",
    "    # åˆ›å»ºä¸€ä¸ªç®€å•çš„é’©å­æ¥æ•è·ç‰¹å¾å›¾\n",
    "    activation = {}\n",
    "    \n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # æ³¨å†Œé’©å­\n",
    "    model.features[0].register_forward_hook(get_activation('conv1'))\n",
    "    model.features[10].register_forward_hook(get_activation('conv5'))\n",
    "    model.features[24].register_forward_hook(get_activation('conv10'))\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªéšæœºè¾“å…¥\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # å¯è§†åŒ–ä¸åŒå±‚çš„ç‰¹å¾å›¾\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for ax, (name, feat) in zip(axes, activation.items()):\n",
    "        # å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œç¬¬ä¸€ä¸ªé€šé“\n",
    "        feature_map = feat[0, 0].numpy()\n",
    "        ax.imshow(feature_map, cmap='viridis')\n",
    "        ax.set_title(f'{name}\\nå½¢çŠ¶: {list(feat.shape)}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('ä¸åŒæ·±åº¦çš„ç‰¹å¾å›¾', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ’¡ æ³¨æ„ç‰¹å¾å›¾å°ºå¯¸éšæ·±åº¦å‡å°ï¼Œè€ŒæŠ½è±¡ç¨‹åº¦å¢åŠ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’» 3. Grad-CAM ç±»æ¿€æ´»å›¾\n",
    "\n",
    "Grad-CAM æ˜¾ç¤ºæ¨¡å‹åœ¨åšå†³ç­–æ—¶\"å…³æ³¨\"å›¾åƒçš„å“ªäº›åŒºåŸŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TORCH:\n",
    "    class GradCAM:\n",
    "        \"\"\"\n",
    "        Grad-CAM å®ç°\n",
    "        \n",
    "        å…¬å¼:\n",
    "            1. Î±_k = GAP(âˆ‚y_c/âˆ‚A_k)  (æ¢¯åº¦çš„å…¨å±€å¹³å‡æ± åŒ–)\n",
    "            2. L_cam = ReLU(Î£_k Î±_k Â· A_k)  (åŠ æƒå’Œ)\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, model, target_layer):\n",
    "            self.model = model\n",
    "            self.target_layer = target_layer\n",
    "            self.gradients = None\n",
    "            self.activations = None\n",
    "            \n",
    "            # æ³¨å†Œé’©å­\n",
    "            target_layer.register_forward_hook(self.save_activation)\n",
    "            target_layer.register_full_backward_hook(self.save_gradient)\n",
    "        \n",
    "        def save_activation(self, module, input, output):\n",
    "            self.activations = output\n",
    "        \n",
    "        def save_gradient(self, module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "        \n",
    "        def __call__(self, x, class_idx=None):\n",
    "            # å‰å‘ä¼ æ’­\n",
    "            output = self.model(x)\n",
    "            \n",
    "            if class_idx is None:\n",
    "                class_idx = output.argmax(dim=1).item()\n",
    "            \n",
    "            # åå‘ä¼ æ’­ï¼ˆå¯¹ç‰¹å®šç±»åˆ«ï¼‰\n",
    "            self.model.zero_grad()\n",
    "            one_hot = torch.zeros_like(output)\n",
    "            one_hot[0, class_idx] = 1\n",
    "            output.backward(gradient=one_hot, retain_graph=True)\n",
    "            \n",
    "            # è®¡ç®— Grad-CAM\n",
    "            gradients = self.gradients.cpu().data.numpy()[0]  # (C, H, W)\n",
    "            activations = self.activations.cpu().data.numpy()[0]  # (C, H, W)\n",
    "            \n",
    "            # æƒé‡ = æ¢¯åº¦çš„å…¨å±€å¹³å‡æ± åŒ–\n",
    "            weights = np.mean(gradients, axis=(1, 2))  # (C,)\n",
    "            \n",
    "            # åŠ æƒå’Œ\n",
    "            cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "            for i, w in enumerate(weights):\n",
    "                cam += w * activations[i]\n",
    "            \n",
    "            # ReLU\n",
    "            cam = np.maximum(cam, 0)\n",
    "            \n",
    "            # å½’ä¸€åŒ–åˆ° [0, 1]\n",
    "            cam = cam - cam.min()\n",
    "            cam = cam / cam.max()\n",
    "            \n",
    "            return cam, class_idx\n",
    "    \n",
    "    print(\"GradCAM ç±»å·²å®šä¹‰\")\n",
    "    print(\"\\nä½¿ç”¨æ–¹æ³•:\")\n",
    "    print(\"  grad_cam = GradCAM(model, model.features[-1])\")\n",
    "    print(\"  cam, class_idx = grad_cam(image_tensor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TORCH:\n",
    "    # æ¼”ç¤º Grad-CAMï¼ˆä½¿ç”¨éšæœºå›¾åƒï¼‰\n",
    "    model = models.resnet18(weights='DEFAULT')\n",
    "    model.eval()\n",
    "    \n",
    "    # åˆ›å»º GradCAM\n",
    "    grad_cam = GradCAM(model, model.layer4)\n",
    "    \n",
    "    # éšæœºè¾“å…¥\n",
    "    x = torch.randn(1, 3, 224, 224)\n",
    "    x.requires_grad = True\n",
    "    \n",
    "    # è®¡ç®— CAM\n",
    "    cam, class_idx = grad_cam(x)\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # åŸå›¾ï¼ˆå–å‰3é€šé“ï¼‰\n",
    "    img = x[0].detach().numpy().transpose(1, 2, 0)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('è¾“å…¥å›¾åƒ')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # CAM çƒ­åŠ›å›¾\n",
    "    import cv2\n",
    "    cam_resized = cv2.resize(cam, (224, 224))\n",
    "    axes[1].imshow(img)\n",
    "    axes[1].imshow(cam_resized, alpha=0.5, cmap='jet')\n",
    "    axes[1].set_title(f'Grad-CAM (é¢„æµ‹ç±»åˆ«: {class_idx})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ çº¢è‰²åŒºåŸŸæ˜¯æ¨¡å‹åšå†³ç­–æ—¶å…³æ³¨çš„åŒºåŸŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ æœ¬ç« å°ç»“\n",
    "\n",
    "### å¯è§†åŒ–æŠ€æœ¯å¯¹æ¯”\n",
    "\n",
    "| æŠ€æœ¯ | ç›®çš„ | æ–¹æ³• |\n",
    "|------|------|------|\n",
    "| å·ç§¯æ ¸å¯è§†åŒ– | çœ‹å­¦åˆ°äº†ä»€ä¹ˆæ»¤æ³¢å™¨ | ç›´æ¥æ˜¾ç¤ºæƒé‡ |\n",
    "| ç‰¹å¾å›¾å¯è§†åŒ– | çœ‹ç‰¹å¾è¡¨ç¤º | æ˜¾ç¤ºä¸­é—´å±‚è¾“å‡º |\n",
    "| Grad-CAM | è§£é‡Šå†³ç­– | æ¢¯åº¦åŠ æƒæ¿€æ´» |\n",
    "\n",
    "### å…³é”®æ´å¯Ÿ\n",
    "\n",
    "- æµ…å±‚ç‰¹å¾ï¼šè¾¹ç¼˜ã€é¢œè‰²ã€çº¹ç†\n",
    "- æ·±å±‚ç‰¹å¾ï¼šå½¢çŠ¶ã€éƒ¨ä»¶ã€è¯­ä¹‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ å®Œæˆ 4.1 CNN æ¨¡å—ï¼\n",
    "\n",
    "æ­å–œå®Œæˆ CNN å­¦ä¹ ï¼æ¥ä¸‹æ¥å¯ä»¥ï¼š\n",
    "\n",
    "1. å®Œæˆ **project_mnist_classifier.py** é¡¹ç›®\n",
    "2. è¿›å…¥ **4.2 RNN** å­¦ä¹ åºåˆ—æ¨¡å‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
