# CNN 架构设计常见问题 (FAQ)

> 本文档整理了学习 CNN 架构时常见的疑惑，以 `SimpleCNN_Numpy` 为例进行解答。

---

## Q1: `__init__` 中的权重为什么要这样初始化？

### 代码示例

```python
class SimpleCNN_Numpy:
    def __init__(self):
        # Conv1: 8个滤波器，1个输入通道，3×3核
        self.conv1_w = np.random.randn(8, 1, 3, 3) * np.sqrt(2.0 / (1*3*3))
        self.conv1_b = np.zeros(8)

        # Conv2: 16个滤波器，8个输入通道，3×3核
        self.conv2_w = np.random.randn(16, 8, 3, 3) * np.sqrt(2.0 / (8*3*3))
        self.conv2_b = np.zeros(16)

        # FC1
        self.fc1_w = np.random.randn(784, 64) * np.sqrt(2.0 / 784)
        self.fc1_b = np.zeros(64)

        # FC2
        self.fc2_w = np.random.randn(64, 10) * np.sqrt(2.0 / 64)
        self.fc2_b = np.zeros(10)
```

### 权重形状 `(C_out, C_in, k_h, k_w)` 的含义

| 维度 | 符号 | 含义 |
|------|------|------|
| 第1维 | C_out | 输出通道数（滤波器数量） |
| 第2维 | C_in | 输入通道数 |
| 第3维 | k_h | 卷积核高度 |
| 第4维 | k_w | 卷积核宽度 |

**为什么这样排列？** 卷积操作时，每个输出通道都需要一个完整的滤波器（覆盖所有输入通道）。形状 `(C_out, C_in, k_h, k_w)` 方便按 `c_out` 索引取出对应滤波器。

### He 初始化的数学原理

```python
np.random.randn(...) * np.sqrt(2.0 / n_in)
```

这是 **He 初始化**（又叫 Kaiming 初始化），专门为 ReLU 激活函数设计。

**为什么不能简单用 `np.random.randn()`？**

- **权重太大** → 前向传播时数值爆炸，梯度也爆炸
- **权重太小** → 信号逐层衰减，梯度消失

**目标**：让每层输出的方差保持稳定（不放大也不缩小）

对于 ReLU 激活函数：
```
Var(output) = Var(input) × (n_in / 2) × Var(W)
```

要让 `Var(output) = Var(input)`，需要：
```
Var(W) = 2 / n_in
std(W) = sqrt(2 / n_in)
```

**各层的 n_in 计算**：

| 层 | n_in 计算 | 值 |
|---|---|---|
| conv1 | 1 × 3 × 3 | 9 |
| conv2 | 8 × 3 × 3 | 72 |
| fc1 | 784 | 784 |
| fc2 | 64 | 64 |

### 偏置为什么初始化为 0？

- 偏置对称性不是问题（不像权重那样需要打破对称）
- 配合 He 初始化的权重，0 偏置足够让网络正常启动
- 训练过程中会自动学习到合适的偏置值

---

## Q2: 为什么输入是 `(1, 28, 28)`？

### 三个维度的含义

| 维度 | 值 | 含义 |
|------|-----|------|
| **1** | C (通道数) | MNIST 是灰度图，只有1个颜色通道 |
| **28** | H (高度) | 图片高度 28 像素 |
| **28** | W (宽度) | 图片宽度 28 像素 |

### 对比：彩色图片 vs 灰度图片

```
灰度图 (MNIST):     (1, 28, 28)   ← 1个通道
彩色图 (RGB):       (3, H, W)     ← 3个通道 (红、绿、蓝)
```

### 为什么不直接用 `(28, 28)`？

虽然灰度图本质上是二维的，但保持 `(C, H, W)` 格式有几个原因：

1. **统一接口**：卷积层设计为处理多通道输入，用 `(1, 28, 28)` 保持接口一致

2. **代码复用**：同一个 `conv2d` 函数既能处理灰度图也能处理彩色图

3. **卷积核匹配**：
   ```python
   # 卷积核形状: (C_out, C_in, k_h, k_w)
   self.conv1_w = np.random.randn(8, 1, 3, 3)
   #                              ↑  ↑
   #                         8个输出 1个输入通道
   ```
   输入的通道维度 `1` 必须和卷积核的 `C_in=1` 匹配

### 数据流示意

```
原始 MNIST 数据: (28, 28) 二维数组
       │
       ▼ reshape 或 expand_dims
CNN 输入格式: (1, 28, 28)
       │
       ▼ conv2d
输出: (8, 28, 28)  ← 8个通道了
```

### 批处理时的形状

实际训练时还会加一个 batch 维度：

```
单张图片:  (1, 28, 28)        → (C, H, W)
一批图片:  (64, 1, 28, 28)    → (N, C, H, W)
           ↑
        batch_size=64
```

---

## Q3: 为什么 Conv1 权重是 `(8, 1, 3, 3)`？

### 每个维度的含义和设计理由

| 维度 | 值 | 含义 | 为什么选这个值？ |
|------|-----|------|------------------|
| **8** | C_out | 输出通道数（滤波器数量） | 设计选择，可以是任意值 |
| **1** | C_in | 输入通道数 | **必须**与输入匹配（MNIST灰度图=1） |
| **3** | k_h | 卷积核高度 | 常用选择，3×3 是最流行的 |
| **3** | k_w | 卷积核宽度 | 同上 |

### 详细解释

#### 1. 为什么 `C_in = 1`？（必须的）

这是**被动决定的**，必须和输入数据的通道数匹配：

```
输入图片: (1, 28, 28)
           ↑
           1个通道（灰度图）

所以 conv1 的 C_in 必须 = 1
```

如果输入是 RGB 彩色图 `(3, H, W)`，那 C_in 就必须是 3。

#### 2. 为什么 `C_out = 8`？（设计选择）

这是**自由选择的**，8 只是一个合理的起点：

- **太少**（如 2）：提取的特征不够丰富
- **太多**（如 128）：对于简单的 MNIST，计算浪费
- **8** 是一个平衡点，足够捕捉边缘、角点等基础特征

你完全可以改成 16、32 或其他值。

#### 3. 为什么 `3×3` 卷积核？（行业标准）

**3×3 是现代 CNN 的黄金标准**，原因：

| 卷积核大小 | 特点 |
|------------|------|
| 1×1 | 太小，只看单个像素，无法捕捉空间关系 |
| 3×3 | 刚好，能看到周围 8 个邻居，计算高效 |
| 5×5 | 可以用两个 3×3 替代，且参数更少 |
| 7×7 | 只在网络开头偶尔使用（如 ResNet 第一层） |

**数学上**：两个 3×3 卷积 = 一个 5×5 感受野，但参数更少：
```
5×5: 25 个参数
3×3 + 3×3: 9 + 9 = 18 个参数（更少！）
```

### 可视化：8 个滤波器在做什么？

```
输入 (1, 28, 28)
        │
        ▼
┌─────────────────────────────────────────┐
│  8 个 3×3 滤波器，每个学习不同的特征：   │
│                                         │
│  滤波器1: 检测 │ 垂直边缘               │
│  滤波器2: 检测 ─ 水平边缘               │
│  滤波器3: 检测 ╱ 斜边缘                 │
│  滤波器4: 检测 ╲ 斜边缘                 │
│  滤波器5: 检测角点                      │
│  滤波器6: 检测纹理                      │
│  滤波器7: ...                           │
│  滤波器8: ...                           │
└─────────────────────────────────────────┘
        │
        ▼
输出 (8, 28, 28)  ← 8 张特征图
```

### 总结

| 参数 | 性质 | 说明 |
|------|------|------|
| `C_in = 1` | **强制** | 必须匹配输入通道数 |
| `C_out = 8` | **可调** | 超参数，可以尝试不同值 |
| `k_h = k_w = 3` | **推荐** | 行业标准，效果好且高效 |

---

## Q4: 池化层必须是 2×2 吗？

### 池化层大小是可以自由选择的

2×2 只是最常用的选择，不是强制的。

### 对比计算：2×2 vs 3×3 池化

#### 当前：2×2 池化

```
输入: (1, 28, 28)
    ↓ conv1 (padding=1)
(8, 28, 28)
    ↓ max_pool 2×2       28 ÷ 2 = 14
(8, 14, 14)
    ↓ conv2 (padding=1)
(16, 14, 14)
    ↓ max_pool 2×2       14 ÷ 2 = 7
(16, 7, 7)
    ↓ flatten
16 × 7 × 7 = 784        ← fc1_w = (784, 64)
```

#### 如果改成：3×3 池化

```
输入: (1, 28, 28)
    ↓ conv1 (padding=1)
(8, 28, 28)
    ↓ max_pool 3×3       28 ÷ 3 = 9（向下取整）
(8, 9, 9)
    ↓ conv2 (padding=1)
(16, 9, 9)
    ↓ max_pool 3×3       9 ÷ 3 = 3
(16, 3, 3)
    ↓ flatten
16 × 3 × 3 = 144        ← fc1_w = (144, 64)
```

### 答案汇总

| 池化大小 | flatten 后维度 | fc1_w 形状 |
|----------|----------------|------------|
| 2×2 | 16 × 7 × 7 = **784** | (784, 64) |
| 3×3 | 16 × 3 × 3 = **144** | (144, 64) |

### 为什么 2×2 最常用？

| 池化大小 | 特点 |
|----------|------|
| **2×2** | 尺寸减半，信息保留较多，最常用 |
| **3×3** | 尺寸缩小更快，信息损失更多 |
| **4×4** | 很少用，下采样太激进 |

**经验法则**：
- 2×2 池化 + stride=2 是标准配置
- 现代网络有时用 stride=2 的卷积代替池化

### 注意：28 不能被 3 整除

```python
28 ÷ 3 = 9.33... → 向下取整为 9
```

这意味着会丢失边缘的一些像素。如果输入尺寸能被池化大小整除会更干净（如 27×27 图片用 3×3 池化）。

---

## 附录：完整架构可视化

```
SimpleCNN_Numpy 架构:

输入 (1, 28, 28)
        │
        ▼
┌─────────────────────────┐
│  Conv1: (8, 1, 3, 3)    │  ← 8个滤波器，每个处理1个通道
│  He init: sqrt(2/9)     │
└─────────────────────────┘
        │
        ▼ ReLU → MaxPool(2×2)

(8, 14, 14)
        │
        ▼
┌─────────────────────────┐
│  Conv2: (16, 8, 3, 3)   │  ← 16个滤波器，每个处理8个通道
│  He init: sqrt(2/72)    │
└─────────────────────────┘
        │
        ▼ ReLU → MaxPool(2×2) → Flatten

(784,)
        │
        ▼
┌─────────────────────────┐
│  FC1: (784, 64)         │
│  He init: sqrt(2/784)   │
└─────────────────────────┘
        │
        ▼ ReLU

(64,)
        │
        ▼
┌─────────────────────────┐
│  FC2: (64, 10)          │  ← 10类输出
│  He init: sqrt(2/64)    │
└─────────────────────────┘
        │
        ▼
输出 (10,) → 分类 logits
```

---

## 参考资料

- [He et al., 2015] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification
- [VGGNet] 证明了 3×3 卷积核的有效性
