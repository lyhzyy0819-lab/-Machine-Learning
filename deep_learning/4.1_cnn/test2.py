"""
卷积层实现（从零开始，包含前向和反向传播）

本文件实现了 2D 卷积层的完整前向和反向传播过程。
"""

import numpy as np


class ConvLayer:
    """
    2D 卷积层（单通道简化版）

    ============================================================
    前向传播公式：
    ============================================================

    Y[i, j] = Σ_m Σ_n X[i+m, j+n] · W[m, n] + b

    符号说明：
        X: 输入特征图，形状 (H, W)
        W: 卷积核，形状 (k, k)
        b: 偏置，标量
        Y: 输出特征图，形状 (H-k+1, W-k+1)
        i, j: 输出位置索引
        m, n: 卷积核内部索引（0 到 k-1）

    ============================================================
    图示：卷积前向传播（4×4 输入，2×2 核）
    ============================================================

    输入 X (4×4):                卷积核 W (2×2):
    ┌─────┬─────┬─────┬─────┐    ┌─────┬─────┐
    │  1  │  2  │  3  │  4  │    │ w00 │ w01 │
    ├─────┼─────┼─────┼─────┤    ├─────┼─────┤
    │  5  │  6  │  7  │  8  │    │ w10 │ w11 │
    ├─────┼─────┼─────┼─────┤    └─────┴─────┘
    │  9  │ 10  │ 11  │ 12  │
    ├─────┼─────┼─────┼─────┤
    │ 13  │ 14  │ 15  │ 16  │
    └─────┴─────┴─────┴─────┘

    输出 Y (3×3) 的计算过程：

    Y[0,0]: i=0, j=0
            窗口 = X[0:2, 0:2] = [[1,2], [5,6]]
            Y[0,0] = 1×w00 + 2×w01 + 5×w10 + 6×w11 + b

            ┌─────┬─────┐─────┬─────┐
            │▓ 1 ▓│▓ 2 ▓│  3  │  4  │
            ├─────┼─────┼─────┼─────┤
            │▓ 5 ▓│▓ 6 ▓│  7  │  8  │
            ├─────┼─────┼─────┼─────┤
            │  9  │ 10  │ 11  │ 12  │
            └─────┴─────┴─────┴─────┘

    Y[0,1]: i=0, j=1
            窗口 = X[0:2, 1:3] = [[2,3], [6,7]]
            Y[0,1] = 2×w00 + 3×w01 + 6×w10 + 7×w11 + b

            ┌─────┬─────┬─────┬─────┐
            │  1  │▓ 2 ▓│▓ 3 ▓│  4  │  ← 窗口右移 1 格
            ├─────┼─────┼─────┼─────┤
            │  5  │▓ 6 ▓│▓ 7 ▓│  8  │
            ├─────┼─────┼─────┼─────┤
            │  9  │ 10  │ 11  │ 12  │
            └─────┴─────┴─────┴─────┘

    Y[1,0]: i=1, j=0
            窗口 = X[1:3, 0:2] = [[5,6], [9,10]]
            Y[1,0] = 5×w00 + 6×w01 + 9×w10 + 10×w11 + b

            ┌─────┬─────┬─────┬─────┐
            │  1  │  2  │  3  │  4  │
            ├─────┼─────┼─────┼─────┤
            │▓ 5 ▓│▓ 6 ▓│  7  │  8  │  ← 窗口下移 1 格
            ├─────┼─────┼─────┼─────┤
            │▓ 9 ▓│▓10 ▓│ 11  │ 12  │
            └─────┴─────┴─────┴─────┘

    注意：卷积的 stride=1，所以窗口每次只移动 1 格
         这与池化不同（池化通常 stride=pool_size，窗口不重叠）

    ============================================================
    反向传播公式：
    ============================================================

    已知：上游梯度 dL/dY（记作 dY）

    1. 权重梯度 dW：
       dW[m, n] = Σ_i Σ_j dY[i, j] · X[i+m, j+n]
       即：dW = X ★ dY（X 和 dY 的互相关/卷积）

    2. 偏置梯度 db：
       db = Σ_i Σ_j dY[i, j]
       即：db = sum(dY)

    3. 输入梯度 dX：
       dX = dY ★_full rot180(W)
       即：对 dY 进行零填充后，与旋转180°的 W 做卷积

    ============================================================
    参数：
        kernel_size: 卷积核大小（正方形，如 3 表示 3×3）
    ============================================================
    """

    def __init__(self, kernel_size):
        """
        初始化卷积层

        参数：
            kernel_size: 卷积核大小（如 3 表示 3×3 核）
        """
        self.kernel_size = kernel_size

        # ========================================
        # He 初始化（Kaiming 初始化）
        # ========================================
        # 公式：W ~ N(0, sqrt(2/n_in))
        # 其中 n_in = kernel_size × kernel_size（输入神经元数）
        # 目的：保持前向传播时信号方差稳定，防止梯度消失/爆炸
        scale = np.sqrt(2.0 / (self.kernel_size * self.kernel_size))
        self.W = np.random.randn(kernel_size, kernel_size) * scale

        # 偏置初始化为 0
        self.b = 0.0

        # ========================================
        # 缓存（反向传播需要用到前向传播时的输入）
        # ========================================
        self.X = None   # 保存输入

        # ========================================
        # 梯度存储
        # ========================================
        self.dw = None  # 权重梯度 dL/dW
        self.db = None  # 偏置梯度 dL/db

    def forward(self, X):
        """
        前向传播

        公式：Y[i, j] = Σ_m Σ_n X[i+m, j+n] · W[m, n] + b

        参数：
            X: 输入特征图，形状 (H, W)

        返回：
            Y: 输出特征图，形状 (H-k+1, W-k+1)

        示例（3×3 核，5×5 输入）：
            输入: (5, 5)
            输出: (5-3+1, 5-3+1) = (3, 3)
        """
        # 保存输入（反向传播时需要）
        self.X = X

        # 获取输入尺寸
        H, W_in = X.shape       # H=高度, W_in=宽度
        k = self.kernel_size    # 卷积核大小

        # ========================================
        # 计算输出尺寸
        # ========================================
        # 公式：out_size = input_size - kernel_size + 1
        # 例如：5 - 3 + 1 = 3
        out_h = H - k + 1
        out_w = W_in - k + 1

        # 初始化输出
        Y = np.zeros((out_h, out_w))

        # ========================================
        # 卷积计算（滑动窗口）
        # ========================================
        # 遍历输出的每个位置 (i, j)
        for i in range(out_h):
            for j in range(out_w):
                # 提取感受野（receptive field）
                # window 是输入 X 中与当前输出位置对应的 k×k 区域
                # X[i:i+k, j:j+k] 表示从 (i,j) 开始的 k×k 窗口
                window = X[i:i+k, j:j+k]

                # 逐元素相乘后求和，再加偏置
                # 这就是公式 Y[i,j] = Σ_m Σ_n X[i+m, j+n] · W[m,n] + b
                Y[i, j] = np.sum(window * self.W) + self.b

        return Y

    def backward(self, dY):
        """
        反向传播

        已知：dY = dL/dY（损失对输出的梯度，从下一层传来）

        计算：
            1. dW = dL/dW（权重梯度，用于更新权重）
            2. db = dL/db（偏置梯度，用于更新偏置）
            3. dX = dL/dX（输入梯度，传给前一层）

        参数：
            dY: 上游梯度，形状 (out_h, out_w)

        返回：
            dX: 输入梯度，形状 (H, W)
        """
        # 取出缓存的输入
        X = self.X
        H, W_in = X.shape           # 输入尺寸
        k = self.kernel_size        # 卷积核大小
        out_h, out_w = dY.shape     # 输出/梯度尺寸

        # ========================================
        # 1. 计算偏置梯度 db
        # ========================================
        # 公式：db = Σ_i Σ_j dY[i, j] = sum(dY)
        #
        # 推导：
        #   前向：Y[i,j] = ... + b（b 加到每个输出位置）
        #   所以每个 dY[i,j] 都要传给 b
        #   db = Σ dY[i,j]
        self.db = np.sum(dY)

        # ========================================
        # 2. 计算权重梯度 dW
        # ========================================
        # 公式：dW[m, n] = Σ_i Σ_j dY[i, j] · X[i+m, j+n]
        #
        # 推导（链式法则）：
        #   W[m,n] 影响所有 Y[i,j]（因为 W 在每个位置都被使用）
        #   dL/dW[m,n] = Σ_i Σ_j (dL/dY[i,j]) · (dY[i,j]/dW[m,n])
        #             = Σ_i Σ_j dY[i,j] · X[i+m, j+n]
        #
        # 直觉：这就是 X 和 dY 的互相关（类似卷积）！
        self.dw = np.zeros_like(self.W)
        for m in range(k):
            for n in range(k):
                # X 中对应的区域：从 (m, n) 开始，大小为 (out_h, out_w)
                # 这正是前向传播时 W[m,n] 所乘的所有 X 元素
                X_region = X[m:m+out_h, n:n+out_w]

                # 对应元素相乘再求和
                self.dw[m, n] = np.sum(dY * X_region)

        # ========================================
        # 3. 计算输入梯度 dX
        # ========================================
        # 公式：dX = dY ★_full rot180(W)
        #
        # 步骤：
        #   (1) 将 W 旋转 180°
        #   (2) 对 dY 进行零填充（padding = k-1）
        #   (3) 执行普通卷积
        #
        # 推导（链式法则）：
        #   X[i,j] 参与了哪些 Y 的计算？
        #   它被 W 的不同位置 W[m,n] 使用，贡献到 Y[i-m, j-n]
        #   所以：dL/dX[i,j] = Σ_m Σ_n dY[i-m, j-n] · W[m,n]
        #   这等价于 dY 与 rot180(W) 的 full 模式卷积

        # 步骤 (1)：旋转 W 180°
        # rot90(W, 2) 表示旋转 90° 两次 = 180°
        w_rot = np.rot90(self.W, 2)

        # 步骤 (2)：对 dY 进行零填充
        # padding 大小 = kernel_size - 1
        # 例如 3×3 核，padding = 2
        # 这样卷积后输出尺寸才能恢复到输入尺寸
        pad = k - 1
        dY_padded = np.pad(dY, pad, mode='constant', constant_values=0)
        # dY: (out_h, out_w) → dY_padded: (out_h + 2*pad, out_w + 2*pad)

        # 步骤 (3)：卷积计算 dX
        dX = np.zeros((H, W_in))
        for i in range(H):
            for j in range(W_in):
                # 从填充后的 dY 中提取 k×k 窗口
                window = dY_padded[i:i+k, j:j+k]
                # 与旋转后的 W 做逐元素乘法并求和
                dX[i, j] = np.sum(window * w_rot)

        return dX


class MaxPoolLayer:
    """
    2D 最大池化层

    ============================================================
    前向传播公式：
    ============================================================

    Y[i, j] = max(窗口内所有值)

    窗口位置计算：
        i_start = i × stride
        j_start = j × stride
        窗口 = X[i_start : i_start+p, j_start : j_start+p]

    ============================================================
    图示：为什么 i_start = i × stride？
    ============================================================

    以 4×4 输入，2×2 池化，stride=2 为例：

    输入 X (4×4):
    ┌─────┬─────┬─────┬─────┐
    │  1  │  3  │  2  │  4  │  行0
    ├─────┼─────┼─────┼─────┤
    │  5  │  6  │  7  │  8  │  行1
    ├─────┼─────┼─────┼─────┤
    │  9  │  2  │  3  │  1  │  行2
    ├─────┼─────┼─────┼─────┤
    │  4  │  5  │  6  │  7  │  行3
    └─────┴─────┴─────┴─────┘
      列0   列1   列2   列3

    输出 Y (2×2) 的计算过程：

    ┌──────────────────────────────────────────────────────────┐
    │ Y[0,0]: i=0, j=0                                         │
    │         i_start = 0 × 2 = 0                              │
    │         j_start = 0 × 2 = 0                              │
    │         窗口 = X[0:2, 0:2] = [[1,3], [5,6]]              │
    │         Y[0,0] = max(1,3,5,6) = 6                        │
    │                                                          │
    │         ┌─────┬─────┐─────┬─────┐                        │
    │         │▓ 1 ▓│▓ 3 ▓│  2  │  4  │                        │
    │         ├─────┼─────┼─────┼─────┤                        │
    │         │▓ 5 ▓│▓ 6 ▓│  7  │  8  │  ← 窗口在这里         │
    │         ├─────┼─────┼─────┼─────┤                        │
    │         │  9  │  2  │  3  │  1  │                        │
    │         └─────┴─────┴─────┴─────┘                        │
    └──────────────────────────────────────────────────────────┘

    ┌──────────────────────────────────────────────────────────┐
    │ Y[0,1]: i=0, j=1                                         │
    │         i_start = 0 × 2 = 0                              │
    │         j_start = 1 × 2 = 2   ← j=1 所以跳过 2 列        │
    │         窗口 = X[0:2, 2:4] = [[2,4], [7,8]]              │
    │         Y[0,1] = max(2,4,7,8) = 8                        │
    │                                                          │
    │         ┌─────┬─────┬─────┬─────┐                        │
    │         │  1  │  3  │▓ 2 ▓│▓ 4 ▓│                        │
    │         ├─────┼─────┼─────┼─────┤                        │
    │         │  5  │  6  │▓ 7 ▓│▓ 8 ▓│  ← 窗口移到右边       │
    │         ├─────┼─────┼─────┼─────┤                        │
    │         │  9  │  2  │  3  │  1  │                        │
    │         └─────┴─────┴─────┴─────┘                        │
    └──────────────────────────────────────────────────────────┘

    ┌──────────────────────────────────────────────────────────┐
    │ Y[1,0]: i=1, j=0                                         │
    │         i_start = 1 × 2 = 2   ← i=1 所以跳过 2 行        │
    │         j_start = 0 × 2 = 0                              │
    │         窗口 = X[2:4, 0:2] = [[9,2], [4,5]]              │
    │         Y[1,0] = max(9,2,4,5) = 9                        │
    │                                                          │
    │         ┌─────┬─────┬─────┬─────┐                        │
    │         │  1  │  3  │  2  │  4  │                        │
    │         ├─────┼─────┼─────┼─────┤                        │
    │         │  5  │  6  │  7  │  8  │                        │
    │         ├─────┼─────┼─────┼─────┤                        │
    │         │▓ 9 ▓│▓ 2 ▓│  3  │  1  │  ← 窗口移到下面       │
    │         ├─────┼─────┼─────┼─────┤                        │
    │         │▓ 4 ▓│▓ 5 ▓│  6  │  7  │                        │
    │         └─────┴─────┴─────┴─────┘                        │
    └──────────────────────────────────────────────────────────┘

    ┌──────────────────────────────────────────────────────────┐
    │ Y[1,1]: i=1, j=1                                         │
    │         i_start = 1 × 2 = 2                              │
    │         j_start = 1 × 2 = 2                              │
    │         窗口 = X[2:4, 2:4] = [[3,1], [6,7]]              │
    │         Y[1,1] = max(3,1,6,7) = 7                        │
    └──────────────────────────────────────────────────────────┘

    最终输出 Y:
    ┌─────┬─────┐
    │  6  │  8  │
    ├─────┼─────┤
    │  9  │  7  │
    └─────┴─────┘

    ============================================================
    核心理解：i_start = i × stride 的含义
    ============================================================

    stride（步幅）表示窗口每次移动的距离

    当 stride = pool_size = 2 时：
    - 窗口不重叠，紧密排列
    - 第 0 个窗口从位置 0 开始 → 0 × 2 = 0
    - 第 1 个窗口从位置 2 开始 → 1 × 2 = 2
    - 第 i 个窗口从位置 i×2 开始 → i × stride

    通用公式：
        输出位置 (i, j) → 输入窗口起始位置 (i × stride, j × stride)

    ============================================================
    反向传播：梯度只传给最大值位置
    ============================================================

    规则：每个窗口中，只有最大值位置接收梯度，其他位置梯度为 0

    图示（4×4 输入，2×2 池化）：

    前向传播时记录了最大值位置：
    ┌─────┬─────┬─────┬─────┐
    │  1  │  3  │  2  │  4  │
    ├─────┼─────┼─────┼─────┤        输出 Y (2×2):
    │  5  │【6】│  7  │【8】│        ┌─────┬─────┐
    ├─────┼─────┼─────┼─────┤        │  6  │  8  │
    │【9】│  2  │  3  │  1  │        ├─────┼─────┤
    ├─────┼─────┼─────┼─────┤        │  9  │  7  │
    │  4  │  5  │  6  │【7】│        └─────┴─────┘
    └─────┴─────┴─────┴─────┘
    【】标记的是每个窗口的最大值

    反向传播时：

    上游梯度 dY (2×2):
    ┌──────┬──────┐
    │ 0.1  │ 0.2  │
    ├──────┼──────┤
    │ 0.3  │ 0.4  │
    └──────┴──────┘

    输入梯度 dX (4×4)：
    ┌──────┬──────┬──────┬──────┐
    │  0   │  0   │  0   │  0   │
    ├──────┼──────┼──────┼──────┤
    │  0   │ 0.1  │  0   │ 0.2  │  ← dY[0,0]=0.1 传给位置(1,1)
    ├──────┼──────┼──────┼──────┤    dY[0,1]=0.2 传给位置(1,3)
    │ 0.3  │  0   │  0   │  0   │  ← dY[1,0]=0.3 传给位置(2,0)
    ├──────┼──────┼──────┼──────┤
    │  0   │  0   │  0   │ 0.4  │  ← dY[1,1]=0.4 传给位置(3,3)
    └──────┴──────┴──────┴──────┘

    直觉理解：
    - 前向传播时，只有最大值"赢得"了输出
    - 反向传播时，梯度只流向"赢家"
    - 其他位置对输出没有贡献，所以梯度为 0
    """

    def __init__(self, pool_size=2, stride=None):
        """
        初始化池化层

        参数：
            pool_size: 池化窗口大小（如 2 表示 2×2 窗口）
            stride: 步幅，默认等于 pool_size（窗口不重叠）
        """
        self.pool_size = pool_size
        # 如果没有指定 stride，默认等于 pool_size（窗口不重叠）
        self.stride = stride if stride is not None else pool_size

        # 缓存
        self.X = None              # 保存输入
        self.max_indices = None    # 保存每个窗口最大值的位置

    def forward(self, X):
        """
        前向传播

        公式：Y[i, j] = max(X[i*s : i*s+p, j*s : j*s+p])

        参数：
            X: 输入特征图，形状 (H, W)

        返回：
            Y: 输出特征图，形状 ((H-p)//s+1, (W-p)//s+1)
        """
        # 保存输入
        self.X = X
        H, W = X.shape
        p = self.pool_size  # 池化窗口大小
        s = self.stride     # 步幅

        # ========================================
        # 计算输出尺寸
        # ========================================
        # 公式：out_size = (input_size - pool_size) // stride + 1
        out_h = (H - p) // s + 1
        out_w = (W - p) // s + 1

        Y = np.zeros((out_h, out_w))
        self.max_indices = []  # 存储每个窗口最大值的位置

        # ========================================
        # 池化计算
        # ========================================
        for i in range(out_h):
            for j in range(out_w):
                # ----------------------------------------
                # 核心：计算窗口在输入中的起始位置
                # ----------------------------------------
                # i_start = i × stride
                # j_start = j × stride
                #
                # 为什么？因为：
                #   - 输出位置 i 对应第 i 个窗口
                #   - 每个窗口间隔 stride 个像素
                #   - 所以第 i 个窗口从第 i×stride 行开始
                #
                # 例如 stride=2：
                #   i=0 → i_start = 0×2 = 0（第1个窗口，从第0行开始）
                #   i=1 → i_start = 1×2 = 2（第2个窗口，从第2行开始）
                i_start = i * s
                j_start = j * s

                # ----------------------------------------
                # 提取池化窗口
                # ----------------------------------------
                # X[i_start : i_start+p, j_start : j_start+p]
                # 含义：从 (i_start, j_start) 开始，取 p×p 大小的区域
                #
                # 例如 i=0, j=0, p=2, s=2:
                #   i_start=0, j_start=0
                #   window = X[0:2, 0:2]
                #          = X 的第0-1行、第0-1列
                #          = 左上角 2×2 区域
                window = X[i_start:i_start+p, j_start:j_start+p]

                # ----------------------------------------
                # 找最大值
                # ----------------------------------------
                Y[i, j] = np.max(window)

                # 记录最大值在原始输入中的位置（反向传播需要）
                max_idx = np.unravel_index(np.argmax(window), window.shape)
                # 转换为全局坐标：窗口内坐标 + 窗口起始位置
                self.max_indices.append((i_start + max_idx[0], j_start + max_idx[1]))

        return Y

    def backward(self, dY):
        """
        反向传播

        规则：梯度只传给最大值的位置，其他位置梯度为 0

        参数：
            dY: 上游梯度，形状 (out_h, out_w)

        返回：
            dX: 输入梯度，形状 (H, W)
        """
        H, W = self.X.shape
        dX = np.zeros((H, W))

        # 将上游梯度分配给对应的最大值位置
        out_h, out_w = dY.shape
        idx = 0
        for i in range(out_h):
            for j in range(out_w):
                # 获取该窗口最大值的全局位置
                max_i, max_j = self.max_indices[idx]
                # 梯度只传给最大值位置
                dX[max_i, max_j] += dY[i, j]
                idx += 1

        return dX


class ReLULayer:
    """
    ReLU 激活函数层（Rectified Linear Unit）

    ============================================================
    前向传播公式：
    ============================================================

    Y = max(0, X)

    即：Y[i] = { X[i]  如果 X[i] > 0
               { 0     如果 X[i] ≤ 0

    ============================================================
    图示：ReLU 函数
    ============================================================

         Y
         ▲
         │         ╱
         │        ╱
         │       ╱
         │      ╱
    ─────┼─────╱──────► X
         │    0
         │
         │

    特点：
    - X > 0 时：输出等于输入（斜率 = 1）
    - X ≤ 0 时：输出为 0（斜率 = 0）

    ============================================================
    反向传播公式：
    ============================================================

    dX = dY ⊙ (X > 0)

    其中 ⊙ 表示逐元素乘法

    推导（链式法则）：
        dY/dX = { 1  如果 X > 0
                { 0  如果 X ≤ 0

        dL/dX = dL/dY × dY/dX
              = dY × (X > 0)

    ============================================================
    图示：ReLU 导数
    ============================================================

         dY/dX
         ▲
         │       ┌──────────
       1 │       │
         │       │
    ─────┼───────┼──────────► X
         │       0
         │
         │

    直觉理解：
    - X > 0 时：梯度完全通过（"门"打开）
    - X ≤ 0 时：梯度被阻断（"门"关闭）

    ============================================================
    为什么使用 ReLU？
    ============================================================

    1. 计算简单：只需比较和取最大值
    2. 缓解梯度消失：正区间梯度恒为 1
    3. 稀疏激活：约 50% 神经元输出为 0
    4. 生物学合理：类似神经元的阈值激活

    潜在问题：
    - 死亡 ReLU：如果 X 总是 < 0，该神经元永远不更新
    - 解决方案：Leaky ReLU、PReLU、ELU 等变体
    """

    def __init__(self):
        """
        初始化 ReLU 层

        ReLU 没有可学习参数，只需要保存输入用于反向传播
        """
        self.X = None  # 缓存输入（反向传播时需要判断正负）

    def forward(self, X):
        """
        前向传播

        公式：Y = max(0, X)

        参数：
            X: 输入，形状任意（如 (N, D) 或 (H, W) 等）

        返回：
            Y: 输出，形状与输入相同

        示例：
            输入: [-2, -1, 0, 1, 2]
            输出: [ 0,  0, 0, 1, 2]
        """
        # 保存输入（反向传播需要）
        self.X = X

        # ========================================
        # ReLU 计算
        # ========================================
        # np.maximum(0, X) 逐元素取 X 和 0 的较大值
        # 等价于：
        #   Y = np.where(X > 0, X, 0)
        # 或者：
        #   Y = X * (X > 0)
        return np.maximum(0, X)

    def backward(self, dY):
        """
        反向传播

        公式：dX = dY ⊙ (X > 0)

        参数：
            dY: 上游梯度，形状与输出相同

        返回：
            dX: 输入梯度，形状与输入相同

        示例：
            前向时 X = [-2, -1, 0, 1, 2]
            mask = (X > 0) = [False, False, False, True, True]
            若 dY = [1, 1, 1, 1, 1]
            则 dX = [0, 0, 0, 1, 1]
        """
        # ========================================
        # 计算 ReLU 梯度
        # ========================================
        # (self.X > 0) 生成布尔数组，True=1, False=0
        # .astype(float) 转换为浮点数（可选，NumPy 会自动转换）
        #
        # 逐元素乘法：
        #   - X > 0 的位置：梯度完全通过
        #   - X ≤ 0 的位置：梯度变为 0
        return dY * (self.X > 0).astype(float)


class FCLayer:
    """
    全连接层（Fully Connected Layer / Dense Layer）

    ============================================================
    前向传播公式：
    ============================================================

    Y = X @ W + b

    展开形式：
        Y[i, j] = Σ_k X[i, k] · W[k, j] + b[j]

    符号说明：
        X: 输入矩阵，形状 (N, in_features)
           N = batch_size（样本数）
           in_features = 每个样本的特征数
        W: 权重矩阵，形状 (in_features, out_features)
        b: 偏置向量，形状 (out_features,)
        Y: 输出矩阵，形状 (N, out_features)

    ============================================================
    图示：全连接层
    ============================================================

    以 in_features=3, out_features=2 为例：

    输入层 (3个神经元)          输出层 (2个神经元)

         [x1] ─────w11─────→ [y1]
              ╲            ╱
               ╲w12   w21╱
         [x2] ──╳────╳───→
              ╱ w22   w31╲
             ╱            ╲
         [x3] ─────w32─────→ [y2]

    每个输出 = 所有输入的加权和 + 偏置：
        y1 = x1·w11 + x2·w21 + x3·w31 + b1
        y2 = x1·w12 + x2·w22 + x3·w32 + b2

    ============================================================
    矩阵形式
    ============================================================

                   W (3×2)
              ┌─────┬─────┐
              │ w11 │ w12 │
              ├─────┼─────┤
              │ w21 │ w22 │
              ├─────┼─────┤
              │ w31 │ w32 │
              └─────┴─────┘

    X (1×3)  @  W (3×2)  +  b (2,)  =  Y (1×2)
    [x1,x2,x3]                          [y1, y2]

    ============================================================
    反向传播公式：
    ============================================================

    已知：dY = dL/dY（上游梯度）

    1. 权重梯度 dW：
       dW = X^T @ dY
       形状：(in_features, out_features)

    2. 偏置梯度 db：
       db = sum(dY, axis=0)
       形状：(out_features,)

    3. 输入梯度 dX：
       dX = dY @ W^T
       形状：(N, in_features)

    ============================================================
    反向传播推导（以单样本为例）
    ============================================================

    前向：y_j = Σ_k x_k · w_kj + b_j

    1. dL/dw_kj（权重梯度）：
       dL/dw_kj = dL/dy_j · dy_j/dw_kj
                = dY_j · x_k
       矩阵形式：dW = X^T @ dY

    2. dL/db_j（偏置梯度）：
       dL/db_j = dL/dy_j · dy_j/db_j
               = dY_j · 1
       向量形式：db = sum(dY, axis=0)

    3. dL/dx_k（输入梯度）：
       dL/dx_k = Σ_j dL/dy_j · dy_j/dx_k
               = Σ_j dY_j · w_kj
       矩阵形式：dX = dY @ W^T

    ============================================================
    参数：
        in_features: 输入特征数
        out_features: 输出特征数
    ============================================================
    """

    def __init__(self, in_features, out_features):
        """
        初始化全连接层

        参数：
            in_features: 输入特征数（上一层输出维度）
            out_features: 输出特征数（该层神经元个数）

        示例：
            FCLayer(784, 64)
            - 输入：784 维向量（如展平后的 28×28 图像）
            - 输出：64 维向量
            - 权重：784 × 64 = 50,176 个参数
            - 偏置：64 个参数
        """
        # ========================================
        # He 初始化（Kaiming 初始化）
        # ========================================
        # 公式：W ~ N(0, sqrt(2/n_in))
        #
        # 为什么用 sqrt(2/n_in)？
        #   - 保持前向传播时信号方差稳定
        #   - 2 是因为 ReLU 会使约一半神经元输出为 0
        #   - n_in 是输入特征数
        #
        # 其他初始化方式：
        #   - Xavier 初始化：sqrt(1/n_in) 或 sqrt(2/(n_in+n_out))
        #   - 适用于 tanh/sigmoid 激活函数
        scale = np.sqrt(2.0 / in_features)
        self.W = np.random.randn(in_features, out_features) * scale

        # ========================================
        # 偏置初始化为 0
        # ========================================
        # 因为 He 初始化的权重已经能让网络正常启动
        self.b = np.zeros(out_features)

        # ========================================
        # 缓存（反向传播需要前向传播时的输入）
        # ========================================
        self.X = None  # 保存输入

        # ========================================
        # 梯度存储
        # ========================================
        self.dW = None  # 权重梯度 dL/dW
        self.db = None  # 偏置梯度 dL/db

    def forward(self, X):
        """
        前向传播

        公式：Y = X @ W + b

        参数：
            X: 输入，形状 (N, in_features) 或 (in_features,)
               N 是 batch_size

        返回：
            Y: 输出，形状 (N, out_features)

        注意：
            如果输入是 1D 向量，会自动 reshape 为 (1, in_features)
        """
        # ========================================
        # 处理 1D 输入
        # ========================================
        # 如果输入是 1D 向量（单样本），reshape 为 2D
        # 例如：(784,) → (1, 784)
        if X.ndim == 1:
            X = X.reshape(1, -1)

        # 保存输入（反向传播需要）
        self.X = X

        # ========================================
        # 矩阵乘法 + 偏置
        # ========================================
        # X @ W: (N, in_features) @ (in_features, out_features)
        #      → (N, out_features)
        #
        # + b: 广播加法
        #      b 的形状 (out_features,) 广播到 (N, out_features)
        #      即每一行都加上相同的偏置向量
        return X @ self.W + self.b

    def backward(self, dY):
        """
        反向传播

        参数：
            dY: 上游梯度，形状 (N, out_features)

        返回：
            dX: 输入梯度，形状 (N, in_features)

        同时计算并存储：
            self.dW: 权重梯度，形状 (in_features, out_features)
            self.db: 偏置梯度，形状 (out_features,)
        """
        # ========================================
        # 处理 1D 梯度
        # ========================================
        # 确保 dY 是 2D
        if dY.ndim == 1:
            dY = dY.reshape(1, -1)

        # ========================================
        # 1. 计算权重梯度 dW
        # ========================================
        # 公式：dW = X^T @ dY
        #
        # 维度验证：
        #   X^T: (in_features, N)
        #   dY:  (N, out_features)
        #   dW:  (in_features, out_features)  ✓ 与 W 形状相同
        #
        # 直觉：
        #   dW[k,j] = Σ_i X[i,k] · dY[i,j]
        #   表示所有样本中，第 k 个输入特征对第 j 个输出的贡献
        self.dW = self.X.T @ dY

        # ========================================
        # 2. 计算偏置梯度 db
        # ========================================
        # 公式：db = sum(dY, axis=0)
        #
        # 维度验证：
        #   dY:  (N, out_features)
        #   db:  (out_features,)  ✓ 与 b 形状相同
        #
        # 直觉：
        #   b_j 加到每个样本的 y_j 上
        #   所以 db_j = Σ_i dY[i,j]（对所有样本的梯度求和）
        self.db = dY.sum(axis=0)

        # ========================================
        # 3. 计算输入梯度 dX
        # ========================================
        # 公式：dX = dY @ W^T
        #
        # 维度验证：
        #   dY:   (N, out_features)
        #   W^T:  (out_features, in_features)
        #   dX:   (N, in_features)  ✓ 与 X 形状相同
        #
        # 直觉：
        #   dX[i,k] = Σ_j dY[i,j] · W[k,j]
        #   第 i 个样本的第 k 个输入特征的梯度
        #   = 所有输出梯度通过对应权重传回来的总和
        dX = dY @ self.W.T

        return dX

# # ============================================================
# # 测试代码
# # ============================================================
# if __name__ == "__main__":
#     print("=" * 60)
#     print("卷积层测试")
#     print("=" * 60)
#
#     # 创建卷积层
#     conv = ConvLayer(kernel_size=3)
#
#     # 创建输入
#     X = np.random.randn(5, 5)
#     print(f"输入 X shape: {X.shape}")
#
#     # 前向传播
#     Y = conv.forward(X)
#     print(f"输出 Y shape: {Y.shape}")
#
#     # 模拟上游梯度
#     dY = np.ones_like(Y)
#     print(f"上游梯度 dY shape: {dY.shape}")
#
#     # 反向传播
#     dX = conv.backward(dY)
#     print(f"输入梯度 dX shape: {dX.shape}")
#     print(f"权重梯度 dW shape: {conv.dw.shape}")
#     print(f"偏置梯度 db: {conv.db:.4f}")
#
#     print("\n" + "=" * 60)
#     print("梯度检验")
#     print("=" * 60)
#
#     # 数值梯度检验
#     eps = 1e-5
#     dW_numerical = np.zeros_like(conv.W)
#
#     for m in range(3):
#         for n in range(3):
#             # W[m,n] + eps
#             conv.W[m, n] += eps
#             Y_plus = conv.forward(X)
#             loss_plus = np.sum(Y_plus ** 2)
#
#             # W[m,n] - eps
#             conv.W[m, n] -= 2 * eps
#             Y_minus = conv.forward(X)
#             loss_minus = np.sum(Y_minus ** 2)
#
#             # 恢复
#             conv.W[m, n] += eps
#
#             # 数值梯度
#             dW_numerical[m, n] = (loss_plus - loss_minus) / (2 * eps)
#
#     # 重新计算解析梯度
#     Y = conv.forward(X)
#     dY_check = 2 * Y  # dL/dY = 2Y（因为 L = sum(Y^2)）
#     conv.backward(dY_check)
#
#     rel_error = np.max(np.abs(conv.dw - dW_numerical)) / (np.max(np.abs(dW_numerical)) + 1e-8)
#     print(f"相对误差: {rel_error:.2e}")
#
#     if rel_error < 1e-5:
#         print("✓ 梯度检验通过！")
#     else:
#         print("✗ 梯度检验失败")
