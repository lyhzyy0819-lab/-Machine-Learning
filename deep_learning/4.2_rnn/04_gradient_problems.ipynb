{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ 4 ç« ï¼šæ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸\n",
    "\n",
    "> ç†è§£ RNN è®­ç»ƒçš„æ ¸å¿ƒéš¾é¢˜ï¼Œä¸º LSTM/GRU åšé“ºå«\n",
    "\n",
    "---\n",
    "\n",
    "## æœ¬ç« ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "- [ ] ç†è§£ä¸ºä»€ä¹ˆ RNN ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸\n",
    "- [ ] ç”¨æ•°å­¦åˆ†ææ¢¯åº¦çš„æŒ‡æ•°è¡°å‡/å¢é•¿\n",
    "- [ ] å¯è§†åŒ–é•¿åºåˆ—ä¸­çš„æ¢¯åº¦è¡Œä¸º\n",
    "- [ ] æŒæ¡ç¼“è§£æ¢¯åº¦é—®é¢˜çš„åŸºæœ¬æŠ€æœ¯\n",
    "- [ ] ç†è§£ LSTM/GRU è®¾è®¡çš„å¿…è¦æ€§\n",
    "\n",
    "---\n",
    "\n",
    "## é—®é¢˜å¼•å…¥ï¼šRNN çš„\"å¥å¿˜ç—‡\"\n",
    "\n",
    "è®©æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸ªç°è±¡ï¼šRNN ä¼¼ä¹æ— æ³•è®°ä½é•¿æœŸä¿¡æ¯\n",
    "\n",
    "```\n",
    "å¥å­ 1ï¼ˆçŸ­æœŸä¾èµ–ï¼‰ï¼š\n",
    "\"The cat sat on the mat.\" â†’ é¢„æµ‹ \"mat\" åªéœ€è¦çœ‹ \"sat on the\"\n",
    "\n",
    "å¥å­ 2ï¼ˆé•¿æœŸä¾èµ–ï¼‰ï¼š\n",
    "\"I grew up in France... [100 words later]... I speak fluent ___\"\n",
    "â†’ é¢„æµ‹ \"French\" éœ€è¦è®°ä½ 100 è¯ä¹‹å‰çš„ \"France\"\n",
    "```\n",
    "\n",
    "ä¸ºä»€ä¹ˆ RNN éš¾ä»¥å¤„ç†é•¿æœŸä¾èµ–ï¼Ÿç­”æ¡ˆå°±åœ¨æ¢¯åº¦ä¸­ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸çš„æ•°å­¦åˆ†æ\n",
    "\n",
    "### 1.1 å›é¡¾ BPTT ä¸­çš„æ¢¯åº¦é“¾\n",
    "\n",
    "RNN çš„éšçŠ¶æ€æ›´æ–°ï¼š\n",
    "$$h_t = \\tanh(W_{hh} \\cdot h_{t-1} + W_{xh} \\cdot x_t + b_h)$$\n",
    "\n",
    "ä»æ—¶é—´ $t$ å›ä¼ åˆ°æ—¶é—´ $k$ çš„æ¢¯åº¦ï¼ˆ$t > k$ï¼‰ï¼š\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial h_k} = \\prod_{i=k+1}^{t} \\frac{\\partial h_i}{\\partial h_{i-1}}$$\n",
    "\n",
    "å…¶ä¸­æ¯ä¸€é¡¹ï¼š\n",
    "$$\\frac{\\partial h_i}{\\partial h_{i-1}} = \\text{diag}(1 - h_i^2) \\cdot W_{hh}$$\n",
    "\n",
    "æ‰€ä»¥ï¼š\n",
    "$$\\frac{\\partial h_t}{\\partial h_k} = \\prod_{i=k+1}^{t} \\text{diag}(1 - h_i^2) \\cdot W_{hh}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 å…³é”®æ´å¯Ÿï¼šçŸ©é˜µä¹˜ç§¯çš„èŒƒæ•°\n",
    "\n",
    "è€ƒè™‘è¿ç»­ä¹˜ä»¥åŒä¸€ä¸ªçŸ©é˜µ $W$ å…± $n$ æ¬¡ï¼š\n",
    "\n",
    "$$W^n$$\n",
    "\n",
    "è¿™ä¸ªä¹˜ç§¯çš„è¡Œä¸ºå–å†³äº $W$ çš„**ç‰¹å¾å€¼**ï¼š\n",
    "\n",
    "è®¾ $W$ çš„æœ€å¤§ç‰¹å¾å€¼ä¸º $\\lambda_{max}$ï¼š\n",
    "\n",
    "| $|\\lambda_{max}|$ | æ•ˆæœ | ç»“æœ |\n",
    "|-------------------|------|------|\n",
    "| $< 1$ | æŒ‡æ•°è¡°å‡ | æ¢¯åº¦æ¶ˆå¤± |\n",
    "| $= 1$ | ç¨³å®š | ç†æƒ³ |\n",
    "| $> 1$ | æŒ‡æ•°å¢é•¿ | æ¢¯åº¦çˆ†ç‚¸ |\n",
    "\n",
    "**æ›´ç³Ÿç³•çš„æ˜¯**ï¼šRNN ä¸­è¿˜æœ‰ tanh çš„å¯¼æ•° $(1 - h^2)$ï¼Œå®ƒçš„å€¼åœ¨ $[0, 1]$ ä¹‹é—´ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†æ¢¯åº¦æ¶ˆå¤±ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matrix_power():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–çŸ©é˜µè¿ä¹˜çš„æ•ˆæœï¼š\n",
    "    å±•ç¤ºä¸åŒç‰¹å¾å€¼å¯¹æ¢¯åº¦çš„å½±å“\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"çŸ©é˜µè¿ä¹˜æ•ˆæœæ¼”ç¤º\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    n_steps = 50\n",
    "    \n",
    "    # ========================================\n",
    "    # æƒ…å†µ 1: |Î»| < 1 â†’ æ¢¯åº¦æ¶ˆå¤±\n",
    "    # ========================================\n",
    "    ax = axes[0]\n",
    "    \n",
    "    # åˆ›å»ºç‰¹å¾å€¼ < 1 çš„çŸ©é˜µ\n",
    "    # ä½¿ç”¨å¯¹è§’çŸ©é˜µä¾¿äºåˆ†æ\n",
    "    eigenvalues_small = [0.9, 0.8, 0.7]\n",
    "    \n",
    "    for ev in eigenvalues_small:\n",
    "        W = ev * np.eye(2)  # ç®€å•çš„å¯¹è§’çŸ©é˜µ\n",
    "        norms = []\n",
    "        W_power = np.eye(2)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            W_power = W_power @ W\n",
    "            norms.append(np.linalg.norm(W_power))\n",
    "        \n",
    "        ax.plot(range(n_steps), norms, label=f'Î» = {ev}')\n",
    "    \n",
    "    ax.set_xlabel('æ—¶é—´æ­¥', fontsize=12)\n",
    "    ax.set_ylabel('||W^t||', fontsize=12)\n",
    "    ax.set_title('æ¢¯åº¦æ¶ˆå¤±: |Î»| < 1', fontsize=14, color='blue')\n",
    "    ax.legend()\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=1e-10, color='red', linestyle='--', label='æ¶ˆå¤±é˜ˆå€¼')\n",
    "    \n",
    "    # ========================================\n",
    "    # æƒ…å†µ 2: |Î»| = 1 â†’ ç¨³å®š\n",
    "    # ========================================\n",
    "    ax = axes[1]\n",
    "    \n",
    "    eigenvalues_one = [1.0, 0.99, 1.01]\n",
    "    \n",
    "    for ev in eigenvalues_one:\n",
    "        W = ev * np.eye(2)\n",
    "        norms = []\n",
    "        W_power = np.eye(2)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            W_power = W_power @ W\n",
    "            norms.append(np.linalg.norm(W_power))\n",
    "        \n",
    "        ax.plot(range(n_steps), norms, label=f'Î» = {ev}')\n",
    "    \n",
    "    ax.set_xlabel('æ—¶é—´æ­¥', fontsize=12)\n",
    "    ax.set_ylabel('||W^t||', fontsize=12)\n",
    "    ax.set_title('è¾¹ç•Œæƒ…å†µ: |Î»| â‰ˆ 1', fontsize=14, color='green')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ========================================\n",
    "    # æƒ…å†µ 3: |Î»| > 1 â†’ æ¢¯åº¦çˆ†ç‚¸\n",
    "    # ========================================\n",
    "    ax = axes[2]\n",
    "    \n",
    "    eigenvalues_large = [1.1, 1.2, 1.3]\n",
    "    \n",
    "    for ev in eigenvalues_large:\n",
    "        W = ev * np.eye(2)\n",
    "        norms = []\n",
    "        W_power = np.eye(2)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            W_power = W_power @ W\n",
    "            norms.append(np.linalg.norm(W_power))\n",
    "        \n",
    "        ax.plot(range(n_steps), norms, label=f'Î» = {ev}')\n",
    "    \n",
    "    ax.set_xlabel('æ—¶é—´æ­¥', fontsize=12)\n",
    "    ax.set_ylabel('||W^t||', fontsize=12)\n",
    "    ax.set_title('æ¢¯åº¦çˆ†ç‚¸: |Î»| > 1', fontsize=14, color='red')\n",
    "    ax.legend()\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°å…·ä½“æ•°å€¼\n",
    "    print(\"\\nå…·ä½“æ•°å€¼ç¤ºä¾‹:\")\n",
    "    print(\"-\" * 40)\n",
    "    for ev, name in [(0.9, \"æ¶ˆå¤±\"), (1.0, \"ç¨³å®š\"), (1.1, \"çˆ†ç‚¸\")]:\n",
    "        result = ev ** 50\n",
    "        print(f\"Î»={ev:.1f}, t=50: Î»^t = {result:.2e} ({name})\")\n",
    "\n",
    "\n",
    "visualize_matrix_power()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 tanh å¯¼æ•°çš„å½±å“\n",
    "\n",
    "tanh çš„å¯¼æ•°ï¼š\n",
    "$$\\frac{d\\tanh(x)}{dx} = 1 - \\tanh^2(x)$$\n",
    "\n",
    "è¿™ä¸ªå¯¼æ•°æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tanh_derivative():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ– tanh åŠå…¶å¯¼æ•°\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    x = np.linspace(-4, 4, 200)\n",
    "    \n",
    "    # å·¦å›¾ï¼štanh å‡½æ•°\n",
    "    ax = axes[0]\n",
    "    y = np.tanh(x)\n",
    "    ax.plot(x, y, 'b-', linewidth=2, label='tanh(x)')\n",
    "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.axhline(y=1, color='r', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=-1, color='r', linestyle='--', alpha=0.5)\n",
    "    ax.fill_between(x, -1, 1, alpha=0.1, color='blue')\n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('tanh(x)', fontsize=12)\n",
    "    ax.set_title('tanh å‡½æ•°: è¾“å‡ºè¢«å‹ç¼©åˆ° [-1, 1]', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    \n",
    "    # å³å›¾ï¼štanh å¯¼æ•°\n",
    "    ax = axes[1]\n",
    "    dy = 1 - np.tanh(x) ** 2  # tanh çš„å¯¼æ•°\n",
    "    ax.plot(x, dy, 'r-', linewidth=2, label=\"tanh'(x) = 1 - tanhÂ²(x)\")\n",
    "    ax.axhline(y=1, color='g', linestyle='--', alpha=0.5, label='æœ€å¤§å€¼ = 1')\n",
    "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.axhline(y=0.25, color='orange', linestyle='--', alpha=0.5, label='å½“ |x|=1 æ—¶çº¦ 0.42')\n",
    "    ax.fill_between(x, 0, dy, alpha=0.2, color='red')\n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel(\"tanh'(x)\", fontsize=12)\n",
    "    ax.set_title('tanh å¯¼æ•°: æ€»æ˜¯ â‰¤ 1, è¿œç¦»åŸç‚¹æ—¶è¶‹è¿‘ 0', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.1, 1.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # å…³é”®ç‚¹åˆ†æ\n",
    "    print(\"\\nå…³é”®è§‚å¯Ÿ:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. tanh'(x) çš„æœ€å¤§å€¼æ˜¯ 1ï¼ˆåœ¨ x=0 å¤„ï¼‰\")\n",
    "    print(\"2. å½“ |x| > 1 æ—¶ï¼Œtanh'(x) < 0.42\")\n",
    "    print(\"3. å½“ |x| > 2 æ—¶ï¼Œtanh'(x) < 0.07\")\n",
    "    print(\"4. å½“ |x| > 3 æ—¶ï¼Œtanh'(x) < 0.01\")\n",
    "    print(\"\\nç»“è®º: tanh çš„å¯¼æ•°å¤§éƒ¨åˆ†æ—¶å€™è¿œå°äº 1ï¼Œè¿™ä¼šåŠ å‰§æ¢¯åº¦æ¶ˆå¤±ï¼\")\n",
    "\n",
    "\n",
    "visualize_tanh_derivative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 ç»„åˆæ•ˆåº”ï¼šå®Œæ•´çš„æ¢¯åº¦åˆ†æ\n",
    "\n",
    "BPTT ä¸­ä»æ—¶é—´ $t$ åˆ°æ—¶é—´ $k$ çš„æ¢¯åº¦ï¼š\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial h_k} = \\prod_{i=k+1}^{t} \\underbrace{\\text{diag}(1 - h_i^2)}_{\\leq 1} \\cdot W_{hh}$$\n",
    "\n",
    "æ¯ä¸€é¡¹çš„èŒƒæ•°å¤§è‡´ä¸ºï¼š\n",
    "$$\\|\\text{diag}(1 - h_i^2) \\cdot W_{hh}\\| \\approx \\|W_{hh}\\| \\cdot \\bar{\\sigma}'$$\n",
    "\n",
    "å…¶ä¸­ $\\bar{\\sigma}'$ æ˜¯å¹³å‡çš„ tanh å¯¼æ•°ï¼ˆé€šå¸¸ < 1ï¼‰ã€‚\n",
    "\n",
    "ç»è¿‡ $(t-k)$ æ­¥åï¼š\n",
    "$$\\left\\|\\frac{\\partial h_t}{\\partial h_k}\\right\\| \\approx \\left(\\|W_{hh}\\| \\cdot \\bar{\\sigma}'\\right)^{t-k}$$\n",
    "\n",
    "**è¿™æ˜¯ä¸€ä¸ªæŒ‡æ•°å‡½æ•°ï¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_flow():\n",
    "    \"\"\"\n",
    "    åˆ†æ RNN ä¸­æ¢¯åº¦æµåŠ¨çš„å®Œæ•´å›¾æ™¯\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RNN æ¢¯åº¦æµåŠ¨åˆ†æ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ä¸åŒ W_hh èŒƒæ•°ä¸‹çš„æ¢¯åº¦è¡Œä¸º\n",
    "    w_norms = [0.5, 0.8, 1.0, 1.2, 1.5]\n",
    "    avg_tanh_deriv = 0.5  # å‡è®¾å¹³å‡ tanh å¯¼æ•°\n",
    "    \n",
    "    seq_lens = np.arange(1, 101)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å·¦å›¾ï¼šæ¢¯åº¦èŒƒæ•°éšæ—¶é—´æ­¥çš„å˜åŒ–\n",
    "    ax = axes[0]\n",
    "    \n",
    "    for w_norm in w_norms:\n",
    "        effective_factor = w_norm * avg_tanh_deriv\n",
    "        gradient_norms = effective_factor ** seq_lens\n",
    "        \n",
    "        label = f'||W||={w_norm}, æœ‰æ•ˆå› å­={effective_factor:.2f}'\n",
    "        ax.semilogy(seq_lens, gradient_norms, label=label, linewidth=2)\n",
    "    \n",
    "    ax.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=1e-10, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=1e10, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('æ—¶é—´æ­¥å·® (t - k)', fontsize=12)\n",
    "    ax.set_ylabel('æ¢¯åº¦èŒƒæ•° (å¯¹æ•°å°ºåº¦)', fontsize=12)\n",
    "    ax.set_title(f'æ¢¯åº¦èŒƒæ•° vs æ—¶é—´æ­¥ (å¹³å‡ tanh\\' = {avg_tanh_deriv})', fontsize=14)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(1e-50, 1e50)\n",
    "    \n",
    "    # æ·»åŠ åŒºåŸŸæ ‡æ³¨\n",
    "    ax.fill_between(seq_lens, 1e-50, 1e-10, alpha=0.1, color='blue', label='æ¢¯åº¦æ¶ˆå¤±åŒº')\n",
    "    ax.fill_between(seq_lens, 1e10, 1e50, alpha=0.1, color='red', label='æ¢¯åº¦çˆ†ç‚¸åŒº')\n",
    "    \n",
    "    # å³å›¾ï¼šæœ‰æ•ˆæ¢¯åº¦è·ç¦»\n",
    "    ax = axes[1]\n",
    "    \n",
    "    # è®¡ç®—æ¢¯åº¦è¡°å‡åˆ°ç‰¹å®šé˜ˆå€¼éœ€è¦çš„æ—¶é—´æ­¥\n",
    "    thresholds = [1e-3, 1e-5, 1e-10]\n",
    "    effective_factors = np.linspace(0.3, 0.99, 50)\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        # æ±‚è§£: factor^n = thresh => n = log(thresh) / log(factor)\n",
    "        steps_to_threshold = np.log(thresh) / np.log(effective_factors)\n",
    "        ax.plot(effective_factors, steps_to_threshold, \n",
    "                label=f'æ¢¯åº¦è¡°å‡åˆ° {thresh}', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('æœ‰æ•ˆå› å­ (||W|| Ã— avg tanh\\')', fontsize=12)\n",
    "    ax.set_ylabel('æ‰€éœ€æ—¶é—´æ­¥æ•°', fontsize=12)\n",
    "    ax.set_title('æ¢¯åº¦æœ‰æ•ˆä¼ æ’­è·ç¦»', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 200)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # å…·ä½“æ•°å€¼ç¤ºä¾‹\n",
    "    print(\"\\nå…·ä½“ç¤ºä¾‹:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"å‡è®¾ ||W_hh|| = 1.0, å¹³å‡ tanh' = 0.5\")\n",
    "    print(\"æœ‰æ•ˆå› å­ = 0.5\")\n",
    "    print(f\"\")\n",
    "    for steps in [10, 20, 50, 100]:\n",
    "        grad = 0.5 ** steps\n",
    "        print(f\"  {steps} æ­¥åæ¢¯åº¦: 0.5^{steps} = {grad:.2e}\")\n",
    "\n",
    "\n",
    "analyze_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šå®éªŒéªŒè¯\n",
    "\n",
    "### 2.1 å®é™… RNN ä¸­çš„æ¢¯åº¦è§‚å¯Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_gradient_in_rnn():\n",
    "    \"\"\"\n",
    "    åœ¨å®é™… RNN ä¸­è§‚å¯Ÿæ¢¯åº¦æ¶ˆå¤±ç°è±¡\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"å®é™… RNN ä¸­çš„æ¢¯åº¦è§‚å¯Ÿ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åˆ›å»ºç®€å•çš„ RNN\n",
    "    class SimpleRNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super().__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # ä¿å­˜æ¯ä¸ªæ—¶é—´æ­¥çš„éšçŠ¶æ€ï¼ˆç”¨äºåˆ†ææ¢¯åº¦ï¼‰\n",
    "            out, _ = self.rnn(x)\n",
    "            # åªç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º\n",
    "            return self.fc(out[:, -1, :])\n",
    "    \n",
    "    # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦\n",
    "    seq_lengths = [10, 20, 50, 100, 200]\n",
    "    hidden_size = 32\n",
    "    input_size = 1\n",
    "    \n",
    "    gradient_norms = {}\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        torch.manual_seed(42)\n",
    "        model = SimpleRNN(input_size, hidden_size)\n",
    "        \n",
    "        # åˆ›å»ºè¾“å…¥ï¼šåœ¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥æœ‰ç‰¹æ®Šä¿¡å·\n",
    "        x = torch.zeros(1, seq_len, input_size)\n",
    "        x[0, 0, 0] = 1.0  # åªåœ¨ t=0 æœ‰è¾“å…¥\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        output = model(x)\n",
    "        \n",
    "        # åå‘ä¼ æ’­\n",
    "        output.backward()\n",
    "        \n",
    "        # è·å– W_hh çš„æ¢¯åº¦\n",
    "        # RNN çš„æƒé‡åŒ…æ‹¬ weight_hh_l0\n",
    "        grad_norm = model.rnn.weight_hh_l0.grad.norm().item()\n",
    "        gradient_norms[seq_len] = grad_norm\n",
    "        \n",
    "        print(f\"åºåˆ—é•¿åº¦ {seq_len:3d}: ||âˆ‚L/âˆ‚W_hh|| = {grad_norm:.6e}\")\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.bar(range(len(seq_lengths)), list(gradient_norms.values()), \n",
    "           tick_label=[str(s) for s in seq_lengths], color='steelblue', edgecolor='navy')\n",
    "    ax.set_xlabel('åºåˆ—é•¿åº¦', fontsize=12)\n",
    "    ax.set_ylabel('æ¢¯åº¦èŒƒæ•°', fontsize=12)\n",
    "    ax.set_title('RNN æ¢¯åº¦èŒƒæ•° vs åºåˆ—é•¿åº¦', fontsize=14)\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # æ·»åŠ è¶‹åŠ¿è¯´æ˜\n",
    "    ax.text(0.95, 0.95, 'æ¢¯åº¦éšåºåˆ—é•¿åº¦\\næŒ‡æ•°çº§è¡°å‡!', \n",
    "            transform=ax.transAxes, ha='right', va='top',\n",
    "            fontsize=12, color='red',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return gradient_norms\n",
    "\n",
    "\n",
    "gradient_norms = observe_gradient_in_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 é€æ—¶é—´æ­¥çš„æ¢¯åº¦åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_per_timestep():\n",
    "    \"\"\"\n",
    "    åˆ†ææ¯ä¸ªæ—¶é—´æ­¥å¯¹æœ€ç»ˆæ¢¯åº¦çš„è´¡çŒ®\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"é€æ—¶é—´æ­¥æ¢¯åº¦åˆ†æ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åˆ›å»ºæ‰‹åŠ¨å®ç°çš„ RNNï¼ˆä¾¿äºæå–æ¯æ­¥æ¢¯åº¦ï¼‰\n",
    "    seq_len = 50\n",
    "    hidden_size = 16\n",
    "    input_size = 1\n",
    "    \n",
    "    # åˆå§‹åŒ–å‚æ•°\n",
    "    torch.manual_seed(42)\n",
    "    W_xh = torch.randn(hidden_size, input_size, requires_grad=True) * 0.1\n",
    "    W_hh = torch.randn(hidden_size, hidden_size, requires_grad=True) * 0.1\n",
    "    W_hy = torch.randn(1, hidden_size, requires_grad=True) * 0.1\n",
    "    \n",
    "    # è¾“å…¥åºåˆ—\n",
    "    x = torch.zeros(seq_len, input_size)\n",
    "    x[0, 0] = 1.0  # åªåœ¨ t=0 æœ‰è¾“å…¥\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­ï¼Œä¿å­˜æ¯ä¸ªæ—¶é—´æ­¥çš„éšçŠ¶æ€\n",
    "    h_list = []\n",
    "    h = torch.zeros(hidden_size)\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        h = torch.tanh(W_xh @ x[t] + W_hh @ h)\n",
    "        h_list.append(h)\n",
    "    \n",
    "    # æœ€ç»ˆè¾“å‡º\n",
    "    y = W_hy @ h_list[-1]\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªéšçŠ¶æ€å¯¹è¾“å‡ºçš„æ¢¯åº¦\n",
    "    grad_norms = []\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦\n",
    "        if h_list[t].grad is not None:\n",
    "            h_list[t].grad.zero_()\n",
    "        \n",
    "        # è®¡ç®— dy/dh_t\n",
    "        grad = torch.autograd.grad(y, h_list[t], retain_graph=True)[0]\n",
    "        grad_norms.append(grad.norm().item())\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å·¦å›¾ï¼šçº¿æ€§å°ºåº¦\n",
    "    ax = axes[0]\n",
    "    ax.plot(range(seq_len), grad_norms, 'b-', linewidth=2)\n",
    "    ax.fill_between(range(seq_len), grad_norms, alpha=0.3)\n",
    "    ax.set_xlabel('æ—¶é—´æ­¥ t', fontsize=12)\n",
    "    ax.set_ylabel('||âˆ‚y/âˆ‚h_t||', fontsize=12)\n",
    "    ax.set_title('å„æ—¶é—´æ­¥éšçŠ¶æ€å¯¹è¾“å‡ºçš„æ¢¯åº¦ï¼ˆçº¿æ€§å°ºåº¦ï¼‰', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ ‡æ³¨\n",
    "    ax.annotate('è¶Šæ—©çš„æ—¶é—´æ­¥\\næ¢¯åº¦è¶Šå°', xy=(10, grad_norms[10]), \n",
    "                xytext=(20, max(grad_norms)*0.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                fontsize=11, color='red')\n",
    "    \n",
    "    # å³å›¾ï¼šå¯¹æ•°å°ºåº¦\n",
    "    ax = axes[1]\n",
    "    ax.semilogy(range(seq_len), grad_norms, 'r-', linewidth=2)\n",
    "    ax.fill_between(range(seq_len), grad_norms, alpha=0.3, color='red')\n",
    "    ax.set_xlabel('æ—¶é—´æ­¥ t', fontsize=12)\n",
    "    ax.set_ylabel('||âˆ‚y/âˆ‚h_t|| (å¯¹æ•°å°ºåº¦)', fontsize=12)\n",
    "    ax.set_title('å„æ—¶é—´æ­¥éšçŠ¶æ€å¯¹è¾“å‡ºçš„æ¢¯åº¦ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ‹ŸåˆæŒ‡æ•°è¡°å‡\n",
    "    from scipy.optimize import curve_fit\n",
    "    def exp_decay(t, a, b):\n",
    "        return a * np.exp(b * t)\n",
    "    \n",
    "    try:\n",
    "        popt, _ = curve_fit(exp_decay, range(seq_len), grad_norms, p0=[1, -0.1])\n",
    "        t_fit = np.linspace(0, seq_len-1, 100)\n",
    "        ax.plot(t_fit, exp_decay(t_fit, *popt), 'g--', linewidth=2, \n",
    "                label=f'æ‹Ÿåˆ: {popt[0]:.2f}Ã—exp({popt[1]:.3f}t)')\n",
    "        ax.legend()\n",
    "        print(f\"\\næŒ‡æ•°æ‹Ÿåˆ: è¡°å‡ç‡ = {popt[1]:.4f}\")\n",
    "        print(f\"æ¯æ­¥è¡°å‡çº¦ {np.exp(popt[1]):.4f} å€\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°ç»Ÿè®¡\n",
    "    print(\"\\næ¢¯åº¦ç»Ÿè®¡:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"t=0 (æœ€è¿œ) æ¢¯åº¦: {grad_norms[0]:.6e}\")\n",
    "    print(f\"t={seq_len//2} (ä¸­é—´) æ¢¯åº¦: {grad_norms[seq_len//2]:.6e}\")\n",
    "    print(f\"t={seq_len-1} (æœ€è¿‘) æ¢¯åº¦: {grad_norms[-1]:.6e}\")\n",
    "    print(f\"\\nè¡°å‡æ¯”ä¾‹: {grad_norms[-1]/grad_norms[0]:.2e} å€\")\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "\n",
    "grad_per_step = analyze_gradient_per_timestep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šé•¿æœŸä¾èµ–ä»»åŠ¡å®éªŒ\n",
    "\n",
    "### 3.1 è®¾è®¡ä¸€ä¸ªéœ€è¦é•¿æœŸè®°å¿†çš„ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_term_memory_task():\n",
    "    \"\"\"\n",
    "    é•¿æœŸä¾èµ–ä»»åŠ¡ï¼šè®°ä½åºåˆ—å¼€å¤´çš„ä¿¡æ¯\n",
    "    \n",
    "    ä»»åŠ¡è®¾è®¡:\n",
    "    - è¾“å…¥: [æ ‡è®°, 0, 0, 0, ..., 0, 0]\n",
    "    - è¾“å‡º: é¢„æµ‹å¼€å¤´çš„æ ‡è®°ï¼ˆ+1 æˆ– -1ï¼‰\n",
    "    - åºåˆ—é•¿åº¦è¶Šé•¿ï¼Œä»»åŠ¡è¶Šéš¾\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"é•¿æœŸä¾èµ–ä»»åŠ¡ï¼šRNN çš„è®°å¿†æé™æµ‹è¯•\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    def generate_data(n_samples, seq_len):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆæ•°æ®:\n",
    "        X: ç¬¬ä¸€ä¸ªä½ç½®æ˜¯ +1 æˆ– -1ï¼Œå…¶ä½™ä¸º 0\n",
    "        Y: ç­‰äºç¬¬ä¸€ä¸ªä½ç½®çš„å€¼\n",
    "        \"\"\"\n",
    "        X = np.zeros((n_samples, seq_len, 1))\n",
    "        Y = np.zeros((n_samples, 1))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            label = np.random.choice([-1, 1])\n",
    "            X[i, 0, 0] = label  # åªåœ¨ç¬¬ä¸€ä¸ªä½ç½®æœ‰ä¿¡å·\n",
    "            Y[i, 0] = label\n",
    "        \n",
    "        return torch.FloatTensor(X), torch.FloatTensor(Y)\n",
    "    \n",
    "    # åˆ›å»º RNN æ¨¡å‹\n",
    "    class MemoryRNN(nn.Module):\n",
    "        def __init__(self, hidden_size):\n",
    "            super().__init__()\n",
    "            self.rnn = nn.RNN(1, hidden_size, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out, _ = self.rnn(x)\n",
    "            return torch.tanh(self.fc(out[:, -1, :]))  # ç”¨ tanh å°†è¾“å‡ºé™åˆ¶åœ¨ [-1, 1]\n",
    "    \n",
    "    # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦\n",
    "    seq_lengths = [5, 10, 20, 50, 100]\n",
    "    hidden_size = 32\n",
    "    n_train = 500\n",
    "    n_test = 100\n",
    "    n_epochs = 100\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        print(f\"\\nåºåˆ—é•¿åº¦ = {seq_len}\")\n",
    "        \n",
    "        # ç”Ÿæˆæ•°æ®\n",
    "        X_train, Y_train = generate_data(n_train, seq_len)\n",
    "        X_test, Y_test = generate_data(n_test, seq_len)\n",
    "        \n",
    "        # åˆ›å»ºæ¨¡å‹\n",
    "        torch.manual_seed(42)\n",
    "        model = MemoryRNN(hidden_size)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # è®­ç»ƒ\n",
    "        train_losses = []\n",
    "        test_accs = []\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(X_train)\n",
    "            loss = criterion(output, Y_train)\n",
    "            loss.backward()\n",
    "            \n",
    "            # æ¢¯åº¦è£å‰ª\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # æµ‹è¯•å‡†ç¡®ç‡\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred = model(X_test)\n",
    "                    # å‡†ç¡®ç‡ï¼šé¢„æµ‹ç¬¦å·æ˜¯å¦æ­£ç¡®\n",
    "                    acc = ((pred > 0) == (Y_test > 0)).float().mean().item()\n",
    "                    test_accs.append((epoch, acc))\n",
    "        \n",
    "        # æœ€ç»ˆå‡†ç¡®ç‡\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_test)\n",
    "            final_acc = ((pred > 0) == (Y_test > 0)).float().mean().item()\n",
    "        \n",
    "        results[seq_len] = {\n",
    "            'final_acc': final_acc,\n",
    "            'train_losses': train_losses,\n",
    "            'test_accs': test_accs\n",
    "        }\n",
    "        \n",
    "        print(f\"  æœ€ç»ˆå‡†ç¡®ç‡: {final_acc*100:.1f}%\")\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å·¦å›¾ï¼šå‡†ç¡®ç‡ vs åºåˆ—é•¿åº¦\n",
    "    ax = axes[0]\n",
    "    accs = [results[s]['final_acc'] * 100 for s in seq_lengths]\n",
    "    bars = ax.bar(range(len(seq_lengths)), accs, \n",
    "                  tick_label=[str(s) for s in seq_lengths],\n",
    "                  color=['green' if a > 70 else 'orange' if a > 55 else 'red' for a in accs],\n",
    "                  edgecolor='black')\n",
    "    ax.axhline(y=50, color='gray', linestyle='--', label='éšæœºçŒœæµ‹ (50%)')\n",
    "    ax.set_xlabel('åºåˆ—é•¿åº¦', fontsize=12)\n",
    "    ax.set_ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)\n",
    "    ax.set_title('RNN è®°å¿†èƒ½åŠ› vs åºåˆ—é•¿åº¦', fontsize=14)\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, acc in zip(bars, accs):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "                f'{acc:.1f}%', ha='center', fontsize=10)\n",
    "    \n",
    "    # å³å›¾ï¼šè®­ç»ƒæ›²çº¿\n",
    "    ax = axes[1]\n",
    "    for seq_len in seq_lengths:\n",
    "        ax.plot(results[seq_len]['train_losses'], label=f'seq_len={seq_len}')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('è®­ç»ƒæŸå¤±', fontsize=12)\n",
    "    ax.set_title('è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ç»“è®º\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ç»“è®º:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"â€¢ RNN åœ¨çŸ­åºåˆ—ä¸Šè¡¨ç°è‰¯å¥½\")\n",
    "    print(\"â€¢ åºåˆ—è¶Šé•¿ï¼Œå‡†ç¡®ç‡è¶Šæ¥è¿‘éšæœºçŒœæµ‹ (50%)\")\n",
    "    print(\"â€¢ è¿™è¯æ˜äº†æ¢¯åº¦æ¶ˆå¤±å¯¼è‡´ RNN æ— æ³•å­¦ä¹ é•¿æœŸä¾èµ–\")\n",
    "    print(\"â€¢ è§£å†³æ–¹æ¡ˆ: LSTM å’Œ GRUï¼ˆä¸‹ä¸€ç« ï¼‰\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "memory_results = long_term_memory_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬å››éƒ¨åˆ†ï¼šç¼“è§£æ¢¯åº¦é—®é¢˜çš„æŠ€æœ¯\n",
    "\n",
    "### 4.1 æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰\n",
    "\n",
    "è§£å†³æ¢¯åº¦çˆ†ç‚¸çš„æœ€ç›´æ¥æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipping_demo():\n",
    "    \"\"\"\n",
    "    æ¢¯åº¦è£å‰ªæ¼”ç¤º\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"æ¢¯åº¦è£å‰ª (Gradient Clipping)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æ¨¡æ‹Ÿä¸€äº›å¯èƒ½çˆ†ç‚¸çš„æ¢¯åº¦\n",
    "    np.random.seed(42)\n",
    "    gradients = np.random.randn(100) * np.random.exponential(2, 100)\n",
    "    \n",
    "    # æ–¹æ³• 1: æŒ‰å€¼è£å‰ª (Clip by Value)\n",
    "    max_val = 1.0\n",
    "    clipped_by_value = np.clip(gradients, -max_val, max_val)\n",
    "    \n",
    "    # æ–¹æ³• 2: æŒ‰èŒƒæ•°è£å‰ª (Clip by Norm)\n",
    "    max_norm = 5.0\n",
    "    grad_norm = np.linalg.norm(gradients)\n",
    "    if grad_norm > max_norm:\n",
    "        clipped_by_norm = gradients * (max_norm / grad_norm)\n",
    "    else:\n",
    "        clipped_by_norm = gradients.copy()\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # åŸå§‹æ¢¯åº¦\n",
    "    ax = axes[0, 0]\n",
    "    ax.bar(range(len(gradients)), gradients, color='blue', alpha=0.7)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.set_title(f'åŸå§‹æ¢¯åº¦ (èŒƒæ•° = {np.linalg.norm(gradients):.2f})', fontsize=12)\n",
    "    ax.set_xlabel('å‚æ•°ç´¢å¼•')\n",
    "    ax.set_ylabel('æ¢¯åº¦å€¼')\n",
    "    ax.set_ylim(-10, 10)\n",
    "    \n",
    "    # æŒ‰å€¼è£å‰ª\n",
    "    ax = axes[0, 1]\n",
    "    ax.bar(range(len(clipped_by_value)), clipped_by_value, color='green', alpha=0.7)\n",
    "    ax.axhline(y=max_val, color='r', linestyle='--', label=f'max = {max_val}')\n",
    "    ax.axhline(y=-max_val, color='r', linestyle='--')\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.set_title(f'æŒ‰å€¼è£å‰ª (max = Â±{max_val})', fontsize=12)\n",
    "    ax.set_xlabel('å‚æ•°ç´¢å¼•')\n",
    "    ax.set_ylabel('æ¢¯åº¦å€¼')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(-10, 10)\n",
    "    \n",
    "    # æŒ‰èŒƒæ•°è£å‰ª\n",
    "    ax = axes[1, 0]\n",
    "    ax.bar(range(len(clipped_by_norm)), clipped_by_norm, color='orange', alpha=0.7)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.set_title(f'æŒ‰èŒƒæ•°è£å‰ª (max_norm = {max_norm}, æ–°èŒƒæ•° = {np.linalg.norm(clipped_by_norm):.2f})', fontsize=12)\n",
    "    ax.set_xlabel('å‚æ•°ç´¢å¼•')\n",
    "    ax.set_ylabel('æ¢¯åº¦å€¼')\n",
    "    ax.set_ylim(-10, 10)\n",
    "    \n",
    "    # æ¯”è¾ƒä¸¤ç§æ–¹æ³•\n",
    "    ax = axes[1, 1]\n",
    "    methods = ['åŸå§‹', 'æŒ‰å€¼è£å‰ª', 'æŒ‰èŒƒæ•°è£å‰ª']\n",
    "    norms = [np.linalg.norm(gradients), np.linalg.norm(clipped_by_value), np.linalg.norm(clipped_by_norm)]\n",
    "    bars = ax.bar(methods, norms, color=['blue', 'green', 'orange'], edgecolor='black')\n",
    "    ax.axhline(y=max_norm, color='r', linestyle='--', label=f'ç›®æ ‡èŒƒæ•° = {max_norm}')\n",
    "    ax.set_title('æ¢¯åº¦èŒƒæ•°æ¯”è¾ƒ', fontsize=12)\n",
    "    ax.set_ylabel('æ¢¯åº¦èŒƒæ•°')\n",
    "    ax.legend()\n",
    "    \n",
    "    for bar, norm in zip(bars, norms):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'{norm:.2f}', ha='center', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ä»£ç ç¤ºä¾‹\n",
    "    print(\"\\nPyTorch æ¢¯åº¦è£å‰ªä»£ç :\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\"\"\n",
    "# æ–¹æ³• 1: æŒ‰èŒƒæ•°è£å‰ªï¼ˆæ¨èï¼‰\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# æ–¹æ³• 2: æŒ‰å€¼è£å‰ª\n",
    "torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "\n",
    "# ä½¿ç”¨ä½ç½®: åœ¨ loss.backward() ä¹‹å, optimizer.step() ä¹‹å‰\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # è£å‰ª\n",
    "optimizer.step()\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "gradient_clipping_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 å…¶ä»–ç¼“è§£æŠ€æœ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_techniques():\n",
    "    \"\"\"\n",
    "    å…¶ä»–ç¼“è§£æ¢¯åº¦é—®é¢˜çš„æŠ€æœ¯\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ç¼“è§£æ¢¯åº¦é—®é¢˜çš„å…¶ä»–æŠ€æœ¯\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    techniques = [\n",
    "        {\n",
    "            \"name\": \"1. æƒé‡åˆå§‹åŒ–\",\n",
    "            \"description\": \"ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–æˆ– Xavier åˆå§‹åŒ–\",\n",
    "            \"code\": \"\"\"\n",
    "# æ­£äº¤åˆå§‹åŒ– (ä¿æŒæ¢¯åº¦ç¨³å®š)\n",
    "nn.init.orthogonal_(rnn.weight_hh_l0)\n",
    "\n",
    "# Xavier åˆå§‹åŒ–\n",
    "nn.init.xavier_uniform_(rnn.weight_ih_l0)\n",
    "\"\"\",\n",
    "            \"effect\": \"é˜²æ­¢åˆå§‹æ¢¯åº¦è¿‡å¤§æˆ–è¿‡å°\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"2. ä½¿ç”¨ ReLU è€Œé tanh\",\n",
    "            \"description\": \"ReLU çš„å¯¼æ•°æ˜¯ 0 æˆ– 1ï¼Œå‡å°‘æ¢¯åº¦æ¶ˆå¤±\",\n",
    "            \"code\": \"\"\"\n",
    "# æ³¨æ„: åŸç‰ˆ RNN ç”¨ tanhï¼Œä½†å¯ä»¥ç”¨ ReLU å˜ä½“\n",
    "class ReLURNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, \n",
    "                         nonlinearity='relu')  # ä½¿ç”¨ ReLU\n",
    "\"\"\",\n",
    "            \"effect\": \"å¯èƒ½å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼Œéœ€è¦é…åˆè£å‰ªä½¿ç”¨\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"3. æ‰¹å½’ä¸€åŒ– / å±‚å½’ä¸€åŒ–\",\n",
    "            \"description\": \"å½’ä¸€åŒ–éšçŠ¶æ€ï¼Œç¨³å®šæ¢¯åº¦æµåŠ¨\",\n",
    "            \"code\": \"\"\"\n",
    "class RNNWithLayerNorm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        outputs = []\n",
    "        for t in range(x.size(1)):\n",
    "            h = self.rnn_cell(x[:, t, :], h)\n",
    "            h = self.layer_norm(h)  # å±‚å½’ä¸€åŒ–\n",
    "            outputs.append(h)\n",
    "        return torch.stack(outputs, dim=1), h\n",
    "\"\"\",\n",
    "            \"effect\": \"æ ‡å‡†åŒ–æ¿€æ´»å€¼ï¼Œé˜²æ­¢å€¼è¿‡å¤§æˆ–è¿‡å°\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"4. æˆªæ–­ BPTT\",\n",
    "            \"description\": \"é™åˆ¶åå‘ä¼ æ’­çš„æ—¶é—´æ­¥æ•°\",\n",
    "            \"code\": \"\"\"\n",
    "# æ¯ 20 æ­¥æˆªæ–­ä¸€æ¬¡æ¢¯åº¦\n",
    "truncation_len = 20\n",
    "for i in range(0, seq_len, truncation_len):\n",
    "    chunk = x[:, i:i+truncation_len, :]\n",
    "    if h is not None:\n",
    "        h = h.detach()  # æˆªæ–­æ¢¯åº¦\n",
    "    output, h = rnn(chunk, h)\n",
    "\"\"\",\n",
    "            \"effect\": \"é™åˆ¶æ¢¯åº¦ä¼ æ’­è·ç¦»ï¼Œä½†å¯èƒ½ä¸¢å¤±é•¿æœŸä¿¡æ¯\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"5. æ®‹å·®è¿æ¥\",\n",
    "            \"description\": \"æ·»åŠ è·³è·ƒè¿æ¥ï¼Œè®©æ¢¯åº¦ç›´æ¥æµè¿‡\",\n",
    "            \"code\": \"\"\"\n",
    "class ResidualRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        # å‡è®¾ input_size == hidden_size\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        out, h_new = self.rnn(x, h)\n",
    "        return out + x, h_new  # æ®‹å·®è¿æ¥\n",
    "\"\"\",\n",
    "            \"effect\": \"æ¢¯åº¦å¯ä»¥ç»•è¿‡ RNN ç›´æ¥ä¼ å›ï¼Œç±»ä¼¼ ResNet\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for tech in techniques:\n",
    "        print(f\"\\n{tech['name']}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"æè¿°: {tech['description']}\")\n",
    "        print(f\"æ•ˆæœ: {tech['effect']}\")\n",
    "        print(f\"ä»£ç ç¤ºä¾‹:{tech['code']}\")\n",
    "    \n",
    "    # æœ€ç»ˆè§£å†³æ–¹æ¡ˆ\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"æœ€ç»ˆè§£å†³æ–¹æ¡ˆ: LSTM å’Œ GRU\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "ä¸Šè¿°æŠ€æœ¯éƒ½åªæ˜¯\"ç¼“è§£\"æ¢¯åº¦é—®é¢˜ï¼ŒçœŸæ­£çš„\"è§£å†³\"éœ€è¦æ”¹å˜æ¶æ„ï¼š\n",
    "\n",
    "LSTM (Long Short-Term Memory):\n",
    "  - å¼•å…¥\"ç»†èƒçŠ¶æ€\" (Cell State) ä½œä¸ºä¿¡æ¯é«˜é€Ÿå…¬è·¯\n",
    "  - ä½¿ç”¨\"é—¨æ§æœºåˆ¶\"æ§åˆ¶ä¿¡æ¯æµåŠ¨\n",
    "  - æ¢¯åº¦å¯ä»¥æ²¿ç»†èƒçŠ¶æ€ç›´æ¥ä¼ é€’\n",
    "\n",
    "GRU (Gated Recurrent Unit):\n",
    "  - LSTM çš„ç®€åŒ–ç‰ˆæœ¬\n",
    "  - æ›´å°‘çš„å‚æ•°ï¼Œä½†æ•ˆæœç›¸è¿‘\n",
    "  \n",
    "ä¸‹ä¸€ç« æˆ‘ä»¬å°†è¯¦ç»†è®²è§£ LSTM çš„åŸç†å’Œå®ç°ï¼\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "other_techniques()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬äº”éƒ¨åˆ†ï¼šæ€»ç»“ä¸å±•æœ›\n",
    "\n",
    "### 5.1 æœ¬ç« æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapter_summary():\n",
    "    \"\"\"\n",
    "    æœ¬ç« çŸ¥è¯†ç‚¹æ€»ç»“\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ç¬¬ 4 ç« æ€»ç»“ï¼šæ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    summary = \"\"\"\n",
    "    \n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         æ ¸å¿ƒé—®é¢˜                                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  BPTT ä¸­çš„æ¢¯åº¦è®¡ç®—ï¼š                                                â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚    âˆ‚h_t     t                                                       â”‚\n",
    "â”‚    â”€â”€â”€â”€ = âˆ [diag(1-h_iÂ²) Â· W_hh]                                   â”‚\n",
    "â”‚    âˆ‚h_k   i=k+1                                                     â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  è¿™æ˜¯ä¸€ä¸ªæŒ‡æ•°å‡½æ•°ï¼                                                 â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  â€¢ |Î»_max(W_hh)| < 1  â†’  æ¢¯åº¦æ¶ˆå¤±  â†’  æ— æ³•å­¦ä¹ é•¿æœŸä¾èµ–              â”‚\n",
    "â”‚  â€¢ |Î»_max(W_hh)| > 1  â†’  æ¢¯åº¦çˆ†ç‚¸  â†’  è®­ç»ƒä¸ç¨³å®š                    â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         ç¼“è§£æ–¹æ³•                                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  æ¢¯åº¦çˆ†ç‚¸ï¼ˆè¾ƒæ˜“è§£å†³ï¼‰:                                              â”‚\n",
    "â”‚    âœ“ æ¢¯åº¦è£å‰ª (Gradient Clipping)                                   â”‚\n",
    "â”‚    âœ“ æ­£ç¡®çš„æƒé‡åˆå§‹åŒ–                                               â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  æ¢¯åº¦æ¶ˆå¤±ï¼ˆè¾ƒéš¾è§£å†³ï¼‰:                                              â”‚\n",
    "â”‚    â–³ ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°                                             â”‚\n",
    "â”‚    â–³ å±‚å½’ä¸€åŒ– (Layer Normalization)                                 â”‚\n",
    "â”‚    â–³ æ®‹å·®è¿æ¥                                                       â”‚\n",
    "â”‚    â˜… LSTM / GRUï¼ˆæ ¹æœ¬è§£å†³æ–¹æ¡ˆï¼‰                                     â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         å®éªŒç»“è®º                                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  1. æ¢¯åº¦éšæ—¶é—´æ­¥æŒ‡æ•°çº§è¡°å‡/å¢é•¿                                     â”‚\n",
    "â”‚  2. RNN çš„æœ‰æ•ˆè®°å¿†é•¿åº¦çº¦ 10-20 æ­¥                                   â”‚\n",
    "â”‚  3. è¶…è¿‡ 50 æ­¥çš„é•¿æœŸä¾èµ–å‡ ä¹æ— æ³•å­¦ä¹                                 â”‚\n",
    "â”‚  4. è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆ LSTM åœ¨ 1997 å¹´è¢«å‘æ˜                            â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "    \n",
    "    print(summary)\n",
    "    \n",
    "    # å¯è§†åŒ–æ€»ç»“\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # ç»˜åˆ¶æ¢¯åº¦è¡Œä¸ºç¤ºæ„å›¾\n",
    "    time_steps = np.arange(0, 100)\n",
    "    \n",
    "    # ä¸‰ç§æƒ…å†µ\n",
    "    vanishing = 0.95 ** time_steps\n",
    "    stable = np.ones_like(time_steps, dtype=float)\n",
    "    exploding = 1.05 ** time_steps\n",
    "    \n",
    "    ax.semilogy(time_steps, vanishing, 'b-', linewidth=2, label='æ¢¯åº¦æ¶ˆå¤± (Î»=0.95)')\n",
    "    ax.semilogy(time_steps, stable, 'g-', linewidth=2, label='ç†æƒ³æƒ…å†µ (Î»=1.0)')\n",
    "    ax.semilogy(time_steps, exploding, 'r-', linewidth=2, label='æ¢¯åº¦çˆ†ç‚¸ (Î»=1.05)')\n",
    "    \n",
    "    # æ ‡æ³¨åŒºåŸŸ\n",
    "    ax.axhspan(1e-10, 1e-5, alpha=0.1, color='blue', label='æ¶ˆå¤±åŒºåŸŸ')\n",
    "    ax.axhspan(1e5, 1e10, alpha=0.1, color='red', label='çˆ†ç‚¸åŒºåŸŸ')\n",
    "    \n",
    "    ax.set_xlabel('æ—¶é—´æ­¥å·® (t - k)', fontsize=12)\n",
    "    ax.set_ylabel('æ¢¯åº¦ç›¸å¯¹å¤§å°', fontsize=12)\n",
    "    ax.set_title('RNN æ¢¯åº¦é—®é¢˜ï¼šæ¶ˆå¤± vs çˆ†ç‚¸', fontsize=14)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(1e-15, 1e15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "chapter_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ä¸‹ä¸€ç« é¢„å‘Š\n",
    "\n",
    "**ç¬¬ 5 ç« ï¼šLSTM æ·±åº¦è§£æ**\n",
    "\n",
    "LSTM å¦‚ä½•è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Ÿ\n",
    "\n",
    "å…³é”®åˆ›æ–°ï¼š\n",
    "1. **ç»†èƒçŠ¶æ€ (Cell State)**: ä¿¡æ¯é«˜é€Ÿå…¬è·¯ï¼Œæ¢¯åº¦å¯ä»¥ç›´æ¥ä¼ é€’\n",
    "2. **é—¨æ§æœºåˆ¶**: å­¦ä¹ ä½•æ—¶è®°ä½ã€ä½•æ—¶é—å¿˜ã€ä½•æ—¶è¾“å‡º\n",
    "\n",
    "ä¸‹ä¸€ç« æˆ‘ä»¬å°†ï¼š\n",
    "- è¯¦ç»†ç†è§£ LSTM çš„æ¯ä¸ªé—¨çš„ä½œç”¨\n",
    "- ç”¨ NumPy ä»é›¶å®ç° LSTM\n",
    "- åˆ†æ LSTM å¦‚ä½•ä¿æŒæ¢¯åº¦ç¨³å®š\n",
    "- åœ¨é•¿æœŸä¾èµ–ä»»åŠ¡ä¸ŠéªŒè¯ LSTM çš„æ•ˆæœ\n",
    "\n",
    "ğŸ‘‰ [05_lstm_deep_dive.ipynb](./05_lstm_deep_dive.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
