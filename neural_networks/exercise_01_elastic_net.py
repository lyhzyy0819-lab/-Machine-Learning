"""
============================================================================
ç»ƒä¹ 1ï¼šElastic Netæ­£åˆ™åŒ– - ä»é›¶åˆ°ä¸€å®Œæ•´å®ç°
============================================================================

ğŸ“š é—®é¢˜èƒŒæ™¯ï¼š
    L1å’ŒL2æ­£åˆ™åŒ–å„æœ‰ä¼˜ç¼ºç‚¹ï¼š
    - L1ï¼šäº§ç”Ÿç¨€ç–è§£ï¼Œä½†æ¢¯åº¦ä¸è¿ç»­
    - L2ï¼šæ¢¯åº¦å¹³æ»‘ï¼Œä½†ä¸äº§ç”Ÿç¨€ç–æ€§

    Elastic Netç»“åˆä¸¤è€…ä¼˜ç‚¹ï¼

ğŸ¯ å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£Elastic Netçš„æ•°å­¦åŸç†
    2. ä»é›¶å®ç°Elastic Netæ­£åˆ™åŒ–
    3. å¯¹æ¯”L1ã€L2ã€Elastic Netçš„æ•ˆæœ
    4. åˆ†æÎ±å‚æ•°å¯¹ç»“æœçš„å½±å“

============================================================================
"""

# ============================================================================
# ç¬¬1éƒ¨åˆ†ï¼šå¯¼å…¥åº“å’Œç¯å¢ƒé…ç½®
# ============================================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
np.random.seed(42)

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'STHeiti']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 70)
print("ç»ƒä¹ 1ï¼šElastic Netæ­£åˆ™åŒ– - ä»é›¶åˆ°ä¸€å®Œæ•´å®ç°")
print("=" * 70)


# ============================================================================
# ç¬¬2éƒ¨åˆ†ï¼šæ•°å­¦åŸç†è¯¦è§£
# ============================================================================
"""
ğŸ“– Elastic Netæ­£åˆ™åŒ–å…¬å¼ï¼š

    L_total = L_CE + Î± * Î» * ||W||â‚ + (1-Î±)/2 * Î» * ||W||â‚‚Â²

å…¶ä¸­ï¼š
    - L_CE: äº¤å‰ç†µæŸå¤±ï¼ˆCross Entropy Lossï¼‰
    - Î» (lambda): æ­£åˆ™åŒ–å¼ºåº¦ï¼Œæ§åˆ¶æƒ©ç½šåŠ›åº¦
    - Î± (alpha): L1/L2æ¯”ä¾‹ç³»æ•°ï¼ŒÎ± âˆˆ [0, 1]
        - Î± = 1: çº¯L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰
        - Î± = 0: çº¯L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰
        - Î± = 0.5: L1å’ŒL2å„å ä¸€åŠ

ğŸ“ æ¢¯åº¦è®¡ç®—ï¼š

    âˆ‚L_total/âˆ‚W = âˆ‚L_CE/âˆ‚W + Î± * Î» * sign(W) + (1-Î±) * Î» * W
                  â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  åŸå§‹æ¢¯åº¦      L1é¡¹æ¢¯åº¦           L2é¡¹æ¢¯åº¦

ğŸ’¡ ä¸ºä»€ä¹ˆElastic Netæ›´å¥½ï¼Ÿ
    1. ç»§æ‰¿L1çš„ç¨€ç–æ€§ï¼šå¯ä»¥è¿›è¡Œç‰¹å¾é€‰æ‹©
    2. ç»§æ‰¿L2çš„ç¨³å®šæ€§ï¼šåœ¨é«˜åº¦ç›¸å…³ç‰¹å¾æ—¶æ›´ç¨³å®š
    3. å¯è°ƒèŠ‚Î±å¹³è¡¡ä¸¤è€…
"""


# ============================================================================
# ç¬¬3éƒ¨åˆ†ï¼šæ¿€æ´»å‡½æ•°å®šä¹‰
# ============================================================================

def relu(z):
    """
    ReLUæ¿€æ´»å‡½æ•° (Rectified Linear Unit)

    æ•°å­¦å…¬å¼ï¼š
        ReLU(z) = max(0, z)

    ç‰¹ç‚¹ï¼š
        - z > 0 æ—¶ï¼Œè¾“å‡º z
        - z <= 0 æ—¶ï¼Œè¾“å‡º 0

    å‚æ•°:
        z: è¾“å…¥å€¼ï¼Œå¯ä»¥æ˜¯æ ‡é‡æˆ–æ•°ç»„

    è¿”å›:
        ä¸è¾“å…¥å½¢çŠ¶ç›¸åŒçš„æ•°ç»„
    """
    return np.maximum(0, z)


def relu_derivative(z):
    """
    ReLUçš„å¯¼æ•°

    æ•°å­¦å…¬å¼ï¼š
        ReLU'(z) = { 1, if z > 0
                   { 0, if z <= 0

    æ³¨æ„ï¼šåœ¨z=0å¤„ï¼Œå¯¼æ•°ç†è®ºä¸Šæœªå®šä¹‰ï¼Œä½†å®è·µä¸­é€šå¸¸å–0

    å‚æ•°:
        z: è¾“å…¥å€¼

    è¿”å›:
        å¯¼æ•°å€¼ï¼ˆ0æˆ–1ï¼‰
    """
    return (z > 0).astype(float)


def sigmoid(z):
    """
    Sigmoidæ¿€æ´»å‡½æ•°

    æ•°å­¦å…¬å¼ï¼š
        Ïƒ(z) = 1 / (1 + e^(-z))

    ç‰¹ç‚¹ï¼š
        - è¾“å‡ºèŒƒå›´ (0, 1)ï¼Œé€‚åˆäºŒåˆ†ç±»
        - å°†ä»»æ„å®æ•°æ˜ å°„åˆ°æ¦‚ç‡å€¼

    å‚æ•°:
        z: è¾“å…¥å€¼

    è¿”å›:
        æ¦‚ç‡å€¼ï¼ŒèŒƒå›´ (0, 1)

    å®ç°ç»†èŠ‚ï¼š
        np.clip(z, -500, 500) é˜²æ­¢æ•°å€¼æº¢å‡º
        - å½“zå¾ˆå¤§æ—¶ï¼Œexp(-z) â‰ˆ 0ï¼Œsigmoid â‰ˆ 1
        - å½“zå¾ˆå°æ—¶ï¼Œexp(-z) å¯èƒ½æº¢å‡ºï¼Œæ‰€ä»¥é™åˆ¶èŒƒå›´
    """
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))


# ============================================================================
# ç¬¬4éƒ¨åˆ†ï¼šElastic Netç¥ç»ç½‘ç»œå®ç°
# ============================================================================

class ElasticNetNetwork:
    """
    å¸¦Elastic Netæ­£åˆ™åŒ–çš„ç¥ç»ç½‘ç»œ

    ç½‘ç»œç»“æ„ï¼š
        è¾“å…¥å±‚ (2) â†’ éšè—å±‚1 (64) â†’ éšè—å±‚2 (64) â†’ éšè—å±‚3 (32) â†’ è¾“å‡ºå±‚ (1)

    Elastic Netå…¬å¼ï¼š
        L_total = L_CE + Î± * Î» * ||W||â‚ + (1-Î±)/2 * Î» * ||W||â‚‚Â²

    å‚æ•°:
        lambda_reg: float, æ­£åˆ™åŒ–å¼ºåº¦ Î»
        alpha: float, L1/L2æ¯”ä¾‹ Î± âˆˆ [0, 1]
            - alpha=1: çº¯L1
            - alpha=0: çº¯L2
            - alpha=0.5: æ··åˆ
    """

    def __init__(self, lambda_reg=0.01, alpha=0.5):
        """
        åˆå§‹åŒ–ç½‘ç»œå‚æ•°

        å‚æ•°:
            lambda_reg: æ­£åˆ™åŒ–å¼ºåº¦ï¼Œå…¸å‹å€¼ 0.001 ~ 0.1
            alpha: L1æ¯”ä¾‹ï¼Œ0è¡¨ç¤ºçº¯L2ï¼Œ1è¡¨ç¤ºçº¯L1
        """
        # =====================================
        # å­˜å‚¨æ­£åˆ™åŒ–è¶…å‚æ•°
        # =====================================
        self.lambda_reg = lambda_reg  # Î»: æ­£åˆ™åŒ–å¼ºåº¦
        self.alpha = alpha            # Î±: L1/L2æ¯”ä¾‹

        # =====================================
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        # =====================================
        # ä½¿ç”¨è¾ƒå¤§çš„åˆå§‹åŒ–å€¼ï¼Œå®¹æ˜“è§‚å¯Ÿè¿‡æ‹Ÿåˆç°è±¡
        # å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨Heåˆå§‹åŒ–æˆ–Xavieråˆå§‹åŒ–

        # ç¬¬1å±‚: 2 â†’ 64
        # W1å½¢çŠ¶: (64, 2), æ¯è¡Œæ˜¯ä¸€ä¸ªç¥ç»å…ƒçš„æƒé‡
        self.W1 = np.random.randn(64, 2) * 0.5
        self.b1 = np.zeros(64)  # åç½®åˆå§‹åŒ–ä¸º0

        # ç¬¬2å±‚: 64 â†’ 64
        self.W2 = np.random.randn(64, 64) * 0.5
        self.b2 = np.zeros(64)

        # ç¬¬3å±‚: 64 â†’ 32
        self.W3 = np.random.randn(32, 64) * 0.5
        self.b3 = np.zeros(32)

        # è¾“å‡ºå±‚: 32 â†’ 1
        self.W4 = np.random.randn(1, 32) * 0.5
        self.b4 = np.zeros(1)

        print(f"ç½‘ç»œåˆå§‹åŒ–å®Œæˆ:")
        print(f"  æ­£åˆ™åŒ–å¼ºåº¦ Î» = {lambda_reg}")
        print(f"  L1/L2æ¯”ä¾‹ Î± = {alpha}")
        print(f"  ç½‘ç»œç»“æ„: [2, 64, 64, 32, 1]")
        print(f"  æ€»å‚æ•°é‡: {self._count_params()}")

    def _count_params(self):
        """è®¡ç®—ç½‘ç»œæ€»å‚æ•°é‡"""
        total = 0
        for W, b in [(self.W1, self.b1), (self.W2, self.b2),
                     (self.W3, self.b3), (self.W4, self.b4)]:
            total += W.size + b.size
        return total

    def forward(self, X):
        """
        å‰å‘ä¼ æ’­

        æ•°æ®æµåŠ¨è¿‡ç¨‹ï¼š
            X â†’ [W1, b1] â†’ ReLU â†’ [W2, b2] â†’ ReLU â†’ [W3, b3] â†’ ReLU â†’ [W4, b4] â†’ Sigmoid â†’ è¾“å‡º

        æ•°å­¦å…¬å¼ï¼š
            z^(l) = a^(l-1) @ W^(l).T + b^(l)  # çº¿æ€§å˜æ¢
            a^(l) = activation(z^(l))          # éçº¿æ€§æ¿€æ´»

        å‚æ•°:
            X: è¾“å…¥æ•°æ®, shape (n_samples, 2)

        è¿”å›:
            output: é¢„æµ‹æ¦‚ç‡, shape (n_samples, 1)
        """
        # =====================================
        # ç¬¬1å±‚ï¼šè¾“å…¥å±‚ â†’ éšè—å±‚1
        # =====================================
        # çº¿æ€§å˜æ¢: z1 = X @ W1.T + b1
        # X: (n_samples, 2), W1.T: (2, 64) â†’ z1: (n_samples, 64)
        self.z1 = X @ self.W1.T + self.b1
        # æ¿€æ´»å‡½æ•°
        self.a1 = relu(self.z1)

        # =====================================
        # ç¬¬2å±‚ï¼šéšè—å±‚1 â†’ éšè—å±‚2
        # =====================================
        self.z2 = self.a1 @ self.W2.T + self.b2
        self.a2 = relu(self.z2)

        # =====================================
        # ç¬¬3å±‚ï¼šéšè—å±‚2 â†’ éšè—å±‚3
        # =====================================
        self.z3 = self.a2 @ self.W3.T + self.b3
        self.a3 = relu(self.z3)

        # =====================================
        # è¾“å‡ºå±‚ï¼šéšè—å±‚3 â†’ è¾“å‡º
        # =====================================
        # ä½¿ç”¨Sigmoidæ¿€æ´»ï¼Œè¾“å‡ºæ¦‚ç‡å€¼
        self.z4 = self.a3 @ self.W4.T + self.b4
        self.a4 = sigmoid(self.z4)

        return self.a4

    def backward(self, X, y_true):
        """
        åå‘ä¼ æ’­ - åŒ…å«Elastic Netæ­£åˆ™åŒ–

        æ ¸å¿ƒå…¬å¼ï¼š
            âˆ‚L/âˆ‚W = âˆ‚L_CE/âˆ‚W + Î±*Î»*sign(W) + (1-Î±)*Î»*W
                    â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    åŸå§‹æ¢¯åº¦      L1æ¢¯åº¦         L2æ¢¯åº¦

        åå‘ä¼ æ’­æµç¨‹ï¼š
            1. è®¡ç®—è¾“å‡ºå±‚è¯¯å·® Î´4 = a4 - y_true
            2. é€å±‚åå‘ä¼ æ’­è¯¯å·®
            3. è®¡ç®—æ¯å±‚çš„æ¢¯åº¦
            4. æ·»åŠ æ­£åˆ™åŒ–é¡¹

        å‚æ•°:
            X: è¾“å…¥æ•°æ®, shape (n_samples, 2)
            y_true: çœŸå®æ ‡ç­¾, shape (n_samples,)

        è¿”å›:
            grads: æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦åˆ—è¡¨
        """
        m = X.shape[0]  # æ ·æœ¬æ•°é‡

        # =====================================
        # è¾“å‡ºå±‚æ¢¯åº¦
        # =====================================
        # å¯¹äºäºŒåˆ†ç±»äº¤å‰ç†µ + Sigmoidï¼Œæ¢¯åº¦ç®€åŒ–ä¸ºï¼š
        # Î´4 = a4 - y_true
        delta4 = (self.a4 - y_true.reshape(-1, 1)) / m

        # æƒé‡æ¢¯åº¦ = åŸå§‹æ¢¯åº¦ + L1é¡¹ + L2é¡¹
        # grad_W4 = Î´4.T @ a3 + Î±*Î»*sign(W4) + (1-Î±)*Î»*W4
        grad_W4 = (delta4.T @ self.a3 +
                   self.alpha * self.lambda_reg * np.sign(self.W4) +  # L1é¡¹
                   (1 - self.alpha) * self.lambda_reg * self.W4)      # L2é¡¹

        # åç½®æ¢¯åº¦ï¼ˆåç½®ä¸æ­£åˆ™åŒ–ï¼ï¼‰
        grad_b4 = np.sum(delta4, axis=0)

        # =====================================
        # ç¬¬3å±‚æ¢¯åº¦
        # =====================================
        # è¯¯å·®åå‘ä¼ æ’­ï¼šÎ´3 = (Î´4 @ W4) * ReLU'(z3)
        delta3 = (delta4 @ self.W4) * relu_derivative(self.z3)

        grad_W3 = (delta3.T @ self.a2 +
                   self.alpha * self.lambda_reg * np.sign(self.W3) +
                   (1 - self.alpha) * self.lambda_reg * self.W3)
        grad_b3 = np.sum(delta3, axis=0)

        # =====================================
        # ç¬¬2å±‚æ¢¯åº¦
        # =====================================
        delta2 = (delta3 @ self.W3) * relu_derivative(self.z2)

        grad_W2 = (delta2.T @ self.a1 +
                   self.alpha * self.lambda_reg * np.sign(self.W2) +
                   (1 - self.alpha) * self.lambda_reg * self.W2)
        grad_b2 = np.sum(delta2, axis=0)

        # =====================================
        # ç¬¬1å±‚æ¢¯åº¦
        # =====================================
        delta1 = (delta2 @ self.W2) * relu_derivative(self.z1)

        grad_W1 = (delta1.T @ X +
                   self.alpha * self.lambda_reg * np.sign(self.W1) +
                   (1 - self.alpha) * self.lambda_reg * self.W1)
        grad_b1 = np.sum(delta1, axis=0)

        # è¿”å›æ‰€æœ‰æ¢¯åº¦
        return [grad_W1, grad_b1, grad_W2, grad_b2,
                grad_W3, grad_b3, grad_W4, grad_b4]

    def get_params(self):
        """è·å–æ‰€æœ‰å‚æ•°"""
        return [self.W1, self.b1, self.W2, self.b2,
                self.W3, self.b3, self.W4, self.b4]

    def set_params(self, params):
        """è®¾ç½®æ‰€æœ‰å‚æ•°"""
        self.W1, self.b1, self.W2, self.b2, \
        self.W3, self.b3, self.W4, self.b4 = params

    def compute_loss(self, X, y_true):
        """
        è®¡ç®—æ€»æŸå¤±ï¼ˆåŒ…å«Elastic Netæ­£åˆ™åŒ–é¡¹ï¼‰

        å…¬å¼ï¼š
            L_total = L_CE + Î±*Î»*||W||â‚ + (1-Î±)/2*Î»*||W||â‚‚Â²

        å‚æ•°:
            X: è¾“å…¥æ•°æ®
            y_true: çœŸå®æ ‡ç­¾

        è¿”å›:
            total_loss: æ€»æŸå¤±å€¼
        """
        # =====================================
        # 1. è®¡ç®—äº¤å‰ç†µæŸå¤±
        # =====================================
        y_pred = self.forward(X)
        epsilon = 1e-15  # é˜²æ­¢log(0)
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

        # äºŒåˆ†ç±»äº¤å‰ç†µ: L = -mean(y*log(p) + (1-y)*log(1-p))
        ce_loss = -np.mean(
            y_true * np.log(y_pred.flatten()) +
            (1 - y_true) * np.log(1 - y_pred.flatten())
        )

        # =====================================
        # 2. è®¡ç®—L1æ­£åˆ™åŒ–é¡¹: Î± * Î» * Î£|W|
        # =====================================
        l1_penalty = self.alpha * self.lambda_reg * (
            np.sum(np.abs(self.W1)) +
            np.sum(np.abs(self.W2)) +
            np.sum(np.abs(self.W3)) +
            np.sum(np.abs(self.W4))
        )

        # =====================================
        # 3. è®¡ç®—L2æ­£åˆ™åŒ–é¡¹: (1-Î±)/2 * Î» * Î£(WÂ²)
        # =====================================
        l2_penalty = (1 - self.alpha) * self.lambda_reg / 2 * (
            np.sum(self.W1 ** 2) +
            np.sum(self.W2 ** 2) +
            np.sum(self.W3 ** 2) +
            np.sum(self.W4 ** 2)
        )

        return ce_loss + l1_penalty + l2_penalty

    def compute_accuracy(self, X, y_true):
        """
        è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡

        å‚æ•°:
            X: è¾“å…¥æ•°æ®
            y_true: çœŸå®æ ‡ç­¾

        è¿”å›:
            accuracy: å‡†ç¡®ç‡ (0~1)
        """
        y_pred = self.forward(X)
        # æ¦‚ç‡ >= 0.5 é¢„æµ‹ä¸ºç±»åˆ«1ï¼Œå¦åˆ™ä¸ºç±»åˆ«0
        predictions = (y_pred >= 0.5).astype(int).flatten()
        return np.mean(predictions == y_true)

    def count_zero_weights(self, threshold=1e-3):
        """
        ç»Ÿè®¡æ¥è¿‘0çš„æƒé‡æ•°é‡ï¼ˆç¨€ç–æ€§æŒ‡æ ‡ï¼‰

        å‚æ•°:
            threshold: åˆ¤æ–­ä¸º0çš„é˜ˆå€¼

        è¿”å›:
            (zero_count, total_count): æ¥è¿‘0çš„æƒé‡æ•°å’Œæ€»æƒé‡æ•°
        """
        total = self.W1.size + self.W2.size + self.W3.size + self.W4.size
        zero_count = (
            np.sum(np.abs(self.W1) < threshold) +
            np.sum(np.abs(self.W2) < threshold) +
            np.sum(np.abs(self.W3) < threshold) +
            np.sum(np.abs(self.W4) < threshold)
        )
        return zero_count, total


# ============================================================================
# ç¬¬5éƒ¨åˆ†ï¼šè®­ç»ƒå‡½æ•°
# ============================================================================

def train_elastic_net(X_train, y_train, X_test, y_test,
                      lambda_reg=0.01, alpha=0.5,
                      n_epochs=500, learning_rate=0.01,
                      verbose=True):
    """
    è®­ç»ƒå¸¦Elastic Netæ­£åˆ™åŒ–çš„ç¥ç»ç½‘ç»œ

    å‚æ•°:
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_test, y_test: æµ‹è¯•æ•°æ®
        lambda_reg: æ­£åˆ™åŒ–å¼ºåº¦
        alpha: L1æ¯”ä¾‹ (0~1)
        n_epochs: è®­ç»ƒè½®æ•°
        learning_rate: å­¦ä¹ ç‡
        verbose: æ˜¯å¦æ‰“å°è®­ç»ƒä¿¡æ¯

    è¿”å›:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        history: è®­ç»ƒå†å²ï¼ˆæŸå¤±å’Œå‡†ç¡®ç‡ï¼‰
    """
    # åˆ›å»ºæ¨¡å‹
    model = ElasticNetNetwork(lambda_reg=lambda_reg, alpha=alpha)

    # è®°å½•è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'test_loss': [],
        'train_acc': [],
        'test_acc': [],
        'sparsity': []
    }

    # =====================================
    # è®­ç»ƒå¾ªç¯
    # =====================================
    for epoch in range(n_epochs):
        # ----- å‰å‘ä¼ æ’­ -----
        model.forward(X_train)

        # ----- åå‘ä¼ æ’­ -----
        grads = model.backward(X_train, y_train)

        # ----- å‚æ•°æ›´æ–°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰ -----
        # W_new = W_old - learning_rate * gradient
        params = model.get_params()
        updated_params = [p - learning_rate * g for p, g in zip(params, grads)]
        model.set_params(updated_params)

        # ----- è®°å½•æŒ‡æ ‡ -----
        if epoch % 10 == 0:
            train_loss = model.compute_loss(X_train, y_train)
            test_loss = model.compute_loss(X_test, y_test)
            train_acc = model.compute_accuracy(X_train, y_train)
            test_acc = model.compute_accuracy(X_test, y_test)
            zero_w, total_w = model.count_zero_weights()
            sparsity = zero_w / total_w * 100

            history['train_loss'].append(train_loss)
            history['test_loss'].append(test_loss)
            history['train_acc'].append(train_acc)
            history['test_acc'].append(test_acc)
            history['sparsity'].append(sparsity)

            if verbose and epoch % 100 == 0:
                print(f"Epoch {epoch:4d}: "
                      f"Train Acc={train_acc:.4f}, "
                      f"Test Acc={test_acc:.4f}, "
                      f"Sparsity={sparsity:.1f}%")

    return model, history


# ============================================================================
# ç¬¬6éƒ¨åˆ†ï¼šå®éªŒå¯¹æ¯”
# ============================================================================

if __name__ == "__main__":

    # =====================================
    # 1. å‡†å¤‡æ•°æ®
    # =====================================
    print("\n" + "=" * 70)
    print("ç¬¬1æ­¥ï¼šå‡†å¤‡æ•°æ®")
    print("=" * 70)

    # ç”Ÿæˆæœˆç‰™å½¢æ•°æ®é›†ï¼ˆç»å…¸çš„éçº¿æ€§åˆ†ç±»é—®é¢˜ï¼‰
    X, y = make_moons(n_samples=200, noise=0.2, random_state=42)

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    # æ•°æ®æ ‡å‡†åŒ–ï¼ˆé‡è¦ï¼ä½¿è®­ç»ƒæ›´ç¨³å®šï¼‰
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")

    # =====================================
    # 2. å¯¹æ¯”ä¸åŒçš„Î±å€¼
    # =====================================
    print("\n" + "=" * 70)
    print("ç¬¬2æ­¥ï¼šå¯¹æ¯”ä¸åŒçš„Î±å€¼")
    print("=" * 70)

    # æµ‹è¯•çš„Î±å€¼ï¼š
    # Î± = 0.0: çº¯L2æ­£åˆ™åŒ–
    # Î± = 0.3: 70% L2 + 30% L1
    # Î± = 0.5: 50% L2 + 50% L1
    # Î± = 0.7: 30% L2 + 70% L1
    # Î± = 1.0: çº¯L1æ­£åˆ™åŒ–
    alphas = [0.0, 0.3, 0.5, 0.7, 1.0]
    results = {}

    for alpha in alphas:
        print(f"\n{'â”€' * 50}")
        print(f"è®­ç»ƒæ¨¡å‹: Î± = {alpha}")
        print(f"{'â”€' * 50}")

        model, history = train_elastic_net(
            X_train, y_train, X_test, y_test,
            lambda_reg=0.01,
            alpha=alpha,
            n_epochs=500,
            learning_rate=0.01,
            verbose=True
        )

        results[alpha] = {
            'model': model,
            'history': history,
            'final_test_acc': history['test_acc'][-1],
            'final_sparsity': history['sparsity'][-1]
        }

        print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {history['test_acc'][-1]:.4f}")
        print(f"æœ€ç»ˆç¨€ç–æ€§: {history['sparsity'][-1]:.1f}%")

    # =====================================
    # 3. å¯è§†åŒ–ç»“æœ
    # =====================================
    print("\n" + "=" * 70)
    print("ç¬¬3æ­¥ï¼šå¯è§†åŒ–ç»“æœ")
    print("=" * 70)

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    epochs_plot = np.arange(0, 50) * 10

    # ----- å›¾1ï¼šæµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯” -----
    ax1 = axes[0, 0]
    for alpha in alphas:
        ax1.plot(epochs_plot, results[alpha]['history']['test_acc'],
                 label=f'Î±={alpha}', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=11)
    ax1.set_ylabel('Test Accuracy', fontsize=11)
    ax1.set_title('æµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”ï¼ˆä¸åŒÎ±å€¼ï¼‰', fontsize=12, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # ----- å›¾2ï¼šç¨€ç–æ€§å¯¹æ¯” -----
    ax2 = axes[0, 1]
    for alpha in alphas:
        ax2.plot(epochs_plot, results[alpha]['history']['sparsity'],
                 label=f'Î±={alpha}', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=11)
    ax2.set_ylabel('Sparsity (%)', fontsize=11)
    ax2.set_title('æƒé‡ç¨€ç–æ€§å¯¹æ¯”ï¼ˆä¸åŒÎ±å€¼ï¼‰', fontsize=12, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # ----- å›¾3ï¼šæœ€ç»ˆç»“æœæŸ±çŠ¶å›¾ -----
    ax3 = axes[1, 0]
    x_pos = np.arange(len(alphas))
    width = 0.35

    final_accs = [results[a]['final_test_acc'] for a in alphas]
    final_sparsities = [results[a]['final_sparsity'] / 100 for a in alphas]

    bars1 = ax3.bar(x_pos - width/2, final_accs, width,
                    label='æµ‹è¯•å‡†ç¡®ç‡', alpha=0.8, color='#3498db')
    bars2 = ax3.bar(x_pos + width/2, final_sparsities, width,
                    label='ç¨€ç–æ€§æ¯”ä¾‹', alpha=0.8, color='#e74c3c')

    ax3.set_xlabel('Î± (L1æ¯”ä¾‹)', fontsize=11)
    ax3.set_ylabel('Value', fontsize=11)
    ax3.set_title('æœ€ç»ˆç»“æœå¯¹æ¯”', fontsize=12, fontweight='bold')
    ax3.set_xticks(x_pos)
    ax3.set_xticklabels([f'{a}' for a in alphas])
    ax3.legend()
    ax3.grid(True, alpha=0.3, axis='y')

    # ----- å›¾4ï¼šç»“è®ºæ€»ç»“ -----
    ax4 = axes[1, 1]
    summary_text = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           Elastic Netæ­£åˆ™åŒ–å®éªŒæ€»ç»“                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                          â•‘
â•‘  ğŸ“ å…¬å¼:                                                â•‘
â•‘     L = L_CE + Î±Â·Î»Â·||W||â‚ + (1-Î±)/2Â·Î»Â·||W||â‚‚Â²           â•‘
â•‘                                                          â•‘
â•‘  ğŸ”¬ å®éªŒç»“è®º:                                            â•‘
â•‘                                                          â•‘
â•‘  Î± = 0 (çº¯L2):                                          â•‘
â•‘    â€¢ ç¨€ç–æ€§æœ€ä½                                          â•‘
â•‘    â€¢ æƒé‡å‡åŒ€ç¼©å°                                        â•‘
â•‘                                                          â•‘
â•‘  Î± = 1 (çº¯L1):                                          â•‘
â•‘    â€¢ ç¨€ç–æ€§æœ€é«˜                                          â•‘
â•‘    â€¢ å¾ˆå¤šæƒé‡å˜ä¸º0                                       â•‘
â•‘                                                          â•‘
â•‘  Î± âˆˆ (0,1) (Elastic Net):                               â•‘
â•‘    â€¢ ç»“åˆä¸¤è€…ä¼˜ç‚¹                                        â•‘
â•‘    â€¢ é€‚åº¦ç¨€ç– + ç¨³å®šè®­ç»ƒ                                 â•‘
â•‘                                                          â•‘
â•‘  ğŸ’¡ æ¨è: Î± = 0.5 æ˜¯å¾ˆå¥½çš„èµ·ç‚¹                          â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    ax4.text(0.02, 0.5, summary_text, fontsize=9, verticalalignment='center',
             family='monospace',
             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.3))
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')

    plt.tight_layout()
    plt.savefig('/Users/lyh/Desktop/ Machine Learning/neural_networks/elastic_net_results.png',
                dpi=150, bbox_inches='tight')
    plt.show()

    # =====================================
    # 4. æ‰“å°æœ€ç»ˆæ€»ç»“
    # =====================================
    print("\n" + "=" * 70)
    print("å®éªŒç»“æœæ±‡æ€»")
    print("=" * 70)
    print(f"\n{'Î±å€¼':<8} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'ç¨€ç–æ€§':<10} {'è¯´æ˜':<20}")
    print("-" * 60)

    descriptions = {
        0.0: "çº¯L2æ­£åˆ™åŒ–",
        0.3: "åL2çš„Elastic Net",
        0.5: "å‡è¡¡Elastic Net",
        0.7: "åL1çš„Elastic Net",
        1.0: "çº¯L1æ­£åˆ™åŒ–"
    }

    for alpha in alphas:
        acc = results[alpha]['final_test_acc']
        sparsity = results[alpha]['final_sparsity']
        desc = descriptions[alpha]
        print(f"{alpha:<8} {acc:<12.4f} {sparsity:<10.1f}% {desc:<20}")

    print("\n" + "=" * 70)
    print("âœ… ç»ƒä¹ 1å®Œæˆï¼")
    print("=" * 70)
    print("""
ğŸ“š å­¦ä¹ è¦ç‚¹:
    1. Elastic Net = L1 + L2æ­£åˆ™åŒ–çš„ç»“åˆ
    2. Î±å‚æ•°æ§åˆ¶L1å’ŒL2çš„æ¯”ä¾‹
    3. Î±è¶Šå¤§ï¼Œç¨€ç–æ€§è¶Šå¼º
    4. å®é™…åº”ç”¨ä¸­ï¼ŒÎ±=0.5æ˜¯å¾ˆå¥½çš„èµ·ç‚¹
    5. å¯ä»¥é€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³çš„Î»å’ŒÎ±
""")
