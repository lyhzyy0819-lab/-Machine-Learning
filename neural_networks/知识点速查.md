# 神经网络模块 - 面试知识点速查（详细版）

> 本文档涵盖神经网络11个核心章节的面试高频考点，侧重结论性知识和实践经验。

---

## 目录
1. [感知机与历史](#1-感知机与历史)
2. [激活函数](#2-激活函数)
3. [多层感知机与前向传播](#3-多层感知机与前向传播)
4. [反向传播算法](#4-反向传播算法)
5. [损失函数与优化器](#5-损失函数与优化器)
6. [正则化技术](#6-正则化技术)
7. [Batch Normalization与Dropout](#7-batch-normalization与dropout)
8. [权重初始化](#8-权重初始化)
9. [训练技巧](#9-训练技巧)
10. [PyTorch/TensorFlow入门](#10-pytorchtensorflow入门)
11. [神经网络回归](#11-神经网络回归)

---

## 1. 感知机与历史

### 核心结论
- **感知机**是最简单的线性二分类器，只能解决**线性可分**问题
- **XOR问题**证明了单层感知机的局限性，推动了多层网络的发展
- **激活函数**的作用是引入非线性，没有它多层网络等于单层

### 关键公式
| 公式 | 说明 |
|------|------|
| $y = \text{sign}(\mathbf{w}^T\mathbf{x} + b)$ | 感知机模型 |
| $\mathbf{w} \leftarrow \mathbf{w} + \eta y_{true}\mathbf{x}$ | 权重更新（只在预测错误时） |

### 历史里程碑
| 年份 | 事件 | 意义 |
|------|------|------|
| 1943 | McCulloch-Pitts神经元 | 首个数学神经元模型 |
| 1958 | Rosenblatt感知机 | **首个可学习的神经网络** |
| 1969 | Minsky《Perceptrons》 | 证明无法解决XOR，导致**第一次AI寒冬** |
| 1986 | 反向传播算法 | 多层网络训练成为可能 |
| 2006 | Hinton深度信念网络 | 深度学习复兴 |
| 2012 | AlexNet | ImageNet夺冠，**深度学习爆发** |
| 2017 | Transformer | 开启大模型时代 |

### 面试问答

**Q1: 感知机能解决XOR问题吗？为什么？**
> **A**: 不能。XOR是线性不可分问题。感知机的决策边界是一条直线（超平面），无法将(0,0)、(1,1)与(0,1)、(1,0)分开。这是因为XOR的正负样本在二维空间中交叉分布，没有任何直线能将它们分开。

**Q2: 如何解决XOR问题？**
> **A**: 使用**多层网络**（至少一个隐藏层）。隐藏层学习中间特征，将原始空间映射到新空间，在新空间中XOR变得线性可分。例如，隐藏层可以学习AND和OR特征，输出层再组合这些特征。

**Q3: 为什么需要激活函数？**
> **A**:
> 1. **引入非线性**：没有激活函数，$f(f(x))$还是线性的，多层等于单层
> 2. **增加表达能力**：非线性让网络能逼近任意复杂函数
> 3. **模拟生物神经元**：神经元有"激活阈值"，不是简单的线性响应

**Q4: 感知机学习规则为什么有效？**
> **A**: 只在**预测错误时更新**权重，更新方向使决策边界向错分样本移动。对于线性可分数据，有限次迭代后必定收敛（感知机收敛定理）。

**Q5: 感知机和逻辑回归的区别？**
> **A**:
> | 特性 | 感知机 | 逻辑回归 |
> |------|--------|---------|
> | 输出 | {-1,1}或{0,1} | 概率[0,1] |
> | 激活函数 | sign（阶跃） | Sigmoid |
> | 损失函数 | 误分类点到超平面距离 | 交叉熵 |
> | 更新时机 | 仅错误时 | 每次都更新 |
> | 优化目标 | 找到分离超平面 | 最大化似然 |

**Q6: 第一次AI寒冬的原因是什么？**
> **A**: 1969年Minsky和Papert在《Perceptrons》书中数学证明了单层感知机无法解决XOR等线性不可分问题。这被过度解读为"神经网络无用"，导致研究经费大幅削减，进入约15年的低谷期。

### 易错点
| 错误 | 正确理解 |
|------|---------|
| 感知机=神经网络 | 感知机只是最简单的单层线性分类器 |
| 感知机能处理任意分类问题 | 只能处理线性可分问题 |
| 多层感知机=多个感知机叠加 | 不对，多层感知机用可微激活函数，可以反向传播 |

---

## 2. 激活函数

### 核心结论
- **隐藏层首选ReLU**：计算快、缓解梯度消失、稀疏激活
- **输出层根据任务选**：二分类→Sigmoid，多分类→Softmax，回归→无激活
- **Sigmoid/Tanh的问题**：梯度消失、计算慢、非零中心（Sigmoid）

### 激活函数速查表

| 函数 | 公式 | 输出范围 | 优点 | 缺点 | 使用场景 |
|------|------|---------|------|------|---------|
| **Sigmoid** | $\frac{1}{1+e^{-z}}$ | (0,1) | 输出可解释为概率 | 梯度消失、非零中心、计算慢 | 二分类输出层 |
| **Tanh** | $\frac{e^z-e^{-z}}{e^z+e^{-z}}$ | (-1,1) | 零中心、梯度比Sigmoid大 | 仍有梯度消失 | LSTM内部、特殊场景 |
| **ReLU** | $\max(0,z)$ | [0,+∞) | 计算快、缓解梯度消失、稀疏 | Dead ReLU问题 | **隐藏层首选** |
| **Leaky ReLU** | $\max(\alpha z, z)$ | (-∞,+∞) | 避免Dead ReLU | 多一个超参数α | ReLU失效时 |
| **PReLU** | $\max(\alpha z, z)$，α可学习 | (-∞,+∞) | α自动学习 | 多参数 | 图像任务 |
| **ELU** | $z$ if $z>0$ else $\alpha(e^z-1)$ | (-α,+∞) | 负区平滑、零均值输出 | 计算exp较慢 | 特殊需求 |
| **GELU** | $z \cdot \Phi(z)$ | (-∞,+∞) | 平滑、理论优雅 | 计算较复杂 | **Transformer/BERT** |
| **Swish** | $z \cdot \sigma(\beta z)$ | (-∞,+∞) | 平滑、自门控 | 计算较复杂 | EfficientNet |
| **Mish** | $z \cdot \tanh(\ln(1 + e^z))$ | (-∞,+∞) | 平滑、非单调 | 计算复杂 | YOLOv4/视觉任务 |
| **Softmax** | $\frac{e^{z_i}}{\sum_j e^{z_j}}$ | (0,1)且和为1 | 输出概率分布 | 只用于输出层 | **多分类输出层** |

### 快速选择指南
```
┌─ 隐藏层 ─────────────────────────────────────────┐
│  默认选择：ReLU                                   │
│  如果出现Dead ReLU：Leaky ReLU (α=0.01)          │
│  如果是Transformer：GELU                          │
│  如果追求最优：Swish                              │
└──────────────────────────────────────────────────┘

┌─ 输出层 ─────────────────────────────────────────┐
│  二分类：Sigmoid                                  │
│  多分类：Softmax                                  │
│  回归：无激活（Linear）                           │
│  多标签分类：Sigmoid（每个输出独立）              │
└──────────────────────────────────────────────────┘
```

### 面试问答

**Q1: 为什么ReLU比Sigmoid更适合深层网络？**
> **A**:
> 1. **计算效率**：ReLU只需max操作，Sigmoid需要exp运算
> 2. **梯度消失**：ReLU正区梯度恒为1，Sigmoid导数最大0.25，深层相乘趋近于0
> 3. **稀疏激活**：ReLU负值为0，约50%神经元不激活，减少计算和过拟合
> 4. **收敛速度**：实验表明ReLU网络收敛快6倍

**Q2: 什么是Dead ReLU问题？如何解决？**
> **A**:
> - **问题**：当神经元输入长期为负，输出恒为0，梯度也为0，权重无法更新，神经元"死亡"
> - **原因**：学习率过大导致权重更新过猛，或初始化不当
> - **解决方案**：
>   1. 使用Leaky ReLU（负区有小斜率α=0.01）
>   2. 使用PReLU（α可学习）
>   3. 降低学习率
>   4. 使用He初始化

**Q3: Sigmoid梯度消失的原因？**
> **A**:
> - Sigmoid导数 $\sigma'(z) = \sigma(z)(1-\sigma(z))$，最大值仅为0.25（当z=0时）
> - 当z很大或很小时，导数趋近于0（饱和区）
> - 深层网络反向传播时，梯度连乘：$0.25^{10} \approx 10^{-6}$，梯度几乎消失

**Q4: Tanh比Sigmoid好在哪？**
> **A**:
> 1. **零中心输出**：Tanh输出(-1,1)，均值接近0；Sigmoid输出(0,1)，均值0.5
> 2. **梯度更大**：Tanh导数最大为1，Sigmoid最大0.25
> 3. **实际效果**：零中心输出使下一层输入更平衡，收敛更快
> - **但仍有梯度消失问题**，所以深层网络还是用ReLU

**Q5: Softmax和Sigmoid的区别？什么时候用哪个？**
> **A**:
> | 特性 | Sigmoid | Softmax |
> |------|---------|---------|
> | 输出 | 单个值∈(0,1) | 向量，每个∈(0,1)，**和为1** |
> | 用途 | 二分类 | 多分类 |
> | 独立性 | 各输出独立 | 各输出互斥 |
> | 多标签 | ✓（每个标签独立判断） | ✗ |

**Q6: GELU为什么在Transformer中表现好？**
> **A**:
> 1. **平滑性**：GELU处处可微，没有ReLU在0点的尖角
> 2. **随机正则化效果**：GELU可以看作对ReLU的随机软化，有轻微正则化效果
> 3. **理论基础**：基于高斯分布的累积分布函数，与Dropout有数学联系
> 4. **实验验证**：BERT、GPT等模型实验表明GELU效果最佳

**Q7: 为什么不能不用激活函数？**
> **A**:
> - 没有激活函数：$y = W_2(W_1 x + b_1) + b_2 = W_{eq}x + b_{eq}$
> - 多层线性变换=一层线性变换，网络表达能力退化
> - 无法学习非线性关系，如XOR、图像边缘、语言语义等

**Q8: 激活函数的导数在反向传播中的作用？**
> **A**:
> - 反向传播公式：$\delta^{(l)} = (W^{(l+1)})^T\delta^{(l+1)} \odot \sigma'(z^{(l)})$
> - 导数$\sigma'$决定了梯度能否有效传递
> - 导数太小→梯度消失，导数太大→梯度爆炸
> - ReLU正区导数=1，梯度无衰减

**Q9: Swish和ReLU的核心区别是什么？**
> **A**:
> - Swish在负区间允许梯度流动（最小值≈-0.278），ReLU负区为0
> - Swish具有"自门控"特性：$\sigma(x)$作为门控制信号通过程度
> - Swish处处平滑可微，ReLU在0点有尖角
> - 当$\beta \to \infty$时，Swish趋近于ReLU

**Q10: Mish相比Swish有什么优势？**
> **A**:
> - Mish使用$\tanh(\text{softplus}(x))$作为门，比Sigmoid更平滑
> - 在视觉任务（如目标检测YOLOv4）中表现优于Swish
> - 负区间有小的负值输出，允许更好的梯度流动
> - 缺点：计算复杂度最高

### 万能近似定理（Universal Approximation Theorem）

**定理表述**（Cybenko 1989; Hornik 1991）：
> 对任意紧集上的连续函数$f$和误差$\epsilon > 0$，存在单隐藏层网络：
> $$F(x) = \sum_{i=1}^{N} v_i \sigma(w_i^T x + b_i)$$
> 满足：$|F(x) - f(x)| < \epsilon$

**关键限制**：
| 限制 | 说明 |
|------|------|
| 只证存在性 | 不说明如何找到这些权重 |
| 可能需要指数级神经元 | 宽度可能随问题复杂度指数增长 |
| 不保证泛化 | 只保证拟合，不保证测试集表现 |
| 实践中深度更有效 | 深网络比宽网络参数效率更高 |

### 易错点
| 错误 | 正确理解 |
|------|---------|
| Softmax用于隐藏层 | Softmax只用于多分类输出层 |
| ReLU导数在0点是1 | 0点导数未定义，通常定义为0 |
| 激活函数越复杂越好 | 不是，ReLU简单但效果好，计算效率也重要 |
| Sigmoid可以用于深层隐藏层 | 会导致严重梯度消失，不推荐 |

### 实践建议
- **默认**：隐藏层用ReLU，简单有效
- **出现Dead ReLU**：换Leaky ReLU或PReLU
- **Transformer模型**：用GELU
- **二分类**：输出层Sigmoid + BCE Loss
- **多分类**：输出层Softmax + CE Loss（实际实现中常合并为CrossEntropyLoss）

---

## 3. 多层感知机与前向传播

### 核心结论
- **万能逼近定理**：单隐藏层网络可以逼近任意连续函数（但可能需要指数级宽度）
- **深度 vs 宽度**：深网络比宽网络更高效（参数更少，表达能力相当）
- **前向传播**就是逐层计算：线性变换 → 激活函数 → 下一层

### 网络结构要点

| 组件 | 说明 | 关键参数 |
|------|------|---------|
| 输入层 | 接收原始特征 | 维度=特征数 |
| 隐藏层 | 学习中间表示 | 层数、每层神经元数 |
| 输出层 | 产生最终预测 | 维度=类别数（分类）或1（回归） |
| 权重W | 线性变换矩阵 | shape=(输入维度, 输出维度) |
| 偏置b | 平移项 | shape=(1, 输出维度) |

### 前向传播公式
$$z^{(l)} = W^{(l)}h^{(l-1)} + b^{(l)}$$
$$h^{(l)} = \sigma(z^{(l)})$$

### 面试问答

**Q1: 为什么需要多层网络？**
> **A**:
> 1. **线性不可分问题**：单层只能画直线，多层能画任意曲线
> 2. **特征层次学习**：浅层学习简单特征（边缘），深层学习复杂特征（人脸）
> 3. **参数效率**：深网络比宽网络用更少参数达到相同表达能力
> 4. **组合爆炸**：深度网络可以指数级组合特征

**Q2: 隐藏层数量和大小怎么选？**
> **A**:
> | 问题复杂度 | 隐藏层数 | 每层大小 |
> |-----------|---------|---------|
> | 简单（线性可分） | 0-1层 | 小 |
> | 中等（XOR等） | 1-2层 | 中等 |
> | 复杂（图像、NLP） | 3-100+层 | 根据任务调整 |
>
> **经验法则**：
> - 先试1-2个隐藏层，不够再加
> - 隐藏层大小：输入和输出维度的几何平均数附近
> - 用交叉验证确定最优

**Q3: 权重初始化为0会怎样？**
> **A**:
> - **对称性问题**：所有神经元权重相同→输出相同→梯度相同→更新后仍相同
> - 网络退化为"一个神经元"，无法学习多样特征
> - **解决**：必须随机初始化打破对称性

**Q4: 什么是万能逼近定理？**
> **A**:
> - **定理**：单隐藏层前馈网络，只要隐藏神经元足够多，可以以任意精度逼近任意连续函数
> - **局限**：
>   1. 可能需要指数级神经元
>   2. 没说明如何找到这些权重
>   3. 实践中深网络更高效

**Q5: 深度和宽度哪个更重要？**
> **A**:
> - **深度更重要**：深网络可以用更少参数表达复杂函数
> - **原因**：深度带来**组合效应**，每层可以复用前一层的特征
> - **例子**：表达n个变量的所有布尔函数，深网络需要O(n)参数，宽网络需要O(2^n)
> - **但也不能太深**：太深会有梯度消失/爆炸，需要ResNet等技巧

**Q6: 前向传播需要保存什么？**
> **A**: 需要保存每层的**激活值**$h^{(l)}$和**线性输出**$z^{(l)}$，因为：
> 1. 反向传播计算梯度需要用到这些值
> 2. 激活函数导数$\sigma'(z)$需要z
> 3. 权重梯度$\frac{\partial L}{\partial W} = \delta \cdot h^T$需要h

**Q7: Batch维度是怎么处理的？**
> **A**:
> - 输入X的shape：(batch_size, input_dim)
> - 权重W的shape：(input_dim, output_dim)
> - 计算：Z = X @ W + b，输出shape：(batch_size, output_dim)
> - 批处理可以**并行计算**，利用GPU加速

**Q8: 神经网络的参数量怎么计算？**
> **A**:
> - 全连接层：$n_{in} \times n_{out} + n_{out}$（权重+偏置）
> - 例：[784, 256, 128, 10]
>   - 第1层：784×256 + 256 = 200,960
>   - 第2层：256×128 + 128 = 32,896
>   - 第3层：128×10 + 10 = 1,290
>   - 总计：235,146参数

### 易错点
| 错误 | 正确理解 |
|------|---------|
| 更多层一定更好 | 不一定，太深会过拟合或梯度问题 |
| 隐藏层越宽越好 | 会过拟合且计算慢 |
| 前向传播不需要保存中间值 | 需要保存，反向传播要用 |
| 偏置也需要随机初始化 | 偏置通常初始化为0即可 |

### 实践建议
- **起步**：1-2个隐藏层，每层64-256神经元
- **调优**：逐步增加深度/宽度，观察验证集性能
- **过拟合**：减少神经元数或加正则化
- **欠拟合**：增加网络容量

---

## 4. 反向传播算法

### 核心结论
- **反向传播**= 链式法则 + 动态规划（缓存中间结果）
- **时间复杂度**：O(参数数量)，比数值梯度O(参数²)高效得多
- **梯度流动**：从输出层向输入层逐层传递

### 关键公式

| 步骤 | 公式 | 说明 |
|------|------|------|
| 输出层误差 | $\delta^{(L)} = \hat{y} - y$ | 交叉熵+Softmax的简化结果 |
| 隐藏层误差 | $\delta^{(l)} = (W^{(l+1)})^T\delta^{(l+1)} \odot \sigma'(z^{(l)})$ | 链式法则反向传播 |
| 权重梯度 | $\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)}(h^{(l-1)})^T$ | 用于更新权重 |
| 偏置梯度 | $\frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}$ | 用于更新偏置 |
| 参数更新 | $\theta \leftarrow \theta - \eta \nabla L$ | 梯度下降 |

### 反向传播流程
```
1. 前向传播：计算所有层的z和h，保存起来
2. 计算输出层误差：δ^(L) = ŷ - y
3. 反向传播误差：从L-1到1，计算每层的δ
4. 计算梯度：dW = δ · h^T，db = δ
5. 更新参数：W = W - lr * dW
```

### 三种求导方式对比

| 方法 | 原理 | 精度 | 速度 | 适用场景 |
|------|------|------|------|---------|
| **符号微分** | 直接对表达式求导 | 精确 | 中等 | 小规模表达式 |
| **数值微分** | 有限差分 $\frac{f(x+\epsilon)-f(x-\epsilon)}{2\epsilon}$ | ~1e-8 | 慢 | 梯度验证 |
| **自动微分** | 链式法则+计算图 | 机器精度 | 快 | **深度学习** |

### 前向模式 vs 反向模式（JVP vs VJP）

| 特性 | 前向模式（JVP） | 反向模式（VJP） |
|------|----------------|----------------|
| **全称** | Jacobian-Vector Product | Vector-Jacobian Product |
| **公式** | $\text{JVP}(\mathbf{v}) = J \cdot \mathbf{v}$ | $\text{VJP}(\mathbf{v}) = J^T \cdot \mathbf{v}$ |
| **传播方向** | 沿计算图正向 | 沿计算图反向 |
| **一次计算** | 一个输入对所有输出的影响 | 所有输入对一个输出的梯度 |
| **复杂度** | O(n)，n=输入维度 | O(m)，m=输出维度 |
| **适用场景** | 输入少、输出多 | **输出少、输入多（神经网络）** |

**深度学习用反向模式的原因**：
- 神经网络有百万级参数（输入多），但只有1个标量损失（输出少）
- 前向模式需要N次传播，反向模式只需1次

### 面试问答

**Q1: 反向传播的本质是什么？**
> **A**:
> - **链式法则的高效实现**：将复合函数的导数分解为各层导数的乘积
> - **动态规划思想**：缓存中间结果，避免重复计算
> - **从后向前计算**：利用已计算的后层梯度来计算前层

**Q2: 为什么反向传播比数值梯度快？**
> **A**:
> | 方法 | 时间复杂度 | 说明 |
> |------|-----------|------|
> | 反向传播 | O(n) | 一次前向+一次反向，得到所有n个参数的梯度 |
> | 数值梯度 | O(n²) | 每个参数需要两次前向传播 |
>
> - 对于百万参数的网络，反向传播快100万倍

**Q3: 梯度消失和梯度爆炸是什么？**
> **A**:
> - **梯度消失**：梯度逐层相乘后趋近于0
>   - 原因：Sigmoid导数<1、权重初始化过小
>   - 后果：浅层无法学习，只有深层更新
>   - 解决：ReLU、BN、残差连接、LSTM
>
> - **梯度爆炸**：梯度逐层相乘后趋向无穷
>   - 原因：权重初始化过大
>   - 后果：参数更新过大，loss变NaN
>   - 解决：梯度裁剪、权重正则化、好的初始化

**Q4: 为什么交叉熵+Softmax的梯度是ŷ-y？**
> **A**:
> - 这是数学上的巧合/设计
> - 交叉熵损失：$L = -\sum y_k \log \hat{y}_k$
> - Softmax：$\hat{y}_k = \frac{e^{z_k}}{\sum e^{z_j}}$
> - 对$z_i$求导时，复杂的求导相互抵消，最终$\frac{\partial L}{\partial z_i} = \hat{y}_i - y_i$
> - **好处**：计算简单，梯度在正确类别和错误类别方向明确

**Q5: 梯度检验怎么做？什么时候需要做？**
> **A**:
> - **方法**：比较解析梯度和数值梯度
>   - 数值梯度：$\frac{f(\theta+\epsilon) - f(\theta-\epsilon)}{2\epsilon}$，$\epsilon \approx 10^{-7}$
>   - 相对误差：$\frac{\|g_{analytic} - g_{numeric}\|}{\|g_{analytic}\| + \|g_{numeric}\|}$
>
> - **判断标准**：
>   - < 1e-7：✓ 完美
>   - < 1e-5：✓ 可接受
>   - < 1e-3：⚠ 有问题
>   - > 1e-3：✗ 肯定有bug
>
> - **何时使用**：实现新层、新损失函数时，验证正确性

**Q6: 批量梯度和单样本梯度的关系？**
> **A**:
> - 批量梯度 = 批内所有样本梯度的**平均值**
> - $\nabla L_{batch} = \frac{1}{m}\sum_{i=1}^{m} \nabla L_i$
> - **优点**：更稳定、可以利用GPU并行
> - **小批量**：折中方案，既有随机性（逃离局部最优）又够稳定

**Q7: 反向传播中的维度匹配问题？**
> **A**:
> - **规则**：梯度的shape必须和参数的shape一致
> - W的shape：(n_in, n_out)
> - dW的shape：也是(n_in, n_out)
> - **计算**：dW = h^T @ δ（注意转置位置）
> - **调试技巧**：打印每个变量的shape，确保维度匹配

**Q8: 计算图和反向传播的关系？**
> **A**:
> - **计算图**：将计算过程表示为有向无环图
> - **节点**：操作（加法、乘法、激活函数等）
> - **边**：数据流动
> - **反向传播**：沿着图的反方向，用链式法则计算梯度
> - **自动微分**：PyTorch/TensorFlow自动构建计算图和执行反向传播

**Q9: 为什么梯度累积要用+=而不是=？**
> **A**:
> - 一个节点可能被多条路径使用（菱形依赖/分支结构）
> - 使用`+=`才能正确累积来自所有路径的梯度
> - 使用`=`会覆盖之前的梯度，导致梯度计算错误

**Q10: 自动微分和数值微分的关键区别？**
> **A**:
> | 特性 | 自动微分 | 数值微分 |
> |------|---------|---------|
> | 精度 | 机器精度(~1e-16) | 受截断误差限制(~1e-8) |
> | 速度 | O(n) | O(n²) |
> | 原理 | 链式法则精确计算 | 有限差分近似 |
> | 用途 | 网络训练 | 仅用于梯度检验 |

### 易错点
| 错误 | 正确理解 |
|------|---------|
| 反向传播只能用于神经网络 | 可用于任何可微分的计算图 |
| 梯度消失只发生在深层网络 | 2-3层Sigmoid网络也可能发生 |
| 梯度裁剪能解决梯度消失 | 不能，只能解决梯度爆炸 |
| 数值梯度可以替代反向传播 | 太慢，只用于调试验证 |

### 实践建议
- **调试反向传播**：用梯度检验验证
- **监控梯度**：打印梯度范数，观察是否消失/爆炸
- **梯度爆炸**：先尝试降低学习率，再考虑梯度裁剪
- **梯度消失**：换ReLU、加BN、用残差连接

---

## 5. 损失函数与优化器

### 核心结论
- **分类任务**：交叉熵损失 + Softmax/Sigmoid
- **回归任务**：MSE（平方误差）或 MAE（绝对误差）
- **优化器首选**：Adam（自适应学习率，默认参数效果好）
- **学习率**：最重要的超参数，过大震荡，过小慢

### 损失函数速查

| 损失函数 | 公式 | 适用任务 | 特点 |
|---------|------|---------|------|
| **交叉熵** | $-\sum y_k \log \hat{y}_k$ | 分类 | 梯度清晰，收敛快 |
| **二值交叉熵** | $-[y\log\hat{y} + (1-y)\log(1-\hat{y})]$ | 二分类 | Sigmoid配对 |
| **Focal Loss** | $-\alpha(1-p_t)^{\gamma}\log(p_t)$ | 类别不平衡 | 抑制易样本，聚焦难样本 |
| **MSE** | $\frac{1}{n}\sum(y-\hat{y})^2$ | 回归 | 对异常值敏感 |
| **MAE** | $\frac{1}{n}\sum\|y-\hat{y}\|$ | 回归 | 对异常值鲁棒 |
| **Huber** | MSE(小误差) + MAE(大误差) | 回归 | 折中方案 |
| **Hinge** | $\max(0, 1-y\cdot\hat{y})$ | SVM | 间隔最大化 |
| **KL散度** | $\sum P \log \frac{P}{Q}$ | 分布差异 | 非对称，可无穷 |
| **JS散度** | $\frac{1}{2}D_{KL}(P\|M) + \frac{1}{2}D_{KL}(Q\|M)$ | 分布差异 | 对称，有界[0,log2] |

### KL散度与JS散度

| 特性 | KL散度 | JS散度 |
|------|-------|-------|
| **对称性** | ❌ 非对称 | ✓ 对称 |
| **范围** | [0, +∞) | [0, log2] |
| **零点** | Q=0时无穷 | 始终有界 |
| **典型应用** | VAE、知识蒸馏 | GAN |

**Focal Loss详解**：
- **公式**：$FL(p_t) = -\alpha_t(1-p_t)^{\gamma}\log(p_t)$
- **调制因子**：$(1-p_t)^{\gamma}$，$p_t$接近1时趋于0，抑制易样本
- **推荐参数**：$\gamma=2$，$\alpha=0.25$（正样本权重）
- **应用场景**：目标检测（如RetinaNet）、极度不平衡分类

### 优化器速查

| 优化器 | 核心思想 | 优点 | 缺点 | 推荐场景 |
|-------|---------|------|------|---------|
| **SGD** | 随机梯度下降 | 简单、泛化好 | 收敛慢、需调lr | 追求最佳泛化 |
| **Momentum** | 累积历史梯度 | 加速收敛 | 多一个超参数 | SGD改进版 |
| **Nesterov** | 在预测位置计算梯度 | 更快收敛、更稳定 | 略复杂 | Momentum升级版 |
| **RMSProp** | 自适应学习率 | 处理非平稳目标 | - | RNN训练 |
| **Adam** | Momentum + RMSProp | 自适应、收敛快 | 可能泛化差 | 快速原型 |
| **AdamW** | Adam + 解耦权重衰减 | 泛化更好 | - | **目前最佳** |
| **Lookahead** | 快慢权重机制 | 稳定、减少方差 | 略增复杂度 | 大模型训练 |

### Nesterov动量详解

**标准Momentum vs Nesterov**：
| 特性 | 标准Momentum | Nesterov |
|------|-------------|----------|
| 梯度计算位置 | 当前位置$\theta_{t-1}$ | 预测位置$\theta_{t-1} - \alpha\beta v_{t-1}$ |
| 问题 | 容易"冲过头" | 提前纠正方向 |
| 效果 | 加速收敛 | 更快收敛、更稳定 |

**Nesterov公式**：
$$v_t = \beta v_{t-1} + \nabla L(\theta_{t-1} - \alpha\beta v_{t-1})$$
$$\theta_t = \theta_{t-1} - \alpha v_t$$

### AdamW详解

**Adam vs AdamW的关键区别**：
- **Adam + L2正则**：正则化梯度被自适应学习率缩放
- **AdamW**：权重衰减直接作用于参数，不受自适应缩放影响

**AdamW公式**：
$$\theta_t = \theta_{t-1} - \alpha\frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon} - \alpha\lambda\theta_{t-1}$$

**为什么AdamW更好**：
- 权重衰减与学习率解耦
- 泛化性能通常优于Adam + L2
- 目前大模型训练的标准选择

### Lookahead优化器

**核心机制**：
- 维护两组权重：快权重$\theta$（内部优化器更新）和慢权重$\phi$
- 每k步同步一次：$\phi = \phi + \alpha(\theta - \phi)$，然后$\theta = \phi$

**优点**：
- 减少优化过程的方差
- 不需要warmup
- 可包装任意优化器（如Lookahead(Adam)）

### Adam公式（必背）
$$m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \quad \text{(一阶矩估计)}$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \quad \text{(二阶矩估计)}$$
$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \quad \text{(偏置修正)}$$
$$\theta \leftarrow \theta - \eta\frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon}$$

**默认参数**：$\beta_1=0.9, \beta_2=0.999, \epsilon=10^{-8}, \eta=0.001$

### 面试问答

**Q1: 为什么分类用交叉熵而不是MSE？**
> **A**:
> 1. **梯度信号**：MSE在预测接近正确时梯度趋近于0，学习缓慢；交叉熵梯度始终清晰
> 2. **概率解释**：交叉熵衡量两个概率分布的差异，更适合分类
> 3. **数学性质**：交叉熵+Softmax的梯度简化为ŷ-y，计算简便
> 4. **实验验证**：交叉熵收敛更快，最终效果更好

**Q2: Adam为什么这么流行？**
> **A**:
> 1. **自适应学习率**：每个参数有自己的学习率，稀疏特征也能有效更新
> 2. **动量加速**：累积历史梯度，加速收敛
> 3. **对超参数不敏感**：默认参数(lr=0.001)适用于大多数情况
> 4. **综合优势**：结合了Momentum和RMSProp的优点

**Q3: 学习率怎么选？**
> **A**:
> | 学习率 | 现象 | 处理 |
> |-------|------|------|
> | 过大 | loss震荡/发散/NaN | 降低10倍 |
> | 过小 | loss下降很慢 | 增大2-10倍 |
> | 合适 | loss稳定下降 | 保持 |
>
> **实践建议**：
> - Adam默认0.001，大多数情况够用
> - SGD一般0.01-0.1
> - 使用学习率调度器（warmup + decay）

**Q4: Adam和SGD该选哪个？**
> **A**:
> | 场景 | 推荐 | 原因 |
> |------|------|------|
> | 快速原型 | Adam | 收敛快，无需调参 |
> | 追求最佳性能 | SGD+Momentum | 泛化可能更好 |
> | 大规模预训练 | AdamW | 权重衰减更正确 |
> | 不确定 | 先Adam，再试SGD | |

**Q5: 什么是学习率预热（Warmup）？**
> **A**:
> - **做法**：训练初期用小学习率，逐渐增大到目标值
> - **原因**：初期参数随机，大学习率可能导致不稳定更新
> - **实现**：前N步线性增长，如从0.0001增到0.001
> - **效果**：训练更稳定，最终效果更好

**Q6: Batch Size对训练有什么影响？**
> **A**:
> | Batch Size | 优点 | 缺点 |
> |------------|------|------|
> | 小（16-32） | 正则化效果、逃离局部最优 | 噪声大、GPU利用率低 |
> | 大（256+） | 稳定、GPU利用率高 | 可能陷入尖锐最小值、泛化差 |
>
> **实践**：通常32-256，根据GPU内存和任务调整

**Q7: 为什么Adam需要偏置修正？**
> **A**:
> - $m_0=0, v_0=0$，初期估计偏向于0
> - 第一步：$m_1 = 0.1 \times g_1$，实际梯度被低估10倍
> - 偏置修正：$\hat{m}_1 = m_1 / (1-0.9^1) = m_1 / 0.1$，恢复正确估计
> - 随着t增大，$(1-\beta^t) \to 1$，修正逐渐消失

**Q8: 什么情况下训练不收敛？**
> **A**:
> - **学习率过大**：loss震荡或变NaN
> - **梯度消失**：loss几乎不变
> - **数据问题**：标签错误、数据未标准化
> - **网络问题**：结构不合理、初始化不当
> - **调试步骤**：检查loss曲线→检查梯度→检查数据→简化网络

**Q9: KL散度和JS散度的区别？什么时候用哪个？**
> **A**:
> | 特性 | KL散度 | JS散度 |
> |------|-------|-------|
> | 对称性 | 非对称 | 对称 |
> | 范围 | [0,+∞) | [0,log2] |
> | 应用 | VAE、知识蒸馏 | GAN |
>
> - KL散度有方向性，$D_{KL}(P\|Q) \neq D_{KL}(Q\|P)$
> - JS散度是KL的对称化：$JS = \frac{1}{2}KL(P\|M) + \frac{1}{2}KL(Q\|M)$，其中$M=\frac{P+Q}{2}$

**Q10: Focal Loss如何解决类别不平衡？**
> **A**:
> - 通过调制因子$(1-p_t)^{\gamma}$动态调整样本权重
> - 易分类样本（$p_t$接近1）：调制因子趋于0，损失被抑制
> - 难分类样本（$p_t$接近0）：调制因子趋于1，保持原损失
> - 相比简单的类别权重，Focal Loss根据**预测置信度**动态调整

**Q11: AdamW和Adam+L2正则化为什么不等价？**
> **A**:
> - 在SGD中两者数学等价
> - 在Adam中：L2正则化的梯度会被自适应学习率$\frac{1}{\sqrt{v_t}}$缩放
> - AdamW的权重衰减直接作用于参数，不经过自适应缩放
> - 实践中AdamW泛化性能更好

### 易错点
| 错误 | 正确理解 |
|------|---------|
| 交叉熵可以直接用于回归 | 不行，交叉熵用于概率分布，回归用MSE/MAE |
| Adam永远比SGD好 | 不一定，SGD有时泛化更好 |
| 学习率越小越安全 | 太小会陷入局部最优或收敛极慢 |
| 大Batch一定更好 | 可能泛化差，需要调整学习率 |
| KL散度是对称的 | 不对，$D_{KL}(P\|Q) \neq D_{KL}(Q\|P)$ |
| Adam的权重衰减=L2正则 | 不等价，Adam中L2被自适应缩放，AdamW解耦 |

### 实践建议
- **起步**：Adam + lr=0.001
- **微调**：如果过拟合，换SGD+Momentum
- **学习率调度**：Cosine Annealing 或 Step Decay
- **监控**：画loss曲线，观察训练和验证的gap

---

## 6. 正则化技术

### 核心结论
- **过拟合**：训练好、测试差，模型记住了噪声
- **正则化**：限制模型复杂度，提高泛化能力
- **常用方法**：L2正则化、Dropout、Early Stopping、数据增强

### 正则化方法对比

| 方法 | 原理 | 优点 | 缺点 | 超参数 |
|------|------|------|------|--------|
| **L2正则化** | 惩罚大权重 | 简单有效 | 需调λ | λ=0.0001~0.01 |
| **L1正则化** | 惩罚权重绝对值 | 产生稀疏解 | 不可微点 | λ |
| **Dropout** | 随机丢弃神经元 | 集成效应 | 训练变慢 | p=0.2~0.5 |
| **Early Stopping** | 验证集性能不提升时停止 | 自动化 | 需验证集 | patience |
| **数据增强** | 增加训练数据多样性 | 最有效 | 需设计增强策略 | - |
| **Batch Norm** | 归一化层输入 | 兼有正则化效果 | 小batch不稳定 | - |

### L1 vs L2 详细对比

| 特性 | L1 正则化 | L2 正则化 |
|------|----------|----------|
| **惩罚项** | $\lambda\sum\|w_i\|$ | $\frac{\lambda}{2}\sum w_i^2$ |
| **梯度** | $\lambda \cdot \text{sign}(w)$（常数） | $\lambda \cdot w$（与w成正比） |
| **权重变化** | 均匀向0收缩，最终变0 | 按比例收缩，不会精确为0 |
| **解的特点** | **稀疏**（很多0） | **平滑**（没有0） |
| **适用场景** | 特征选择、稀疏模型 | 防止过拟合（更常用） |
| **几何解释** | 菱形约束区域 | 圆形约束区域 |

### 偏差-方差分解

**核心公式**：
$$\text{MSE} = \text{Bias}^2 + \text{Variance} + \sigma^2$$

| 组成部分 | 含义 | 影响因素 |
|---------|------|---------|
| **Bias²** | 预测期望与真实值的差距（系统性误差） | 模型复杂度↑ → Bias↓ |
| **Variance** | 预测在不同训练集上的波动（稳定性） | 数据量↑/正则化↑ → Variance↓ |
| **σ²** | 数据本身的噪声（不可约误差） | 无法消除 |

**打靶类比**：
- **高偏差**：所有射击点偏离靶心（不准确但稳定）→ 欠拟合
- **高方差**：射击点分散在靶心周围（平均准确但不稳定）→ 过拟合
- **理想**：所有射击点集中在靶心（准确且稳定）

**诊断与解决**：
| 问题 | 症状 | 解决方案 |
|------|------|---------|
| 高偏差（欠拟合） | 训练误差高，测试误差也高，两者接近 | 增加模型复杂度、添加特征、减少正则化 |
| 高方差（过拟合） | 训练误差低，测试误差高，差距大 | 增加数据、增加正则化、早停、集成学习 |

### Dropout作为集成学习

**集成效应解释**：
- 有n个可Dropout单元的网络包含$2^n$个子网络
- 训练时每次前向传播使用不同的随机掩码 = 采样一个子网络
- 推理时使用全部单元 = 所有子网络的加权平均

**Monte Carlo Dropout**：
- 推理时保持Dropout开启，多次采样并平均
- 可以估计**预测不确定性**
- 用于模型置信度估计和贝叶斯深度学习

### 高级数据增强

| 方法 | 公式/描述 | 特点 | 应用场景 |
|------|---------|------|---------|
| **Mixup** | $\tilde{x} = \lambda x_i + (1-\lambda)x_j$，$\lambda \sim \text{Beta}(\alpha, \alpha)$ | 全局线性混合，软标签 | 通用分类 |
| **CutMix** | 用$x_j$的区域替换$x_i$的对应区域 | 局部替换，保留像素 | 分类、检测 |
| **Cutout** | 随机擦除图像的正方形区域 | 简单有效 | 图像分类 |
| **RandAugment** | 随机选N个操作，强度M | 超参数少，自适应 | 小数据集 |

**Mixup vs CutMix**：
| 特性 | Mixup | CutMix |
|------|-------|--------|
| 混合方式 | 全局像素加权平均 | 局部区域替换 |
| 像素保留 | 所有像素被混合 | 部分像素完整保留 |
| 标签处理 | $\tilde{y} = \lambda y_i + (1-\lambda)y_j$ | 标签按面积比混合 |
| 优势 | 平滑决策边界 | 更好的定位能力 |

**RandAugment超参数**：
- N：每张图应用的增强操作数（通常2-3）
- M：增强强度（0-30，通常5-15）
- 优势：相比AutoAugment，避免昂贵的搜索过程

### 面试问答

**Q1: 什么是过拟合？如何诊断？**
> **A**:
> - **定义**：模型在训练集表现好，测试集表现差
> - **本质**：模型记住了训练数据的噪声，而非学习真正的规律
> - **诊断**：
>   - 画学习曲线：训练loss↓，验证loss↑ → 过拟合
>   - 比较准确率：训练99%，测试70% → 过拟合
>   - gap越大，过拟合越严重

**Q2: 为什么L1产生稀疏解而L2不会？**
> **A**:
> - **L1梯度**：$\text{sign}(w)$，是常数±1，不管w多小，都以恒定速度向0收缩
> - **L2梯度**：$w$，与权重成正比，w越小梯度越小，收缩越慢，永远不会精确为0
> - **几何解释**：L1的菱形约束更容易在坐标轴上（某些维度=0）取得最优解

**Q3: Dropout为什么有效？**
> **A**:
> 1. **集成效应**：每次训练用不同子网络，推理时相当于集成多个模型
> 2. **防止共适应**：神经元不能依赖特定的其他神经元，必须独立有效
> 3. **强制冗余表示**：每个特征由多个神经元学习
> 4. **类似噪声注入**：增加训练难度，提高鲁棒性

**Q4: Dropout训练和推理时有什么区别？**
> **A**:
> | 阶段 | Inverted Dropout（推荐） | Standard Dropout |
> |------|-------------------------|------------------|
> | 训练 | 丢弃 + 除以(1-p)缩放 | 仅丢弃 |
> | 推理 | 直接使用 | 乘以(1-p)缩放 |
>
> - Inverted Dropout更方便：推理时无需修改

**Q5: 正则化强度λ怎么选？**
> **A**:
> - **太大**：欠拟合，模型过于简单
> - **太小**：正则化效果不明显
> - **选择方法**：
>   1. 从0.001开始，尝试0.0001, 0.001, 0.01, 0.1
>   2. 用交叉验证选最佳
>   3. 画验证曲线（λ vs 验证性能）

**Q6: Early Stopping怎么实现？**
> **A**:
> ```python
> best_val_loss = float('inf')
> patience_counter = 0
>
> for epoch in range(max_epochs):
>     train_loss = train_one_epoch()
>     val_loss = validate()
>
>     if val_loss < best_val_loss:
>         best_val_loss = val_loss
>         save_model()  # 保存最佳模型
>         patience_counter = 0
>     else:
>         patience_counter += 1
>         if patience_counter >= patience:
>             break  # 早停
>
> load_best_model()  # 加载最佳模型
> ```

**Q7: 数据增强有哪些常见方法？**
> **A**:
> | 领域 | 增强方法 |
> |------|---------|
> | 图像 | 翻转、旋转、裁剪、颜色变换、Mixup、CutMix |
> | 文本 | 同义词替换、回译、随机插入/删除 |
> | 音频 | 时间拉伸、音调变化、加噪声 |
> | 通用 | Dropout、特征噪声 |

**Q8: 多种正则化方法可以同时使用吗？**
> **A**:
> - **可以**，通常组合使用效果更好
> - **常见组合**：
>   - L2正则化 + Dropout + Early Stopping
>   - BN + Dropout（注意：BN已有正则化效果，Dropout率可以降低）
>   - 数据增强 + L2 + Dropout
> - **注意**：正则化太强会欠拟合，需要平衡

**Q9: 偏差和方差的trade-off如何理解？**
> **A**:
> - 模型复杂度与总误差呈U形关系
> - 简单模型：Bias高，Variance低 → 总误差由Bias主导
> - 复杂模型：Bias低，Variance高 → 总误差由Variance主导
> - **最优点**：Bias和Variance达到平衡
> - **增加数据**：主要降低Variance，对Bias帮助有限

**Q10: 为什么增加训练数据主要降低方差而非偏差？**
> **A**:
> - **偏差**由模型复杂度决定，与数据量关系不大
> - **方差**反映模型对训练数据的敏感度
> - 数据量增加 → 模型不易被单个样本影响 → Variance下降
> - 如果是高偏差问题，增加数据作用有限，需要增加模型复杂度

**Q11: Mixup为什么能提高泛化？**
> **A**:
> - 创造训练集中不存在的"虚拟样本"
> - 强迫模型在类别之间学习**线性插值**
> - 产生更平滑的决策边界，减少过拟合
> - 软标签提供更丰富的监督信号

### 易错点
| 错误 | 正确理解 |
|------|---------|
| 只有L2叫正则化 | 任何限制模型复杂度的方法都是正则化 |
| Dropout训练推理一样 | 推理时必须关闭Dropout |
| 正则化越强越好 | 太强会欠拟合 |
| 偏置项也要正则化 | 通常不正则化偏置，只正则化权重 |

### 实践建议
- **第一选择**：数据增强（最有效且不增加模型复杂度）
- **标配**：L2正则化（λ=0.0001~0.001）+ Early Stopping
- **全连接层**：加Dropout（p=0.2~0.5）
- **卷积层**：Dropout率低一些（p=0.1~0.3）或不用

---

## 7. Batch Normalization与Dropout

### 核心结论
- **BN作用**：归一化层输入，稳定训练，允许更大学习率，有轻微正则化效果
- **Dropout作用**：随机丢弃神经元，防止过拟合
- **推荐顺序**：Conv/Dense → BN → ReLU → Dropout

### Batch Normalization 要点

| 方面 | 说明 |
|------|------|
| **训练时** | 用当前batch的均值/方差归一化 |
| **推理时** | 用累积的running_mean/running_var |
| **可学习参数** | γ（缩放）和 β（平移） |
| **放置位置** | 激活函数之前（更常见）或之后 |
| **主要作用** | 加速收敛、稳定训练、正则化 |

### BN公式
$$\hat{x} = \frac{x - \mu_{batch}}{\sqrt{\sigma^2_{batch} + \epsilon}}$$
$$y = \gamma \hat{x} + \beta$$

### 归一化变体对比

| 方法 | 归一化维度 | 依赖batch | 适用场景 | 特点 |
|------|-----------|----------|---------|------|
| **BatchNorm** | (N)跨样本 | ✓ | 大batch CNN | 最常用，小batch不稳定 |
| **LayerNorm** | (C,H,W)跨特征 | ✗ | Transformer/RNN | 序列友好，不依赖batch |
| **GroupNorm** | (C/G,H,W)分组 | ✗ | 小batch CNN | 分组归一化，G通常=32 |
| **InstanceNorm** | (H,W)单通道 | ✗ | 风格迁移 | 每个通道独立归一化 |

**GroupNorm公式**：
- 将C个通道分为G组，每组C/G个通道
- 对每个样本每组独立计算均值和方差
- 特例：G=1时=LayerNorm，G=C时=InstanceNorm

**归一化选择决策树**：
```
场景判断
├─ 大batch CNN？ → BatchNorm
├─ 小batch CNN或batch=1推理？ → GroupNorm (G=32)
├─ Transformer/RNN/序列模型？ → LayerNorm
└─ 风格迁移？ → InstanceNorm
```

### Dropout 要点

| 方面 | Inverted Dropout（推荐） |
|------|-------------------------|
| 训练 | $y = \frac{x \cdot mask}{1-p}$ |
| 推理 | $y = x$（无需修改） |
| p的含义 | 丢弃概率（不是保留概率） |
| 位置 | 全连接层之后，卷积层较少用 |

### 面试问答

**Q1: BN为什么能加速训练？**
> **A**:
> 1. **减少内部协变量偏移**：每层输入分布稳定，不需要频繁适应
> 2. **允许更大学习率**：归一化后梯度更稳定，不易爆炸
> 3. **减少对初始化的敏感性**：即使初始化不好，BN也能纠正
> 4. **平滑损失曲面**：使优化更容易

**Q2: BN训练和推理为什么不同？**
> **A**:
> - **训练**：每个batch计算自己的均值/方差，同时更新running统计
> - **推理**：
>   - 可能只有1个样本，无法计算batch统计
>   - 需要确定性输出，不能依赖batch内其他样本
>   - 使用训练时累积的running_mean/running_var
> - **切换**：PyTorch用`model.train()`和`model.eval()`

**Q3: BN的γ和β为什么需要？**
> **A**:
> - 归一化后，输出固定为均值0、方差1
> - 但这可能不是最优分布，比如ReLU需要正值输入
> - γ（缩放）和β（平移）让网络**自己学习最优分布**
> - 极端情况：γ=σ, β=μ可以恢复原始分布

**Q4: BN在小batch时为什么不稳定？**
> **A**:
> - 小batch（如1-4）时，均值/方差估计不准确
> - 随机性太大，训练不稳定
> - **解决方案**：
>   - 使用Layer Normalization（对单个样本归一化）
>   - 使用Group Normalization
>   - 增大batch size

**Q5: Layer Norm vs Batch Norm？**
> **A**:
> | 特性 | Batch Norm | Layer Norm |
> |------|-----------|------------|
> | 归一化维度 | 跨样本（N维） | 跨特征（C,H,W维） |
> | 依赖batch | 是 | 否 |
> | 小batch | 不稳定 | 稳定 |
> | 推理时 | 需要running统计 | 无需 |
> | 主要用途 | CNN | **Transformer、RNN** |

**Q6: Dropout率怎么选？**
> **A**:
> | 场景 | 推荐Dropout率 |
> |------|--------------|
> | 全连接层 | 0.5（经典值） |
> | 卷积层 | 0.2-0.3（或不用） |
> | 输出层 | 不使用 |
> | 小数据集 | 0.5-0.7（更强正则化） |
> | 大数据集 | 0.1-0.3（或不用） |
> | 配合BN | 降低到0.2-0.3 |

**Q7: BN和Dropout可以一起用吗？**
> **A**:
> - **可以**，但需要注意：
>   - BN本身有正则化效果，Dropout率可以降低
>   - 顺序：BN → Activation → Dropout
>   - 有研究表明同时用可能有冲突，效果不一定更好
> - **现代实践**：很多模型只用BN不用Dropout

**Q8: 推理时忘记关Dropout会怎样？**
> **A**:
> - 每次推理结果不同（随机性）
> - 输出不稳定，预测不可靠
> - 性能下降（因为部分信息被丢弃）
> - **解决**：确保推理时调用`model.eval()`

**Q9: GroupNorm相比BatchNorm的优势？**
> **A**:
> - 不依赖batch size，小batch时仍稳定
> - 推理时无需维护running statistics
> - 在目标检测、分割等小batch场景表现更好
> - 缺点：大batch时效果可能略逊于BatchNorm

**Q10: 为什么Transformer用LayerNorm而不是BatchNorm？**
> **A**:
> - 序列长度可变，batch统计不稳定
> - 自注意力机制需要每个位置独立处理
> - LayerNorm对单个样本的所有特征归一化，不依赖batch
> - 在NLP任务中表现更好

### BN vs Dropout 选择

| 情况 | 建议 |
|------|------|
| CNN图像分类 | BN为主，Dropout可选 |
| 全连接网络 | 两者都用 |
| Transformer | LayerNorm，通常不用Dropout |
| 小数据集 | 强调Dropout |
| 大数据集 | 强调BN |

### 易错点
| 错误 | 正确理解 |
|------|---------|
| 推理时用batch统计 | 推理必须用running统计 |
| Dropout率0.5表示保留50% | p=0.5表示**丢弃**50%，保留50% |
| BN放在激活函数之后更好 | 放之前更常见，两者都可以 |
| BN和Dropout作用相同 | BN主要加速训练，Dropout主要防过拟合 |

### 实践建议
- **层顺序**：Linear → BN → ReLU → Dropout → 下一层
- **CNN**：每个卷积块加BN
- **全连接**：BN + Dropout(0.3-0.5)
- **推理**：务必调用`model.eval()`
- **调试**：如果训练推理结果差异大，检查BN/Dropout模式

---

## 8. 权重初始化

### 核心结论
- **零初始化**：绝对不能用，对称性问题导致网络无法学习
- **ReLU系列**：使用He初始化（$\sigma^2 = 2/n_{in}$）
- **Sigmoid/Tanh**：使用Xavier初始化（$\sigma^2 = 2/(n_{in}+n_{out})$）
- **偏置**：初始化为0

### 初始化方法速查

| 方法 | 公式 | 适用激活函数 | 框架API |
|------|------|-------------|---------|
| **He Normal** | $W \sim N(0, \sqrt{2/n_{in}})$ | ReLU, Leaky ReLU | `kaiming_normal_` |
| **He Uniform** | $W \sim U[-\sqrt{6/n_{in}}, \sqrt{6/n_{in}}]$ | ReLU | `kaiming_uniform_` |
| **Xavier Normal** | $W \sim N(0, \sqrt{2/(n_{in}+n_{out})})$ | Sigmoid, Tanh | `xavier_normal_` |
| **Xavier Uniform** | $W \sim U[-\sqrt{6/(n_{in}+n_{out})}, \sqrt{6/(n_{in}+n_{out})}]$ | Sigmoid, Tanh | `xavier_uniform_` |
| **Zeros** | $b = 0$ | 偏置初始化 | `zeros_` |

### 快速选择指南
```
激活函数是ReLU/LeakyReLU/PReLU?
├─ 是 → He初始化 (kaiming)
└─ 否 → Xavier初始化 (glorot)

偏置 → 初始化为0
```

### 面试问答

**Q1: 为什么不能全零初始化？**
> **A**:
> - **对称性问题**：所有神经元权重相同→输出相同→梯度相同→更新后仍相同
> - 网络无论多深，都等价于一个神经元
> - 无法学习多样化的特征
> - **偏置可以为0**：因为偏置不参与对称性问题

**Q2: Xavier初始化的核心思想？**
> **A**:
> - **目标**：保持前向和反向传播时信号方差不变
> - **推导思路**：
>   1. 线性层输出方差：$\text{Var}(z) = n_{in} \cdot \text{Var}(W) \cdot \text{Var}(x)$
>   2. 要使$\text{Var}(z) = \text{Var}(x)$，需要$\text{Var}(W) = 1/n_{in}$
>   3. 考虑反向传播，需要$\text{Var}(W) = 1/n_{out}$
>   4. 折中：$\text{Var}(W) = 2/(n_{in}+n_{out})$

**Q3: He初始化为什么方差要乘2？**
> **A**:
> - **ReLU特性**：约50%神经元输出为0，方差减半
> - 为了补偿这个减半效应，初始化方差乘2
> - 公式：$\text{Var}(W) = 2/n_{in}$
> - 这样经过ReLU后，输出方差仍与输入相当

**Q4: He初始化为什么只用$n_{in}$不用$n_{out}$？**
> **A**:
> - ReLU的反向传播梯度只有0和1，没有连续缩放
> - 前向传播方差稳定更重要
> - 现代优化器（Adam）能自适应调整
> - 简化计算，实践效果足够好

**Q5: 有BN还需要好的初始化吗？**
> **A**:
> - **需要**，原因：
>   1. 训练初期BN的running统计不准确
>   2. 极端初始化可能导致数值溢出
>   3. 好的初始化能加速收敛
>   4. 第一层和最后一层通常没有BN
> - **结论**：BN缓解但不能替代好的初始化

**Q6: 偏置为什么初始化为0？**
> **A**:
> 1. **无对称性问题**：偏置是每个神经元独立的，即使都为0，权重不同就能打破对称性
> 2. **作用是平移**：训练中会自动调整到合适值
> 3. **简单有效**：随机初始化偏置没有额外好处

**Q7: 不同初始化对训练的影响？**
> **A**:
> | 初始化 | 现象 |
> |-------|------|
> | 过小 | 激活值逐层衰减→梯度消失→浅层不学习 |
> | 过大 | 激活值逐层爆炸→梯度爆炸→训练不稳定 |
> | 合适 | 激活值方差稳定→梯度流通畅→训练顺利 |

**Q8: 迁移学习时怎么初始化？**
> **A**:
> - **预训练层**：加载预训练权重，不重新初始化
> - **新增层**：用He/Xavier初始化
> - **微调策略**：
>   - 冻结预训练层，只训练新增层
>   - 或用很小的学习率微调全部

### 易错点
| 错误 | 正确理解 |
|------|---------|
| 初始化只影响训练速度 | 也影响最终性能 |
| 随机初始化就行 | 必须控制方差，否则梯度问题 |
| 标准正态分布N(0,1)可以 | 方差太大，需要缩放 |
| BN后初始化不重要 | 仍然重要，BN只是缓解 |

### 实践建议
- **默认**：ReLU + He初始化
- **检查**：打印前几层激活值的均值和方差，应该稳定
- **预训练**：优先使用预训练权重
- **调试**：如果训练不稳定，检查初始化是否合适

---

## 9. 训练技巧

### 核心结论
- **学习率**是最重要的超参数，配合学习率调度器使用
- **梯度裁剪**解决梯度爆炸，主要用于RNN
- **早停**自动选择最佳模型，防止过拟合
- **Warmup**在训练初期使用小学习率，稳定训练

### 学习率调度器

| 调度器 | 公式/描述 | 特点 | 适用场景 |
|-------|---------|------|---------|
| **Step Decay** | 每N个epoch乘γ | 简单、阶梯下降 | 传统CV |
| **Exponential** | $\eta_t = \eta_0 e^{-\lambda t}$ | 平滑指数下降 | 一般任务 |
| **Cosine Annealing** | 余弦曲线下降 | 平滑、可重启 | 常用 |
| **OneCycleLR** | 先增后降 | 收敛快、泛化好 | **推荐** |
| **ReduceOnPlateau** | 性能停滞时降低 | 自适应 | 不确定收敛速度 |

### One Cycle学习率策略

**核心思想**：先升后降，配合动量反向调度

**阶段划分**：
```
lr:  小 ────→ 大 ────────────→ 极小
      预热      峰值           退火
      (10%)    (30%)         (60%)

momentum: 高 ────→ 低 ────────→ 高
```

**公式**：
- 预热阶段：$lr(t) = lr_{init} + (lr_{max} - lr_{init}) \cdot \frac{t}{T_1}$
- 退火阶段：$lr(t) = lr_{final} + \frac{lr_{max} - lr_{final}}{2}(1 + \cos(\pi \cdot \frac{t-T_1}{T-T_1}))$

**推荐参数**：
- `pct_start=0.3`（前30%用于预热）
- `div_factor=25`（初始lr = max_lr/25）
- `final_div_factor=10000`（最终lr极小）
- `anneal_strategy='cos'`（余弦退火）

**为什么有效**：
- 高学习率阶段快速探索损失空间
- 低学习率阶段精细收敛
- 动量协同调度增强稳定性

### 残差连接（ResNet）

**核心公式**：
$$y = F(x) + x$$

其中$F(x)$是残差块（通常为Conv→BN→ReLU→Conv→BN）

**梯度流分析**：
$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \left(\frac{\partial F}{\partial x} + I\right)$$

- 即使$\frac{\partial F}{\partial x}$很小，$I$（恒等项）保证梯度直通
- 解决了深层网络的**退化问题**（不是过拟合，是优化困难）

**为什么残差学习更容易**：
- 学习$F(x) = 0$（恒等映射）比学习$H(x) = x$更容易
- 残差块只需学习"微调"，而非完整映射

**设计要点**：
| 类型 | 结构 | 特点 |
|------|------|------|
| 基础块 | 3×3→3×3 | ResNet-18/34 |
| 瓶颈块 | 1×1→3×3→1×1 | ResNet-50/101/152，参数更少 |
| Pre-norm | Norm→Act→Conv | 训练更稳定 |
| Post-norm | Conv→Norm→Act | 信号传播更清晰 |

### DenseNet密集连接

**核心公式**：
$$x_l = H_l([x_0, x_1, ..., x_{l-1}])$$

- 拼接所有前序层的输出作为输入
- Growth Rate $k$：每层增加$k$个特征通道

**与ResNet对比**：
| 特性 | ResNet | DenseNet |
|------|--------|----------|
| 连接方式 | 相加 | 拼接 |
| 特征复用 | 间接 | 直接 |
| 参数效率 | 较低 | 较高 |
| 内存占用 | 较低 | 较高 |
| 梯度流 | 跳跃连接 | 密集连接 |

**超参数**：
- Growth Rate $k$：12-32（控制宽度增长）
- 压缩因子$\theta$：0.5（过渡层减少通道数）

### 梯度裁剪

| 方法 | 公式 | 特点 |
|------|------|------|
| **范数裁剪**（推荐） | $g = g \cdot \frac{\theta}{\|g\|}$ if $\|g\| > \theta$ | 保持梯度方向 |
| 值裁剪 | $g_i = \text{clip}(g_i, -\theta, \theta)$ | 可能改变方向 |

### 面试问答

**Q1: 学习率调度有什么作用？**
> **A**:
> 1. **初期用大学习率**：快速下降到低损失区域
> 2. **后期用小学习率**：精细调整，收敛到更优解
> 3. **避免震荡**：小学习率减少在最优点附近的震荡
> 4. **跳出局部最优**：周期性重启可以探索新区域

**Q2: Warmup有什么用？**
> **A**:
> - **问题**：训练初期参数随机，大学习率可能导致不稳定更新
> - **做法**：前若干步从0线性增加到目标学习率
> - **效果**：
>   - 训练更稳定
>   - 特别对大batch、大模型有效
>   - Transformer标配

**Q3: 梯度爆炸和梯度裁剪的关系？**
> **A**:
> - **梯度爆炸**：梯度范数过大，参数更新过猛
> - **梯度裁剪**：当梯度范数超过阈值时，等比例缩小
> - **范数裁剪好处**：保持梯度方向，只缩小幅度
> - **常用阈值**：1.0, 5.0, 10.0

**Q4: Early Stopping的patience怎么设？**
> **A**:
> - **定义**：验证性能连续patience个epoch不提升时停止
> - **建议**：
>   - 小数据集：5-10
>   - 大数据集：10-20
>   - 训练较慢的模型：20-50
> - **太小**：可能过早停止，错过最优
> - **太大**：浪费时间，可能过拟合

**Q5: 如何诊断训练问题？**
> **A**:
> | 现象 | 原因 | 解决 |
> |------|------|------|
> | loss=NaN | 梯度爆炸 | 降低学习率、梯度裁剪 |
> | loss不降 | 学习率太小/网络太浅 | 增大学习率/增加深度 |
> | 验证loss上升 | 过拟合 | 正则化/Dropout/早停 |
> | 训练不稳定 | 学习率太大 | 降低学习率 |

**Q6: 混合精度训练是什么？**
> **A**:
> - **做法**：用FP16（半精度）计算前向和反向，FP32保存权重
> - **优点**：
>   - 减少显存占用（约一半）
>   - 加速计算（Tensor Core）
> - **注意**：需要损失缩放（Loss Scaling）防止精度问题
> - **框架支持**：PyTorch `torch.cuda.amp`

**Q7: 梯度累积怎么用？**
> **A**:
> - **场景**：GPU显存不够放大batch
> - **做法**：
>   1. 多次前向+反向，累积梯度
>   2. 累积N次后，平均并更新参数
> - **效果**：等效于N倍batch size
> - **代码关键**：不在每次backward后清零梯度

**Q8: 如何加速训练？**
> **A**:
> | 方法 | 效果 |
> |------|------|
> | 更大batch + 更大学习率 | 并行度提高 |
> | 混合精度（FP16） | 显存减半，速度加倍 |
> | 数据加载优化 | 消除IO瓶颈 |
> | 预训练+微调 | 减少训练时间 |
> | 分布式训练 | 多卡加速 |

**Q9: 残差连接为什么能训练很深的网络？**
> **A**:
> - 提供梯度直通路径：$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}(\frac{\partial F}{\partial x} + I)$
> - 即使$\frac{\partial F}{\partial x}$很小，恒等项$I$保证梯度流通
> - 解决的是**退化问题**（优化困难），不是过拟合
> - 学习残差$F(x)=0$比学习恒等映射$H(x)=x$更容易

**Q10: One Cycle为什么比普通学习率调度更好？**
> **A**:
> - 高学习率阶段快速探索损失空间，避免陷入局部最优
> - 与动量反向协同：lr高时momentum低（激进探索），lr低时momentum高（稳定收敛）
> - 收敛速度通常快2-3倍
> - 泛化性能往往更好

**Q11: DenseNet相比ResNet为什么参数更少？**
> **A**:
> - DenseNet通过特征拼接实现**直接复用**，无需重复学习
> - 每层只需学习$k$个新特征（Growth Rate），而非完整特征图
> - ResNet每层独立学习，特征复用是间接的
> - 但DenseNet内存占用更高（需保存所有中间特征）

### 易错点
| 错误 | 正确理解 |
|------|---------|
| 梯度裁剪能解决梯度消失 | 只能解决梯度爆炸 |
| patience=1就停止 | 太激进，建议至少5-10 |
| warmup对所有任务都有效 | 主要对大模型、大batch有效 |
| 学习率调度只在后期有用 | 整个训练过程都有影响 |

### 实践建议
- **标配**：Adam + Cosine调度 + 早停
- **大模型**：加Warmup（前5-10%步数）
- **RNN/LSTM**：加梯度裁剪（阈值1.0-5.0）
- **显存不够**：梯度累积或混合精度

---

## 10. PyTorch/TensorFlow入门

### 框架对比

| 方面 | PyTorch | TensorFlow 2.x / Keras |
|------|---------|----------------------|
| **编程范式** | 动态图（define-by-run） | 默认动态图，支持静态图 |
| **调试** | 易（普通Python调试） | 较难（tf.function需特殊处理） |
| **生产部署** | TorchServe, ONNX | TFLite, TF Serving, TensorRT |
| **移动端** | 较弱 | 强（TFLite） |
| **社区** | 学术界主流 | 工业界常用 |
| **代码风格** | 更Pythonic | 更函数式 |

### 核心概念对照

| 概念 | PyTorch | TensorFlow/Keras |
|------|---------|------------------|
| 张量 | `torch.Tensor` | `tf.Tensor` |
| 自动求导 | `requires_grad=True` + `.backward()` | `tf.GradientTape()` |
| 模型定义 | 继承`nn.Module` | `Sequential`或`Model`子类 |
| 前向传播 | `forward()`方法 | `call()`方法或`__call__` |
| 损失函数 | `nn.CrossEntropyLoss()` | `losses.SparseCategoricalCrossentropy()` |
| 优化器 | `optim.Adam(params, lr)` | `optimizers.Adam(lr)` |
| 训练模式切换 | `model.train()` / `model.eval()` | `training=True/False`参数 |

### 面试问答

**Q1: PyTorch和TensorFlow选哪个？**
> **A**:
> | 场景 | 推荐 | 原因 |
> |------|------|------|
> | 学术研究 | PyTorch | 灵活、论文代码多用PyTorch |
> | 快速原型 | PyTorch | 调试方便 |
> | 生产部署 | TensorFlow | 部署工具完善 |
> | 移动端 | TensorFlow | TFLite成熟 |
> | 大公司 | 看团队栈 | 统一技术栈 |

**Q2: PyTorch中`requires_grad`的作用？**
> **A**:
> - 标记张量需要计算梯度
> - PyTorch会为该张量构建计算图
> - 调用`.backward()`时计算梯度
> - 模型参数默认`requires_grad=True`

**Q3: 为什么需要`optimizer.zero_grad()`？**
> **A**:
> - PyTorch的梯度是**累加**的，不会自动清零
> - 原因：支持梯度累积等高级功能
> - 忘记清零：梯度会越来越大，参数更新错误
> - 必须在每次`.backward()`前调用

**Q4: `model.train()`和`model.eval()`有什么区别？**
> **A**:
> | 模式 | Dropout | BatchNorm | 用途 |
> |------|---------|-----------|------|
> | `train()` | 启用丢弃 | 用batch统计 | 训练时 |
> | `eval()` | 禁用 | 用running统计 | 验证/推理时 |
>
> - 忘记切换会导致验证结果不准确

**Q5: `with torch.no_grad()`的作用？**
> **A**:
> - 禁用梯度计算和计算图构建
> - **优点**：减少内存占用，加速计算
> - **用途**：验证、推理、不需要梯度的计算
> - **注意**：不能替代`model.eval()`

**Q6: PyTorch和NumPy怎么互转？**
> **A**:
> ```python
> # NumPy → PyTorch
> tensor = torch.from_numpy(numpy_array)
>
> # PyTorch → NumPy
> numpy_array = tensor.numpy()        # CPU张量
> numpy_array = tensor.cpu().numpy()  # GPU张量先转CPU
> ```

**Q7: Keras的Sequential和Functional API区别？**
> **A**:
> | 特性 | Sequential | Functional API |
> |------|-----------|----------------|
> | 模型结构 | 线性堆叠 | 任意DAG |
> | 多输入/输出 | 不支持 | 支持 |
> | 共享层 | 不支持 | 支持 |
> | 使用难度 | 简单 | 稍复杂 |
> | 适用场景 | 简单模型 | 复杂模型 |

**Q8: GPU训练的关键代码？**
> **A**:
> ```python
> # PyTorch
> device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
> model = model.to(device)
> inputs, labels = inputs.to(device), labels.to(device)
>
> # TensorFlow（通常自动检测）
> # 或手动指定
> with tf.device('/GPU:0'):
>     model = create_model()
> ```

### 易错点
| 错误 | 正确做法 |
|------|---------|
| 验证时忘记`model.eval()` | 验证前调用`model.eval()` |
| 验证时忘记`torch.no_grad()` | 用`with torch.no_grad():`包裹 |
| 忘记`optimizer.zero_grad()` | 每次backward前清零 |
| GPU张量直接转numpy | 先`.cpu()`再`.numpy()` |

### 实践建议
- **学习**：先学PyTorch，上手更快
- **训练循环**：PyTorch手写（灵活），Keras用`fit()`（方便）
- **调试**：充分利用PyTorch的动态图特性
- **部署**：考虑ONNX格式，跨框架通用

---

## 11. 神经网络回归

### 核心结论
- **输出层**：无激活函数（线性输出）
- **损失函数**：MSE（平方误差）或 MAE（绝对误差）
- **评估指标**：R²、RMSE、MAE
- **数据预处理**：特征和目标都需要标准化

### 回归 vs 分类

| 方面 | 回归 | 分类 |
|------|------|------|
| **输出** | 连续数值 | 离散类别 |
| **输出层激活** | 无（Linear） | Softmax/Sigmoid |
| **损失函数** | MSE/MAE/Huber | 交叉熵 |
| **评估指标** | R²/RMSE/MAE | 准确率/F1/AUC |
| **例子** | 房价、温度预测 | 图像分类、情感分析 |

### 损失函数选择

| 损失函数 | 公式 | 特点 | 使用场景 |
|---------|------|------|---------|
| **MSE** | $\frac{1}{n}\sum(y-\hat{y})^2$ | 对大误差惩罚重 | 数据干净，无异常值 |
| **MAE** | $\frac{1}{n}\sum\|y-\hat{y}\|$ | 对异常值鲁棒 | 有异常值 |
| **Huber** | 混合MSE和MAE | 折中方案 | 不确定数据质量 |

### 评估指标

| 指标 | 公式 | 解释 | 范围 |
|------|------|------|------|
| **R²** | $1 - \frac{\sum(y-\hat{y})^2}{\sum(y-\bar{y})^2}$ | 解释方差比例 | (-∞,1]，1最好 |
| **RMSE** | $\sqrt{\frac{1}{n}\sum(y-\hat{y})^2}$ | 均方根误差 | [0,∞)，0最好 |
| **MAE** | $\frac{1}{n}\sum\|y-\hat{y}\|$ | 平均绝对误差 | [0,∞)，0最好 |
| **MAPE** | $\frac{100}{n}\sum\|\frac{y-\hat{y}}{y}\|$ | 百分比误差 | [0,∞)，0最好 |

### 面试问答

**Q1: 回归输出层为什么不加激活函数？**
> **A**:
> - 回归目标值没有范围限制（如房价可以很高）
> - 激活函数会限制输出范围（Sigmoid→[0,1]，ReLU→[0,∞)）
> - 线性输出可以表示任意实数

**Q2: MSE和MAE怎么选？**
> **A**:
> | 情况 | 选择 | 原因 |
> |------|------|------|
> | 数据干净 | MSE | 梯度随误差变化，收敛快 |
> | 有异常值 | MAE | 对异常值鲁棒 |
> | 不确定 | Huber | 综合两者优点 |

**Q3: R²可以为负吗？什么意思？**
> **A**:
> - **可以**为负
> - R²<0表示模型预测还不如直接用均值预测
> - 说明模型非常差，可能有严重问题
> - R²=0表示和均值预测一样

**Q4: 为什么回归任务要标准化目标值？**
> **A**:
> 1. **数值稳定**：避免大数值导致梯度问题
> 2. **学习率通用**：标准化后同样的学习率适用于不同量纲
> 3. **收敛更快**：输出在合理范围内
> 4. **注意**：预测后要反标准化回原始尺度

**Q5: 特征标准化怎么做？**
> **A**:
> ```python
> # 1. 只用训练集fit（防止数据泄露）
> scaler = StandardScaler()
> X_train_scaled = scaler.fit_transform(X_train)
>
> # 2. 用同一个scaler transform验证/测试集
> X_val_scaled = scaler.transform(X_val)
> X_test_scaled = scaler.transform(X_test)
> ```

**Q6: 如何处理多输出回归？**
> **A**:
> - **方法1**：输出层神经元数=输出维度
> - **方法2**：多个独立网络，每个预测一个输出
> - **损失**：各输出损失加权求和
> - **注意**：不同输出可能需要不同的标准化

**Q7: 回归模型过拟合的表现？**
> **A**:
> - 训练R²很高（接近1），验证R²明显低
> - 训练RMSE很低，验证RMSE明显高
> - **解决**：正则化、减少模型复杂度、增加数据

**Q8: 神经网络回归和线性回归的区别？**
> **A**:
> | 特性 | 线性回归 | 神经网络回归 |
> |------|---------|-------------|
> | 非线性 | 不能（需手动特征） | 自动学习 |
> | 参数量 | 少 | 多 |
> | 可解释性 | 高（系数有意义） | 低（黑盒） |
> | 小数据 | 更好 | 容易过拟合 |
> | 复杂关系 | 需要特征工程 | 自动捕获 |

### 易错点
| 错误 | 正确做法 |
|------|---------|
| 输出层加ReLU | 回归不加激活函数 |
| 用验证集fit scaler | 只用训练集fit |
| 忘记反标准化 | 预测后要转回原始尺度 |
| 用R²评估不同数据集 | R²只能同数据集比较 |

### 实践建议
- **起步**：MSE损失 + Adam优化器
- **有异常值**：换MAE或Huber
- **多输出**：权衡各输出的重要性调整权重
- **评估**：同时看R²、RMSE、MAE

---

## 综合速查表

### 激活函数选择
| 场景 | 推荐 |
|------|------|
| 隐藏层 | ReLU |
| 二分类输出 | Sigmoid |
| 多分类输出 | Softmax |
| 回归输出 | 无激活 |
| Transformer | GELU |

### 初始化选择
| 激活函数 | 初始化 |
|----------|--------|
| ReLU系列 | He (kaiming) |
| Sigmoid/Tanh | Xavier (glorot) |
| 偏置 | 零初始化 |

### 损失函数选择
| 任务 | 损失函数 |
|------|---------|
| 二分类 | Binary Cross Entropy |
| 多分类 | Cross Entropy |
| 回归（干净数据） | MSE |
| 回归（有异常值） | MAE或Huber |

### 优化器选择
| 场景 | 推荐 |
|------|------|
| 默认/快速原型 | Adam |
| 追求最佳泛化 | SGD+Momentum |
| 大模型训练 | AdamW |
| 需要稳定性 | Lookahead(AdamW) |
| 快速收敛 | One Cycle调度 |

### 归一化选择
| 场景 | 推荐 |
|------|------|
| 大batch CNN | BatchNorm |
| 小batch/batch=1 | GroupNorm |
| Transformer/RNN | LayerNorm |
| 风格迁移 | InstanceNorm |

### 数据增强选择
| 场景 | 推荐 |
|------|------|
| 图像分类 | RandAugment + Mixup/CutMix |
| 目标检测 | CutMix（保留定位能力） |
| 小数据集 | 强增强 + Mixup |
| 医学图像 | 轻度几何变换 |

### 正则化选择
| 问题 | 解决方案 |
|------|---------|
| 过拟合 | L2 + Dropout + Early Stopping |
| 梯度消失 | ReLU + He初始化 + BN |
| 梯度爆炸 | 梯度裁剪 + 降低学习率 |

---

## 必背公式

```
1. Sigmoid: σ(z) = 1/(1+e^(-z))
2. ReLU: max(0,z)
3. Softmax: e^(zi)/Σe^(zj)
4. 交叉熵: L = -Σy·log(ŷ)
5. MSE: L = (1/n)Σ(y-ŷ)²
6. He初始化: W ~ N(0, √(2/n_in))
7. Xavier初始化: W ~ N(0, √(2/(n_in+n_out)))
8. BN: y = γ·(x-μ)/√(σ²+ε) + β
9. L2正则: L_total = L + λ/2·||W||²
10. Adam: θ ← θ - η·m̂/(√v̂+ε)
11. R²: 1 - SS_res/SS_tot
12. Mish: x·tanh(ln(1+e^x))
13. Swish: x·σ(βx)
14. KL散度: Σ P·log(P/Q)
15. Focal Loss: -α(1-p_t)^γ·log(p_t)
16. 偏差-方差分解: MSE = Bias² + Var + σ²
17. 残差连接: y = F(x) + x
18. Mixup: x̃ = λx_i + (1-λ)x_j
19. AdamW: θ = θ - α·m̂/(√v̂+ε) - αλθ
20. Nesterov: v_t = βv_{t-1} + ∇L(θ - αβv)
```

---

## 面试必会问题 Top 20

1. **ReLU比Sigmoid好在哪？** → 计算快、缓解梯度消失、稀疏激活
2. **为什么不能零初始化？** → 对称性问题，神经元无法学习不同特征
3. **BN训练和推理的区别？** → 训练用batch统计，推理用running统计
4. **Adam和SGD选哪个？** → 快速原型用Adam，追求最佳泛化用SGD
5. **L1和L2正则的区别？** → L1稀疏化，L2均匀缩小
6. **过拟合怎么解决？** → L2+Dropout+Early Stopping+数据增强
7. **梯度消失怎么解决？** → ReLU+BN+残差连接+He初始化
8. **梯度爆炸怎么解决？** → 梯度裁剪+降低学习率
9. **Xavier和He的区别？** → He考虑ReLU方差减半，乘以2补偿
10. **Dropout为什么有效？** → 集成效应+防止共适应
11. **为什么分类用交叉熵不用MSE？** → 梯度更清晰，收敛更快
12. **学习率怎么选？** → 从0.001开始，看loss曲线调整
13. **回归输出层为什么不加激活？** → 输出需要是任意实数
14. **反向传播的本质？** → 链式法则的高效实现
15. **感知机为什么不能解决XOR？** → 线性不可分问题
16. **KL散度和JS散度的区别？** → KL非对称可无穷，JS对称有界
17. **Focal Loss如何解决类别不平衡？** → 调制因子$(1-p_t)^{\gamma}$抑制易样本
18. **偏差和方差的trade-off？** → 模型复杂度↑→偏差↓方差↑，呈U形曲线
19. **为什么深度学习用反向模式自动微分？** → 输出少输入多，反向模式只需一次传播
20. **残差连接为什么能训练很深的网络？** → 恒等项$I$保证梯度直通，解决退化问题

---

*最后更新：2025年12月*
