import numpy as np
import matplotlib.pyplot as plt


class Mixup:
    """
    Mixupæ•°æ®å¢å¼º - ä»é›¶å®ç°

    è®ºæ–‡: "mixup: Beyond Empirical Risk Minimization" (ICLR 2018)

    æ ¸å¿ƒæ€æƒ³:
        - é€šè¿‡çº¿æ€§æ’å€¼æ··åˆä¸¤ä¸ªæ ·æœ¬åŠå…¶æ ‡ç­¾
        - åˆ›å»ºè™šæ‹Ÿçš„è®­ç»ƒæ ·æœ¬ï¼Œæ‰©å±•è®­ç»ƒåˆ†å¸ƒ
        - äº§ç”Ÿæ›´å¹³æ»‘çš„å†³ç­–è¾¹ç•Œï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

    ä¼˜ç‚¹:
        - å®ç°ç®€å•
        - æ— é¢å¤–è®¡ç®—å¼€é”€ï¼ˆè®­ç»ƒæ—¶æ··åˆï¼‰
        - å¯¹å„ç§æ¨¡å‹å’Œä»»åŠ¡éƒ½æœ‰æ•ˆ
    """

    def __init__(self, alpha=0.2):
        """
        åˆå§‹åŒ–Mixup

        å‚æ•°:
            alpha: float, Betaåˆ†å¸ƒçš„å‚æ•°
                   alpha=0: ä¸æ··åˆï¼ˆé€€åŒ–ä¸ºæ ‡å‡†è®­ç»ƒï¼‰
                   alpha=1: å‡åŒ€åˆ†å¸ƒï¼Œå¼ºæ··åˆ
                   æ¨èå€¼: 0.2 - 0.4
        """
        self.alpha = alpha

    def sample_lambda(self):
        """
        ä»Betaåˆ†å¸ƒé‡‡æ ·æ··åˆç³»æ•°Î»

        è¿”å›:
            float: Î» âˆˆ [0, 1]

        Betaåˆ†å¸ƒçš„æ€§è´¨:
            - Î± = Î² æ—¶åˆ†å¸ƒå¯¹ç§°
            - Î± < 1 æ—¶åˆ†å¸ƒå‘ˆUå½¢ï¼Œå€¾å‘äºå–æ¥è¿‘0æˆ–1çš„å€¼
            - Î± > 1 æ—¶åˆ†å¸ƒå‘ˆå€’Uå½¢ï¼Œå€¾å‘äºå–æ¥è¿‘0.5çš„å€¼
        """
        if self.alpha > 0:
            # ä»Beta(Î±, Î±)é‡‡æ ·
            lam = np.random.beta(self.alpha, self.alpha)
        else:
            # alpha=0æ—¶ä¸æ··åˆ
            lam = 1.0
        return lam

    def mix_samples(self, x1, y1, x2, y2):
        """
        æ··åˆä¸¤ä¸ªæ ·æœ¬

        å‚æ•°:
            x1, x2: numpyæ•°ç»„, è¾“å…¥å›¾åƒ
            y1, y2: numpyæ•°ç»„, one-hotç¼–ç çš„æ ‡ç­¾

        è¿”å›:
            mixed_x: æ··åˆåçš„å›¾åƒ
            mixed_y: æ··åˆåçš„æ ‡ç­¾ï¼ˆè½¯æ ‡ç­¾ï¼‰
            lam: ä½¿ç”¨çš„æ··åˆç³»æ•°

        å…¬å¼:
            xÌƒ = Î» * x1 + (1-Î») * x2
            á»¹ = Î» * y1 + (1-Î») * y2
        """
        lam = self.sample_lambda()

        # æ··åˆå›¾åƒ
        mixed_x = lam * x1 + (1 - lam) * x2

        # æ··åˆæ ‡ç­¾ï¼ˆè½¯æ ‡ç­¾ï¼‰
        mixed_y = lam * y1 + (1 - lam) * y2

        return mixed_x, mixed_y, lam

    def mix_batch(self, batch_x, batch_y):
        """
        å¯¹ä¸€ä¸ªbatchè¿›è¡ŒMixup

        å‚æ•°:
            batch_x: numpyæ•°ç»„, shape: (batch_size, H, W, C)
            batch_y: numpyæ•°ç»„, shape: (batch_size, n_classes), one-hotç¼–ç 

        è¿”å›:
            mixed_x: æ··åˆåçš„batch
            mixed_y: æ··åˆåçš„æ ‡ç­¾
            lam: ä½¿ç”¨çš„æ··åˆç³»æ•°

        ç­–ç•¥:
            å°†batchæ‰“ä¹±ï¼Œç„¶åä¸åŸbatchæ··åˆ
            è¿™æ ·æ¯ä¸ªæ ·æœ¬éƒ½ä¸å¦ä¸€ä¸ªéšæœºæ ·æœ¬æ··åˆ
        """
        batch_size = len(batch_x)
        lam = self.sample_lambda()

        # éšæœºæ‰“ä¹±ç´¢å¼•
        shuffle_indices = np.random.permutation(batch_size)

        # æ··åˆ
        mixed_x = lam * batch_x + (1 - lam) * batch_x[shuffle_indices]
        mixed_y = lam * batch_y + (1 - lam) * batch_y[shuffle_indices]

        return mixed_x, mixed_y, lam

def exercise_1_gridmask():
    """
    ç»ƒä¹ 1: å®ç°GridMask
    æç¤º:
        1. åˆ›å»ºä¸€ä¸ªç½‘æ ¼æ©ç 
        2. å°†æ©ç åº”ç”¨åˆ°å›¾åƒä¸Š
    """
    # åœ¨è¿™é‡Œå¡«å†™ä½ çš„ä»£ç 
    pass


# =============================================================================
# ç»ƒä¹ 3: å®Œæ•´å®ç°MixUpè®­ç»ƒå¾ªç¯
# =============================================================================

def exercise_3_mixup_training():
    """
    ç»ƒä¹ 3ï¼ˆæŒ‘æˆ˜ï¼‰: å®Œæ•´å®ç°MixUpè®­ç»ƒå¾ªç¯

    æœ¬ç»ƒä¹ å°†å®ç°ä¸€ä¸ªå®Œæ•´çš„ä½¿ç”¨Mixupæ•°æ®å¢å¼ºçš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
    2. ç®€å•ç¥ç»ç½‘ç»œå®šä¹‰ï¼ˆä»é›¶å®ç°ï¼‰
    3. æ”¯æŒè½¯æ ‡ç­¾çš„äº¤å‰ç†µæŸå¤±
    4. Mixupè®­ç»ƒå¾ªç¯
    5. æ•ˆæœå¯¹æ¯”å¯è§†åŒ–

    å­¦ä¹ ç›®æ ‡ï¼š
    - ç†è§£Mixupå¦‚ä½•ä¸è®­ç»ƒå¾ªç¯é›†æˆ
    - æŒæ¡è½¯æ ‡ç­¾ï¼ˆsoft labelsï¼‰çš„æŸå¤±è®¡ç®—
    - è§‚å¯ŸMixupçš„æ­£åˆ™åŒ–æ•ˆæœ
    """

    print("=" * 70)
    print("ç»ƒä¹ 3: å®Œæ•´å®ç°MixUpè®­ç»ƒå¾ªç¯")
    print("=" * 70)

    # =========================================================================
    # ç¬¬1éƒ¨åˆ†: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
    # =========================================================================
    print("\nğŸ“¦ ç¬¬1éƒ¨åˆ†: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
    print("-" * 50)

    from sklearn.datasets import load_digits
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler

    # åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†
    # digitsæ•°æ®é›†: 1797ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯8x8=64ç»´çš„ç°åº¦å›¾åƒï¼Œå…±10ä¸ªç±»åˆ«(0-9)
    digits = load_digits()
    X, y = digits.data, digits.target

    print(f"æ•°æ®é›†å¤§å°: {X.shape[0]} æ ·æœ¬")
    print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]} (8x8åƒç´ )")
    print(f"ç±»åˆ«æ•°é‡: {len(np.unique(y))} (æ•°å­—0-9)")

    # æ•°æ®æ ‡å‡†åŒ–
    # æ ‡å‡†åŒ–å…¬å¼: X_scaled = (X - Î¼) / Ïƒ
    # è¿™æœ‰åŠ©äºåŠ é€Ÿè®­ç»ƒæ”¶æ•›ï¼Œé¿å…æŸäº›ç‰¹å¾ä¸»å¯¼æ¢¯åº¦
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # åˆ’åˆ†æ•°æ®é›†: 60%è®­ç»ƒ, 20%éªŒè¯, 20%æµ‹è¯•
    X_train, X_temp, y_train, y_temp = train_test_split(
        X_scaled, y, test_size=0.4, random_state=42, stratify=y
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )

    print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")

    # å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç 
    # Mixupéœ€è¦å¯¹æ ‡ç­¾è¿›è¡Œæ··åˆï¼Œå› æ­¤å¿…é¡»ä½¿ç”¨one-hotæ ¼å¼
    # ä¾‹å¦‚: æ ‡ç­¾3 â†’ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
    def to_one_hot(y, n_classes):
        """
        å°†æ•´æ•°æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç 

        å‚æ•°:
            y: numpyæ•°ç»„, shape (n_samples,), æ•´æ•°æ ‡ç­¾
            n_classes: int, ç±»åˆ«æ€»æ•°

        è¿”å›:
            one_hot: numpyæ•°ç»„, shape (n_samples, n_classes)

        ç¤ºä¾‹:
            y = [0, 2, 1]
            n_classes = 3
            è¿”å›: [[1,0,0], [0,0,1], [0,1,0]]
        """
        n_samples = len(y)
        one_hot = np.zeros((n_samples, n_classes))
        # ä½¿ç”¨é«˜çº§ç´¢å¼•: one_hot[è¡Œç´¢å¼•, åˆ—ç´¢å¼•] = 1
        one_hot[np.arange(n_samples), y] = 1
        return one_hot

    n_classes = 10
    y_train_onehot = to_one_hot(y_train, n_classes)
    y_val_onehot = to_one_hot(y_val, n_classes)
    y_test_onehot = to_one_hot(y_test, n_classes)

    print(f"æ ‡ç­¾å½¢çŠ¶ï¼ˆone-hotï¼‰: {y_train_onehot.shape}")

    # =========================================================================
    # ç¬¬2éƒ¨åˆ†: ç®€å•ç¥ç»ç½‘ç»œå®šä¹‰ï¼ˆä»é›¶å®ç°ï¼‰
    # =========================================================================
    print("\nğŸ§  ç¬¬2éƒ¨åˆ†: ç¥ç»ç½‘ç»œå®šä¹‰")
    print("-" * 50)

    class SimpleMLP:
        """
        ç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆä»é›¶å®ç°ï¼‰

        ç½‘ç»œç»“æ„: è¾“å…¥å±‚(64) â†’ éšè—å±‚(128, ReLU) â†’ è¾“å‡ºå±‚(10, Softmax)

        å…³é”®ç‰¹æ€§:
        - æ”¯æŒè½¯æ ‡ç­¾è®­ç»ƒï¼ˆç”¨äºMixupï¼‰
        - Heåˆå§‹åŒ–ï¼ˆé€‚ç”¨äºReLUæ¿€æ´»ï¼‰
        - å®Œæ•´çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­å®ç°

        å‚æ•°:
            input_size: int, è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_size: int, éšè—å±‚ç¥ç»å…ƒæ•°é‡
            output_size: int, è¾“å‡ºç±»åˆ«æ•°é‡
            learning_rate: float, å­¦ä¹ ç‡
        """

        def __init__(self, input_size=64, hidden_size=128, output_size=10, learning_rate=0.01):
            """
            åˆå§‹åŒ–ç½‘ç»œå‚æ•°

            ä½¿ç”¨Heåˆå§‹åŒ–:
                W ~ N(0, sqrt(2/n_in))

            è¿™ç§åˆå§‹åŒ–æ–¹å¼ç‰¹åˆ«é€‚åˆReLUæ¿€æ´»å‡½æ•°ï¼Œ
            å¯ä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜
            """
            self.lr = learning_rate

            # éšè—å±‚å‚æ•°
            # W1 shape: (input_size, hidden_size) = (64, 128)
            # Heåˆå§‹åŒ–: æ ‡å‡†å·® = sqrt(2 / fan_in)
            self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)
            self.b1 = np.zeros(hidden_size)

            # è¾“å‡ºå±‚å‚æ•°
            # W2 shape: (hidden_size, output_size) = (128, 10)
            self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)
            self.b2 = np.zeros(output_size)

            # ç¼“å­˜ï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰
            self.cache = {}

        def relu(self, z):
            """
            ReLUæ¿€æ´»å‡½æ•°: f(z) = max(0, z)

            ä¼˜ç‚¹:
            - è®¡ç®—ç®€å•
            - ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
            - ç¨€ç–æ¿€æ´»ï¼ˆéƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºä¸º0ï¼‰
            """
            return np.maximum(0, z)

        def relu_derivative(self, z):
            """
            ReLUçš„å¯¼æ•°: f'(z) = 1 if z > 0 else 0

            æ³¨æ„: åœ¨z=0å¤„å¯¼æ•°æœªå®šä¹‰ï¼Œè¿™é‡Œå–0
            """
            return (z > 0).astype(float)

        def softmax(self, z):
            """
            Softmaxæ¿€æ´»å‡½æ•°: softmax(z_i) = exp(z_i) / Î£_j exp(z_j)

            å°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆæ‰€æœ‰è¾“å‡ºå’Œä¸º1ï¼‰

            æ•°å€¼ç¨³å®šæ€§æŠ€å·§:
            - å‡å»æœ€å¤§å€¼é¿å…expæº¢å‡º
            - exp(z - max(z)) / Î£exp(z - max(z)) æ•°å­¦ä¸Šç­‰ä»·
            """
            # å‡å»æ¯è¡Œçš„æœ€å¤§å€¼ï¼Œé¿å…æ•°å€¼æº¢å‡º
            z_shifted = z - np.max(z, axis=1, keepdims=True)
            exp_z = np.exp(z_shifted)
            return exp_z / np.sum(exp_z, axis=1, keepdims=True)

        def forward(self, X):
            """
            å‰å‘ä¼ æ’­

            è®¡ç®—æµç¨‹:
                z1 = X @ W1 + b1        # éšè—å±‚çº¿æ€§å˜æ¢
                h1 = relu(z1)           # éšè—å±‚æ¿€æ´»
                z2 = h1 @ W2 + b2       # è¾“å‡ºå±‚çº¿æ€§å˜æ¢
                y_pred = softmax(z2)    # è¾“å‡ºå±‚æ¿€æ´»ï¼ˆæ¦‚ç‡ï¼‰

            å‚æ•°:
                X: numpyæ•°ç»„, shape (batch_size, input_size)

            è¿”å›:
                y_pred: numpyæ•°ç»„, shape (batch_size, output_size), é¢„æµ‹æ¦‚ç‡
            """
            # éšè—å±‚
            # z1 shape: (batch_size, hidden_size)
            z1 = X @ self.W1 + self.b1
            h1 = self.relu(z1)

            # è¾“å‡ºå±‚
            # z2 shape: (batch_size, output_size)
            z2 = h1 @ self.W2 + self.b2
            y_pred = self.softmax(z2)

            # ç¼“å­˜ä¸­é—´ç»“æœï¼ˆåå‘ä¼ æ’­éœ€è¦ï¼‰
            self.cache = {
                'X': X,  # è¾“å…¥
                'z1': z1,  # éšè—å±‚çº¿æ€§è¾“å‡º
                'h1': h1,  # éšè—å±‚æ¿€æ´»è¾“å‡º
                'z2': z2,  # è¾“å‡ºå±‚çº¿æ€§è¾“å‡º
                'y_pred': y_pred  # æœ€ç»ˆé¢„æµ‹
            }

            return y_pred

        def backward(self, y_true):
            """
            åå‘ä¼ æ’­ - è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°

            å…³é”®å…¬å¼ï¼ˆäº¤å‰ç†µ + Softmaxçš„ç®€åŒ–ï¼‰:
                Î´2 = y_pred - y_true              # è¾“å‡ºå±‚è¯¯å·®
                dW2 = h1.T @ Î´2 / m               # W2æ¢¯åº¦
                db2 = mean(Î´2, axis=0)            # b2æ¢¯åº¦

                Î´1 = (Î´2 @ W2.T) * relu'(z1)     # éšè—å±‚è¯¯å·®
                dW1 = X.T @ Î´1 / m                # W1æ¢¯åº¦
                db1 = mean(Î´1, axis=0)            # b1æ¢¯åº¦

            å‚æ•°:
                y_true: numpyæ•°ç»„, shape (batch_size, output_size)
                        å¯ä»¥æ˜¯ç¡¬æ ‡ç­¾ï¼ˆone-hotï¼‰æˆ–è½¯æ ‡ç­¾ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰

            æ³¨æ„:
                - Mixupè®­ç»ƒæ—¶ï¼Œy_trueæ˜¯è½¯æ ‡ç­¾ï¼Œå¦‚[0.7, 0, 0.3, 0, ...]
                - äº¤å‰ç†µ+Softmaxçš„æ¢¯åº¦å…¬å¼å¯¹è½¯æ ‡ç­¾åŒæ ·é€‚ç”¨ï¼
            """
            # è·å–ç¼“å­˜
            X = self.cache['X']
            h1 = self.cache['h1']
            z1 = self.cache['z1']
            y_pred = self.cache['y_pred']

            m = X.shape[0]  # batchå¤§å°

            # ===== è¾“å‡ºå±‚æ¢¯åº¦ =====
            # äº¤å‰ç†µæŸå¤±å¯¹Softmaxè¾“å‡ºçš„æ¢¯åº¦ç®€åŒ–ä¸º: Î´2 = y_pred - y_true
            # è¿™ä¸ªä¼˜ç¾çš„ç»“æœæ¥è‡ªäºå¯¹ -Î£y*log(p) çš„æ±‚å¯¼
            # æ— è®ºyæ˜¯ç¡¬æ ‡ç­¾è¿˜æ˜¯è½¯æ ‡ç­¾ï¼Œè¿™ä¸ªå…¬å¼éƒ½æˆç«‹ï¼
            delta2 = y_pred - y_true  # shape: (m, output_size)

            # W2çš„æ¢¯åº¦: dL/dW2 = h1.T @ delta2 / m
            # å½¢çŠ¶: (hidden_size, m) @ (m, output_size) = (hidden_size, output_size)
            dW2 = h1.T @ delta2 / m

            # b2çš„æ¢¯åº¦: å¯¹batchå–å¹³å‡
            db2 = np.mean(delta2, axis=0)

            # ===== éšè—å±‚æ¢¯åº¦ =====
            # è¯¯å·®åå‘ä¼ æ’­: delta1 = (delta2 @ W2.T) * relu'(z1)
            # å½¢çŠ¶: (m, output_size) @ (output_size, hidden_size) = (m, hidden_size)
            delta1 = (delta2 @ self.W2.T) * self.relu_derivative(z1)

            # W1çš„æ¢¯åº¦
            dW1 = X.T @ delta1 / m

            # b1çš„æ¢¯åº¦
            db1 = np.mean(delta1, axis=0)

            # ===== å‚æ•°æ›´æ–°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰=====
            # Î¸_new = Î¸_old - lr * gradient
            self.W2 -= self.lr * dW2
            self.b2 -= self.lr * db2
            self.W1 -= self.lr * dW1
            self.b1 -= self.lr * db1

        def predict(self, X):
            """
            é¢„æµ‹ç±»åˆ«æ ‡ç­¾

            è¿”å›:
                é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•ï¼ˆå–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ï¼‰
            """
            y_pred = self.forward(X)
            return np.argmax(y_pred, axis=1)

        def copy(self):
            """
            å¤åˆ¶æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”å®éªŒï¼Œç¡®ä¿ä¸¤ä¸ªæ¨¡å‹åˆå§‹å‚æ•°ç›¸åŒï¼‰
            """
            new_model = SimpleMLP(
                input_size=self.W1.shape[0],
                hidden_size=self.W1.shape[1],
                output_size=self.W2.shape[1],
                learning_rate=self.lr
            )
            new_model.W1 = self.W1.copy()
            new_model.b1 = self.b1.copy()
            new_model.W2 = self.W2.copy()
            new_model.b2 = self.b2.copy()
            return new_model

    print("SimpleMLPç±»å®šä¹‰å®Œæˆ")
    print("ç½‘ç»œç»“æ„: 64 â†’ 128 (ReLU) â†’ 10 (Softmax)")

    # =========================================================================
    # ç¬¬3éƒ¨åˆ†: æŸå¤±å‡½æ•°å®šä¹‰
    # =========================================================================
    print("\nğŸ“‰ ç¬¬3éƒ¨åˆ†: æŸå¤±å‡½æ•°å®šä¹‰")
    print("-" * 50)

    def soft_cross_entropy(y_pred, y_true):
        """
        æ”¯æŒè½¯æ ‡ç­¾çš„äº¤å‰ç†µæŸå¤±

        å…¬å¼: L = -Î£ y_true * log(y_pred)

        ä¸ç¡¬æ ‡ç­¾çš„åŒºåˆ«:
        - ç¡¬æ ‡ç­¾: y_trueæ˜¯one-hotï¼Œå¦‚[0, 0, 1, 0, ...]ï¼Œåªæœ‰ä¸€ä¸ª1
        - è½¯æ ‡ç­¾: y_trueæ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œå¦‚[0.3, 0, 0.7, 0, ...]ï¼Œå’Œä¸º1

        Mixupè®­ç»ƒæ—¶ä½¿ç”¨è½¯æ ‡ç­¾:
        - å¦‚æœå›¾åƒA(æ ‡ç­¾çŒ«)å’Œå›¾åƒB(æ ‡ç­¾ç‹—)ä»¥Î»=0.7æ··åˆ
        - æ··åˆåçš„æ ‡ç­¾ä¸º: 0.7*çŒ« + 0.3*ç‹—
        - è¿™å°±æ˜¯è½¯æ ‡ç­¾ï¼

        å‚æ•°:
            y_pred: numpyæ•°ç»„, shape (batch_size, n_classes), é¢„æµ‹æ¦‚ç‡
            y_true: numpyæ•°ç»„, shape (batch_size, n_classes), çœŸå®æ ‡ç­¾ï¼ˆå¯ä»¥æ˜¯è½¯æ ‡ç­¾ï¼‰

        è¿”å›:
            loss: float, å¹³å‡æŸå¤±å€¼
        """
        # æ•°å€¼ç¨³å®šæ€§: é¿å…log(0)
        epsilon = 1e-10
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

        # äº¤å‰ç†µè®¡ç®—: -Î£ y_true * log(y_pred)
        # å¯¹æ¯ä¸ªæ ·æœ¬: è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„åŠ æƒå¯¹æ•°å’Œ
        # ç„¶åå¯¹batchå–å¹³å‡
        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

        return loss

    def accuracy(y_pred, y_true):
        """
        è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡

        å‚æ•°:
            y_pred: é¢„æµ‹æ¦‚ç‡æˆ–é¢„æµ‹æ ‡ç­¾
            y_true: çœŸå®æ ‡ç­¾ï¼ˆå¯ä»¥æ˜¯one-hotæˆ–æ•´æ•°å½¢å¼ï¼‰

        è¿”å›:
            å‡†ç¡®ç‡ (0åˆ°1ä¹‹é—´)
        """
        # å¦‚æœy_predæ˜¯æ¦‚ç‡ï¼Œè½¬æ¢ä¸ºç±»åˆ«
        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:
            pred_labels = np.argmax(y_pred, axis=1)
        else:
            pred_labels = y_pred

        # å¦‚æœy_trueæ˜¯one-hotï¼Œè½¬æ¢ä¸ºç±»åˆ«
        if len(y_true.shape) > 1 and y_true.shape[1] > 1:
            true_labels = np.argmax(y_true, axis=1)
        else:
            true_labels = y_true

        return np.mean(pred_labels == true_labels)

    print("æŸå¤±å‡½æ•°: soft_cross_entropy (æ”¯æŒè½¯æ ‡ç­¾)")
    print("è¯„ä¼°å‡½æ•°: accuracy")

    # =========================================================================
    # ç¬¬4éƒ¨åˆ†: è®­ç»ƒå‡½æ•°å®šä¹‰
    # =========================================================================
    print("\nğŸ”„ ç¬¬4éƒ¨åˆ†: è®­ç»ƒå‡½æ•°å®šä¹‰")
    print("-" * 50)

    def create_batches(X, y, batch_size, shuffle=True):
        """
        åˆ›å»ºmini-batchæ•°æ®ç”Ÿæˆå™¨

        å‚æ•°:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            batch_size: æ¯ä¸ªbatchçš„å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®

        ç”Ÿæˆ:
            (batch_X, batch_y) å…ƒç»„
        """
        n_samples = X.shape[0]
        indices = np.arange(n_samples)

        if shuffle:
            np.random.shuffle(indices)

        for start in range(0, n_samples, batch_size):
            end = min(start + batch_size, n_samples)
            batch_indices = indices[start:end]
            yield X[batch_indices], y[batch_indices]

    def train_with_mixup(model, X_train, y_train, X_val, y_val,
                         mixup_alpha=0.2, epochs=50, batch_size=32):
        """
        ä½¿ç”¨Mixupè¿›è¡Œè®­ç»ƒ

        Mixupè®­ç»ƒçš„æ ¸å¿ƒæµç¨‹:
        1. ä»è®­ç»ƒæ•°æ®ä¸­é‡‡æ ·ä¸€ä¸ªbatch
        2. å¯¹batchåº”ç”¨Mixupå¢å¼ºï¼ˆæ··åˆå›¾åƒå’Œæ ‡ç­¾ï¼‰
        3. ç”¨æ··åˆåçš„æ•°æ®è¿›è¡Œå‰å‘ä¼ æ’­
        4. ç”¨è½¯æ ‡ç­¾è®¡ç®—æŸå¤±
        5. åå‘ä¼ æ’­æ›´æ–°å‚æ•°

        å‚æ•°:
            model: SimpleMLPå®ä¾‹
            X_train, y_train: è®­ç»ƒæ•°æ®ï¼ˆy_trainåº”ä¸ºone-hotæ ¼å¼ï¼‰
            X_val, y_val: éªŒè¯æ•°æ®
            mixup_alpha: Mixupçš„Î±å‚æ•°ï¼ˆæ§åˆ¶æ··åˆå¼ºåº¦ï¼‰
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°

        è¿”å›:
            history: å­—å…¸ï¼ŒåŒ…å«è®­ç»ƒå’ŒéªŒè¯çš„æŸå¤±/å‡†ç¡®ç‡å†å²
        """
        # åˆ›å»ºMixupå¢å¼ºå™¨
        mixup = Mixup(alpha=mixup_alpha)

        # è®°å½•è®­ç»ƒå†å²
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }

        for epoch in range(epochs):
            epoch_losses = []
            epoch_correct = 0
            epoch_total = 0

            # éå†æ‰€æœ‰batch
            for batch_X, batch_y in create_batches(X_train, y_train, batch_size):
                # ========== Mixupçš„æ ¸å¿ƒæ­¥éª¤ ==========

                # æ­¥éª¤1: åº”ç”¨Mixupå¢å¼º
                # mix_batchä¼š:
                #   - æ‰“ä¹±batchå†…çš„æ ·æœ¬é¡ºåº
                #   - å°†åŸæ ·æœ¬ä¸æ‰“ä¹±åçš„æ ·æœ¬æŒ‰Î»æ··åˆ
                #   - Î»ä»Beta(alpha, alpha)åˆ†å¸ƒé‡‡æ ·
                mixed_X, mixed_y, lam = mixup.mix_batch(batch_X, batch_y)

                # æ­¥éª¤2: å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨æ··åˆåçš„å›¾åƒï¼‰
                y_pred = model.forward(mixed_X)

                # æ­¥éª¤3: è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨è½¯æ ‡ç­¾ï¼ï¼‰
                # è¿™é‡Œmixed_yæ˜¯è½¯æ ‡ç­¾ï¼Œå¦‚[0.7, 0, 0.3, 0, ...]
                loss = soft_cross_entropy(y_pred, mixed_y)
                epoch_losses.append(loss)

                # æ­¥éª¤4: åå‘ä¼ æ’­ï¼ˆä½¿ç”¨è½¯æ ‡ç­¾è®¡ç®—æ¢¯åº¦ï¼‰
                # äº¤å‰ç†µ+Softmaxçš„æ¢¯åº¦å…¬å¼å¯¹è½¯æ ‡ç­¾åŒæ ·é€‚ç”¨
                model.backward(mixed_y)

                # ç»Ÿè®¡å‡†ç¡®ç‡ï¼ˆç”¨åŸå§‹æ ‡ç­¾è¯„ä¼°ï¼‰
                pred_labels = np.argmax(y_pred, axis=1)
                true_labels = np.argmax(batch_y, axis=1)  # ä½¿ç”¨åŸå§‹æ ‡ç­¾
                epoch_correct += np.sum(pred_labels == true_labels)
                epoch_total += len(batch_y)

            # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
            train_loss = np.mean(epoch_losses)
            train_acc = epoch_correct / epoch_total

            # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
            val_pred = model.forward(X_val)
            val_loss = soft_cross_entropy(val_pred, y_val)
            val_acc = accuracy(val_pred, y_val)

            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)

            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch + 1:3d}/{epochs}: "
                      f"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, "
                      f"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}")

        return history

    def train_without_mixup(model, X_train, y_train, X_val, y_val,
                            epochs=50, batch_size=32):
        """
        æ ‡å‡†è®­ç»ƒï¼ˆä¸ä½¿ç”¨Mixupï¼‰- ä½œä¸ºå¯¹ç…§ç»„

        ä¸Mixupè®­ç»ƒçš„åŒºåˆ«:
        - ä¸æ··åˆå›¾åƒ
        - ä½¿ç”¨ç¡¬æ ‡ç­¾ï¼ˆone-hotï¼‰
        - å…¶ä»–å®Œå…¨ç›¸åŒ

        å‚æ•°å’Œè¿”å›å€¼ä¸train_with_mixupç›¸åŒ
        """
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }

        for epoch in range(epochs):
            epoch_losses = []
            epoch_correct = 0
            epoch_total = 0

            for batch_X, batch_y in create_batches(X_train, y_train, batch_size):
                # æ ‡å‡†è®­ç»ƒï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
                y_pred = model.forward(batch_X)
                loss = soft_cross_entropy(y_pred, batch_y)  # batch_yæ˜¯ç¡¬æ ‡ç­¾(one-hot)
                epoch_losses.append(loss)

                model.backward(batch_y)

                pred_labels = np.argmax(y_pred, axis=1)
                true_labels = np.argmax(batch_y, axis=1)
                epoch_correct += np.sum(pred_labels == true_labels)
                epoch_total += len(batch_y)

            train_loss = np.mean(epoch_losses)
            train_acc = epoch_correct / epoch_total

            val_pred = model.forward(X_val)
            val_loss = soft_cross_entropy(val_pred, y_val)
            val_acc = accuracy(val_pred, y_val)

            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)

            if (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch + 1:3d}/{epochs}: "
                      f"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, "
                      f"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}")

        return history

    print("è®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆ")
    print("  - train_with_mixup: ä½¿ç”¨Mixupå¢å¼ºè®­ç»ƒ")
    print("  - train_without_mixup: æ ‡å‡†è®­ç»ƒï¼ˆå¯¹ç…§ç»„ï¼‰")

    # =========================================================================
    # ç¬¬5éƒ¨åˆ†: å¯¹æ¯”å®éªŒ
    # =========================================================================
    print("\nğŸ”¬ ç¬¬5éƒ¨åˆ†: å¯¹æ¯”å®éªŒ")
    print("-" * 50)

    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤
    np.random.seed(42)

    # åˆ›å»ºä¸¤ä¸ªç›¸åŒåˆå§‹åŒ–çš„æ¨¡å‹
    # è¿™æ ·æˆ‘ä»¬å¯ä»¥å…¬å¹³åœ°å¯¹æ¯”Mixupçš„æ•ˆæœ
    model_baseline = SimpleMLP(input_size=64, hidden_size=128, output_size=10, learning_rate=0.1)
    model_mixup = model_baseline.copy()

    # è®­ç»ƒå‚æ•°
    epochs = 100
    batch_size = 32
    mixup_alpha = 0.4  # Mixupçš„Î±å‚æ•°

    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  - Epochs: {epochs}")
    print(f"  - Batch size: {batch_size}")
    print(f"  - Learning rate: 0.1")
    print(f"  - Mixup Î±: {mixup_alpha}")

    # è®­ç»ƒæ ‡å‡†æ¨¡å‹ï¼ˆæ— Mixupï¼‰
    print(f"\nğŸ“Š è®­ç»ƒæ ‡å‡†æ¨¡å‹ï¼ˆæ— Mixupï¼‰...")
    history_baseline = train_without_mixup(
        model_baseline, X_train, y_train_onehot, X_val, y_val_onehot,
        epochs=epochs, batch_size=batch_size
    )

    # è®­ç»ƒMixupæ¨¡å‹
    print(f"\nğŸ“Š è®­ç»ƒMixupæ¨¡å‹ï¼ˆÎ±={mixup_alpha}ï¼‰...")
    history_mixup = train_with_mixup(
        model_mixup, X_train, y_train_onehot, X_val, y_val_onehot,
        mixup_alpha=mixup_alpha, epochs=epochs, batch_size=batch_size
    )

    # =========================================================================
    # ç¬¬6éƒ¨åˆ†: æµ‹è¯•é›†è¯„ä¼°
    # =========================================================================
    print("\nğŸ“ˆ ç¬¬6éƒ¨åˆ†: æµ‹è¯•é›†è¯„ä¼°")
    print("-" * 50)

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_pred_baseline = model_baseline.forward(X_test)
    test_acc_baseline = accuracy(test_pred_baseline, y_test_onehot)

    test_pred_mixup = model_mixup.forward(X_test)
    test_acc_mixup = accuracy(test_pred_mixup, y_test_onehot)

    print(f"\næµ‹è¯•é›†å‡†ç¡®ç‡:")
    print(f"  - æ ‡å‡†è®­ç»ƒ: {test_acc_baseline:.4f} ({test_acc_baseline * 100:.2f}%)")
    print(f"  - Mixupè®­ç»ƒ: {test_acc_mixup:.4f} ({test_acc_mixup * 100:.2f}%)")
    print(f"  - æå‡: {(test_acc_mixup - test_acc_baseline) * 100:+.2f}%")

    # =========================================================================
    # ç¬¬7éƒ¨åˆ†: å¯è§†åŒ–å¯¹æ¯”
    # =========================================================================
    print("\nğŸ“Š ç¬¬7éƒ¨åˆ†: å¯è§†åŒ–å¯¹æ¯”")
    print("-" * 50)

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    epochs_range = range(1, epochs + 1)

    # å­å›¾1: è®­ç»ƒæŸå¤±å¯¹æ¯”
    axes[0, 0].plot(epochs_range, history_baseline['train_loss'],
                    label='æ ‡å‡†è®­ç»ƒ', color='blue', linewidth=2)
    axes[0, 0].plot(epochs_range, history_mixup['train_loss'],
                    label='Mixupè®­ç»ƒ', color='red', linewidth=2)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('æŸå¤±')
    axes[0, 0].set_title('è®­ç»ƒæŸå¤±å¯¹æ¯”')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # å­å›¾2: éªŒè¯æŸå¤±å¯¹æ¯”
    axes[0, 1].plot(epochs_range, history_baseline['val_loss'],
                    label='æ ‡å‡†è®­ç»ƒ', color='blue', linewidth=2)
    axes[0, 1].plot(epochs_range, history_mixup['val_loss'],
                    label='Mixupè®­ç»ƒ', color='red', linewidth=2)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('æŸå¤±')
    axes[0, 1].set_title('éªŒè¯æŸå¤±å¯¹æ¯”')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # å­å›¾3: è®­ç»ƒå‡†ç¡®ç‡å¯¹æ¯”
    axes[1, 0].plot(epochs_range, history_baseline['train_acc'],
                    label='æ ‡å‡†è®­ç»ƒ', color='blue', linewidth=2)
    axes[1, 0].plot(epochs_range, history_mixup['train_acc'],
                    label='Mixupè®­ç»ƒ', color='red', linewidth=2)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('å‡†ç¡®ç‡')
    axes[1, 0].set_title('è®­ç»ƒå‡†ç¡®ç‡å¯¹æ¯”')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # å­å›¾4: éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”
    axes[1, 1].plot(epochs_range, history_baseline['val_acc'],
                    label='æ ‡å‡†è®­ç»ƒ', color='blue', linewidth=2)
    axes[1, 1].plot(epochs_range, history_mixup['val_acc'],
                    label='Mixupè®­ç»ƒ', color='red', linewidth=2)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('å‡†ç¡®ç‡')
    axes[1, 1].set_title('éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.suptitle('Mixupæ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

    # è¿‡æ‹Ÿåˆåˆ†æå›¾
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # æ ‡å‡†è®­ç»ƒçš„è¿‡æ‹Ÿåˆåˆ†æ
    axes[0].plot(epochs_range, history_baseline['train_acc'],
                 label='è®­ç»ƒå‡†ç¡®ç‡', color='blue', linewidth=2)
    axes[0].plot(epochs_range, history_baseline['val_acc'],
                 label='éªŒè¯å‡†ç¡®ç‡', color='blue', linestyle='--', linewidth=2)
    axes[0].fill_between(epochs_range,
                         history_baseline['train_acc'],
                         history_baseline['val_acc'],
                         alpha=0.3, color='blue', label='è¿‡æ‹Ÿåˆå·®è·')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('å‡†ç¡®ç‡')
    axes[0].set_title('æ ‡å‡†è®­ç»ƒ - è¿‡æ‹Ÿåˆåˆ†æ')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Mixupè®­ç»ƒçš„è¿‡æ‹Ÿåˆåˆ†æ
    axes[1].plot(epochs_range, history_mixup['train_acc'],
                 label='è®­ç»ƒå‡†ç¡®ç‡', color='red', linewidth=2)
    axes[1].plot(epochs_range, history_mixup['val_acc'],
                 label='éªŒè¯å‡†ç¡®ç‡', color='red', linestyle='--', linewidth=2)
    axes[1].fill_between(epochs_range,
                         history_mixup['train_acc'],
                         history_mixup['val_acc'],
                         alpha=0.3, color='red', label='è¿‡æ‹Ÿåˆå·®è·')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('å‡†ç¡®ç‡')
    axes[1].set_title('Mixupè®­ç»ƒ - è¿‡æ‹Ÿåˆåˆ†æ')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.suptitle('Mixupçš„æ­£åˆ™åŒ–æ•ˆæœåˆ†æ', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

    # =========================================================================
    # ç¬¬8éƒ¨åˆ†: ç»“æœæ€»ç»“
    # =========================================================================
    print("\n" + "=" * 70)
    print("ğŸ“‹ å®éªŒç»“è®º")
    print("=" * 70)

    # è®¡ç®—è¿‡æ‹Ÿåˆç¨‹åº¦ï¼ˆè®­ç»ƒ-éªŒè¯å‡†ç¡®ç‡å·®ï¼‰
    overfit_baseline = history_baseline['train_acc'][-1] - history_baseline['val_acc'][-1]
    overfit_mixup = history_mixup['train_acc'][-1] - history_mixup['val_acc'][-1]

    print(f"\n1. æµ‹è¯•é›†å‡†ç¡®ç‡:")
    print(f"   - æ ‡å‡†è®­ç»ƒ: {test_acc_baseline * 100:.2f}%")
    print(f"   - Mixupè®­ç»ƒ: {test_acc_mixup * 100:.2f}%")

    print(f"\n2. è¿‡æ‹Ÿåˆç¨‹åº¦ï¼ˆè®­ç»ƒ-éªŒè¯å‡†ç¡®ç‡å·®ï¼‰:")
    print(f"   - æ ‡å‡†è®­ç»ƒ: {overfit_baseline * 100:.2f}%")
    print(f"   - Mixupè®­ç»ƒ: {overfit_mixup * 100:.2f}%")

    print(f"\n3. Mixupçš„æ­£åˆ™åŒ–æ•ˆæœ:")
    if test_acc_mixup > test_acc_baseline:
        print(f"   âœ“ Mixupæå‡äº†æµ‹è¯•å‡†ç¡®ç‡ {(test_acc_mixup - test_acc_baseline) * 100:.2f}%")
    if overfit_mixup < overfit_baseline:
        print(f"   âœ“ Mixupå‡å°‘äº†è¿‡æ‹Ÿåˆç¨‹åº¦ {(overfit_baseline - overfit_mixup) * 100:.2f}%")

    print(f"\n4. å…³é”®æ´å¯Ÿ:")
    print("   - Mixupé€šè¿‡æ··åˆæ ·æœ¬åˆ›å»ºè™šæ‹Ÿè®­ç»ƒæ•°æ®ï¼Œå¢åŠ äº†æ•°æ®å¤šæ ·æ€§")
    print("   - è½¯æ ‡ç­¾è®­ç»ƒé¼“åŠ±æ¨¡å‹äº§ç”Ÿæ›´å¹³æ»‘çš„å†³ç­–è¾¹ç•Œ")
    print("   - Mixupæ˜¯ä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºå’Œæ­£åˆ™åŒ–æŠ€æœ¯")
    print("   - é€‚ç”¨äºå„ç§å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå®ç°ç®€å•ï¼Œæ•ˆæœæ˜¾è‘—")

    print("\n" + "=" * 70)
    print("ç»ƒä¹ 3å®Œæˆï¼")
    print("=" * 70)

    return {
        'model_baseline': model_baseline,
        'model_mixup': model_mixup,
        'history_baseline': history_baseline,
        'history_mixup': history_mixup,
        'test_acc_baseline': test_acc_baseline,
        'test_acc_mixup': test_acc_mixup
    }


results = exercise_3_mixup_training()