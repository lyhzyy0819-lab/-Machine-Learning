{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬9ç« ï¼šæ·±åº¦ç½‘ç»œè®­ç»ƒæŠ€å·§ä¸è¯Šæ–­\n",
    "\n",
    "> **å­¦ä¹ ç›®æ ‡**\n",
    "> - æŒæ¡å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼ˆStepã€Exponentialã€Cosineã€Warm Restartï¼‰\n",
    "> - ç†è§£æ¢¯åº¦è£å‰ªçš„åŸç†å’Œå®ç°\n",
    "> - å­¦ä¹ æ¢¯åº¦ç´¯ç§¯æŠ€æœ¯ï¼ˆå°æ‰¹é‡æ¨¡æ‹Ÿå¤§æ‰¹é‡ï¼‰\n",
    "> - æŒæ¡è®­ç»ƒè¯Šæ–­æ–¹æ³•ï¼ˆå­¦ä¹ æ›²çº¿ã€è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆè¯†åˆ«ï¼‰\n",
    "> - å­¦ä¼šç³»ç»ŸåŒ–çš„è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç« å†…å®¹\n",
    "\n",
    "æ·±åº¦ç½‘ç»œè®­ç»ƒæ˜¯ä¸€é—¨**è‰ºæœ¯+ç§‘å­¦**çš„ç»“åˆï¼š\n",
    "\n",
    "### 1ï¸âƒ£ å­¦ä¹ ç‡è°ƒåº¦ï¼ˆLearning Rate Schedulingï¼‰\n",
    "- **ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ** å›ºå®šå­¦ä¹ ç‡éš¾ä»¥å¹³è¡¡æ¢ç´¢å’Œç²¾è°ƒ\n",
    "- **æ ¸å¿ƒç­–ç•¥ï¼š**\n",
    "  - Step Decayï¼šé˜¶æ®µæ€§é™ä½å­¦ä¹ ç‡\n",
    "  - Exponential Decayï¼šæŒ‡æ•°è¡°å‡\n",
    "  - Cosine Annealingï¼šä½™å¼¦é€€ç«ï¼ˆå¹³æ»‘ä¸‹é™ï¼‰\n",
    "  - Warm Restartï¼šå‘¨æœŸæ€§é‡å¯ï¼ˆè·³å‡ºå±€éƒ¨æœ€ä¼˜ï¼‰\n",
    "\n",
    "### 2ï¸âƒ£ æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰\n",
    "- **é—®é¢˜ï¼š** æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´è®­ç»ƒä¸ç¨³å®š\n",
    "- **è§£å†³æ–¹æ¡ˆï¼š**\n",
    "  - Norm Clippingï¼šé™åˆ¶æ¢¯åº¦çš„L2èŒƒæ•°\n",
    "  - Value Clippingï¼šé™åˆ¶æ¢¯åº¦çš„ç»å¯¹å€¼\n",
    "\n",
    "### 3ï¸âƒ£ æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰\n",
    "- **åº”ç”¨åœºæ™¯ï¼š** å†…å­˜ä¸è¶³æ—¶æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒ\n",
    "- **åŸç†ï¼š** å¤šä¸ªå°æ‰¹é‡ç´¯ç§¯æ¢¯åº¦åå†æ›´æ–°\n",
    "\n",
    "### 4ï¸âƒ£ è®­ç»ƒè¯Šæ–­ï¼ˆTraining Diagnosticsï¼‰\n",
    "- **å­¦ä¹ æ›²çº¿åˆ†æ**\n",
    "- **è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆè¯†åˆ«**\n",
    "- **å†³ç­–æ ‘å¼é—®é¢˜è¯Šæ–­**\n",
    "\n",
    "### 5ï¸âƒ£ è¶…å‚æ•°è°ƒä¼˜ï¼ˆHyperparameter Tuningï¼‰\n",
    "- ç³»ç»ŸåŒ–è°ƒå‚ç­–ç•¥\n",
    "- å¯¹æ¯”å®éªŒè®¾è®¡\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict, Callable\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤ºå’Œæ ·å¼\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 1ï¸âƒ£ å­¦ä¹ ç‡è°ƒåº¦ï¼ˆLearning Rate Schedulingï¼‰\n\n### ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ ç‡è°ƒåº¦ï¼Ÿ\n\n**å›ºå®šå­¦ä¹ ç‡çš„é—®é¢˜ï¼š**\n\n1. **åˆæœŸè®­ç»ƒ**ï¼šéœ€è¦è¾ƒå¤§å­¦ä¹ ç‡å¿«é€Ÿä¸‹é™loss\n2. **åæœŸç²¾è°ƒ**ï¼šéœ€è¦è¾ƒå°å­¦ä¹ ç‡ç²¾ç»†ä¼˜åŒ–ï¼Œé¿å…éœ‡è¡\n\n**è§£å†³æ–¹æ¡ˆï¼šåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡**\n\n### ğŸ“Š å¸¸è§è°ƒåº¦ç­–ç•¥å¯¹æ¯”\n\n| ç­–ç•¥ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |\n|------|------|----------|\n| **Step Decay** | é˜¶æ¢¯å¼ä¸‹é™ | è®­ç»ƒepochè¾ƒå°‘ï¼Œéœ€æ˜ç¡®ä¸‹é™ç‚¹ |\n| **Exponential Decay** | æŒ‡æ•°å¹³æ»‘ä¸‹é™ | é•¿æœŸè®­ç»ƒï¼Œå¸Œæœ›å¹³æ»‘è¿‡æ¸¡ |\n| **Cosine Annealing** | ä½™å¼¦æ›²çº¿ä¸‹é™ | å‘¨æœŸæ€§è®­ç»ƒï¼Œé¿å…è¿‡æ—©æ”¶æ•› |\n| **Warm Restart** | å‘¨æœŸæ€§é‡å¯ | è·³å‡ºå±€éƒ¨æœ€ä¼˜ï¼Œæ¢ç´¢æ›´å¤šè§£ |\n\n---\n\n### ğŸ“ æ•°å­¦åŸç†\n\n#### 1. Step Decayï¼ˆé˜¶æ¢¯è¡°å‡ï¼‰\n$$\n\\eta_t = \\eta_0 \\times \\gamma^{\\lfloor \\frac{t}{T} \\rfloor}\n$$\n- $\\eta_0$: åˆå§‹å­¦ä¹ ç‡\n- $\\gamma$: è¡°å‡å› å­ï¼ˆå¦‚0.1ï¼‰\n- $T$: è¡°å‡å‘¨æœŸï¼ˆæ¯Tä¸ªepochè¡°å‡ä¸€æ¬¡ï¼‰\n\n**ç¤ºä¾‹ï¼š**\n- $\\eta_0 = 0.1, \\gamma = 0.1, T = 30$\n- Epoch 0-29: $\\eta = 0.1$\n- Epoch 30-59: $\\eta = 0.01$\n- Epoch 60-89: $\\eta = 0.001$\n\n---\n\n#### 2. Exponential Decayï¼ˆæŒ‡æ•°è¡°å‡ï¼‰\n$$\n\\eta_t = \\eta_0 \\times e^{-\\lambda t}\n$$\n- $\\lambda$: è¡°å‡ç‡ï¼ˆå¦‚0.01ï¼‰\n- $t$: å½“å‰epoch\n\n**ç‰¹ç‚¹ï¼š** å¹³æ»‘è¿ç»­ä¸‹é™\n\n---\n\n#### 3. Cosine Annealingï¼ˆä½™å¼¦é€€ç«ï¼‰\n$$\n\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min}) \\left(1 + \\cos\\left(\\frac{t}{T_{max}} \\pi\\right)\\right)\n$$\n- $\\eta_{max}$: æœ€å¤§å­¦ä¹ ç‡\n- $\\eta_{min}$: æœ€å°å­¦ä¹ ç‡\n- $T_{max}$: å‘¨æœŸé•¿åº¦\n\n**ç‰¹ç‚¹ï¼š** ä» $\\eta_{max}$ å¹³æ»‘ä¸‹é™åˆ° $\\eta_{min}$ï¼Œç±»ä¼¼ä½™å¼¦æ›²çº¿\n\n---\n\n#### 4. Warm Restartï¼ˆå¸¦é‡å¯çš„ä½™å¼¦é€€ç«ï¼‰\n$$\n\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min}) \\left(1 + \\cos\\left(\\frac{t_{epoch}}{T_i} \\pi\\right)\\right)\n$$\n- $t_{epoch}$: å½“å‰å‘¨æœŸå†…çš„epoch\n- $T_i$: ç¬¬iä¸ªå‘¨æœŸçš„é•¿åº¦ï¼ˆå¯é€’å¢ï¼š$T_i = T_0 \\times T_{mult}^i$ï¼‰\n\n**ç‰¹ç‚¹ï¼š** å‘¨æœŸæ€§é‡å¯åˆ° $\\eta_{max}$ï¼Œå¸®åŠ©è·³å‡ºå±€éƒ¨æœ€ä¼˜\n\n---\n\n### ğŸ’» ä»é›¶å®ç°ï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class LearningRateScheduler:\n    \"\"\"\n    å­¦ä¹ ç‡è°ƒåº¦å™¨åŸºç±»\n    \n    æ‰€æœ‰è°ƒåº¦å™¨éƒ½ç»§æ‰¿æ­¤ç±»ï¼Œå®ç° step() æ–¹æ³•æ¥è®¡ç®—å½“å‰epochçš„å­¦ä¹ ç‡\n    \"\"\"\n    \n    def __init__(self, initial_lr: float):\n        \"\"\"\n        å‚æ•°:\n            initial_lr: åˆå§‹å­¦ä¹ ç‡\n        \"\"\"\n        self.initial_lr = initial_lr\n        self.current_lr = initial_lr\n        self.epoch = 0\n        self.history = []  # è®°å½•å­¦ä¹ ç‡å†å²\n    \n    def step(self) -> float:\n        \"\"\"\n        è®¡ç®—å½“å‰epochçš„å­¦ä¹ ç‡ï¼ˆå­ç±»å®ç°ï¼‰\n        \n        è¿”å›:\n            å½“å‰å­¦ä¹ ç‡\n        \"\"\"\n        raise NotImplementedError\n    \n    def get_lr(self) -> float:\n        \"\"\"è·å–å½“å‰å­¦ä¹ ç‡\"\"\"\n        return self.current_lr\n    \n    def update(self) -> float:\n        \"\"\"\n        æ›´æ–°å­¦ä¹ ç‡ï¼ˆæ¯ä¸ªepochè°ƒç”¨ä¸€æ¬¡ï¼‰\n        \n        è¿”å›:\n            æ›´æ–°åçš„å­¦ä¹ ç‡\n        \"\"\"\n        self.current_lr = self.step()\n        self.history.append(self.current_lr)\n        self.epoch += 1\n        return self.current_lr\n\n\n# ===== 1. Step Decay =====\nclass StepDecayScheduler(LearningRateScheduler):\n    \"\"\"\n    é˜¶æ¢¯è¡°å‡è°ƒåº¦å™¨\n    \n    å…¬å¼: Î·_t = Î·_0 Ã— Î³^(âŒŠt/TâŒ‹)\n    æ¯Tä¸ªepochè¡°å‡ä¸€æ¬¡\n    \"\"\"\n    \n    def __init__(self, initial_lr: float, decay_rate: float = 0.1, decay_steps: int = 30):\n        \"\"\"\n        å‚æ•°:\n            initial_lr: åˆå§‹å­¦ä¹ ç‡ (Î·_0)\n            decay_rate: è¡°å‡ç‡ (Î³)ï¼Œé€šå¸¸ä¸º0.1\n            decay_steps: è¡°å‡æ­¥æ•° (T)ï¼Œæ¯Tä¸ªepochè¡°å‡ä¸€æ¬¡\n        \"\"\"\n        super().__init__(initial_lr)\n        self.decay_rate = decay_rate\n        self.decay_steps = decay_steps\n    \n    def step(self) -> float:\n        \"\"\"\n        è®¡ç®—å½“å‰å­¦ä¹ ç‡\n        \n        ç¤ºä¾‹:\n            initial_lr=0.1, decay_rate=0.1, decay_steps=30\n            Epoch 0-29:  Î· = 0.1 Ã— 0.1^0 = 0.1\n            Epoch 30-59: Î· = 0.1 Ã— 0.1^1 = 0.01\n            Epoch 60-89: Î· = 0.1 Ã— 0.1^2 = 0.001\n        \"\"\"\n        # âŒŠt/TâŒ‹: å½“å‰åœ¨ç¬¬å‡ ä¸ªè¡°å‡å‘¨æœŸ\n        decay_factor = self.epoch // self.decay_steps\n        return self.initial_lr * (self.decay_rate ** decay_factor)\n\n\n# ===== 2. Exponential Decay =====\nclass ExponentialDecayScheduler(LearningRateScheduler):\n    \"\"\"\n    æŒ‡æ•°è¡°å‡è°ƒåº¦å™¨\n    \n    å…¬å¼: Î·_t = Î·_0 Ã— e^(-Î»t)\n    å¹³æ»‘è¿ç»­åœ°è¡°å‡\n    \"\"\"\n    \n    def __init__(self, initial_lr: float, decay_rate: float = 0.05):\n        \"\"\"\n        å‚æ•°:\n            initial_lr: åˆå§‹å­¦ä¹ ç‡ (Î·_0)\n            decay_rate: è¡°å‡ç‡ (Î»)ï¼Œæ§åˆ¶è¡°å‡é€Ÿåº¦\n        \"\"\"\n        super().__init__(initial_lr)\n        self.decay_rate = decay_rate\n    \n    def step(self) -> float:\n        \"\"\"\n        è®¡ç®—å½“å‰å­¦ä¹ ç‡\n        \n        ç¤ºä¾‹:\n            initial_lr=0.1, decay_rate=0.05\n            Epoch 0:  Î· = 0.1 Ã— e^(-0.05Ã—0) = 0.100\n            Epoch 10: Î· = 0.1 Ã— e^(-0.05Ã—10) = 0.061\n            Epoch 20: Î· = 0.1 Ã— e^(-0.05Ã—20) = 0.037\n        \"\"\"\n        return self.initial_lr * np.exp(-self.decay_rate * self.epoch)\n\n\n# ===== 3. Cosine Annealing =====\nclass CosineAnnealingScheduler(LearningRateScheduler):\n    \"\"\"\n    ä½™å¼¦é€€ç«è°ƒåº¦å™¨\n    \n    å…¬å¼: Î·_t = Î·_min + 0.5(Î·_max - Î·_min)(1 + cos(t/T_max Ã— Ï€))\n    ä»Î·_maxå¹³æ»‘ä¸‹é™åˆ°Î·_min\n    \"\"\"\n    \n    def __init__(self, initial_lr: float, min_lr: float = 0.0, T_max: int = 100):\n        \"\"\"\n        å‚æ•°:\n            initial_lr: æœ€å¤§å­¦ä¹ ç‡ (Î·_max)\n            min_lr: æœ€å°å­¦ä¹ ç‡ (Î·_min)\n            T_max: å‘¨æœŸé•¿åº¦ï¼ˆç»è¿‡T_maxä¸ªepochåé™åˆ°æœ€å°ï¼‰\n        \"\"\"\n        super().__init__(initial_lr)\n        self.max_lr = initial_lr\n        self.min_lr = min_lr\n        self.T_max = T_max\n    \n    def step(self) -> float:\n        \"\"\"\n        è®¡ç®—å½“å‰å­¦ä¹ ç‡\n        \n        ç‰¹ç‚¹:\n        - Epoch 0: Î· = Î·_max (cos(0) = 1)\n        - Epoch T_max/2: Î· = (Î·_max + Î·_min)/2 (cos(Ï€/2) = 0)\n        - Epoch T_max: Î· = Î·_min (cos(Ï€) = -1)\n        \"\"\"\n        # cos(t/T_max Ã— Ï€): ä»1ä¸‹é™åˆ°-1\n        # (1 + cos(...)): ä»2ä¸‹é™åˆ°0\n        # 0.5 Ã— (1 + cos(...)): ä»1ä¸‹é™åˆ°0\n        cosine = np.cos(np.pi * (self.epoch % self.T_max) / self.T_max)\n        return self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + cosine)\n\n\n# ===== 4. Warm Restart (SGDR) =====\nclass WarmRestartScheduler(LearningRateScheduler):\n    \"\"\"\n    å¸¦é‡å¯çš„ä½™å¼¦é€€ç«ï¼ˆStochastic Gradient Descent with Warm Restartsï¼‰\n    \n    å…¬å¼: Î·_t = Î·_min + 0.5(Î·_max - Î·_min)(1 + cos(t_epoch/T_i Ã— Ï€))\n    å‘¨æœŸæ€§é‡å¯åˆ°Î·_maxï¼Œå‘¨æœŸé•¿åº¦å¯é€’å¢\n    \"\"\"\n    \n    def __init__(self, initial_lr: float, min_lr: float = 0.0, \n                 T_0: int = 10, T_mult: float = 2.0):\n        \"\"\"\n        å‚æ•°:\n            initial_lr: æœ€å¤§å­¦ä¹ ç‡ (Î·_max)\n            min_lr: æœ€å°å­¦ä¹ ç‡ (Î·_min)\n            T_0: ç¬¬ä¸€ä¸ªå‘¨æœŸçš„é•¿åº¦\n            T_mult: å‘¨æœŸé•¿åº¦çš„ä¹˜æ•°ï¼ˆ>1åˆ™å‘¨æœŸé€’å¢ï¼Œ=1åˆ™å‘¨æœŸå›ºå®šï¼‰\n        \n        ç¤ºä¾‹:\n            T_0=10, T_mult=2\n            ç¬¬1ä¸ªå‘¨æœŸ: Epoch 0-9 (é•¿åº¦10)\n            ç¬¬2ä¸ªå‘¨æœŸ: Epoch 10-29 (é•¿åº¦20)\n            ç¬¬3ä¸ªå‘¨æœŸ: Epoch 30-69 (é•¿åº¦40)\n        \"\"\"\n        super().__init__(initial_lr)\n        self.max_lr = initial_lr\n        self.min_lr = min_lr\n        self.T_0 = T_0\n        self.T_mult = T_mult\n        \n        self.T_current = T_0  # å½“å‰å‘¨æœŸé•¿åº¦\n        self.epoch_in_cycle = 0  # å½“å‰å‘¨æœŸå†…çš„epoch\n        self.cycle_count = 0  # å·²å®Œæˆçš„å‘¨æœŸæ•°\n    \n    def step(self) -> float:\n        \"\"\"\n        è®¡ç®—å½“å‰å­¦ä¹ ç‡\n        \n        ç‰¹ç‚¹:\n        - æ¯ä¸ªå‘¨æœŸå¼€å§‹: Î· = Î·_max\n        - å‘¨æœŸä¸­é—´: Î·é€æ¸ä¸‹é™\n        - å‘¨æœŸç»“æŸ: Î· = Î·_minï¼Œç„¶åé‡å¯åˆ°Î·_max\n        \"\"\"\n        # åœ¨å½“å‰å‘¨æœŸå†…ä½¿ç”¨ä½™å¼¦é€€ç«\n        cosine = np.cos(np.pi * self.epoch_in_cycle / self.T_current)\n        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + cosine)\n        \n        # æ›´æ–°å‘¨æœŸä¿¡æ¯\n        self.epoch_in_cycle += 1\n        \n        # å¦‚æœå½“å‰å‘¨æœŸç»“æŸï¼Œé‡å¯åˆ°ä¸‹ä¸€ä¸ªå‘¨æœŸ\n        if self.epoch_in_cycle >= self.T_current:\n            self.cycle_count += 1\n            self.epoch_in_cycle = 0\n            # ä¸‹ä¸€ä¸ªå‘¨æœŸçš„é•¿åº¦\n            self.T_current = int(self.T_0 * (self.T_mult ** self.cycle_count))\n        \n        return lr\n\n\nprint(\"âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨å®ç°å®Œæˆ\")\nprint(\"   - StepDecayScheduler: é˜¶æ¢¯è¡°å‡\")\nprint(\"   - ExponentialDecayScheduler: æŒ‡æ•°è¡°å‡\")\nprint(\"   - CosineAnnealingScheduler: ä½™å¼¦é€€ç«\")\nprint(\"   - WarmRestartScheduler: å¸¦é‡å¯çš„ä½™å¼¦é€€ç«\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ“ ç»¼åˆæ€»ç»“\n\n### ğŸ¯ æ ¸å¿ƒæŠ€å·§é€ŸæŸ¥è¡¨\n\n| æŠ€å·§ | é—®é¢˜ | è§£å†³æ–¹æ¡ˆ | ä½•æ—¶ä½¿ç”¨ |\n|------|------|----------|----------|\n| **å­¦ä¹ ç‡è°ƒåº¦** | å­¦ä¹ ç‡å›ºå®šå¯¼è‡´æ”¶æ•›æ…¢æˆ–éœ‡è¡ | Step/Exponential/Cosine/Warm Restart | å‡ ä¹æ‰€æœ‰è®­ç»ƒä»»åŠ¡ |\n| **æ¢¯åº¦è£å‰ª** | æ¢¯åº¦çˆ†ç‚¸ï¼Œlosså˜NaN | Norm Clipping | RNNã€æ·±å±‚ç½‘ç»œ |\n| **æ¢¯åº¦ç´¯ç§¯** | å†…å­˜ä¸è¶³ï¼Œæ— æ³•ç”¨å¤§æ‰¹é‡ | ç´¯ç§¯å¤šä¸ªå°æ‰¹é‡æ¢¯åº¦ | GPUå†…å­˜å—é™æ—¶ |\n| **è®­ç»ƒè¯Šæ–­** | ä¸çŸ¥é“è®­ç»ƒå“ªé‡Œå‡ºé—®é¢˜ | å­¦ä¹ æ›²çº¿åˆ†æã€å†³ç­–æ ‘ | è°ƒè¯•è®­ç»ƒé—®é¢˜ |\n\n---\n\n### ğŸš€ å®æˆ˜å»ºè®®\n\n#### 1ï¸âƒ£ åˆå§‹åŒ–è®­ç»ƒæµç¨‹ï¼ˆæ¨èé…ç½®ï¼‰\n\n```python\n# å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä¼˜å…ˆé€‰æ‹©ï¼‰\nscheduler = CosineAnnealingScheduler(initial_lr=0.001, min_lr=1e-6, T_max=100)\n# æˆ–è€…\nscheduler = WarmRestartScheduler(initial_lr=0.001, T_0=20, T_mult=2)\n\n# æ¢¯åº¦è£å‰ªï¼ˆå¦‚æœè®­ç»ƒä¸ç¨³å®šï¼‰\nif gradient_exploding:\n    gradients, _ = clip_gradient_norm(gradients, max_norm=1.0)\n\n# è®­ç»ƒè¯Šæ–­ï¼ˆæ¯Nä¸ªepochæ£€æŸ¥ä¸€æ¬¡ï¼‰\nif epoch % 10 == 0:\n    diagnose_training(train_loss_history, val_loss_history)\n```\n\n---\n\n#### 2ï¸âƒ£ è¶…å‚æ•°è°ƒä¼˜é¡ºåºï¼ˆé‡è¦æ€§ä»é«˜åˆ°ä½ï¼‰\n\n1. **å­¦ä¹ ç‡** â­â­â­â­â­ï¼ˆæœ€é‡è¦ï¼ï¼‰\n   - å…ˆå°è¯•ï¼š0.1, 0.01, 0.001, 0.0001\n   - ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦é€æ¸é™ä½\n\n2. **æ‰¹é‡å¤§å°** â­â­â­â­\n   - å¸¸è§å€¼ï¼š32, 64, 128, 256\n   - å†…å­˜ä¸å¤Ÿï¼Ÿç”¨æ¢¯åº¦ç´¯ç§¯\n\n3. **ç½‘ç»œç»“æ„** â­â­â­\n   - å±‚æ•°ã€ç¥ç»å…ƒæ•°\n   - æ ¹æ®æ¬ æ‹Ÿåˆ/è¿‡æ‹Ÿåˆè°ƒæ•´\n\n4. **æ­£åˆ™åŒ–** â­â­\n   - L2æ­£åˆ™åŒ–ç³»æ•°ã€Dropoutç‡\n   - å…ˆè®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå†åŠ æ­£åˆ™åŒ–\n\n5. **ä¼˜åŒ–å™¨** â­\n   - Adamï¼ˆä¸‡èƒ½é€‰æ‹©ï¼‰\n   - SGD+Momentumï¼ˆç²¾è°ƒç”¨ï¼‰\n\n---\n\n#### 3ï¸âƒ£ è®­ç»ƒé—®é¢˜å¿«é€Ÿè¯Šæ–­\n\n```\nLossæ˜¯NaN/Infï¼Ÿ\nâ”œâ”€ YES â†’ é™ä½å­¦ä¹ ç‡ + æ¢¯åº¦è£å‰ª + æ£€æŸ¥åˆå§‹åŒ–\nâ””â”€ NO  â†’ ç»§ç»­\n\nè®­ç»ƒLossé«˜ä¸”ä¸é™ï¼Ÿ\nâ”œâ”€ YES â†’ æ¬ æ‹Ÿåˆï¼šå¢åŠ æ¨¡å‹å®¹é‡/è®­ç»ƒæ›´ä¹…/é™ä½æ­£åˆ™åŒ–\nâ””â”€ NO  â†’ ç»§ç»­\n\nè®­ç»ƒLossä½ï¼ŒéªŒè¯Lossé«˜ï¼Ÿ\nâ”œâ”€ YES â†’ è¿‡æ‹Ÿåˆï¼šå¢åŠ æ­£åˆ™åŒ–/Dropout/æ•°æ®å¢å¼º\nâ””â”€ NO  â†’ è®­ç»ƒçŠ¶æ€è‰¯å¥½ï¼\n```\n\n---\n\n### ğŸ’¡ æœ€ä½³å®è·µ\n\n1. **æ°¸è¿œä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦**\n   - æ¨èï¼šCosine Annealing æˆ– Warm Restart\n   - è‡³å°‘ä½¿ç”¨ Step Decay\n\n2. **ç›‘æ§æ¢¯åº¦èŒƒæ•°**\n   - å¦‚æœæ¢¯åº¦èŒƒæ•°çªç„¶å˜å¤§ï¼ˆ>10ï¼‰ï¼Œä½¿ç”¨æ¢¯åº¦è£å‰ª\n\n3. **å®šæœŸè¯Šæ–­**\n   - æ¯10ä¸ªepochæ£€æŸ¥å­¦ä¹ æ›²çº¿\n   - æ ¹æ®è¯Šæ–­ç»“æœåŠæ—¶è°ƒæ•´\n\n4. **å®éªŒè®°å½•**\n   - è®°å½•æ¯æ¬¡å®éªŒçš„è¶…å‚æ•°å’Œç»“æœ\n   - å¯¹æ¯”ä¸åŒé…ç½®çš„æ•ˆæœ\n\n5. **è€å¿ƒè°ƒå‚**\n   - æ·±åº¦å­¦ä¹ è®­ç»ƒæ˜¯è¯•é”™è¿‡ç¨‹\n   - ç³»ç»ŸåŒ–åœ°è°ƒå‚ï¼Œè€Œä¸æ˜¯éšæœºå°è¯•\n\n---\n\n## ğŸ‹ï¸ ç»ƒä¹ é¢˜\n\n### â­â­ ç»ƒä¹ 1ï¼šå®ç°æ¢¯åº¦ç´¯ç§¯\n\n**ä»»åŠ¡ï¼š** å®ç°ä¸€ä¸ªç®€å•çš„è®­ç»ƒå¾ªç¯ï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒã€‚\n\n**è¦æ±‚ï¼š**\n- å°æ‰¹é‡å¤§å°ï¼š16\n- ç´¯ç§¯æ­¥æ•°ï¼š4ï¼ˆæ¨¡æ‹Ÿbatch_size=64ï¼‰\n- åœ¨ç´¯ç§¯4ä¸ªæ‰¹æ¬¡åå†æ›´æ–°å‚æ•°\n\n**æç¤ºï¼š**\n```python\n# ä¼ªä»£ç \nfor epoch in epochs:\n    accumulated_gradients = [0, 0, ...]\n    for i, mini_batch in enumerate(data_loader):\n        # 1. å‰å‘ä¼ æ’­\n        # 2. è®¡ç®—æ¢¯åº¦\n        # 3. ç´¯ç§¯æ¢¯åº¦\n        accumulated_gradients += gradients\n        \n        # 4. æ¯accumulation_stepsä¸ªæ‰¹æ¬¡æ›´æ–°ä¸€æ¬¡\n        if (i + 1) % accumulation_steps == 0:\n            # å¹³å‡æ¢¯åº¦\n            # æ›´æ–°å‚æ•°\n            # æ¸…é›¶ç´¯ç§¯æ¢¯åº¦\n```\n\n---\n\n### â­â­â­ ç»ƒä¹ 2ï¼šå¯¹æ¯”ä¸åŒå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥\n\n**ä»»åŠ¡ï¼š** åœ¨MNISTæ•°æ®é›†ä¸Šè®­ç»ƒåŒä¸€ä¸ªç½‘ç»œï¼Œå¯¹æ¯”4ç§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥çš„æ•ˆæœã€‚\n\n**è¦æ±‚ï¼š**\n1. ä½¿ç”¨ç›¸åŒçš„ç½‘ç»œæ¶æ„\n2. å¯¹æ¯”ï¼šFixed LRã€Step Decayã€Cosine Annealingã€Warm Restart\n3. ç»˜åˆ¶lossæ›²çº¿å¯¹æ¯”å›¾\n4. åˆ†æå“ªç§ç­–ç•¥æ”¶æ•›æœ€å¿«ã€æœ€ç»ˆæ•ˆæœæœ€å¥½\n\n---\n\n### â­â­â­â­ ç»ƒä¹ 3ï¼šå®ç°å®Œæ•´çš„è®­ç»ƒç›‘æ§ç³»ç»Ÿ\n\n**ä»»åŠ¡ï¼š** å®ç°ä¸€ä¸ªè®­ç»ƒç›‘æ§ç±»ï¼Œé›†æˆæœ¬ç« æ‰€æœ‰æŠ€æœ¯ã€‚\n\n**è¦æ±‚ï¼š**\n- è‡ªåŠ¨ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦\n- è‡ªåŠ¨æ¢¯åº¦è£å‰ª\n- è‡ªåŠ¨è¯Šæ–­è®­ç»ƒé—®é¢˜\n- æå‰åœæ­¢ï¼ˆEarly Stoppingï¼‰\n- ä¿å­˜æœ€ä½³æ¨¡å‹\n\n**å‚è€ƒæ¡†æ¶ï¼š**\n```python\nclass TrainingMonitor:\n    def __init__(self, model, lr_scheduler, gradient_clip_max_norm=1.0):\n        pass\n    \n    def train_epoch(self, X_train, y_train):\n        # è®­ç»ƒä¸€ä¸ªepoch\n        pass\n    \n    def validate(self, X_val, y_val):\n        # éªŒè¯\n        pass\n    \n    def should_stop(self):\n        # åˆ¤æ–­æ˜¯å¦æå‰åœæ­¢\n        pass\n    \n    def diagnose(self):\n        # è¯Šæ–­è®­ç»ƒçŠ¶æ€\n        pass\n```\n\n---\n\n### â­â­â­â­â­ ç»ƒä¹ 4ï¼šè¶…å‚æ•°ç½‘æ ¼æœç´¢\n\n**ä»»åŠ¡ï¼š** ç³»ç»ŸåŒ–åœ°æœç´¢æœ€ä½³è¶…å‚æ•°ç»„åˆã€‚\n\n**è¦æ±‚ï¼š**\n1. æœç´¢ç©ºé—´ï¼š\n   - å­¦ä¹ ç‡ï¼š[0.1, 0.01, 0.001]\n   - æ‰¹é‡å¤§å°ï¼š[32, 64, 128]\n   - å­¦ä¹ ç‡è°ƒåº¦ï¼š[Step, Cosine]\n   - æ­£åˆ™åŒ–ï¼š[0.0, 0.01, 0.001]\n\n2. å¯¹æ¯ä¸ªç»„åˆè®­ç»ƒ10ä¸ªepoch\n3. è®°å½•æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡\n4. æ‰¾å‡ºæœ€ä½³é…ç½®\n\n---\n\n## ğŸ“ å­¦ä¹ æ£€æŸ¥ç‚¹\n\nå®Œæˆæœ¬ç« åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š\n\n- [ ] **ç†è§£å¹¶å®ç°** 4ç§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥\n- [ ] **æŒæ¡æ¢¯åº¦è£å‰ª** çš„åŸç†å’Œä¸¤ç§æ–¹æ³•\n- [ ] **å®ç°æ¢¯åº¦ç´¯ç§¯** æ¥æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒ\n- [ ] **ä½¿ç”¨è®­ç»ƒè¯Šæ–­å·¥å…·** è¯†åˆ«è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ\n- [ ] **ç³»ç»ŸåŒ–è°ƒå‚** è€Œä¸æ˜¯éšæœºå°è¯•\n- [ ] **é›†æˆæ‰€æœ‰æŠ€æœ¯** æ„å»ºå®Œæ•´çš„è®­ç»ƒæµç¨‹\n\n---\n\n## ğŸ“š æ‰©å±•é˜…è¯»\n\n### è®ºæ–‡\n\n1. **SGDR: Stochastic Gradient Descent with Warm Restarts**\n   - Loshchilov & Hutter, 2017\n   - Cosine Annealing with Warm Restartsçš„åŸå§‹è®ºæ–‡\n\n2. **On the difficulty of training Recurrent Neural Networks**\n   - Pascanu et al., 2013\n   - æ¢¯åº¦è£å‰ªçš„ç†è®ºåŸºç¡€\n\n3. **Cyclical Learning Rates for Training Neural Networks**\n   - Smith, 2017\n   - å¾ªç¯å­¦ä¹ ç‡ç­–ç•¥\n\n### å®è·µèµ„æº\n\n- **PyTorchå­¦ä¹ ç‡è°ƒåº¦å™¨**: `torch.optim.lr_scheduler`\n- **TensorFlow Callbacks**: è®­ç»ƒç›‘æ§å’Œæå‰åœæ­¢\n- **Weights & Biases**: å®éªŒè¿½è¸ªå·¥å…·\n\n---\n\n**æ­å–œï¼ä½ å·²ç»æŒæ¡äº†æ·±åº¦ç½‘ç»œè®­ç»ƒçš„æ ¸å¿ƒæŠ€å·§ï¼ğŸ‰**\n\nè¿™äº›æŠ€æœ¯æ˜¯æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆçš„å¿…å¤‡å·¥å…·ï¼Œèƒ½å¤Ÿå¸®åŠ©ä½ ï¼š\n- âœ… æ›´å¿«åœ°è®­ç»ƒæ¨¡å‹\n- âœ… é¿å…è®­ç»ƒå´©æºƒ\n- âœ… ç³»ç»ŸåŒ–åœ°è§£å†³è®­ç»ƒé—®é¢˜\n- âœ… æ‰¾åˆ°æœ€ä½³è¶…å‚æ•°é…ç½®\n\n**ä¸‹ä¸€æ­¥ï¼š** ç¬¬10ç« å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨PyTorchå’ŒTensorFlowï¼Œå°†è¿™äº›æŠ€æœ¯åº”ç”¨åˆ°ç”Ÿäº§çº§é¡¹ç›®ä¸­ï¼",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class EarlyStopping:\n    \"\"\"\n    æ—©åœï¼ˆEarly Stoppingï¼‰å®ç°\n    \n    ç›‘æ§éªŒè¯é›†æ€§èƒ½ï¼Œå½“è¿ç»­patienceä¸ªepochæ²¡æœ‰æå‡æ—¶åœæ­¢è®­ç»ƒ\n    \n    ä½¿ç”¨ç¤ºä¾‹:\n    --------\n    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n    \n    for epoch in range(max_epochs):\n        train_loss = train_one_epoch(...)\n        val_loss = validate(...)\n        \n        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢\n        if early_stopping.step(val_loss, model):\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n    \n    # åŠ è½½æœ€ä½³æ¨¡å‹\n    model = early_stopping.load_best_model()\n    \"\"\"\n    \n    def __init__(self, patience: int = 10, min_delta: float = 0.0, \n                 mode: str = 'min', verbose: bool = True):\n        \"\"\"\n        å‚æ•°:\n        -----\n        patience : int\n            å®¹å¿çš„epochæ•°ï¼ˆè¿ç»­patienceä¸ªepochæ²¡æå‡å°±åœæ­¢ï¼‰\n        min_delta : float\n            æœ€å°æ”¹è¿›é‡ï¼ˆå°äºæ­¤å€¼è§†ä¸ºæ²¡æœ‰æ”¹è¿›ï¼‰\n            ä¾‹å¦‚ï¼šval_lossä»0.5é™åˆ°0.499ï¼Œmin_delta=0.002åˆ™è§†ä¸ºæ— æ”¹è¿›\n        mode : str\n            'min'ï¼ˆç›‘æ§lossï¼‰æˆ– 'max'ï¼ˆç›‘æ§accuracyï¼‰\n        verbose : bool\n            æ˜¯å¦æ‰“å°ä¿¡æ¯\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.mode = mode\n        self.verbose = verbose\n        \n        # çŠ¶æ€å˜é‡\n        self.counter = 0  # æ²¡æœ‰æ”¹è¿›çš„epochè®¡æ•°\n        self.best_score = None  # æœ€ä½³éªŒè¯æ€§èƒ½\n        self.best_model = None  # æœ€ä½³æ¨¡å‹ï¼ˆæƒé‡çš„æ·±æ‹·è´ï¼‰\n        self.early_stop = False  # æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ\n        self.val_score_history = []  # éªŒè¯æ€§èƒ½å†å²\n        \n        # æ ¹æ®modeè®¾ç½®æ¯”è¾ƒå‡½æ•°\n        if mode == 'min':\n            self.is_better = lambda new, best: new < best - min_delta\n        elif mode == 'max':\n            self.is_better = lambda new, best: new > best + min_delta\n        else:\n            raise ValueError(f\"mode must be 'min' or 'max', got {mode}\")\n    \n    def step(self, val_score: float, model=None) -> bool:\n        \"\"\"\n        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ\n        \n        å‚æ•°:\n        -----\n        val_score : float\n            å½“å‰epochçš„éªŒè¯æ€§èƒ½ï¼ˆlossæˆ–accuracyï¼‰\n        model : object, optional\n            æ¨¡å‹å¯¹è±¡ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡ï¼‰\n        \n        è¿”å›:\n        -----\n        bool\n            æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ\n        \"\"\"\n        self.val_score_history.append(val_score)\n        \n        # ç¬¬ä¸€æ¬¡è¯„ä¼°\n        if self.best_score is None:\n            self.best_score = val_score\n            if model is not None:\n                self._save_model(model)\n            if self.verbose:\n                print(f\"  ğŸ¯ åˆå§‹éªŒè¯æ€§èƒ½: {val_score:.4f}\")\n            return False\n        \n        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›\n        if self.is_better(val_score, self.best_score):\n            # æœ‰æ”¹è¿›ï¼šæ›´æ–°æœ€ä½³æ€§èƒ½ï¼Œé‡ç½®è®¡æ•°å™¨\n            improvement = abs(val_score - self.best_score)\n            self.best_score = val_score\n            self.counter = 0\n            \n            if model is not None:\n                self._save_model(model)\n            \n            if self.verbose:\n                print(f\"  âœ… éªŒè¯æ€§èƒ½æå‡ {improvement:.4f} â†’ æ–°æœ€ä½³: {val_score:.4f}\")\n        else:\n            # æ²¡æœ‰æ”¹è¿›ï¼šå¢åŠ è®¡æ•°å™¨\n            self.counter += 1\n            \n            if self.verbose:\n                print(f\"  â³ éªŒè¯æ€§èƒ½æœªæå‡ï¼ˆ{self.counter}/{self.patience}ï¼‰ \"\n                      f\"å½“å‰: {val_score:.4f} vs æœ€ä½³: {self.best_score:.4f}\")\n            \n            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if self.verbose:\n                    print(f\"  ğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­{self.patience}ä¸ªepochæ— æ”¹è¿›\")\n                return True\n        \n        return False\n    \n    def _save_model(self, model):\n        \"\"\"ä¿å­˜æ¨¡å‹æƒé‡çš„æ·±æ‹·è´\"\"\"\n        import copy\n        # è¿™é‡Œç®€å•åœ°ä¿å­˜æ•´ä¸ªæ¨¡å‹çš„æ·±æ‹·è´\n        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯èƒ½åªä¿å­˜æƒé‡å­—å…¸\n        self.best_model = copy.deepcopy(model)\n    \n    def load_best_model(self):\n        \"\"\"åŠ è½½æœ€ä½³æ¨¡å‹\"\"\"\n        if self.best_model is None:\n            raise ValueError(\"æ²¡æœ‰ä¿å­˜çš„æœ€ä½³æ¨¡å‹\")\n        return self.best_model\n\n\n# ===== æ¼”ç¤ºæ—©åœ =====\nprint(\"=\"*70)\nprint(\"  æ—©åœï¼ˆEarly Stoppingï¼‰æ¼”ç¤º\")\nprint(\"=\"*70)\n\n# æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹\nprint(\"\\\\nåœºæ™¯1ï¼šæ­£å¸¸æ—©åœï¼ˆè¿‡æ‹Ÿåˆï¼‰\")\nprint(\"-\"*70)\n\n# æ¨¡æ‹ŸéªŒè¯lossï¼šå…ˆä¸‹é™ï¼Œç„¶åå¼€å§‹ä¸Šå‡ï¼ˆè¿‡æ‹Ÿåˆï¼‰\nval_losses = [1.0, 0.8, 0.6, 0.5, 0.48, 0.47, 0.48, 0.50, 0.52, 0.53, 0.54, 0.55]\n\nearly_stopping = EarlyStopping(patience=3, min_delta=0.01, mode='min', verbose=True)\n\nfor epoch, val_loss in enumerate(val_losses):\n    print(f\"\\\\nEpoch {epoch+1}:\")\n    if early_stopping.step(val_loss):\n        print(f\"\\\\n  è®­ç»ƒåœ¨Epoch {epoch+1}åœæ­¢\")\n        print(f\"  æœ€ä½³éªŒè¯loss: {early_stopping.best_score:.4f} \"\n              f\"(Epoch {epoch+1 - early_stopping.patience})\")\n        break\n\nprint(\"\\\\n\"*2 + \"=\"*70)\nprint(\"åœºæ™¯2ï¼šè®­ç»ƒå®Œæˆï¼ˆæŒç»­æ”¹è¿›ï¼‰\")\nprint(\"-\"*70)\n\n# æ¨¡æ‹ŸéªŒè¯lossï¼šæŒç»­ä¸‹é™\nval_losses_good = [1.0, 0.8, 0.6, 0.5, 0.4, 0.35, 0.32, 0.30, 0.28]\n\nearly_stopping2 = EarlyStopping(patience=3, min_delta=0.01, mode='min', verbose=True)\n\nstopped = False\nfor epoch, val_loss in enumerate(val_losses_good):\n    print(f\"\\\\nEpoch {epoch+1}:\")\n    if early_stopping2.step(val_loss):\n        stopped = True\n        break\n\nif not stopped:\n    print(f\"\\\\n  âœ… è®­ç»ƒæ­£å¸¸å®Œæˆï¼ˆ{len(val_losses_good)} epochsï¼‰\")\n    print(f\"  æœ€ç»ˆéªŒè¯loss: {early_stopping2.best_score:.4f}\")\n\nprint(\"\\\\n\" + \"=\"*70)\nprint(\"ğŸ’¡ æ—©åœä½¿ç”¨å»ºè®®ï¼š\")\nprint(\"=\"*70)\nprint(\"  1ï¸âƒ£  patienceè®¾ç½®ï¼šé€šå¸¸10-20ä¸ªepoch\")\nprint(\"  2ï¸âƒ£  min_deltaï¼šæ ¹æ®lossé‡çº§è®¾ç½®ï¼ˆå¦‚loss~1.0ï¼Œè®¾0.001ï¼‰\")\nprint(\"  3ï¸âƒ£  ä¿å­˜æœ€ä½³æ¨¡å‹ï¼šé¿å…è¿”å›è¿‡æ‹Ÿåˆçš„æ¨¡å‹\")\nprint(\"  4ï¸âƒ£  ç»“åˆå­¦ä¹ ç‡è°ƒåº¦ï¼šåœ¨å­¦ä¹ ç‡é™ä½å‰ç»™è¶³å¤Ÿè€å¿ƒ\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 5ï¸âƒ£ æ—©åœï¼ˆEarly Stoppingï¼‰\n\n### ğŸ›‘ ä»€ä¹ˆæ˜¯æ—©åœï¼Ÿ\n\n**é—®é¢˜**ï¼šè®­ç»ƒå¤ªä¹…ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä½†ä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™è¯¥åœ\n\n**è§£å†³æ–¹æ¡ˆ**ï¼šç›‘æ§éªŒè¯é›†æ€§èƒ½ï¼Œå½“ä¸å†æå‡æ—¶è‡ªåŠ¨åœæ­¢è®­ç»ƒ\n\n### ğŸ“ åŸç†\n\n**æ ¸å¿ƒæ€æƒ³**ï¼š\n1. æ¯ä¸ªepochåè¯„ä¼°éªŒè¯é›†æ€§èƒ½\n2. å¦‚æœéªŒè¯æ€§èƒ½æå‡ï¼Œä¿å­˜æ¨¡å‹å¹¶é‡ç½®è€å¿ƒè®¡æ•°å™¨\n3. å¦‚æœéªŒè¯æ€§èƒ½ä¸æå‡ï¼Œå¢åŠ è€å¿ƒè®¡æ•°å™¨\n4. å½“è€å¿ƒè®¡æ•°å™¨è¶…è¿‡é˜ˆå€¼ï¼Œåœæ­¢è®­ç»ƒ\n\n### ğŸ’» ä»é›¶å®ç°ï¼šæ—©åœ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def diagnose_training(train_loss_history: List[float], val_loss_history: List[float], \n                       train_acc_history: List[float] = None, val_acc_history: List[float] = None):\n    \"\"\"\n    è®­ç»ƒè¯Šæ–­å†³ç­–æ ‘\n    \n    æ ¹æ®è®­ç»ƒå’ŒéªŒè¯loss/accçš„è¡¨ç°ï¼Œè¯Šæ–­è®­ç»ƒé—®é¢˜å¹¶ç»™å‡ºå»ºè®®\n    \n    å‚æ•°:\n        train_loss_history: è®­ç»ƒé›†losså†å²\n        val_loss_history: éªŒè¯é›†losså†å²\n        train_acc_history: è®­ç»ƒé›†å‡†ç¡®ç‡å†å²ï¼ˆå¯é€‰ï¼‰\n        val_acc_history: éªŒè¯é›†å‡†ç¡®ç‡å†å²ï¼ˆå¯é€‰ï¼‰\n    \"\"\"\n    print(\"\\\\n\" + \"=\"*70)\n    print(\"  ğŸ©º è®­ç»ƒè¯Šæ–­æŠ¥å‘Š\")\n    print(\"=\"*70)\n    \n    # åŸºæœ¬ç»Ÿè®¡\n    final_train_loss = train_loss_history[-1]\n    final_val_loss = val_loss_history[-1]\n    loss_gap = abs(final_val_loss - final_train_loss)\n    \n    # è®¡ç®—lossè¶‹åŠ¿ï¼ˆæœ€è¿‘10ä¸ªepochçš„å¹³å‡å˜åŒ–ï¼‰\n    recent_epochs = min(10, len(train_loss_history) // 2)\n    train_trend = np.mean(np.diff(train_loss_history[-recent_epochs:]))\n    val_trend = np.mean(np.diff(val_loss_history[-recent_epochs:]))\n    \n    print(f\"\\\\nğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n    print(f\"  è®­ç»ƒé›†Loss: {final_train_loss:.4f}\")\n    print(f\"  éªŒè¯é›†Loss: {final_val_loss:.4f}\")\n    print(f\"  Losså·®è·: {loss_gap:.4f}\")\n    \n    if train_acc_history and val_acc_history:\n        final_train_acc = train_acc_history[-1]\n        final_val_acc = val_acc_history[-1]\n        print(f\"  è®­ç»ƒé›†å‡†ç¡®ç‡: {final_train_acc:.2%}\")\n        print(f\"  éªŒè¯é›†å‡†ç¡®ç‡: {final_val_acc:.2%}\")\n    \n    print(f\"\\\\nğŸ“ˆ è¶‹åŠ¿åˆ†æï¼ˆæœ€è¿‘{recent_epochs}ä¸ªepochï¼‰:\")\n    print(f\"  è®­ç»ƒLossè¶‹åŠ¿: {train_trend:+.5f} {'ğŸ“‰ ä¸‹é™' if train_trend < 0 else 'ğŸ“ˆ ä¸Šå‡'}\")\n    print(f\"  éªŒè¯Lossè¶‹åŠ¿: {val_trend:+.5f} {'ğŸ“‰ ä¸‹é™' if val_trend < 0 else 'ğŸ“ˆ ä¸Šå‡'}\")\n    \n    # ===== å†³ç­–æ ‘è¯Šæ–­ =====\n    print(f\\\"\\\\nğŸ” è¯Šæ–­ç»“æœ:\\\")\n    \n    # 1. æ£€æŸ¥è¿‡æ‹Ÿåˆ\n    if loss_gap > 0.5 and final_train_loss < 0.5:\n        print(\\\"\\\\nâŒ **ä¸¥é‡è¿‡æ‹Ÿåˆï¼**\\\")\n        print(\\\"  ç—‡çŠ¶: è®­ç»ƒlosså¾ˆä½ï¼Œä½†éªŒè¯losså¾ˆé«˜\\\")\n        print(\\\"  ğŸ’Š å»ºè®®:\")\n        print(\\\"     1. å¢åŠ æ­£åˆ™åŒ–ï¼ˆL2ã€Dropoutï¼‰\\\")\n        print(\\\"     2. ä½¿ç”¨æ•°æ®å¢å¼º\\\")\n        print(\\\"     3. å‡å°æ¨¡å‹å®¹é‡\\\")\n        print(\\\"     4. æå‰åœæ­¢è®­ç»ƒ\\\")\n        print(\\\"     5. æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®\\\")\n    \n    elif loss_gap > 0.2 and val_trend > 0:\n        print(\\\"\\\\nâš ï¸  **è½»åº¦è¿‡æ‹Ÿåˆ**\\\")\n        print(\\\"  ç—‡çŠ¶: éªŒè¯losså¼€å§‹ä¸Šå‡ï¼Œè®­ç»ƒlossç»§ç»­ä¸‹é™\\\")\n        print(\\\"  ğŸ’Š å»ºè®®:\")\n        print(\\\"     1. æ·»åŠ Dropout\\\")\n        print(\\\"     2. å¢åŠ L2æ­£åˆ™åŒ–\\\")\n        print(\\\"     3. è€ƒè™‘æå‰åœæ­¢\\\")\n    \n    # 2. æ£€æŸ¥æ¬ æ‹Ÿåˆ\n    elif final_train_loss > 1.0 and train_trend > -0.001:\n        print(\\\"\\\\nâŒ **æ¬ æ‹Ÿåˆï¼**\\\")\n        print(\\\"  ç—‡çŠ¶: è®­ç»ƒlosså’ŒéªŒè¯losséƒ½å¾ˆé«˜ï¼Œä¸”ä¸å†ä¸‹é™\\\")\n        print(\\\"  ğŸ’Š å»ºè®®:\")\n        print(\\\"     1. å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆæ›´å¤šå±‚/ç¥ç»å…ƒï¼‰\\\")\n        print(\\\"     2. è®­ç»ƒæ›´å¤šepoch\\\")\n        print(\\\"     3. é™ä½æ­£åˆ™åŒ–å¼ºåº¦\\\")\n        print(\\\"     4. ä½¿ç”¨æ›´å¼ºçš„ä¼˜åŒ–å™¨ï¼ˆå¦‚Adamï¼‰\\\")\n        print(\\\"     5. æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å°\\\")\n    \n    # 3. æ£€æŸ¥æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸\n    elif np.isnan(final_train_loss) or np.isinf(final_train_loss):\n        print(\\\"\\\\nğŸ’¥ **è®­ç»ƒå´©æºƒï¼ˆæ¢¯åº¦çˆ†ç‚¸ï¼‰ï¼**\\\")\n        print(\\\"  ç—‡çŠ¶: Losså˜æˆNaNæˆ–Inf\\\")\n        print(\\\"  ğŸ’Š å»ºè®®:\")\n        print(\\\"     1. é™ä½å­¦ä¹ ç‡\\\")\n        print(\\\"     2. ä½¿ç”¨æ¢¯åº¦è£å‰ª\\\")\n        print(\\\"     3. æ£€æŸ¥æƒé‡åˆå§‹åŒ–\\\")\n        print(\\\"     4. ä½¿ç”¨Batch Normalization\\\")\n    \n    # 4. è®­ç»ƒåœæ»\n    elif abs(train_trend) < 0.001 and final_train_loss > 0.5:\n        print(\\\"\\\\nâš ï¸  **è®­ç»ƒåœæ»**\\\")\n        print(\\\"  ç—‡çŠ¶: Lossä¸å†ä¸‹é™ï¼Œä½†è¿˜æ²¡æ”¶æ•›\\\")\n        print(\\\"  ğŸ’Š å»ºè®®:\")\n        print(\\\"     1. é™ä½å­¦ä¹ ç‡ï¼ˆä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦ï¼‰\\\")\n        print(\\\"     2. æ£€æŸ¥æ˜¯å¦é™·å…¥å±€éƒ¨æœ€ä¼˜\\\")\n        print(\\\"     3. å°è¯•Warm Restart\\\")\n        print(\\\"     4. å¢åŠ batch size\\\")\n    \n    # 5. è®­ç»ƒæ­£å¸¸\n    else:\n        print(\\\"\\\\nâœ… **è®­ç»ƒçŠ¶æ€è‰¯å¥½ï¼**\\\")\n        print(\\\"  è®­ç»ƒlosså’ŒéªŒè¯losséƒ½åœ¨ä¸‹é™ï¼Œä¸”å·®è·åˆç†\\\")\n        if train_trend < -0.001:\n            print(\\\"  å»ºè®®: ç»§ç»­è®­ç»ƒï¼Œå¯èƒ½è¿˜èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–\\\")\n        else:\n            print(\\\"  å»ºè®®: å·²æ¥è¿‘æ”¶æ•›ï¼Œå¯ä»¥è€ƒè™‘åœæ­¢è®­ç»ƒ\\\")\n    \n    print(\\\"\\\\n\\\" + \\\"=\\\"*70)\n\n\n# ===== ç¤ºä¾‹ï¼šè¯Šæ–­ä¸åŒçš„è®­ç»ƒæƒ…å†µ =====\nprint(\\\"\\\\nç¤ºä¾‹1: è¿‡æ‹Ÿåˆæƒ…å†µ\\\")\ntrain_loss_overfitting = [1.0, 0.8, 0.6, 0.4, 0.2, 0.1, 0.05]\nval_loss_overfitting = [1.0, 0.85, 0.75, 0.7, 0.75, 0.85, 0.9]\ndiagnose_training(train_loss_overfitting, val_loss_overfitting)\n\nprint(\\\"\\\\n\\\" + \\\"-\\\"*70)\nprint(\\\"\\\\nç¤ºä¾‹2: æ¬ æ‹Ÿåˆæƒ…å†µ\\\")\ntrain_loss_underfitting = [2.0, 1.8, 1.7, 1.65, 1.6, 1.59, 1.58]\nval_loss_underfitting = [2.1, 1.9, 1.8, 1.75, 1.7, 1.69, 1.68]\ndiagnose_training(train_loss_underfitting, val_loss_underfitting)\n\nprint(\\\"\\\\n\\\" + \\\"-\\\"*70)\nprint(\\\"\\\\nç¤ºä¾‹3: è®­ç»ƒæ­£å¸¸\\\")\ntrain_loss_good = [1.5, 1.2, 0.9, 0.7, 0.5, 0.4, 0.3]\nval_loss_good = [1.5, 1.25, 0.95, 0.75, 0.58, 0.5, 0.45]\ndiagnose_training(train_loss_good, val_loss_good)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 3ï¸âƒ£ æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰\n\n### ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦æ¢¯åº¦ç´¯ç§¯ï¼Ÿ\n\n**é—®é¢˜åœºæ™¯ï¼š**\n- æƒ³ç”¨batch_size=128è®­ç»ƒï¼Œä½†GPUå†…å­˜åªå¤Ÿbatch_size=32\n- å¤§æ‰¹é‡è®­ç»ƒæ›´ç¨³å®šï¼Œä½†ç¡¬ä»¶èµ„æºä¸è¶³\n\n**è§£å†³æ–¹æ¡ˆï¼šæ¢¯åº¦ç´¯ç§¯**\n\nç”¨å¤šä¸ªå°æ‰¹é‡ï¼ˆå¦‚4ä¸ªbatch_size=32ï¼‰ç´¯ç§¯æ¢¯åº¦ï¼Œå†æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œæ•ˆæœç­‰åŒäºbatch_size=128ï¼\n\n### ğŸ“ åŸç†\n\n**æ­£å¸¸è®­ç»ƒï¼ˆå¤§æ‰¹é‡ï¼‰ï¼š**\n$$\n\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta; \\mathcal{B}_{128})\n$$\n\n**æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§æ‰¹é‡ï¼‰ï¼š**\n$$\n\\begin{aligned}\ng_{accum} &= \\sum_{i=1}^{4} \\nabla_\\theta L(\\theta; \\mathcal{B}_{32}^{(i)}) \\\\\n\\theta &\\leftarrow \\theta - \\eta \\cdot \\frac{g_{accum}}{4}\n\\end{aligned}\n$$\n\n**å…³é”®ï¼š** ç´¯ç§¯4ä¸ªå°æ‰¹é‡çš„æ¢¯åº¦ï¼Œç„¶åé™¤ä»¥4ï¼ˆå¹³å‡ï¼‰ï¼Œå†æ›´æ–°å‚æ•°\n\n---\n\n### ğŸ’» ä»é›¶å®ç°ï¼šæ¢¯åº¦ç´¯ç§¯\n\n---\n\n## 4ï¸âƒ£ è®­ç»ƒè¯Šæ–­ï¼ˆTraining Diagnosticsï¼‰\n\n### ğŸ©º å¦‚ä½•åˆ¤æ–­è®­ç»ƒå‡ºäº†é—®é¢˜ï¼Ÿ\n\nè®­ç»ƒæ·±åº¦ç½‘ç»œå°±åƒçœ‹ç—…ï¼Œéœ€è¦ï¼š\n1. **è§‚å¯Ÿç—‡çŠ¶**ï¼šLossæ›²çº¿ã€å‡†ç¡®ç‡\n2. **è¯Šæ–­ç—…å› **ï¼šè¿‡æ‹Ÿåˆï¼Ÿæ¬ æ‹Ÿåˆï¼Ÿæ¢¯åº¦é—®é¢˜ï¼Ÿ\n3. **å¯¹ç—‡ä¸‹è¯**ï¼šè°ƒæ•´è¶…å‚æ•°ã€æ”¹æ¨¡å‹ç»“æ„\n\n### ğŸ“Š æ ¸å¿ƒè¯Šæ–­å·¥å…·\n\n#### 1. å­¦ä¹ æ›²çº¿åˆ†æ\n\n**å¥½çš„å­¦ä¹ æ›²çº¿ï¼š**\n- è®­ç»ƒlosså’ŒéªŒè¯losséƒ½ä¸‹é™\n- ä¸¤è€…å·®è·ä¸å¤§\n\n**è¿‡æ‹Ÿåˆï¼š**\n- è®­ç»ƒlossæŒç»­ä¸‹é™\n- éªŒè¯lossä¸Šå‡æˆ–ä¸å†ä¸‹é™\n- **è§£å†³æ–¹æ¡ˆï¼š** æ­£åˆ™åŒ–ã€Dropoutã€æ›´å¤šæ•°æ®\n\n**æ¬ æ‹Ÿåˆï¼š**\n- è®­ç»ƒlosså’ŒéªŒè¯losséƒ½å¾ˆé«˜\n- ä¸¤è€…éƒ½ä¸å†ä¸‹é™\n- **è§£å†³æ–¹æ¡ˆï¼š** å¢åŠ æ¨¡å‹å®¹é‡ã€è®­ç»ƒæ›´ä¹…ã€é™ä½æ­£åˆ™åŒ–\n\n#### 2. å†³ç­–æ ‘è¯Šæ–­",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def clip_gradient_norm(gradients: List[np.ndarray], max_norm: float = 1.0) -> Tuple[List[np.ndarray], float]:\n    \"\"\"\n    æ¢¯åº¦èŒƒæ•°è£å‰ªï¼ˆæ¨èæ–¹æ³•ï¼‰\n    \n    å…¬å¼: g â† (max_norm / ||g||) Ã— g  if ||g|| > max_norm\n    \n    å‚æ•°:\n        gradients: æ¢¯åº¦åˆ—è¡¨ [dW1, db1, dW2, db2, ...]\n        max_norm: æœ€å¤§æ¢¯åº¦èŒƒæ•°ï¼ˆé˜ˆå€¼ï¼‰\n    \n    è¿”å›:\n        è£å‰ªåçš„æ¢¯åº¦, åŸå§‹æ¢¯åº¦èŒƒæ•°\n    \"\"\"\n    # 1. è®¡ç®—æ‰€æœ‰æ¢¯åº¦çš„L2èŒƒæ•°\n    # ||g|| = sqrt(sum(g_i^2)) for all parameters\n    total_norm = 0.0\n    for grad in gradients:\n        total_norm += np.sum(grad ** 2)  # ç´¯åŠ æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦å¹³æ–¹å’Œ\n    total_norm = np.sqrt(total_norm)  # å¼€å¹³æ–¹å¾—åˆ°L2èŒƒæ•°\n    \n    # 2. å¦‚æœèŒƒæ•°è¶…è¿‡é˜ˆå€¼ï¼Œè¿›è¡Œè£å‰ª\n    clip_coef = max_norm / (total_norm + 1e-6)  # è®¡ç®—ç¼©æ”¾ç³»æ•°\n    \n    if clip_coef < 1:  # éœ€è¦è£å‰ª\n        clipped_gradients = [grad * clip_coef for grad in gradients]\n        return clipped_gradients, total_norm\n    else:  # ä¸éœ€è¦è£å‰ª\n        return gradients, total_norm\n\n\ndef clip_gradient_value(gradients: List[np.ndarray], clip_value: float = 1.0) -> List[np.ndarray]:\n    \"\"\"\n    æ¢¯åº¦å€¼è£å‰ªï¼ˆé€å…ƒç´ è£å‰ªï¼‰\n    \n    å…¬å¼: g_i â† clip(g_i, -clip_value, clip_value)\n    \n    å‚æ•°:\n        gradients: æ¢¯åº¦åˆ—è¡¨\n        clip_value: è£å‰ªé˜ˆå€¼\n    \n    è¿”å›:\n        è£å‰ªåçš„æ¢¯åº¦\n    \"\"\"\n    # å°†æ¯ä¸ªæ¢¯åº¦çš„å…ƒç´ é™åˆ¶åœ¨ [-clip_value, clip_value]\n    clipped_gradients = [np.clip(grad, -clip_value, clip_value) for grad in gradients]\n    return clipped_gradients\n\n\n# ===== æ¼”ç¤ºæ¢¯åº¦çˆ†ç‚¸ vs æ¢¯åº¦è£å‰ª =====\ndef demonstrate_gradient_clipping():\n    \"\"\"\n    æ¼”ç¤ºæ¢¯åº¦è£å‰ªçš„æ•ˆæœ\n    \"\"\"\n    print(\"=\"*60)\n    print(\"  æ¢¯åº¦è£å‰ªæ¼”ç¤º\")\n    print(\"=\"*60)\n    \n    # æ¨¡æ‹Ÿä¸€ä¸ªæ¢¯åº¦çˆ†ç‚¸çš„åœºæ™¯\n    # å‡è®¾æˆ‘ä»¬æœ‰3ä¸ªå‚æ•°çŸ©é˜µçš„æ¢¯åº¦\n    dW1 = np.random.randn(10, 20) * 100  # æ¢¯åº¦å¾ˆå¤§ï¼\n    dW2 = np.random.randn(20, 30) * 100\n    dW3 = np.random.randn(30, 10) * 100\n    \n    gradients = [dW1, dW2, dW3]\n    \n    # è®¡ç®—åŸå§‹æ¢¯åº¦èŒƒæ•°\n    original_norm = 0.0\n    for grad in gradients:\n        original_norm += np.sum(grad ** 2)\n    original_norm = np.sqrt(original_norm)\n    \n    print(f\"\\\\nåŸå§‹æ¢¯åº¦èŒƒæ•°: {original_norm:.2f}\")\n    print(f\"  dW1 èŒƒæ•°: {np.linalg.norm(dW1):.2f}\")\n    print(f\"  dW2 èŒƒæ•°: {np.linalg.norm(dW2):.2f}\")\n    print(f\"  dW3 èŒƒæ•°: {np.linalg.norm(dW3):.2f}\")\n    \n    # ä½¿ç”¨æ¢¯åº¦èŒƒæ•°è£å‰ª\n    max_norm = 10.0\n    clipped_grads, _ = clip_gradient_norm(gradients, max_norm=max_norm)\n    \n    clipped_norm = 0.0\n    for grad in clipped_grads:\n        clipped_norm += np.sum(grad ** 2)\n    clipped_norm = np.sqrt(clipped_norm)\n    \n    print(f\"\\\\nè£å‰ªåæ¢¯åº¦èŒƒæ•°: {clipped_norm:.2f} (é˜ˆå€¼: {max_norm})\")\n    print(f\"  dW1 èŒƒæ•°: {np.linalg.norm(clipped_grads[0]):.2f}\")\n    print(f\"  dW2 èŒƒæ•°: {np.linalg.norm(clipped_grads[1]):.2f}\")\n    print(f\"  dW3 èŒƒæ•°: {np.linalg.norm(clipped_grads[2]):.2f}\")\n    \n    print(f\"\\\\nç¼©æ”¾ç³»æ•°: {max_norm / original_norm:.4f}\")\n    print(f\"âœ… æ¢¯åº¦èŒƒæ•°ä» {original_norm:.2f} é™ä½åˆ° {clipped_norm:.2f}\")\n    print(\"=\"*60)\n\ndemonstrate_gradient_clipping()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 2ï¸âƒ£ æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰\n\n### ğŸ”¥ æ¢¯åº¦çˆ†ç‚¸é—®é¢˜\n\n**ä»€ä¹ˆæ˜¯æ¢¯åº¦çˆ†ç‚¸ï¼Ÿ**\n\nåœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶å¯èƒ½ä¼šæŒ‡æ•°çº§å¢é•¿ï¼š\n\n$$\n\\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial W^{(L)}} \\times \\prod_{l=2}^{L} \\frac{\\partial W^{(l)}}{\\partial W^{(l-1)}}\n$$\n\nå¦‚æœæ¯å±‚æ¢¯åº¦éƒ½ >1ï¼Œè¿ä¹˜ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼\n\n**åæœï¼š**\n- å‚æ•°æ›´æ–°è¿‡å¤§\n- losså˜æˆNaN\n- è®­ç»ƒå´©æºƒ\n\n---\n\n### âœ‚ï¸ æ¢¯åº¦è£å‰ªç­–ç•¥\n\n#### 1. Norm Clippingï¼ˆèŒƒæ•°è£å‰ªï¼‰â­ æ¨è\n\n**åŸç†ï¼š** é™åˆ¶æ¢¯åº¦å‘é‡çš„L2èŒƒæ•°ä¸è¶…è¿‡é˜ˆå€¼\n\n$$\n\\mathbf{g} \\leftarrow \\begin{cases}\n\\mathbf{g} & \\text{if } \\|\\mathbf{g}\\| \\leq \\theta \\\\\n\\frac{\\theta}{\\|\\mathbf{g}\\|} \\mathbf{g} & \\text{if } \\|\\mathbf{g}\\| > \\theta\n\\end{cases}\n$$\n\n**ä¼˜ç‚¹ï¼š**\n- ä¿æŒæ¢¯åº¦æ–¹å‘ä¸å˜\n- åªç¼©æ”¾å¹…åº¦\n\n#### 2. Value Clippingï¼ˆå€¼è£å‰ªï¼‰\n\n**åŸç†ï¼š** é™åˆ¶æ¢¯åº¦çš„æ¯ä¸ªå…ƒç´ åœ¨ $[-\\theta, \\theta]$\n\n$$\ng_i \\leftarrow \\text{clip}(g_i, -\\theta, \\theta)\n$$\n\n**ç¼ºç‚¹ï¼š**\n- æ”¹å˜æ¢¯åº¦æ–¹å‘\n- ä¸å¦‚Norm Clippingå¸¸ç”¨\n\n---\n\n### ğŸ’» ä»é›¶å®ç°ï¼šæ¢¯åº¦è£å‰ª",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def visualize_lr_schedules(n_epochs=100):\n    \"\"\"\n    å¯è§†åŒ–å¯¹æ¯”ä¸åŒå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥\n    \n    å‚æ•°:\n        n_epochs: è®­ç»ƒçš„æ€»epochæ•°\n    \"\"\"\n    # åˆ›å»ºä¸åŒçš„è°ƒåº¦å™¨\n    schedulers = {\n        'Step Decay': StepDecayScheduler(initial_lr=0.1, decay_rate=0.5, decay_steps=30),\n        'Exponential Decay': ExponentialDecayScheduler(initial_lr=0.1, decay_rate=0.03),\n        'Cosine Annealing': CosineAnnealingScheduler(initial_lr=0.1, min_lr=0.001, T_max=n_epochs),\n        'Warm Restart': WarmRestartScheduler(initial_lr=0.1, min_lr=0.001, T_0=20, T_mult=1.5)\n    }\n    \n    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œè®°å½•å­¦ä¹ ç‡\n    histories = {}\n    for name, scheduler in schedulers.items():\n        lr_history = []\n        for epoch in range(n_epochs):\n            lr = scheduler.update()\n            lr_history.append(lr)\n        histories[name] = lr_history\n    \n    # ç»˜åˆ¶å¯¹æ¯”å›¾\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.flatten()\n    \n    for idx, (name, history) in enumerate(histories.items()):\n        ax = axes[idx]\n        ax.plot(history, linewidth=2.5, color=sns.color_palette()[idx])\n        ax.set_title(f'{name}', fontsize=14, fontweight='bold')\n        ax.set_xlabel('Epoch', fontsize=12)\n        ax.set_ylabel('Learning Rate', fontsize=12)\n        ax.grid(True, alpha=0.3)\n        ax.set_xlim(0, n_epochs-1)\n        \n        # æ·»åŠ å…³é”®ä¿¡æ¯\n        final_lr = history[-1]\n        ax.text(0.98, 0.98, f'æœ€ç»ˆå­¦ä¹ ç‡: {final_lr:.5f}', \n                transform=ax.transAxes, ha='right', va='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n                fontsize=10)\n    \n    plt.tight_layout()\n    plt.savefig('lr_schedules_comparison.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # å¯¹æ¯”æ€»ç»“\n    print(\"\\\\n\" + \"=\"*60)\n    print(\"  å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥å¯¹æ¯”æ€»ç»“\")\n    print(\"=\"*60)\n    for name, history in histories.items():\n        initial = history[0]\n        final = history[-1]\n        min_lr = min(history)\n        max_lr = max(history)\n        print(f\"\\\\n{name}:\")\n        print(f\"  åˆå§‹: {initial:.5f} | æœ€ç»ˆ: {final:.5f}\")\n        print(f\"  èŒƒå›´: [{min_lr:.5f}, {max_lr:.5f}]\")\n        print(f\"  ç‰¹ç‚¹: \", end=\"\")\n        if name == 'Step Decay':\n            print(\"é˜¶æ¢¯å¼ä¸‹é™ï¼Œé€‚åˆæ˜ç¡®çŸ¥é“ä½•æ—¶é™ä½å­¦ä¹ ç‡\")\n        elif name == 'Exponential Decay':\n            print(\"å¹³æ»‘è¿ç»­ä¸‹é™ï¼Œé€‚åˆé•¿æœŸè®­ç»ƒ\")\n        elif name == 'Cosine Annealing':\n            print(\"ä½™å¼¦å¹³æ»‘ä¸‹é™ï¼Œæ”¶æ•›æ›´ç¨³å®š\")\n        elif name == 'Warm Restart':\n            print(\"å‘¨æœŸæ€§é‡å¯ï¼Œå¸®åŠ©è·³å‡ºå±€éƒ¨æœ€ä¼˜\")\n    print(\"=\"*60)\n\n# è¿è¡Œå¯è§†åŒ–\nvisualize_lr_schedules(n_epochs=100)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### ğŸ“Š å¯è§†åŒ–å¯¹æ¯”ï¼šä¸åŒè°ƒåº¦ç­–ç•¥",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}