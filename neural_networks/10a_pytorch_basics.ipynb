{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬10ç« Aï¼šPyTorchåŸºç¡€å…¥é—¨\n",
    "\n",
    "> **ä»NumPyæ‰‹å†™ç¥ç»ç½‘ç»œåˆ°æ·±åº¦å­¦ä¹ æ¡†æ¶çš„æ¡¥æ¢**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "\n",
    "1. **ç†è§£ä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ æ¡†æ¶**ï¼šå¯¹æ¯”NumPyæ‰‹å†™å®ç°çš„ç—›ç‚¹\n",
    "2. **æŒæ¡PyTorch Tensor**ï¼šå¼ é‡åˆ›å»ºã€è¿ç®—ã€ä¸NumPyå¯¹æ¯”\n",
    "3. **ç†è§£è‡ªåŠ¨å¾®åˆ†Autograd**ï¼šå‘Šåˆ«æ‰‹å†™åå‘ä¼ æ’­\n",
    "4. **ä½¿ç”¨nn.Moduleå®šä¹‰æ¨¡å‹**ï¼šå¯¹æ¯”NumPyç‰ˆMLPä¸PyTorchç‰ˆMLP\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç³»åˆ—ç« èŠ‚\n",
    "\n",
    "| ç« èŠ‚ | ä¸»é¢˜ | å†…å®¹ |\n",
    "|------|------|------|\n",
    "| **10a** | PyTorchåŸºç¡€ | Tensorã€Autogradã€nn.Module |\n",
    "| 10b | PyTorchè®­ç»ƒ | æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€å®Œæ•´è®­ç»ƒå¾ªç¯ã€å®æˆ˜é¡¹ç›® |\n",
    "| 10c | TensorFlow/Keras | Keras APIã€æ¡†æ¶å¯¹æ¯”ã€æ€»ç»“ç»ƒä¹  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1ï¼šä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 å›é¡¾ï¼šNumPyå®ç°çš„ç—›ç‚¹\n",
    "\n",
    "åœ¨å‰9ç« ä¸­ï¼Œæˆ‘ä»¬ä»é›¶ç”¨NumPyå®ç°äº†å®Œæ•´çš„ç¥ç»ç½‘ç»œï¼š\n",
    "\n",
    "- âœ… **æ„ŸçŸ¥æœºå’ŒMLP**ï¼ˆç¬¬1-3ç« ï¼‰\n",
    "- âœ… **åå‘ä¼ æ’­ç®—æ³•**ï¼ˆç¬¬4ç« ï¼‰\n",
    "- âœ… **æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨**ï¼ˆç¬¬5ç« ï¼‰\n",
    "- âœ… **æ­£åˆ™åŒ–æŠ€æœ¯**ï¼ˆç¬¬6-7ç« ï¼‰\n",
    "- âœ… **æƒé‡åˆå§‹åŒ–å’Œè®­ç»ƒæŠ€å·§**ï¼ˆç¬¬8-9ç« ï¼‰\n",
    "\n",
    "### è¿™å¾ˆé‡è¦ï¼Œä½†ä¹Ÿå¾ˆç—›è‹¦...\n",
    "\n",
    "è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æ‰‹å†™åå‘ä¼ æ’­æœ‰å¤šå¤æ‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# å›é¡¾ï¼šNumPyä»é›¶å®ç°çš„ç¥ç»ç½‘ç»œï¼ˆæ¥è‡ªç¬¬3-4ç« ï¼‰\n",
    "# ============================================================\n",
    "# è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰å¿…é¡»æ‰‹å†™çš„ä»£ç ...\n",
    "\n",
    "class NumpyMLP:\n",
    "    \"\"\"\n",
    "    NumPyå®ç°çš„å¤šå±‚æ„ŸçŸ¥æœº\n",
    "    \n",
    "    ç»“æ„: è¾“å…¥å±‚ -> éšè—å±‚(ReLU) -> è¾“å‡ºå±‚(Softmax)\n",
    "    \n",
    "    ç—›ç‚¹ï¼š\n",
    "    1. å¿…é¡»æ‰‹åŠ¨æ¨å¯¼æ¯ä¸€å±‚çš„æ¢¯åº¦å…¬å¼\n",
    "    2. å¿…é¡»æ‰‹åŠ¨å®ç°é“¾å¼æ³•åˆ™\n",
    "    3. å®¹æ˜“å‡ºé”™ï¼Œè°ƒè¯•å›°éš¾\n",
    "    4. æ— æ³•ä½¿ç”¨GPUåŠ é€Ÿ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # ----- æ‰‹åŠ¨åˆå§‹åŒ–æƒé‡ï¼ˆXavieråˆå§‹åŒ–ï¼‰ -----\n",
    "        # éœ€è¦è‡ªå·±å®ç°åˆå§‹åŒ–ç­–ç•¥\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLUæ¿€æ´»å‡½æ•°ï¼šéœ€è¦æ‰‹åŠ¨å®ç°\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"ReLUå¯¼æ•°ï¼šåå‘ä¼ æ’­éœ€è¦\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmaxå‡½æ•°ï¼šéœ€è¦æ‰‹åŠ¨å®ç°æ•°å€¼ç¨³å®šç‰ˆæœ¬\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å¿…é¡»æ‰‹åŠ¨ä¿å­˜ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­ï¼\n",
    "        \"\"\"\n",
    "        # ç¬¬ä¸€å±‚ï¼šçº¿æ€§å˜æ¢ + ReLU\n",
    "        self.z1 = X @ self.W1 + self.b1        # ä¿å­˜z1ç”¨äºåå‘ä¼ æ’­\n",
    "        self.a1 = self.relu(self.z1)           # ä¿å­˜a1ç”¨äºåå‘ä¼ æ’­\n",
    "        \n",
    "        # ç¬¬äºŒå±‚ï¼šçº¿æ€§å˜æ¢ + Softmax\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2  # ä¿å­˜z2\n",
    "        self.a2 = self.softmax(self.z2)        # è¾“å‡ºæ¦‚ç‡\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        åå‘ä¼ æ’­ - è¿™æ˜¯æœ€ç—›è‹¦çš„éƒ¨åˆ†ï¼\n",
    "        \n",
    "        å¿…é¡»æ‰‹åŠ¨æ¨å¯¼æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼š\n",
    "        - éœ€è¦ç†è§£é“¾å¼æ³•åˆ™\n",
    "        - éœ€è¦å¤„ç†çŸ©é˜µæ±‚å¯¼\n",
    "        - å®¹æ˜“å‡ºé”™ï¼\n",
    "        \n",
    "        å‚æ•°:\n",
    "            X: è¾“å…¥æ•°æ®, shape (batch_size, input_size)\n",
    "            y_true: çœŸå®æ ‡ç­¾(one-hot), shape (batch_size, output_size)\n",
    "            y_pred: é¢„æµ‹æ¦‚ç‡, shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # batch size\n",
    "        \n",
    "        # ----- è¾“å‡ºå±‚æ¢¯åº¦ -----\n",
    "        # dL/dz2 = y_pred - y_trueï¼ˆäº¤å‰ç†µ+Softmaxçš„ç®€åŒ–å½¢å¼ï¼‰\n",
    "        dz2 = y_pred - y_true  # (batch_size, output_size)\n",
    "        \n",
    "        # dL/dW2 = a1^T @ dz2\n",
    "        # è¿™é‡Œéœ€è¦ç†è§£çŸ©é˜µæ±‚å¯¼çš„è½¬ç½®è§„åˆ™ï¼\n",
    "        self.dW2 = (self.a1.T @ dz2) / m\n",
    "        \n",
    "        # dL/db2 = sum(dz2, axis=0)\n",
    "        self.db2 = np.sum(dz2, axis=0) / m\n",
    "        \n",
    "        # ----- éšè—å±‚æ¢¯åº¦ï¼ˆé“¾å¼æ³•åˆ™ç»§ç»­ï¼‰ -----\n",
    "        # dL/da1 = dz2 @ W2^T\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        \n",
    "        # dL/dz1 = dL/da1 * relu'(z1)\n",
    "        # å¿…é¡»ä¿å­˜z1æ‰èƒ½è®¡ç®—ReLUå¯¼æ•°ï¼\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        \n",
    "        # dL/dW1 = X^T @ dz1\n",
    "        self.dW1 = (X.T @ dz1) / m\n",
    "        \n",
    "        # dL/db1 = sum(dz1, axis=0)\n",
    "        self.db1 = np.sum(dz1, axis=0) / m\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        \"\"\"\n",
    "        æ‰‹åŠ¨æ›´æ–°å‚æ•°\n",
    "        \n",
    "        è¿™åªæ˜¯æœ€ç®€å•çš„SGDï¼\n",
    "        å¦‚æœè¦å®ç°Adamï¼Œè¿˜éœ€è¦é¢å¤–50+è¡Œä»£ç ...\n",
    "        \"\"\"\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"  NumPyå®ç°çš„ç—›ç‚¹æ€»ç»“\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "âŒ ç—›ç‚¹1ï¼šæ‰‹åŠ¨æ¨å¯¼æ¢¯åº¦å…¬å¼\n",
    "   - æ¯å¢åŠ ä¸€å±‚ï¼Œå°±è¦æ¨å¯¼æ–°çš„æ¢¯åº¦\n",
    "   - çŸ©é˜µæ±‚å¯¼å®¹æ˜“å‡ºé”™\n",
    "\n",
    "âŒ ç—›ç‚¹2ï¼šæ‰‹åŠ¨ä¿å­˜ä¸­é—´å˜é‡\n",
    "   - forwardå¿…é¡»ä¿å­˜z1, a1ç­‰\n",
    "   - backwardæ‰èƒ½ç”¨æ¥è®¡ç®—æ¢¯åº¦\n",
    "\n",
    "âŒ ç—›ç‚¹3ï¼šæ‰‹åŠ¨å®ç°ä¼˜åŒ–å™¨\n",
    "   - SGDç®€å•ï¼Œä½†Adaméœ€è¦50+è¡Œä»£ç \n",
    "   - è¿˜æœ‰AdaGrad, RMSpropç­‰...\n",
    "\n",
    "âŒ ç—›ç‚¹4ï¼šæ— GPUåŠ é€Ÿ\n",
    "   - NumPyåªèƒ½ç”¨CPU\n",
    "   - å¤§è§„æ¨¡æ•°æ®è®­ç»ƒå¾ˆæ…¢\n",
    "\n",
    "âŒ ç—›ç‚¹5ï¼šä»£ç éš¾ä»¥ç»´æŠ¤\n",
    "   - ç½‘ç»œç»“æ„æ”¹å˜éœ€è¦é‡å†™backward\n",
    "   - ä¸åŒæ¿€æ´»å‡½æ•°éœ€è¦ä¸åŒçš„å¯¼æ•°å®ç°\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 æ·±åº¦å­¦ä¹ æ¡†æ¶è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ\n",
    "\n",
    "### ğŸš€ æ¡†æ¶çš„æ ¸å¿ƒä»·å€¼\n",
    "\n",
    "| ç—›ç‚¹ | NumPyæ‰‹å†™ | æ¡†æ¶è§£å†³æ–¹æ¡ˆ |\n",
    "|------|-----------|-------------|\n",
    "| **æ¢¯åº¦è®¡ç®—** | æ‰‹åŠ¨æ¨å¯¼æ¯å±‚å…¬å¼ | **è‡ªåŠ¨å¾®åˆ†(Autograd)** |\n",
    "| **ä¸­é—´å˜é‡** | æ‰‹åŠ¨ä¿å­˜a1, z1ç­‰ | **è®¡ç®—å›¾è‡ªåŠ¨è®°å½•** |\n",
    "| **ä¼˜åŒ–å™¨** | æ¯ä¸ªä¼˜åŒ–å™¨50+è¡Œä»£ç  | **ä¸€è¡Œè°ƒç”¨** `optim.Adam()` |\n",
    "| **GPUåŠ é€Ÿ** | ä¸æ”¯æŒ | **`.to(device)`** è‡ªåŠ¨è½¬ç§» |\n",
    "| **ä»£ç ç»´æŠ¤** | æ”¹ç»“æ„å°±è¦é‡å†™backward | **åªéœ€ä¿®æ”¹forward** |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¥ ä¸¤å¤§ä¸»æµæ¡†æ¶å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | PyTorch | TensorFlow/Keras |\n",
    "|------|---------|------------------|\n",
    "| **å¼€å‘è€…** | Meta (Facebook) | Google |\n",
    "| **ç¼–ç¨‹é£æ ¼** | åŠ¨æ€å›¾ï¼ˆPythonicï¼‰ | Kerasé«˜å±‚APIç®€æ´ |\n",
    "| **å­¦æœ¯ç ”ç©¶** | â­â­â­â­â­ ä¸»æµ | â­â­â­ |\n",
    "| **å·¥ä¸šéƒ¨ç½²** | â­â­â­â­ | â­â­â­â­â­ æˆç†Ÿ |\n",
    "| **è°ƒè¯•ä½“éªŒ** | åƒPythonä¸€æ ·è°ƒè¯• | TF2.0åæ”¹è¿›å¾ˆå¤§ |\n",
    "| **å­¦ä¹ æ›²çº¿** | é€‚ä¸­ | Kerasæç®€ï¼ŒTFåº•å±‚å¤æ‚ |\n",
    "\n",
    "**æœ¬ç³»åˆ—ç­–ç•¥ï¼š**\n",
    "- **10a-10b**ï¼šæ·±å…¥å­¦ä¹  **PyTorch**ï¼ˆå­¦æœ¯ç•Œé¦–é€‰ï¼Œä»£ç æ›´æ¥è¿‘åŸç†ï¼‰\n",
    "- **10c**ï¼šå¿«é€Ÿä¸Šæ‰‹ **Keras**ï¼ˆå·¥ä¸šç•Œå¸¸ç”¨ï¼Œäº†è§£é«˜å±‚APIä¾¿åˆ©æ€§ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  æ·±åº¦å­¦ä¹ ç¯å¢ƒæ£€æµ‹\n",
      "============================================================\n",
      "\n",
      "ğŸ“± ç³»ç»Ÿä¿¡æ¯:\n",
      "   æ“ä½œç³»ç»Ÿ: Darwin arm64\n",
      "   Pythonç‰ˆæœ¬: 3.8.18\n",
      "\n",
      "------------------------------------------------------------\n",
      "âœ… PyTorchç‰ˆæœ¬: 2.4.1\n",
      "   ğŸ MPSå¯ç”¨: Apple Silicon GPUåŠ é€Ÿ\n",
      "   MPSæµ‹è¯•: âœ“ æ­£å¸¸å·¥ä½œ\n",
      "\n",
      "   ğŸ“ æ¨èè®¾å¤‡: mps\n",
      "\n",
      "------------------------------------------------------------\n",
      "âœ… é€šç”¨åº“åŠ è½½å®Œæˆ (numpy, matplotlib)\n",
      "\n",
      "============================================================\n",
      "âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ç¯å¢ƒæ£€æµ‹ï¼šæ£€æŸ¥PyTorchå®‰è£…å’ŒGPUæ”¯æŒ\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"  æ·±åº¦å­¦ä¹ ç¯å¢ƒæ£€æµ‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ----- ç³»ç»Ÿä¿¡æ¯ -----\n",
    "print(f\"\\nğŸ“± ç³»ç»Ÿä¿¡æ¯:\")\n",
    "print(f\"   æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.machine()}\")\n",
    "print(f\"   Pythonç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "# ----- PyTorchæ£€æµ‹ -----\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "    \n",
    "    # æ£€æµ‹åŠ é€Ÿè®¾å¤‡\n",
    "    # CUDAï¼ˆNVIDIA GPUï¼‰\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"   ğŸ® CUDAå¯ç”¨: GPU = {torch.cuda.get_device_name(0)}\")\n",
    "    # MPSï¼ˆApple Silicon GPUï¼‰\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(f\"   ğŸ MPSå¯ç”¨: Apple Silicon GPUåŠ é€Ÿ\")\n",
    "        # æµ‹è¯•MPSæ˜¯å¦æ­£å¸¸\n",
    "        try:\n",
    "            _ = torch.randn(100, 100, device=device) @ torch.randn(100, 100, device=device)\n",
    "            print(f\"   MPSæµ‹è¯•: âœ“ æ­£å¸¸å·¥ä½œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"   MPSæµ‹è¯•: âœ— å›é€€åˆ°CPU\")\n",
    "            device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"   GPUåŠ é€Ÿ: ä¸å¯ç”¨ï¼ˆä½¿ç”¨CPUï¼‰\")\n",
    "    \n",
    "    print(f\"\\n   ğŸ“ æ¨èè®¾å¤‡: {device}\")\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorchæœªå®‰è£…\")\n",
    "    print(\"   å®‰è£…å‘½ä»¤: pip install torch torchvision\")\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    device = None\n",
    "\n",
    "# ----- é€šç”¨åº“ -----\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ç»˜å›¾è®¾ç½®\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "print(\"âœ… é€šç”¨åº“åŠ è½½å®Œæˆ (numpy, matplotlib)\")\n",
    "\n",
    "# ----- è®¾å¤‡é€‰æ‹©è¾…åŠ©å‡½æ•° -----\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¡ç®—è®¾å¤‡\n",
    "    \n",
    "    ä¼˜å…ˆçº§: CUDA > MPS > CPU\n",
    "    \n",
    "    ä½¿ç”¨ç¤ºä¾‹:\n",
    "        device = get_device()\n",
    "        model = model.to(device)\n",
    "        data = data.to(device)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2ï¼šPyTorchåŸºç¡€\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Tensorå¼ é‡ï¼šæ¯”NumPyæ›´å¼ºçš„å¤šç»´æ•°ç»„\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯Tensorï¼Ÿ\n",
    "\n",
    "Tensorï¼ˆå¼ é‡ï¼‰æ˜¯PyTorchçš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼š\n",
    "\n",
    "- **æœ¬è´¨**ï¼šå¤šç»´æ•°ç»„ï¼ˆç±»ä¼¼NumPyçš„ndarrayï¼‰\n",
    "- **ä¼˜åŠ¿1**ï¼šå¯ä»¥åœ¨GPUä¸Šè¿è¡Œ\n",
    "- **ä¼˜åŠ¿2**ï¼šæ”¯æŒè‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰\n",
    "\n",
    "### ç»´åº¦å¯¹åº”\n",
    "\n",
    "| ç»´åº¦ | åç§° | ç¤ºä¾‹ |\n",
    "|------|------|------|\n",
    "| 0ç»´ | æ ‡é‡(scalar) | æŸå¤±å€¼ 3.14 |\n",
    "| 1ç»´ | å‘é‡(vector) | åç½® [0.1, 0.2, 0.3] |\n",
    "| 2ç»´ | çŸ©é˜µ(matrix) | æƒé‡ W (128, 64) |\n",
    "| 3ç»´ | 3é˜¶å¼ é‡ | æ‰¹é‡å›¾åƒ (batch, height, width) |\n",
    "| 4ç»´ | 4é˜¶å¼ é‡ | å½©è‰²å›¾åƒ (batch, channels, H, W) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.1 å¼ é‡åˆ›å»º\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£ åˆ›å»ºå¼ é‡çš„å¤šç§æ–¹å¼:\n",
      "\n",
      "ä»åˆ—è¡¨åˆ›å»º: tensor([1, 2, 3, 4])\n",
      "  å½¢çŠ¶(shape): torch.Size([4])\n",
      "  æ•°æ®ç±»å‹(dtype): torch.int64\n",
      "  è®¾å¤‡(device): cpu\n",
      "\n",
      "æŒ‡å®šfloat32ç±»å‹: tensor([1., 2., 3.]), dtype=torch.float32\n",
      "\n",
      "åˆ›å»ºç‰¹æ®Šå¼ é‡:\n",
      "  å…¨é›¶ (2,3):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "  å…¨ä¸€ (3,4):\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "  å•ä½çŸ©é˜µ (3,3):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "\n",
      "åˆ›å»ºéšæœºå¼ é‡:\n",
      "  æ­£æ€åˆ†å¸ƒN(0,1):\n",
      "tensor([[-0.3815,  0.4283,  0.9874],\n",
      "        [ 0.2134,  0.0991,  0.6444]])\n",
      "\n",
      "  å‡åŒ€åˆ†å¸ƒU(0,1):\n",
      "tensor([[0.5992, 0.8686, 0.0689],\n",
      "        [0.5072, 0.9290, 0.3112]])\n",
      "\n",
      "  éšæœºæ•´æ•°[0,10):\n",
      "tensor([[2, 1, 5],\n",
      "        [5, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"  2.1 å¼ é‡åˆ›å»º\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# 1. åˆ›å»ºå¼ é‡çš„å¤šç§æ–¹å¼\n",
    "# ============================================================\n",
    "print(\"\\n1ï¸âƒ£ åˆ›å»ºå¼ é‡çš„å¤šç§æ–¹å¼:\")\n",
    "\n",
    "# ----- ä»Pythonåˆ—è¡¨åˆ›å»º -----\n",
    "# è¿™æ˜¯æœ€ç›´è§‚çš„æ–¹å¼ï¼Œç±»ä¼¼ np.array()\n",
    "x_list = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"\\nä»åˆ—è¡¨åˆ›å»º: {x_list}\")\n",
    "print(f\"  å½¢çŠ¶(shape): {x_list.shape}\")\n",
    "print(f\"  æ•°æ®ç±»å‹(dtype): {x_list.dtype}\")\n",
    "print(f\"  è®¾å¤‡(device): {x_list.device}\")\n",
    "\n",
    "# ----- æŒ‡å®šæ•°æ®ç±»å‹ -----\n",
    "# ç¥ç»ç½‘ç»œé€šå¸¸ä½¿ç”¨float32\n",
    "x_float = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(f\"\\næŒ‡å®šfloat32ç±»å‹: {x_float}, dtype={x_float.dtype}\")\n",
    "\n",
    "# ----- åˆ›å»ºç‰¹æ®Šå¼ é‡ -----\n",
    "print(\"\\nåˆ›å»ºç‰¹æ®Šå¼ é‡:\")\n",
    "\n",
    "# å…¨é›¶å¼ é‡ï¼ˆå¸¸ç”¨äºåˆå§‹åŒ–åç½®ï¼‰\n",
    "zeros = torch.zeros(2, 3)  # 2x3çŸ©é˜µ\n",
    "print(f\"  å…¨é›¶ (2,3):\\n{zeros}\")\n",
    "\n",
    "# å…¨ä¸€å¼ é‡\n",
    "ones = torch.ones(3, 4)\n",
    "print(f\"\\n  å…¨ä¸€ (3,4):\\n{ones}\")\n",
    "\n",
    "# å•ä½çŸ©é˜µï¼ˆå¸¸ç”¨äºåˆå§‹åŒ–æŸäº›æƒé‡ï¼‰\n",
    "eye = torch.eye(3)  # 3x3å•ä½çŸ©é˜µ\n",
    "print(f\"\\n  å•ä½çŸ©é˜µ (3,3):\\n{eye}\")\n",
    "\n",
    "# ----- éšæœºå¼ é‡ -----\n",
    "print(\"\\nåˆ›å»ºéšæœºå¼ é‡:\")\n",
    "\n",
    "# æ ‡å‡†æ­£æ€åˆ†å¸ƒ N(0, 1)ï¼ˆæœ€å¸¸ç”¨äºæƒé‡åˆå§‹åŒ–ï¼‰\n",
    "randn = torch.randn(2, 3)\n",
    "print(f\"  æ­£æ€åˆ†å¸ƒN(0,1):\\n{randn}\")\n",
    "\n",
    "# å‡åŒ€åˆ†å¸ƒ U(0, 1)\n",
    "rand = torch.rand(2, 3)\n",
    "print(f\"\\n  å‡åŒ€åˆ†å¸ƒU(0,1):\\n{rand}\")\n",
    "\n",
    "# æŒ‡å®šèŒƒå›´çš„æ•´æ•°\n",
    "randint = torch.randint(low=0, high=10, size=(2, 3))\n",
    "print(f\"\\n  éšæœºæ•´æ•°[0,10):\\n{randint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.1 å¼ é‡ä¸NumPyå¯¹æ¯”\n",
      "============================================================\n",
      "\n",
      "2ï¸âƒ£ Tensorä¸NumPy ndarrayå¯¹æ¯”:\n",
      "\n",
      "NumPy array:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n",
      "PyTorch tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "------------------------------------------------------------\n",
      "å¸¸ç”¨æ“ä½œå¯¹æ¯”:\n",
      "------------------------------------------------------------\n",
      "æ“ä½œ                   NumPy                     PyTorch                  \n",
      "------------------------------------------------------------\n",
      "åˆ›å»ºé›¶çŸ©é˜µ                np.zeros((2,3))           torch.zeros(2,3)         \n",
      "åˆ›å»ºéšæœºçŸ©é˜µ               np.random.randn(2,3)      torch.randn(2,3)         \n",
      "çŸ©é˜µä¹˜æ³•                 A @ B æˆ– np.dot(A,B)       A @ B æˆ– torch.mm(A,B)    \n",
      "é€å…ƒç´ ä¹˜æ³•                A * B                     A * B                    \n",
      "æ±‚å’Œ                   np.sum(A)                 torch.sum(A) æˆ– A.sum()   \n",
      "å½¢çŠ¶                   A.shape                   A.shape æˆ– A.size()       \n",
      "è½¬ç½®                   A.T                       A.T                      \n",
      "reshape              A.reshape(3,2)            A.reshape(3,2) æˆ– view    \n",
      "------------------------------------------------------------\n",
      "\n",
      "3ï¸âƒ£ NumPyä¸Tensorç›¸äº’è½¬æ¢:\n",
      "\n",
      "åŸå§‹NumPy: [1. 2. 3.]\n",
      "torch.from_numpy(): tensor([1., 2., 3.], dtype=torch.float64)  âš ï¸ å…±äº«å†…å­˜\n",
      "torch.tensor(): tensor([1., 2., 3.], dtype=torch.float64)  âœ“ ç‹¬ç«‹æ‹·è´\n",
      "\n",
      "ä¿®æ”¹NumPy[0]=999å:\n",
      "  from_numpyç‰ˆæœ¬ä¹Ÿå˜äº†: tensor([999.,   2.,   3.], dtype=torch.float64)\n",
      "  tensorç‰ˆæœ¬ä¸å˜: tensor([1., 2., 3.], dtype=torch.float64)\n",
      "\n",
      "Tensor -> NumPy: [4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.1 å¼ é‡ä¸NumPyå¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Tensor vs NumPy ndarrayï¼šå‡ ä¹ä¸€æ ·çš„æ“ä½œï¼\n",
    "# ============================================================\n",
    "print(\"\\n2ï¸âƒ£ Tensorä¸NumPy ndarrayå¯¹æ¯”:\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# åˆ›å»ºç›¸åŒæ•°æ®\n",
    "np_array = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "torch_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\nNumPy array:\\n{np_array}\")\n",
    "print(f\"\\nPyTorch tensor:\\n{torch_tensor}\")\n",
    "\n",
    "# ----- æ“ä½œå¯¹æ¯”è¡¨ -----\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"å¸¸ç”¨æ“ä½œå¯¹æ¯”:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'æ“ä½œ':<20} {'NumPy':<25} {'PyTorch':<25}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'åˆ›å»ºé›¶çŸ©é˜µ':<20} {'np.zeros((2,3))':<25} {'torch.zeros(2,3)':<25}\")\n",
    "print(f\"{'åˆ›å»ºéšæœºçŸ©é˜µ':<20} {'np.random.randn(2,3)':<25} {'torch.randn(2,3)':<25}\")\n",
    "print(f\"{'çŸ©é˜µä¹˜æ³•':<20} {'A @ B æˆ– np.dot(A,B)':<25} {'A @ B æˆ– torch.mm(A,B)':<25}\")\n",
    "print(f\"{'é€å…ƒç´ ä¹˜æ³•':<20} {'A * B':<25} {'A * B':<25}\")\n",
    "print(f\"{'æ±‚å’Œ':<20} {'np.sum(A)':<25} {'torch.sum(A) æˆ– A.sum()':<25}\")\n",
    "print(f\"{'å½¢çŠ¶':<20} {'A.shape':<25} {'A.shape æˆ– A.size()':<25}\")\n",
    "print(f\"{'è½¬ç½®':<20} {'A.T':<25} {'A.T':<25}\")\n",
    "print(f\"{'reshape':<20} {'A.reshape(3,2)':<25} {'A.reshape(3,2) æˆ– view':<25}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# ----- ç›¸äº’è½¬æ¢ -----\n",
    "print(\"\\n3ï¸âƒ£ NumPyä¸Tensorç›¸äº’è½¬æ¢:\")\n",
    "\n",
    "# NumPy -> Tensor\n",
    "np_data = np.array([1.0, 2.0, 3.0])\n",
    "tensor_from_np = torch.from_numpy(np_data)  # å…±äº«å†…å­˜ï¼\n",
    "tensor_copy = torch.tensor(np_data)          # æ‹·è´æ•°æ®\n",
    "\n",
    "print(f\"\\nåŸå§‹NumPy: {np_data}\")\n",
    "print(f\"torch.from_numpy(): {tensor_from_np}  âš ï¸ å…±äº«å†…å­˜\")\n",
    "print(f\"torch.tensor(): {tensor_copy}  âœ“ ç‹¬ç«‹æ‹·è´\")\n",
    "\n",
    "# éªŒè¯å†…å­˜å…±äº«\n",
    "np_data[0] = 999\n",
    "print(f\"\\nä¿®æ”¹NumPy[0]=999å:\")\n",
    "print(f\"  from_numpyç‰ˆæœ¬ä¹Ÿå˜äº†: {tensor_from_np}\")\n",
    "print(f\"  tensorç‰ˆæœ¬ä¸å˜: {tensor_copy}\")\n",
    "\n",
    "# Tensor -> NumPy\n",
    "tensor_data = torch.tensor([4.0, 5.0, 6.0])\n",
    "np_from_tensor = tensor_data.numpy()  # å…±äº«å†…å­˜\n",
    "print(f\"\\nTensor -> NumPy: {np_from_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.1 å¼ é‡è¿ç®—\n",
      "============================================================\n",
      "\n",
      "4ï¸âƒ£ å¼ é‡è¿ç®—:\n",
      "a = \n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "b = \n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "\n",
      "åŸºæœ¬è¿ç®—:\n",
      "a + b = \n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "\n",
      "a * b (é€å…ƒç´ ) = \n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "\n",
      "a @ b (çŸ©é˜µä¹˜æ³•) = \n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "ç»Ÿè®¡è¿ç®—:\n",
      "x = \n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "æ€»å’Œ: 21.0\n",
      "å‡å€¼: 3.5\n",
      "æœ€å¤§å€¼: 6.0\n",
      "æŒ‰è¡Œæ±‚å’Œ (dim=1): tensor([ 6., 15.])\n",
      "æŒ‰åˆ—æ±‚å’Œ (dim=0): tensor([5., 7., 9.])\n",
      "\n",
      "å¹¿æ’­æœºåˆ¶:\n",
      "ones(2,3) + [1,2,3] = \n",
      "tensor([[2., 3., 4.],\n",
      "        [2., 3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.1 å¼ é‡è¿ç®—\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# 3. å¼ é‡è¿ç®—ï¼šä¸NumPyå‡ ä¹ä¸€è‡´\n",
    "# ============================================================\n",
    "print(\"\\n4ï¸âƒ£ å¼ é‡è¿ç®—:\")\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•å¼ é‡\n",
    "a = torch.tensor([[1.0, 2.0], \n",
    "                  [3.0, 4.0]])\n",
    "b = torch.tensor([[5.0, 6.0], \n",
    "                  [7.0, 8.0]])\n",
    "\n",
    "print(f\"a = \\n{a}\")\n",
    "print(f\"\\nb = \\n{b}\")\n",
    "\n",
    "# ----- åŸºæœ¬è¿ç®— -----\n",
    "print(\"\\nåŸºæœ¬è¿ç®—:\")\n",
    "\n",
    "# åŠ æ³•\n",
    "print(f\"a + b = \\n{a + b}\")\n",
    "\n",
    "# é€å…ƒç´ ä¹˜æ³•ï¼ˆHadamardç§¯ï¼‰\n",
    "print(f\"\\na * b (é€å…ƒç´ ) = \\n{a * b}\")\n",
    "\n",
    "# çŸ©é˜µä¹˜æ³•ï¼ˆæœ€é‡è¦ï¼ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒè¿ç®—ï¼‰\n",
    "print(f\"\\na @ b (çŸ©é˜µä¹˜æ³•) = \\n{a @ b}\")\n",
    "# ç­‰ä»·äº: torch.mm(a, b) æˆ– torch.matmul(a, b)\n",
    "\n",
    "# ----- ç»Ÿè®¡è¿ç®— -----\n",
    "print(\"\\nç»Ÿè®¡è¿ç®—:\")\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "print(f\"x = \\n{x}\")\n",
    "print(f\"\\næ€»å’Œ: {x.sum()}\")\n",
    "print(f\"å‡å€¼: {x.mean()}\")\n",
    "print(f\"æœ€å¤§å€¼: {x.max()}\")\n",
    "print(f\"æŒ‰è¡Œæ±‚å’Œ (dim=1): {x.sum(dim=1)}\")\n",
    "print(f\"æŒ‰åˆ—æ±‚å’Œ (dim=0): {x.sum(dim=0)}\")\n",
    "\n",
    "# ----- å¹¿æ’­æœºåˆ¶ï¼ˆä¸NumPyç›¸åŒï¼‰ -----\n",
    "print(\"\\nå¹¿æ’­æœºåˆ¶:\")\n",
    "matrix = torch.ones(2, 3)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "result = matrix + vector  # è‡ªåŠ¨å¹¿æ’­\n",
    "print(f\"ones(2,3) + [1,2,3] = \\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.1 å¼ é‡å½¢çŠ¶æ“ä½œ\n",
      "============================================================\n",
      "\n",
      "5ï¸âƒ£ å½¢çŠ¶æ“ä½œ:\n",
      "\n",
      "åŸå§‹å¼ é‡: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "å½¢çŠ¶: torch.Size([12])\n",
      "\n",
      "reshape(3,4):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "view(4,3):\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "\n",
      "reshape(2,-1) è‡ªåŠ¨æ¨æ–­:\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "\n",
      "flatten(): tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\n",
      "è½¬ç½®æ“ä½œ:\n",
      "åŸçŸ©é˜µ (2,3):\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "è½¬ç½® .T (3,2):\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "\n",
      "3Då¼ é‡å½¢çŠ¶: torch.Size([2, 3, 4])\n",
      "transpose(0,2)å: torch.Size([4, 3, 2])\n",
      "permute(2,0,1)å: torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.1 å¼ é‡å½¢çŠ¶æ“ä½œ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# 4. å½¢çŠ¶æ“ä½œï¼šreshape, view, transpose\n",
    "# ============================================================\n",
    "print(\"\\n5ï¸âƒ£ å½¢çŠ¶æ“ä½œ:\")\n",
    "\n",
    "x = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
    "print(f\"\\nåŸå§‹å¼ é‡: {x}\")\n",
    "print(f\"å½¢çŠ¶: {x.shape}\")\n",
    "\n",
    "# ----- reshapeï¼ˆæ”¹å˜å½¢çŠ¶ï¼‰ -----\n",
    "x_3x4 = x.reshape(3, 4)\n",
    "print(f\"\\nreshape(3,4):\\n{x_3x4}\")\n",
    "\n",
    "# ----- viewï¼ˆç±»ä¼¼reshapeï¼Œä½†è¦æ±‚å†…å­˜è¿ç»­ï¼‰ -----\n",
    "x_4x3 = x.view(4, 3)\n",
    "print(f\"\\nview(4,3):\\n{x_4x3}\")\n",
    "\n",
    "# ----- ä½¿ç”¨-1è‡ªåŠ¨æ¨æ–­ç»´åº¦ -----\n",
    "x_auto = x.reshape(2, -1)  # è‡ªåŠ¨è®¡ç®—åˆ—æ•°ï¼š12/2=6\n",
    "print(f\"\\nreshape(2,-1) è‡ªåŠ¨æ¨æ–­:\\n{x_auto}\")\n",
    "\n",
    "# ----- flattenï¼ˆå±•å¹³ï¼‰ -----\n",
    "x_flat = x_3x4.flatten()\n",
    "print(f\"\\nflatten(): {x_flat}\")\n",
    "\n",
    "# ----- transposeï¼ˆè½¬ç½®ï¼‰ -----\n",
    "print(\"\\nè½¬ç½®æ“ä½œ:\")\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "print(f\"åŸçŸ©é˜µ (2,3):\\n{matrix}\")\n",
    "print(f\"\\nè½¬ç½® .T (3,2):\\n{matrix.T}\")\n",
    "\n",
    "# é«˜ç»´å¼ é‡çš„transpose\n",
    "tensor_3d = torch.randn(2, 3, 4)\n",
    "print(f\"\\n3Då¼ é‡å½¢çŠ¶: {tensor_3d.shape}\")\n",
    "print(f\"transpose(0,2)å: {tensor_3d.transpose(0, 2).shape}\")\n",
    "print(f\"permute(2,0,1)å: {tensor_3d.permute(2, 0, 1).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.1 ç´¢å¼•ä¸åˆ‡ç‰‡ + GPUè½¬ç§»\n",
      "============================================================\n",
      "\n",
      "6ï¸âƒ£ ç´¢å¼•å’Œåˆ‡ç‰‡:\n",
      "åŸå§‹å¼ é‡:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "\n",
      "x[0, :] ç¬¬1è¡Œ: tensor([1, 2, 3])\n",
      "x[:, 1] ç¬¬2åˆ—: tensor([2, 5, 8])\n",
      "x[1:, 1:] å³ä¸‹è§’2x2:\n",
      "tensor([[5, 6],\n",
      "        [8, 9]])\n",
      "x[0, 2] å•ä¸ªå…ƒç´ : 3\n",
      "\n",
      "å¸ƒå°”æ©ç  x > 5:\n",
      "tensor([[False, False, False],\n",
      "        [False, False,  True],\n",
      "        [ True,  True,  True]])\n",
      "æ»¡è¶³æ¡ä»¶çš„å…ƒç´ : tensor([6, 7, 8, 9])\n",
      "\n",
      "============================================================\n",
      "7ï¸âƒ£ è®¾å¤‡è½¬ç§»ï¼ˆGPUåŠ é€Ÿï¼‰:\n",
      "ä½¿ç”¨è®¾å¤‡: MPS (Apple Silicon GPU)\n",
      "\n",
      "åŸå§‹å¼ é‡è®¾å¤‡: cpu\n",
      "è½¬ç§»åè®¾å¤‡: mps:0\n",
      "GPUä¸Šè®¡ç®—ç»“æœè®¾å¤‡: mps:0\n",
      "è½¬å›CPU: cpu\n",
      "\n",
      "âš ï¸ æ³¨æ„: GPUå¼ é‡å¿…é¡»å…ˆ.cpu()æ‰èƒ½.numpy()\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.1 ç´¢å¼•ä¸åˆ‡ç‰‡ + GPUè½¬ç§»\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# 5. ç´¢å¼•å’Œåˆ‡ç‰‡ï¼ˆä¸NumPyå®Œå…¨ä¸€è‡´ï¼‰\n",
    "# ============================================================\n",
    "print(\"\\n6ï¸âƒ£ ç´¢å¼•å’Œåˆ‡ç‰‡:\")\n",
    "\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "print(f\"åŸå§‹å¼ é‡:\\n{x}\")\n",
    "\n",
    "print(f\"\\nx[0, :] ç¬¬1è¡Œ: {x[0, :]}\")\n",
    "print(f\"x[:, 1] ç¬¬2åˆ—: {x[:, 1]}\")\n",
    "print(f\"x[1:, 1:] å³ä¸‹è§’2x2:\\n{x[1:, 1:]}\")\n",
    "print(f\"x[0, 2] å•ä¸ªå…ƒç´ : {x[0, 2]}\")\n",
    "\n",
    "# å¸ƒå°”ç´¢å¼•\n",
    "mask = x > 5\n",
    "print(f\"\\nå¸ƒå°”æ©ç  x > 5:\\n{mask}\")\n",
    "print(f\"æ»¡è¶³æ¡ä»¶çš„å…ƒç´ : {x[mask]}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. GPU/MPSè®¾å¤‡è½¬ç§»\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7ï¸âƒ£ è®¾å¤‡è½¬ç§»ï¼ˆGPUåŠ é€Ÿï¼‰:\")\n",
    "\n",
    "# æ£€æµ‹å¯ç”¨è®¾å¤‡\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"ä½¿ç”¨è®¾å¤‡: CUDA (NVIDIA GPU)\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"ä½¿ç”¨è®¾å¤‡: MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"ä½¿ç”¨è®¾å¤‡: CPU\")\n",
    "\n",
    "# åˆ›å»ºCPUå¼ é‡\n",
    "x_cpu = torch.randn(3, 3)\n",
    "print(f\"\\nåŸå§‹å¼ é‡è®¾å¤‡: {x_cpu.device}\")\n",
    "\n",
    "# è½¬ç§»åˆ°GPU/MPS\n",
    "x_gpu = x_cpu.to(device)\n",
    "print(f\"è½¬ç§»åè®¾å¤‡: {x_gpu.device}\")\n",
    "\n",
    "# åœ¨GPUä¸Šè¿ç®—\n",
    "y_gpu = x_gpu @ x_gpu.T\n",
    "print(f\"GPUä¸Šè®¡ç®—ç»“æœè®¾å¤‡: {y_gpu.device}\")\n",
    "\n",
    "# è½¬å›CPUï¼ˆä¾‹å¦‚ç”¨äºç»˜å›¾æˆ–NumPyæ“ä½œï¼‰\n",
    "y_cpu = y_gpu.cpu()\n",
    "print(f\"è½¬å›CPU: {y_cpu.device}\")\n",
    "\n",
    "# è½¬æ¢ä¸ºNumPyï¼ˆå¿…é¡»å…ˆè½¬åˆ°CPUï¼ï¼‰\n",
    "y_numpy = y_cpu.numpy()\n",
    "print(f\"\\nâš ï¸ æ³¨æ„: GPUå¼ é‡å¿…é¡»å…ˆ.cpu()æ‰èƒ½.numpy()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰ï¼šå‘Šåˆ«æ‰‹å†™åå‘ä¼ æ’­ï¼\n",
    "\n",
    "### ğŸ¯ è¿™æ˜¯PyTorchæœ€æ ¸å¿ƒçš„åŠŸèƒ½\n",
    "\n",
    "åœ¨ç¬¬4ç« ï¼Œæˆ‘ä»¬æ‰‹å†™äº†ç—›è‹¦çš„åå‘ä¼ æ’­ä»£ç ã€‚ç°åœ¨ï¼ŒPyTorchå¯ä»¥**è‡ªåŠ¨å®Œæˆ**ï¼\n",
    "\n",
    "### å·¥ä½œåŸç†\n",
    "\n",
    "1. è®¾ç½® `requires_grad=True`ï¼šå‘Šè¯‰PyTorchè¦è·Ÿè¸ªè¿™ä¸ªå¼ é‡çš„æ“ä½œ\n",
    "2. æ‰§è¡Œå‰å‘è®¡ç®—ï¼šPyTorchåœ¨åå°æ„å»º**è®¡ç®—å›¾**\n",
    "3. è°ƒç”¨ `.backward()`ï¼šè‡ªåŠ¨è®¡ç®—æ‰€æœ‰æ¢¯åº¦\n",
    "4. è¯»å– `.grad` å±æ€§ï¼šè·å–è®¡ç®—å¥½çš„æ¢¯åº¦\n",
    "\n",
    "### å¯¹æ¯”\n",
    "\n",
    "| æ­¥éª¤ | NumPyæ‰‹å†™ | PyTorch Autograd |\n",
    "|------|-----------|------------------|\n",
    "| å®šä¹‰å‡½æ•° | å†™ä»£ç  | å†™ä»£ç  |\n",
    "| æ¨å¯¼æ¢¯åº¦ | æ‰‹åŠ¨ç”¨ç¬”æ¨å¯¼å…¬å¼ | **ä¸éœ€è¦** |\n",
    "| å®ç°æ¢¯åº¦ | å†™ä»£ç å®ç°å…¬å¼ | **è°ƒç”¨.backward()** |\n",
    "| éªŒè¯æ­£ç¡®æ€§ | æ•°å€¼æ¢¯åº¦æ£€éªŒ | **æ¡†æ¶ä¿è¯æ­£ç¡®** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.2 è‡ªåŠ¨å¾®åˆ†åŸºç¡€\n",
      "============================================================\n",
      "\n",
      "ç¤ºä¾‹1: è®¡ç®— f(x) = xÂ² åœ¨ x=3 å¤„çš„æ¢¯åº¦\n",
      "      æ•°å­¦æ¨å¯¼: df/dx = 2x = 2Ã—3 = 6\n",
      "\n",
      "æ­¥éª¤1: x = 3.0\n",
      "       requires_grad = True\n",
      "\n",
      "æ­¥éª¤2: y = xÂ² = 9.0\n",
      "       grad_fn = <PowBackward0 object at 0x103fbfee0>\n",
      "\n",
      "æ­¥éª¤3: è°ƒç”¨ y.backward()\n",
      "\n",
      "æ­¥éª¤4: x.grad = 6.0\n",
      "âœ… ç»“æœæ­£ç¡®ï¼dy/dx = 2Ã—3 = 6\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.2 è‡ªåŠ¨å¾®åˆ†åŸºç¡€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# ç¤ºä¾‹1ï¼šç®€å•å‡½æ•°çš„æ¢¯åº¦è®¡ç®—\n",
    "# ============================================================\n",
    "print(\"\\nç¤ºä¾‹1: è®¡ç®— f(x) = xÂ² åœ¨ x=3 å¤„çš„æ¢¯åº¦\")\n",
    "print(\"      æ•°å­¦æ¨å¯¼: df/dx = 2x = 2Ã—3 = 6\")\n",
    "\n",
    "# æ­¥éª¤1: åˆ›å»ºå¼ é‡ï¼Œè®¾ç½®requires_grad=True\n",
    "# è¿™å‘Šè¯‰PyTorchï¼šè¯·è·Ÿè¸ªå¯¹è¿™ä¸ªå¼ é‡çš„æ‰€æœ‰æ“ä½œ\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "print(f\"\\næ­¥éª¤1: x = {x}\")\n",
    "print(f\"       requires_grad = {x.requires_grad}\")\n",
    "\n",
    "# æ­¥éª¤2: å‰å‘è®¡ç®—\n",
    "# PyTorchåœ¨åå°è®°å½•ï¼šyæ˜¯xçš„å¹³æ–¹\n",
    "y = x ** 2\n",
    "print(f\"\\næ­¥éª¤2: y = xÂ² = {y}\")\n",
    "print(f\"       grad_fn = {y.grad_fn}\")  # è®°å½•äº†å¦‚ä½•è®¡ç®—çš„\n",
    "\n",
    "# æ­¥éª¤3: åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰\n",
    "y.backward()\n",
    "print(f\"\\næ­¥éª¤3: è°ƒç”¨ y.backward()\")\n",
    "\n",
    "# æ­¥éª¤4: è¯»å–æ¢¯åº¦\n",
    "print(f\"\\næ­¥éª¤4: x.grad = {x.grad}\")\n",
    "print(f\"âœ… ç»“æœæ­£ç¡®ï¼dy/dx = 2Ã—3 = 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.2 å¤šå˜é‡å‡½æ•°çš„æ¢¯åº¦\n",
      "============================================================\n",
      "\n",
      "ç¤ºä¾‹2: è®¡ç®— f(x,y) = xÂ² + 2xy + yÂ² çš„æ¢¯åº¦\n",
      "      âˆ‚f/âˆ‚x = 2x + 2y\n",
      "      âˆ‚f/âˆ‚y = 2x + 2y\n",
      "      åœ¨(x=2, y=3)å¤„: âˆ‚f/âˆ‚x = âˆ‚f/âˆ‚y = 2Ã—2+2Ã—3 = 10\n",
      "\n",
      "f(2,3) = 25.0\n",
      "\n",
      "âˆ‚f/âˆ‚x = 10.0 (ç†è®ºå€¼: 10)\n",
      "âˆ‚f/âˆ‚y = 10.0 (ç†è®ºå€¼: 10)\n",
      "\n",
      "âœ… å¤šå˜é‡å‡½æ•°çš„æ¢¯åº¦ä¹Ÿè‡ªåŠ¨è®¡ç®—ï¼\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.2 å¤šå˜é‡å‡½æ•°çš„æ¢¯åº¦\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# ç¤ºä¾‹2ï¼šå¤šå˜é‡å‡½æ•°\n",
    "# ============================================================\n",
    "print(\"\\nç¤ºä¾‹2: è®¡ç®— f(x,y) = xÂ² + 2xy + yÂ² çš„æ¢¯åº¦\")\n",
    "print(\"      âˆ‚f/âˆ‚x = 2x + 2y\")\n",
    "print(\"      âˆ‚f/âˆ‚y = 2x + 2y\")\n",
    "print(\"      åœ¨(x=2, y=3)å¤„: âˆ‚f/âˆ‚x = âˆ‚f/âˆ‚y = 2Ã—2+2Ã—3 = 10\")\n",
    "\n",
    "# åˆ›å»ºå˜é‡\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# å‰å‘è®¡ç®—\n",
    "f = x**2 + 2*x*y + y**2\n",
    "print(f\"\\nf(2,3) = {f.item()}\")\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "f.backward()\n",
    "\n",
    "# è¯»å–æ¢¯åº¦\n",
    "print(f\"\\nâˆ‚f/âˆ‚x = {x.grad.item()} (ç†è®ºå€¼: 10)\")\n",
    "print(f\"âˆ‚f/âˆ‚y = {y.grad.item()} (ç†è®ºå€¼: 10)\")\n",
    "print(\"\\nâœ… å¤šå˜é‡å‡½æ•°çš„æ¢¯åº¦ä¹Ÿè‡ªåŠ¨è®¡ç®—ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.2 å¯¹æ¯”ï¼šæ‰‹å†™é“¾å¼æ³•åˆ™ vs Autograd\n",
      "============================================================\n",
      "\n",
      "ç¤ºä¾‹3: å¤åˆå‡½æ•° f(x) = sin(xÂ²) åœ¨ x=1 å¤„çš„æ¢¯åº¦\n",
      "\n",
      "æ‰‹åŠ¨æ¨å¯¼é“¾å¼æ³•åˆ™:\n",
      "  è®¾ u = xÂ²\n",
      "  f = sin(u)\n",
      "  df/dx = df/du Ã— du/dx\n",
      "        = cos(u) Ã— 2x\n",
      "        = cos(xÂ²) Ã— 2x\n",
      "        = cos(1) Ã— 2 â‰ˆ 1.0806\n",
      "\n",
      "æ‰‹åŠ¨è®¡ç®—: 1.080605\n",
      "PyTorchè‡ªåŠ¨: 1.080605\n",
      "\n",
      "âœ… å®Œå…¨ä¸€è‡´ï¼PyTorchè‡ªåŠ¨åº”ç”¨äº†é“¾å¼æ³•åˆ™ï¼\n",
      "\n",
      "------------------------------------------------------------\n",
      "å…³é”®ä¼˜åŠ¿:\n",
      "  - æ— éœ€æ‰‹åŠ¨æ¨å¯¼: cos(xÂ²)Ã—2x\n",
      "  - æ— éœ€æ‹…å¿ƒå‡ºé”™\n",
      "  - ä»»æ„å¤æ‚çš„å¤åˆå‡½æ•°éƒ½èƒ½è‡ªåŠ¨å¤„ç†\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.2 å¯¹æ¯”ï¼šæ‰‹å†™é“¾å¼æ³•åˆ™ vs Autograd\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# ç¤ºä¾‹3ï¼šå¤åˆå‡½æ•°ï¼ˆå±•ç¤ºé“¾å¼æ³•åˆ™çš„è‡ªåŠ¨åº”ç”¨ï¼‰\n",
    "# ============================================================\n",
    "print(\"\\nç¤ºä¾‹3: å¤åˆå‡½æ•° f(x) = sin(xÂ²) åœ¨ x=1 å¤„çš„æ¢¯åº¦\")\n",
    "print(\"\")\n",
    "print(\"æ‰‹åŠ¨æ¨å¯¼é“¾å¼æ³•åˆ™:\")\n",
    "print(\"  è®¾ u = xÂ²\")\n",
    "print(\"  f = sin(u)\")\n",
    "print(\"  df/dx = df/du Ã— du/dx\")\n",
    "print(\"        = cos(u) Ã— 2x\")\n",
    "print(\"        = cos(xÂ²) Ã— 2x\")\n",
    "print(\"        = cos(1) Ã— 2 â‰ˆ 1.0806\")\n",
    "\n",
    "# ----- æ‰‹å†™è®¡ç®— -----\n",
    "import math\n",
    "x_val = 1.0\n",
    "manual_grad = math.cos(x_val**2) * 2 * x_val\n",
    "print(f\"\\næ‰‹åŠ¨è®¡ç®—: {manual_grad:.6f}\")\n",
    "\n",
    "# ----- PyTorchè‡ªåŠ¨è®¡ç®— -----\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "f = torch.sin(x**2)\n",
    "f.backward()\n",
    "print(f\"PyTorchè‡ªåŠ¨: {x.grad.item():.6f}\")\n",
    "\n",
    "print(\"\\nâœ… å®Œå…¨ä¸€è‡´ï¼PyTorchè‡ªåŠ¨åº”ç”¨äº†é“¾å¼æ³•åˆ™ï¼\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"å…³é”®ä¼˜åŠ¿:\")\n",
    "print(\"  - æ— éœ€æ‰‹åŠ¨æ¨å¯¼: cos(xÂ²)Ã—2x\")\n",
    "print(\"  - æ— éœ€æ‹…å¿ƒå‡ºé”™\")\n",
    "print(\"  - ä»»æ„å¤æ‚çš„å¤åˆå‡½æ•°éƒ½èƒ½è‡ªåŠ¨å¤„ç†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.2 æ¨¡æ‹Ÿç¥ç»ç½‘ç»œå±‚çš„è‡ªåŠ¨æ±‚å¯¼\n",
      "============================================================\n",
      "\n",
      "ç¤ºä¾‹4: æ¨¡æ‹Ÿçº¿æ€§å±‚ y = Wx + b\n",
      "\n",
      "è¿™æ­£æ˜¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒè¿ç®—ï¼\n",
      "PyTorchä¼šè‡ªåŠ¨è®¡ç®— âˆ‚L/âˆ‚W å’Œ âˆ‚L/âˆ‚b\n",
      "\n",
      "è¾“å…¥ x: tensor([1., 2., 3.])\n",
      "æƒé‡ W å½¢çŠ¶: torch.Size([2, 3])\n",
      "åç½® b å½¢çŠ¶: torch.Size([2])\n",
      "\n",
      "è¾“å‡º y = Wx + b: tensor([-3.8251,  0.4256], grad_fn=<AddBackward0>)\n",
      "æŸå¤± L = sum(y): -3.3995\n",
      "\n",
      "è‡ªåŠ¨è®¡ç®—çš„æ¢¯åº¦:\n",
      "  âˆ‚L/âˆ‚W:\n",
      "tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "\n",
      "  âˆ‚L/âˆ‚b: tensor([1., 1.])\n",
      "\n",
      "------------------------------------------------------------\n",
      "å¯¹æ¯”NumPyå®ç°:\n",
      "  NumPy: éœ€è¦æ‰‹åŠ¨æ¨å¯¼ dL/dW = x^T @ dL/dy\n",
      "  PyTorch: loss.backward() ä¸€è¡Œæå®šï¼\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.2 æ¨¡æ‹Ÿç¥ç»ç½‘ç»œå±‚çš„è‡ªåŠ¨æ±‚å¯¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# ç¤ºä¾‹4ï¼šæ¨¡æ‹Ÿç¥ç»ç½‘ç»œçš„å‰å‘å’Œåå‘ä¼ æ’­\n",
    "# ============================================================\n",
    "print(\"\\nç¤ºä¾‹4: æ¨¡æ‹Ÿçº¿æ€§å±‚ y = Wx + b\")\n",
    "print(\"\")\n",
    "print(\"è¿™æ­£æ˜¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒè¿ç®—ï¼\")\n",
    "print(\"PyTorchä¼šè‡ªåŠ¨è®¡ç®— âˆ‚L/âˆ‚W å’Œ âˆ‚L/âˆ‚b\")\n",
    "\n",
    "# è¾“å…¥æ•°æ®ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"\\nè¾“å…¥ x: {x}\")\n",
    "\n",
    "# æ¨¡å‹å‚æ•°ï¼ˆéœ€è¦æ¢¯åº¦ï¼ï¼‰\n",
    "W = torch.randn(2, 3, requires_grad=True)  # æƒé‡çŸ©é˜µ\n",
    "b = torch.randn(2, requires_grad=True)      # åç½®å‘é‡\n",
    "print(f\"æƒé‡ W å½¢çŠ¶: {W.shape}\")\n",
    "print(f\"åç½® b å½¢çŠ¶: {b.shape}\")\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "y = W @ x + b  # çº¿æ€§å˜æ¢\n",
    "print(f\"\\nè¾“å‡º y = Wx + b: {y}\")\n",
    "\n",
    "# å‡è®¾æŸå¤±å‡½æ•°æ˜¯è¾“å‡ºçš„å’Œï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰\n",
    "loss = y.sum()\n",
    "print(f\"æŸå¤± L = sum(y): {loss.item():.4f}\")\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "loss.backward()\n",
    "\n",
    "# æŸ¥çœ‹æ¢¯åº¦\n",
    "print(f\"\\nè‡ªåŠ¨è®¡ç®—çš„æ¢¯åº¦:\")\n",
    "print(f\"  âˆ‚L/âˆ‚W:\\n{W.grad}\")\n",
    "print(f\"\\n  âˆ‚L/âˆ‚b: {b.grad}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"å¯¹æ¯”NumPyå®ç°:\")\n",
    "print(\"  NumPy: éœ€è¦æ‰‹åŠ¨æ¨å¯¼ dL/dW = x^T @ dL/dy\")\n",
    "print(\"  PyTorch: loss.backward() ä¸€è¡Œæå®šï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.2 è®¡ç®—å›¾å¯è§†åŒ–ç†è§£\n",
      "============================================================\n",
      "\n",
      "è®¡ç®—å›¾ç¤ºä¾‹: y = (a + b) Ã— c\n",
      "\n",
      "å‰å‘ä¼ æ’­ï¼ˆæ„å»ºè®¡ç®—å›¾ï¼‰:\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "    [a=2]  [b=3]                    [c=4]\n",
      "       \\    /                          |\n",
      "        \\  /                           |\n",
      "       [Add]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  [Mul]\n",
      "         |                            |\n",
      "      [t=5]                        [y=20]\n",
      "\n",
      "    æ“ä½œè®°å½•: t = a + b, y = t Ã— c\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨å¾®åˆ†ï¼‰:\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "    dy/dy = 1ï¼ˆä»è¾“å‡ºå¼€å§‹ï¼‰\n",
      "       â†“\n",
      "    dy/dt = c = 4ï¼ˆä¹˜æ³•çš„æ¢¯åº¦ï¼‰\n",
      "    dy/dc = t = 5\n",
      "       â†“\n",
      "    dy/da = dy/dt Ã— dt/da = 4 Ã— 1 = 4ï¼ˆé“¾å¼æ³•åˆ™ï¼‰\n",
      "    dy/db = dy/dt Ã— dt/db = 4 Ã— 1 = 4\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ä»£ç éªŒè¯:\n",
      "  y = (a+b)Ã—c = (2+3)Ã—4 = 20.0\n",
      "  dy/da = 4.0 (ç†è®ºå€¼: c = 4)\n",
      "  dy/db = 4.0 (ç†è®ºå€¼: c = 4)\n",
      "  dy/dc = 5.0 (ç†è®ºå€¼: a+b = 5)\n",
      "\n",
      "âœ… PyTorchè‡ªåŠ¨å®Œæˆäº†é“¾å¼æ³•åˆ™çš„è®¡ç®—ï¼\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.2 è®¡ç®—å›¾å¯è§†åŒ–ç†è§£\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# è®¡ç®—å›¾çš„æ¦‚å¿µè§£é‡Šï¼ˆæ–‡å­—å¯è§†åŒ–ï¼‰\n",
    "# ============================================================\n",
    "print(\"\"\"\n",
    "è®¡ç®—å›¾ç¤ºä¾‹: y = (a + b) Ã— c\n",
    "\n",
    "å‰å‘ä¼ æ’­ï¼ˆæ„å»ºè®¡ç®—å›¾ï¼‰:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "    [a=2]  [b=3]                    [c=4]\n",
    "       \\    /                          |\n",
    "        \\  /                           |\n",
    "       [Add]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  [Mul]\n",
    "         |                            |\n",
    "      [t=5]                        [y=20]\n",
    "\n",
    "    æ“ä½œè®°å½•: t = a + b, y = t Ã— c\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨å¾®åˆ†ï¼‰:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "    dy/dy = 1ï¼ˆä»è¾“å‡ºå¼€å§‹ï¼‰\n",
    "       â†“\n",
    "    dy/dt = c = 4ï¼ˆä¹˜æ³•çš„æ¢¯åº¦ï¼‰\n",
    "    dy/dc = t = 5\n",
    "       â†“\n",
    "    dy/da = dy/dt Ã— dt/da = 4 Ã— 1 = 4ï¼ˆé“¾å¼æ³•åˆ™ï¼‰\n",
    "    dy/db = dy/dt Ã— dt/db = 4 Ã— 1 = 4\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# ç”¨ä»£ç éªŒè¯\n",
    "print(\"ä»£ç éªŒè¯:\")\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "t = a + b\n",
    "y = t * c\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(f\"  y = (a+b)Ã—c = (2+3)Ã—4 = {y.item()}\")\n",
    "print(f\"  dy/da = {a.grad.item()} (ç†è®ºå€¼: c = 4)\")\n",
    "print(f\"  dy/db = {b.grad.item()} (ç†è®ºå€¼: c = 4)\")\n",
    "print(f\"  dy/dc = {c.grad.item()} (ç†è®ºå€¼: a+b = 5)\")\n",
    "print(\"\\nâœ… PyTorchè‡ªåŠ¨å®Œæˆäº†é“¾å¼æ³•åˆ™çš„è®¡ç®—ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.2 Autogradæ³¨æ„äº‹é¡¹\n",
      "============================================================\n",
      "\n",
      "âš ï¸ æ³¨æ„äº‹é¡¹1: æ¢¯åº¦ä¼šç´¯ç§¯\n",
      "----------------------------------------\n",
      "ç¬¬ä¸€æ¬¡backwardå: x.grad = 4.0\n",
      "ç¬¬äºŒæ¬¡backwardå: x.grad = 8.0 (ç´¯ç§¯äº†ï¼)\n",
      "æ¸…é›¶åå†backward: x.grad = 4.0 (æ­£ç¡®ï¼)\n",
      "\n",
      "âš ï¸ æ³¨æ„äº‹é¡¹2: æ¨ç†æ—¶ç¦ç”¨æ¢¯åº¦ï¼ˆèŠ‚çœå†…å­˜ï¼‰\n",
      "----------------------------------------\n",
      "è®­ç»ƒæ¨¡å¼ - grad_fnå­˜åœ¨: True\n",
      "æ¨ç†æ¨¡å¼ - grad_fnå­˜åœ¨: False\n",
      "\n",
      "ğŸ’¡ å…³é”®ç‚¹:\n",
      "  - è®­ç»ƒå¾ªç¯ä¸­: ä¿æŒrequires_grad=True\n",
      "  - æ¨ç†/éªŒè¯æ—¶: ä½¿ç”¨with torch.no_grad():\n",
      "  - æ¯ä¸ªbatch: ç”¨optimizer.zero_grad()æ¸…é›¶æ¢¯åº¦\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.2 Autogradæ³¨æ„äº‹é¡¹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# é‡è¦çš„ä½¿ç”¨æ³¨æ„äº‹é¡¹\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nâš ï¸ æ³¨æ„äº‹é¡¹1: æ¢¯åº¦ä¼šç´¯ç§¯\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡è®¡ç®—\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"ç¬¬ä¸€æ¬¡backwardå: x.grad = {x.grad}\")\n",
    "\n",
    "# ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆä¸æ¸…é›¶ï¼‰\n",
    "y2 = x ** 2\n",
    "y2.backward()\n",
    "print(f\"ç¬¬äºŒæ¬¡backwardå: x.grad = {x.grad} (ç´¯ç§¯äº†ï¼)\")\n",
    "\n",
    "# æ­£ç¡®åšæ³•ï¼šæ¯æ¬¡backwardå‰æ¸…é›¶\n",
    "x.grad.zero_()  # å°±åœ°æ“ä½œæ¸…é›¶\n",
    "y3 = x ** 2\n",
    "y3.backward()\n",
    "print(f\"æ¸…é›¶åå†backward: x.grad = {x.grad} (æ­£ç¡®ï¼)\")\n",
    "\n",
    "print(\"\\nâš ï¸ æ³¨æ„äº‹é¡¹2: æ¨ç†æ—¶ç¦ç”¨æ¢¯åº¦ï¼ˆèŠ‚çœå†…å­˜ï¼‰\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "x = torch.randn(1000, 1000, requires_grad=True)\n",
    "\n",
    "# è®­ç»ƒæ—¶ï¼šéœ€è¦æ¢¯åº¦\n",
    "y_train = x @ x.T\n",
    "print(f\"è®­ç»ƒæ¨¡å¼ - grad_fnå­˜åœ¨: {y_train.grad_fn is not None}\")\n",
    "\n",
    "# æ¨ç†æ—¶ï¼šä½¿ç”¨torch.no_grad()ç¦ç”¨æ¢¯åº¦\n",
    "with torch.no_grad():\n",
    "    y_eval = x @ x.T\n",
    "    print(f\"æ¨ç†æ¨¡å¼ - grad_fnå­˜åœ¨: {y_eval.grad_fn is not None}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®ç‚¹:\")\n",
    "print(\"  - è®­ç»ƒå¾ªç¯ä¸­: ä¿æŒrequires_grad=True\")\n",
    "print(\"  - æ¨ç†/éªŒè¯æ—¶: ä½¿ç”¨with torch.no_grad():\")\n",
    "print(\"  - æ¯ä¸ªbatch: ç”¨optimizer.zero_grad()æ¸…é›¶æ¢¯åº¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 nn.Moduleï¼šå®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "\n",
    "### ğŸ—ï¸ æ¨¡å‹å®šä¹‰çš„æ ¸å¿ƒç±»\n",
    "\n",
    "`torch.nn.Module` æ˜¯æ‰€æœ‰ç¥ç»ç½‘ç»œçš„åŸºç±»ã€‚\n",
    "\n",
    "### å®šä¹‰æ¨¡å‹çš„ä¸¤ç§æ–¹å¼\n",
    "\n",
    "| æ–¹å¼ | é€‚ç”¨åœºæ™¯ | çµæ´»æ€§ |\n",
    "|------|----------|--------|\n",
    "| **ç»§æ‰¿nn.Module** | ä»»æ„å¤æ‚æ¨¡å‹ | â­â­â­â­â­ |\n",
    "| **nn.Sequential** | ç®€å•é¡ºåºæ¨¡å‹ | â­â­â­ |\n",
    "\n",
    "### æ ¸å¿ƒæ­¥éª¤\n",
    "\n",
    "1. ç»§æ‰¿ `nn.Module`\n",
    "2. åœ¨ `__init__` ä¸­å®šä¹‰å±‚\n",
    "3. åœ¨ `forward` ä¸­å®šä¹‰å‰å‘ä¼ æ’­é€»è¾‘\n",
    "4. **ä¸éœ€è¦å®šä¹‰backwardï¼** (Autogradè‡ªåŠ¨å¤„ç†)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.3 ä½¿ç”¨nn.Moduleå®šä¹‰MLP\n",
      "============================================================\n",
      "\n",
      "æ¨¡å‹ç»“æ„:\n",
      "PyTorchMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "æ¨¡å‹å‚æ•°:\n",
      "  fc1.weight: å½¢çŠ¶ torch.Size([128, 784])\n",
      "  fc1.bias: å½¢çŠ¶ torch.Size([128])\n",
      "  fc2.weight: å½¢çŠ¶ torch.Size([10, 128])\n",
      "  fc2.bias: å½¢çŠ¶ torch.Size([10])\n",
      "\n",
      "æ€»å‚æ•°æ•°é‡: 101,770\n",
      "  = 784Ã—128 + 128 + 128Ã—10 + 10\n",
      "  = 100352 + 128 + 1280 + 10\n",
      "  = 101,770\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"  2.3 ä½¿ç”¨nn.Moduleå®šä¹‰MLP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# æ–¹å¼1ï¼šç»§æ‰¿nn.Moduleï¼ˆæ¨èï¼Œæœ€çµæ´»ï¼‰\n",
    "# ============================================================\n",
    "\n",
    "class PyTorchMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorchå®ç°çš„å¤šå±‚æ„ŸçŸ¥æœº\n",
    "    \n",
    "    ç»“æ„: è¾“å…¥å±‚ -> éšè—å±‚(ReLU) -> è¾“å‡ºå±‚\n",
    "    \n",
    "    å¯¹æ¯”NumPyå®ç°çš„ä¼˜åŠ¿:\n",
    "    1. æ— éœ€æ‰‹åŠ¨åˆå§‹åŒ–æƒé‡ï¼ˆnn.Linearè‡ªåŠ¨å¤„ç†ï¼‰\n",
    "    2. æ— éœ€æ‰‹åŠ¨å®ç°æ¿€æ´»å‡½æ•°ï¼ˆF.reluç›´æ¥è°ƒç”¨ï¼‰\n",
    "    3. æ— éœ€å®ç°backwardï¼ˆAutogradè‡ªåŠ¨å¤„ç†ï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–ç½‘ç»œå±‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦\n",
    "            hidden_size: éšè—å±‚ç¥ç»å…ƒæ•°\n",
    "            output_size: è¾“å‡ºç±»åˆ«æ•°\n",
    "        \"\"\"\n",
    "        # å¿…é¡»è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼\n",
    "        super().__init__()\n",
    "        \n",
    "        # å®šä¹‰ç½‘ç»œå±‚\n",
    "        # nn.Linear: å…¨è¿æ¥å±‚ï¼Œè‡ªåŠ¨åˆå§‹åŒ–æƒé‡å’Œåç½®\n",
    "        # ç­‰ä»·äºNumPyä¸­çš„: W @ x + b\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # æ³¨æ„ï¼šä¸éœ€è¦æ‰‹åŠ¨åˆå§‹åŒ–æƒé‡ï¼\n",
    "        # nn.Linearé»˜è®¤ä½¿ç”¨Kaimingåˆå§‹åŒ–\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            x: è¾“å…¥å¼ é‡, shape (batch_size, input_size)\n",
    "        \n",
    "        è¿”å›:\n",
    "            è¾“å‡ºå¼ é‡, shape (batch_size, output_size)\n",
    "            \n",
    "        æ³¨æ„ï¼š\n",
    "        - ä¸éœ€è¦ä¿å­˜ä¸­é—´å˜é‡ï¼Autogradä¼šè‡ªåŠ¨è®°å½•\n",
    "        - ä¸éœ€è¦å®ç°backwardï¼Autogradä¼šè‡ªåŠ¨è®¡ç®—\n",
    "        \"\"\"\n",
    "        # ç¬¬ä¸€å±‚ + ReLUæ¿€æ´»\n",
    "        x = self.fc1(x)      # çº¿æ€§å˜æ¢\n",
    "        x = F.relu(x)        # ReLUæ¿€æ´»\n",
    "        \n",
    "        # ç¬¬äºŒå±‚ï¼ˆè¾“å‡ºå±‚ä¸åŠ æ¿€æ´»ï¼ŒæŸå¤±å‡½æ•°ä¼šå¤„ç†ï¼‰\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# å®ä¾‹åŒ–æ¨¡å‹\n",
    "model = PyTorchMLP(input_size=784, hidden_size=128, output_size=10)\n",
    "\n",
    "print(\"\\næ¨¡å‹ç»“æ„:\")\n",
    "print(model)\n",
    "\n",
    "# æŸ¥çœ‹å‚æ•°\n",
    "print(\"\\n\\næ¨¡å‹å‚æ•°:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: å½¢çŠ¶ {param.shape}\")\n",
    "\n",
    "# è®¡ç®—æ€»å‚æ•°é‡\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\næ€»å‚æ•°æ•°é‡: {total_params:,}\")\n",
    "print(f\"  = 784Ã—128 + 128 + 128Ã—10 + 10\")\n",
    "print(f\"  = {784*128} + {128} + {128*10} + {10}\")\n",
    "print(f\"  = {784*128 + 128 + 128*10 + 10:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.3 æµ‹è¯•å‰å‘ä¼ æ’­\n",
      "============================================================\n",
      "\n",
      "è¾“å…¥å½¢çŠ¶: torch.Size([16, 784])\n",
      "  (batch_size=16, features=784)\n",
      "\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([16, 10])\n",
      "  (batch_size=16, num_classes=10)\n",
      "\n",
      "ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å‡ºï¼ˆlogitsï¼‰:\n",
      "  [-0.47005534 -0.53800935  0.28363952 -0.22265495 -0.09798245 -0.32043105\n",
      "  0.00300348 -0.23516689  0.08927251 -0.15305798]\n",
      "\n",
      "ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ¦‚ç‡:\n",
      "  [0.07171478 0.06700338 0.1523822  0.09184454 0.10403942 0.0832894\n",
      " 0.11509476 0.09070254 0.12546475 0.09846433]\n",
      "  æ¦‚ç‡å’Œ: 1.0000\n",
      "\n",
      "âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.3 æµ‹è¯•å‰å‘ä¼ æ’­\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åˆ›å»ºå‡æ•°æ®æµ‹è¯•\n",
    "batch_size = 16\n",
    "fake_input = torch.randn(batch_size, 784)  # æ¨¡æ‹ŸMNISTå›¾åƒ\n",
    "print(f\"\\nè¾“å…¥å½¢çŠ¶: {fake_input.shape}\")\n",
    "print(f\"  (batch_size=16, features=784)\")\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "output = model(fake_input)\n",
    "print(f\"\\nè¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "print(f\"  (batch_size=16, num_classes=10)\")\n",
    "\n",
    "# æŸ¥çœ‹è¾“å‡ºï¼ˆæœªç»è¿‡softmaxï¼Œæ˜¯logitsï¼‰\n",
    "print(f\"\\nç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å‡ºï¼ˆlogitsï¼‰:\")\n",
    "print(f\"  {output[0].detach().numpy()}\")\n",
    "\n",
    "# è½¬æ¢ä¸ºæ¦‚ç‡\n",
    "probs = F.softmax(output, dim=1)\n",
    "print(f\"\\nç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ¦‚ç‡:\")\n",
    "print(f\"  {probs[0].detach().numpy()}\")\n",
    "print(f\"  æ¦‚ç‡å’Œ: {probs[0].sum().item():.4f}\")\n",
    "\n",
    "print(\"\\nâœ… å‰å‘ä¼ æ’­æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.3 nn.Sequentialç®€åŒ–å†™æ³•\n",
      "============================================================\n",
      "\n",
      "Sequentialæ¨¡å‹ç»“æ„:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "è¾“å‡ºå½¢çŠ¶: torch.Size([16, 10])\n",
      "\n",
      "------------------------------------------------------------\n",
      "ä¸¤ç§æ–¹å¼å¯¹æ¯”:\n",
      "------------------------------------------------------------\n",
      "\n",
      "nn.Moduleç±»ï¼ˆç»§æ‰¿æ–¹å¼ï¼‰:\n",
      "  âœ… æœ€çµæ´»ï¼Œå¯ä»¥å®ç°ä»»æ„å¤æ‚çš„forwardé€»è¾‘\n",
      "  âœ… å¯ä»¥æœ‰å¤šä¸ªè¾“å…¥/è¾“å‡º\n",
      "  âœ… å¯ä»¥åœ¨forwardä¸­åŠ å…¥æ¡ä»¶åˆ¤æ–­\n",
      "  âš ï¸ ä»£ç ç¨å¤š\n",
      "\n",
      "nn.Sequential:\n",
      "  âœ… ä»£ç ç®€æ´\n",
      "  âš ï¸ åªèƒ½å®ç°é¡ºåºå †å çš„æ¨¡å‹\n",
      "  âš ï¸ ä¸èƒ½æœ‰åˆ†æ”¯ç»“æ„\n",
      "  âš ï¸ ä¸èƒ½æœ‰å¤šä¸ªè¾“å…¥/è¾“å‡º\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.3 nn.Sequentialç®€åŒ–å†™æ³•\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# æ–¹å¼2ï¼šä½¿ç”¨nn.Sequentialï¼ˆé€‚åˆç®€å•é¡ºåºæ¨¡å‹ï¼‰\n",
    "# ============================================================\n",
    "\n",
    "# ä¸ä¸Šé¢çš„PyTorchMLPå®Œå…¨ç­‰ä»·\n",
    "model_sequential = nn.Sequential(\n",
    "    nn.Linear(784, 128),   # ç¬¬ä¸€å±‚\n",
    "    nn.ReLU(),             # æ¿€æ´»å‡½æ•°\n",
    "    nn.Linear(128, 10)     # ç¬¬äºŒå±‚\n",
    ")\n",
    "\n",
    "print(\"\\nSequentialæ¨¡å‹ç»“æ„:\")\n",
    "print(model_sequential)\n",
    "\n",
    "# æµ‹è¯•å‰å‘ä¼ æ’­\n",
    "output_seq = model_sequential(fake_input)\n",
    "print(f\"\\nè¾“å‡ºå½¢çŠ¶: {output_seq.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ä¸¤ç§æ–¹å¼å¯¹æ¯”:\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "nn.Moduleç±»ï¼ˆç»§æ‰¿æ–¹å¼ï¼‰:\n",
    "  âœ… æœ€çµæ´»ï¼Œå¯ä»¥å®ç°ä»»æ„å¤æ‚çš„forwardé€»è¾‘\n",
    "  âœ… å¯ä»¥æœ‰å¤šä¸ªè¾“å…¥/è¾“å‡º\n",
    "  âœ… å¯ä»¥åœ¨forwardä¸­åŠ å…¥æ¡ä»¶åˆ¤æ–­\n",
    "  âš ï¸ ä»£ç ç¨å¤š\n",
    "\n",
    "nn.Sequential:\n",
    "  âœ… ä»£ç ç®€æ´\n",
    "  âš ï¸ åªèƒ½å®ç°é¡ºåºå †å çš„æ¨¡å‹\n",
    "  âš ï¸ ä¸èƒ½æœ‰åˆ†æ”¯ç»“æ„\n",
    "  âš ï¸ ä¸èƒ½æœ‰å¤šä¸ªè¾“å…¥/è¾“å‡º\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.3 æ ¸å¿ƒå¯¹æ¯”ï¼šNumPy MLP vs PyTorch MLP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# å¹¶æ’å¯¹æ¯”ï¼šNumPyå®ç° vs PyTorchå®ç°\n",
    "# ============================================================\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    NumPyå®ç° vs PyTorchå®ç°                       â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                   â•‘\n",
    "â•‘  ã€åˆå§‹åŒ–æƒé‡ã€‘                                                    â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘\n",
    "â•‘  NumPy:                         â”‚ PyTorch:                        â•‘\n",
    "â•‘  W1 = np.random.randn(784,128)  â”‚ self.fc1 = nn.Linear(784, 128)  â•‘\n",
    "â•‘       * np.sqrt(2/784)          â”‚ # è‡ªåŠ¨Xavieråˆå§‹åŒ–ï¼            â•‘\n",
    "â•‘  b1 = np.zeros(128)             â”‚                                 â•‘\n",
    "â•‘  W2 = np.random.randn(128,10)   â”‚ self.fc2 = nn.Linear(128, 10)   â•‘\n",
    "â•‘       * np.sqrt(2/128)          â”‚                                 â•‘\n",
    "â•‘  b2 = np.zeros(10)              â”‚                                 â•‘\n",
    "â•‘                                                                   â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                   â•‘\n",
    "â•‘  ã€å‰å‘ä¼ æ’­ã€‘                                                      â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘\n",
    "â•‘  NumPy:                         â”‚ PyTorch:                        â•‘\n",
    "â•‘  self.z1 = X @ W1 + b1          â”‚ x = self.fc1(x)                 â•‘\n",
    "â•‘  self.a1 = np.maximum(0, z1)    â”‚ x = F.relu(x)                   â•‘\n",
    "â•‘  self.z2 = a1 @ W2 + b2         â”‚ x = self.fc2(x)                 â•‘\n",
    "â•‘  # å¿…é¡»ä¿å­˜ä¸­é—´å˜é‡ï¼            â”‚ # ä¸éœ€è¦ä¿å­˜ï¼                   â•‘\n",
    "â•‘                                                                   â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                   â•‘\n",
    "â•‘  ã€åå‘ä¼ æ’­ã€‘                                                      â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘\n",
    "â•‘  NumPy (30+è¡Œä»£ç ):             â”‚ PyTorch:                        â•‘\n",
    "â•‘  dz2 = y_pred - y_true          â”‚                                 â•‘\n",
    "â•‘  dW2 = a1.T @ dz2 / m           â”‚ loss.backward()                 â•‘\n",
    "â•‘  db2 = np.sum(dz2, axis=0)/m    â”‚ # ä¸€è¡Œæå®šï¼                     â•‘\n",
    "â•‘  da1 = dz2 @ W2.T               â”‚                                 â•‘\n",
    "â•‘  dz1 = da1 * (z1 > 0)           â”‚                                 â•‘\n",
    "â•‘  dW1 = X.T @ dz1 / m            â”‚                                 â•‘\n",
    "â•‘  db1 = np.sum(dz1, axis=0)/m    â”‚                                 â•‘\n",
    "â•‘                                                                   â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                   â•‘\n",
    "â•‘  ã€å‚æ•°æ›´æ–°ã€‘                                                      â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘\n",
    "â•‘  NumPy:                         â”‚ PyTorch:                        â•‘\n",
    "â•‘  W1 -= lr * dW1                 â”‚ optimizer.step()                â•‘\n",
    "â•‘  b1 -= lr * db1                 â”‚ # ä¸€è¡Œæå®šæ‰€æœ‰å‚æ•°ï¼             â•‘\n",
    "â•‘  W2 -= lr * dW2                 â”‚                                 â•‘\n",
    "â•‘  b2 -= lr * db2                 â”‚                                 â•‘\n",
    "â•‘                                                                   â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "print(\"ä»£ç è¡Œæ•°å¯¹æ¯”:\")\n",
    "print(\"  NumPy MLP: ~80è¡Œï¼ˆå«backwardã€SGDã€Adamç­‰ï¼‰\")\n",
    "print(\"  PyTorch MLP: ~15è¡Œ\")\n",
    "print(\"\\nå¼€å‘æ•ˆç‡: PyTorchæå‡çº¦5å€ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  2.3 æ›´å¤æ‚çš„æ¨¡å‹ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "é«˜çº§MLPç»“æ„:\n",
      "AdvancedMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "\n",
      "æ€»å‚æ•°: 243,658\n",
      "å¯è®­ç»ƒå‚æ•°: 243,658\n",
      "\n",
      "æµ‹è¯•: è¾“å…¥torch.Size([8, 784]) -> è¾“å‡ºtorch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2.3 æ›´å¤æ‚çš„æ¨¡å‹ç¤ºä¾‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# ç¤ºä¾‹ï¼šå¤šéšè—å±‚ + Dropout + BatchNorm\n",
    "# ============================================================\n",
    "\n",
    "class AdvancedMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    æ›´å¤æ‚çš„MLPï¼ŒåŒ…å«:\n",
    "    - å¤šä¸ªéšè—å±‚\n",
    "    - Batch Normalizationï¼ˆç¬¬7ç« å­¦è¿‡ï¼‰\n",
    "    - Dropoutæ­£åˆ™åŒ–ï¼ˆç¬¬7ç« å­¦è¿‡ï¼‰\n",
    "    \n",
    "    ç»“æ„: 784 -> 256 -> 128 -> 64 -> 10\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, num_classes=10, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ç¬¬ä¸€å±‚: 784 -> 256\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch Normalization\n",
    "        \n",
    "        # ç¬¬äºŒå±‚: 256 -> 128\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # ç¬¬ä¸‰å±‚: 128 -> 64\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # è¾“å‡ºå±‚: 64 -> 10\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "        \n",
    "        # Dropoutå±‚\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ç¬¬ä¸€å±‚: Linear -> BN -> ReLU -> Dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # ç¬¬äºŒå±‚\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # ç¬¬ä¸‰å±‚\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # è¾“å‡ºå±‚ï¼ˆæ— æ¿€æ´»ï¼‰\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "advanced_model = AdvancedMLP()\n",
    "\n",
    "print(\"\\né«˜çº§MLPç»“æ„:\")\n",
    "print(advanced_model)\n",
    "\n",
    "# å‚æ•°ç»Ÿè®¡\n",
    "total = sum(p.numel() for p in advanced_model.parameters())\n",
    "trainable = sum(p.numel() for p in advanced_model.parameters() if p.requires_grad)\n",
    "print(f\"\\næ€»å‚æ•°: {total:,}\")\n",
    "print(f\"å¯è®­ç»ƒå‚æ•°: {trainable:,}\")\n",
    "\n",
    "# æµ‹è¯•\n",
    "advanced_model.eval()  # è¯„ä¼°æ¨¡å¼ï¼ˆBNå’ŒDropoutè¡Œä¸ºä¸åŒï¼‰\n",
    "test_input = torch.randn(8, 784)\n",
    "test_output = advanced_model(test_input)\n",
    "print(f\"\\næµ‹è¯•: è¾“å…¥{test_input.shape} -> è¾“å‡º{test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ æœ¬ç« å°ç»“\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µå›é¡¾\n",
    "\n",
    "| æ¦‚å¿µ | è¯´æ˜ | å¯¹æ¯”NumPy |\n",
    "|------|------|----------|\n",
    "| **Tensor** | å¤šç»´æ•°ç»„ï¼Œæ”¯æŒGPU | ç±»ä¼¼ndarray |\n",
    "| **requires_grad** | å¯ç”¨æ¢¯åº¦è·Ÿè¸ª | æ— æ­¤åŠŸèƒ½ |\n",
    "| **backward()** | è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ | éœ€æ‰‹å†™ |\n",
    "| **nn.Module** | æ¨¡å‹åŸºç±» | æ— æ­¤æ¦‚å¿µ |\n",
    "| **nn.Linear** | å…¨è¿æ¥å±‚ | æ‰‹å†™W@x+b |\n",
    "\n",
    "### âœ… å­¦ä¹ æ£€æŸ¥ç‚¹\n",
    "\n",
    "å®Œæˆæœ¬ç« åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š\n",
    "\n",
    "- [ ] ç†è§£ä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
    "- [ ] åˆ›å»ºå’Œæ“ä½œPyTorch Tensor\n",
    "- [ ] ç†è§£Autogradè‡ªåŠ¨å¾®åˆ†çš„å·¥ä½œåŸç†\n",
    "- [ ] ä½¿ç”¨nn.Moduleå®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "- [ ] å¯¹æ¯”NumPyå®ç°å’ŒPyTorchå®ç°çš„åŒºåˆ«\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€ç« é¢„å‘Š\n",
    "\n",
    "**10b: PyTorchå®Œæ•´è®­ç»ƒ**\n",
    "\n",
    "- æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ï¼ˆå¯¹æ¯”æ‰‹å†™SGD/Adamï¼‰\n",
    "- DataLoaderæ•°æ®åŠ è½½\n",
    "- å®Œæ•´è®­ç»ƒå¾ªç¯\n",
    "- æ¨¡å‹ä¿å­˜ä¸åŠ è½½\n",
    "- å®æˆ˜ï¼šé¸¢å°¾èŠ±åˆ†ç±»ã€æ‰‹å†™æ•°å­—è¯†åˆ«"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
