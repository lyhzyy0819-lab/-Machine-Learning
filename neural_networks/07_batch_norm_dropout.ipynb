{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬7ç« ï¼šæ‰¹å½’ä¸€åŒ–ä¸Dropout\n",
    "\n",
    "> **å­¦ä¹ ç›®æ ‡ï¼š**\n",
    "> - ç†è§£Batch Normalizationçš„åŸç†å’Œä½œç”¨\n",
    "> - æŒæ¡Dropoutçš„å·¥ä½œæœºåˆ¶\n",
    "> - ç†è§£è®­ç»ƒæ¨¡å¼å’Œæ¨ç†æ¨¡å¼çš„å·®å¼‚\n",
    "> - å­¦ä¼šç»„åˆä½¿ç”¨å¤šç§æ­£åˆ™åŒ–æŠ€æœ¯\n",
    "> - äº†è§£Layer Normalizationç­‰å˜ä½“\n",
    ">\n",
    "> **å­¦ä¹ æ—¶é—´ï¼š** 3-4å°æ—¶\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ä¸ºä»€ä¹ˆéœ€è¦Batch Normalizationå’ŒDropoutï¼Ÿ\n",
    "\n",
    "### æ·±åº¦ç½‘ç»œçš„æŒ‘æˆ˜\n",
    "\n",
    "1. **å†…éƒ¨åå˜é‡åç§»ï¼ˆInternal Covariate Shiftï¼‰**\n",
    "   - æ¯å±‚è¾“å…¥çš„åˆ†å¸ƒéšè®­ç»ƒä¸æ–­å˜åŒ–\n",
    "   - å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œéœ€è¦å°å­¦ä¹ ç‡\n",
    "   - **è§£å†³æ–¹æ¡ˆ**ï¼šBatch Normalization\n",
    "\n",
    "2. **è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰**\n",
    "   - ç½‘ç»œè®°ä½è®­ç»ƒæ•°æ®ç»†èŠ‚\n",
    "   - æ³›åŒ–èƒ½åŠ›å·®\n",
    "   - **è§£å†³æ–¹æ¡ˆ**ï¼šDropout\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– ç›®å½•\n",
    "\n",
    "1. **Batch Normalizationï¼ˆæ‰¹å½’ä¸€åŒ–ï¼‰**\n",
    "   - åŸç†å’Œæ•°å­¦æ¨å¯¼\n",
    "   - è®­ç»ƒæ—¶å’Œæ¨ç†æ—¶çš„ä¸åŒ\n",
    "   - ä»é›¶å®ç°\n",
    "   - å¯è®­ç»ƒå‚æ•°Î³å’ŒÎ²\n",
    "\n",
    "2. **Dropout**\n",
    "   - å·¥ä½œåŸç†\n",
    "   - è®­ç»ƒæ¨¡å¼ vs æ¨ç†æ¨¡å¼\n",
    "   - ä»é›¶å®ç°\n",
    "   - Inverted Dropout\n",
    "\n",
    "3. **Layer Normalization**\n",
    "   - BN vs LNå¯¹æ¯”\n",
    "   - é€‚ç”¨åœºæ™¯\n",
    "\n",
    "4. **ç»¼åˆå®æˆ˜**\n",
    "   - MNISTæ•°æ®é›†\n",
    "   - å¯¹æ¯”ä¸åŒæ­£åˆ™åŒ–æŠ€æœ¯ç»„åˆ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ç¯å¢ƒé…ç½®å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# æå–ç»Ÿè®¡é‡\niterations = range(n_iterations)\nmeans_without_bn = [stat['mean'] for stat in net_without_bn.layer2_input_stats]\nstds_without_bn = [stat['std'] for stat in net_without_bn.layer2_input_stats]\nmeans_with_bn = [stat['mean'] for stat in net_with_bn.layer2_input_stats]\nstds_with_bn = [stat['std'] for stat in net_with_bn.layer2_input_stats]\n\n# åˆ›å»ºå¯è§†åŒ–\nfig = plt.figure(figsize=(16, 10))\ngs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n\n# ========== ç¬¬1è¡Œï¼šå‡å€¼å’Œæ ‡å‡†å·®å˜åŒ– ==========\nax1 = fig.add_subplot(gs[0, 0])\nax1.plot(iterations, means_without_bn, color='#e74c3c', linewidth=2, label='æ— BN')\nax1.fill_between(iterations, \n                  np.array(means_without_bn) - np.array(stds_without_bn),\n                  np.array(means_without_bn) + np.array(stds_without_bn),\n                  color='#e74c3c', alpha=0.2)\nax1.set_xlabel('è®­ç»ƒè¿­ä»£æ¬¡æ•°', fontsize=11)\nax1.set_ylabel('ç¬¬2å±‚è¾“å…¥å€¼', fontsize=11)\nax1.set_title('âŒ æ— BNï¼šåˆ†å¸ƒå‰§çƒˆå˜åŒ–ï¼ˆå†…éƒ¨åå˜é‡åç§»ï¼‰', fontsize=12, fontweight='bold')\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3)\n\nax2 = fig.add_subplot(gs[0, 1])\nax2.plot(iterations, means_with_bn, color='#2ecc71', linewidth=2, label='æœ‰BN')\nax2.fill_between(iterations,\n                  np.array(means_with_bn) - np.array(stds_with_bn),\n                  np.array(means_with_bn) + np.array(stds_with_bn),\n                  color='#2ecc71', alpha=0.2)\nax2.set_xlabel('è®­ç»ƒè¿­ä»£æ¬¡æ•°', fontsize=11)\nax2.set_ylabel('ç¬¬2å±‚è¾“å…¥å€¼', fontsize=11)\nax2.set_title('âœ… æœ‰BNï¼šåˆ†å¸ƒç¨³å®šï¼ˆå‡å€¼â‰ˆ0ï¼Œæ–¹å·®â‰ˆ1ï¼‰', fontsize=12, fontweight='bold')\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\n# ========== ç¬¬2è¡Œï¼šåˆ†å¸ƒçš„ç›´æ–¹å›¾å¯¹æ¯”ï¼ˆé€‰æ‹©å‡ ä¸ªæ—¶é—´ç‚¹ï¼‰==========\nsnapshots = [0, 15, 30, 45]  # é€‰æ‹©4ä¸ªæ—¶é—´ç‚¹\ncolors_timeline = ['#3498db', '#9b59b6', '#f39c12', '#e74c3c']\n\nax3 = fig.add_subplot(gs[1, 0])\nfor i, snapshot_idx in enumerate(snapshots):\n    data = net_without_bn.layer2_input_stats[snapshot_idx]['data'].flatten()\n    ax3.hist(data, bins=30, alpha=0.5, label=f'è¿­ä»£{snapshot_idx}', \n             color=colors_timeline[i], density=True)\nax3.set_xlabel('æ¿€æ´»å€¼', fontsize=11)\nax3.set_ylabel('å¯†åº¦', fontsize=11)\nax3.set_title('æ— BNï¼šåˆ†å¸ƒéšè®­ç»ƒä¸æ–­åç§»', fontsize=12, fontweight='bold')\nax3.legend(fontsize=9)\nax3.grid(True, alpha=0.3, axis='y')\n\nax4 = fig.add_subplot(gs[1, 1])\nfor i, snapshot_idx in enumerate(snapshots):\n    data = net_with_bn.layer2_input_stats[snapshot_idx]['data'].flatten()\n    ax4.hist(data, bins=30, alpha=0.5, label=f'è¿­ä»£{snapshot_idx}',\n             color=colors_timeline[i], density=True)\nax4.set_xlabel('æ¿€æ´»å€¼', fontsize=11)\nax4.set_ylabel('å¯†åº¦', fontsize=11)\nax4.set_title('æœ‰BNï¼šåˆ†å¸ƒå§‹ç»ˆä¿æŒæ ‡å‡†åŒ–', fontsize=12, fontweight='bold')\nax4.legend(fontsize=9)\nax4.grid(True, alpha=0.3, axis='y')\n\n# ========== ç¬¬3è¡Œï¼šç»Ÿè®¡é‡å˜åŒ–æ›²çº¿ ==========\nax5 = fig.add_subplot(gs[2, 0])\nax5.plot(iterations, means_without_bn, color='#e74c3c', linewidth=2, label='å‡å€¼')\nax5.plot(iterations, stds_without_bn, color='#3498db', linewidth=2, label='æ ‡å‡†å·®')\nax5.set_xlabel('è®­ç»ƒè¿­ä»£æ¬¡æ•°', fontsize=11)\nax5.set_ylabel('ç»Ÿè®¡é‡æ•°å€¼', fontsize=11)\nax5.set_title('æ— BNï¼šå‡å€¼å’Œæ–¹å·®éƒ½åœ¨å˜åŒ–', fontsize=12, fontweight='bold')\nax5.legend(fontsize=10)\nax5.grid(True, alpha=0.3)\n\nax6 = fig.add_subplot(gs[2, 1])\nax6.plot(iterations, means_with_bn, color='#e74c3c', linewidth=2, label='å‡å€¼')\nax6.plot(iterations, stds_with_bn, color='#3498db', linewidth=2, label='æ ‡å‡†å·®')\nax6.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='ç›®æ ‡å‡å€¼=0')\nax6.axhline(y=1, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='ç›®æ ‡æ ‡å‡†å·®=1')\nax6.set_xlabel('è®­ç»ƒè¿­ä»£æ¬¡æ•°', fontsize=11)\nax6.set_ylabel('ç»Ÿè®¡é‡æ•°å€¼', fontsize=11)\nax6.set_title('æœ‰BNï¼šå‡å€¼â‰ˆ0ï¼Œæ ‡å‡†å·®â‰ˆ1ï¼ˆç¨³å®šï¼‰', fontsize=12, fontweight='bold')\nax6.legend(fontsize=9)\nax6.grid(True, alpha=0.3)\n\nplt.suptitle('å†…éƒ¨åå˜é‡åç§»ï¼ˆInternal Covariate Shiftï¼‰å¯è§†åŒ–', \n             fontsize=14, fontweight='bold', y=0.995)\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ” è§‚å¯Ÿè¦ç‚¹ï¼š\")\nprint(\"=\"*70)\nprint(\"\\nã€é—®é¢˜ã€‘æ— BNçš„ç½‘ç»œï¼š\")\nprint(\"  1. ç¬¬2å±‚è¾“å…¥çš„å‡å€¼å’Œæ–¹å·®éšè®­ç»ƒä¸æ–­å˜åŒ–ï¼ˆä¸Šå›¾å·¦ä¾§ï¼‰\")\nprint(\"  2. åˆ†å¸ƒåœ¨ä¸åŒè¿­ä»£ä¸­åç§»å¾ˆå¤§ï¼ˆä¸­å›¾å·¦ä¾§ï¼Œæ›²çº¿åˆ†æ•£ï¼‰\")\nprint(\"  3. ç»Ÿè®¡é‡ä¸ç¨³å®šï¼Œéš¾ä»¥é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ï¼ˆä¸‹å›¾å·¦ä¾§ï¼‰\")\nprint(\"\\nã€è§£å†³æ–¹æ¡ˆã€‘æœ‰BNçš„ç½‘ç»œï¼š\")\nprint(\"  4. ç¬¬2å±‚è¾“å…¥å§‹ç»ˆä¿æŒå‡å€¼â‰ˆ0ï¼Œæ–¹å·®â‰ˆ1ï¼ˆä¸Šå›¾å³ä¾§ï¼‰\")\nprint(\"  5. åˆ†å¸ƒåœ¨ä¸åŒè¿­ä»£ä¸­é‡å è‰¯å¥½ï¼ˆä¸­å›¾å³ä¾§ï¼Œæ›²çº¿é›†ä¸­ï¼‰\")\nprint(\"  6. ç»Ÿè®¡é‡ç¨³å®šï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ï¼ˆä¸‹å›¾å³ä¾§ï¼‰\")\nprint(\"\\nğŸ’¡ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦Batch Normalizationï¼\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# æ¨¡æ‹Ÿå†…éƒ¨åå˜é‡åç§»çš„æ¼”ç¤º\nnp.random.seed(42)\n\n# ç”Ÿæˆç®€å•çš„åˆ†ç±»æ•°æ®\nn_samples = 500\nX_demo = np.random.randn(n_samples, 10)\ny_demo = (X_demo[:, 0] + X_demo[:, 1] > 0).astype(int)\n\n# ç®€å•çš„2å±‚ç½‘ç»œï¼ˆç”¨äºæ¼”ç¤ºï¼‰\nclass SimpleNetwork:\n    \"\"\"ç®€å•ç½‘ç»œï¼Œç”¨äºæ¼”ç¤ºå†…éƒ¨åå˜é‡åç§»\"\"\"\n    def __init__(self, input_size=10, hidden_size=20, use_bn=False):\n        self.W1 = np.random.randn(hidden_size, input_size) * 0.5\n        self.b1 = np.zeros(hidden_size)\n        self.W2 = np.random.randn(1, hidden_size) * 0.5\n        self.b2 = np.zeros(1)\n        self.use_bn = use_bn\n        \n        # è®°å½•ç¬¬2å±‚è¾“å…¥çš„ç»Ÿè®¡é‡\n        self.layer2_input_stats = []\n        \n        if use_bn:\n            self.bn = BatchNormalization(hidden_size)\n    \n    def forward(self, X, record_stats=True):\n        # ç¬¬1å±‚\n        z1 = X @ self.W1.T + self.b1\n        a1 = relu(z1)\n        \n        # å¦‚æœä½¿ç”¨BN\n        if self.use_bn:\n            a1 = self.bn.forward(a1, training=True)\n        \n        # è®°å½•ç¬¬2å±‚çš„è¾“å…¥ç»Ÿè®¡é‡\n        if record_stats:\n            self.layer2_input_stats.append({\n                'mean': np.mean(a1),\n                'std': np.std(a1),\n                'data': a1.copy()  # ä¿å­˜ä¸€ä»½æ•°æ®ç”¨äºå¯è§†åŒ–\n            })\n        \n        # ç¬¬2å±‚\n        z2 = a1 @ self.W2.T + self.b2\n        return z2\n    \n    def train_step(self, X, y, lr=0.01):\n        \"\"\"ç®€åŒ–çš„è®­ç»ƒæ­¥éª¤ï¼ˆåªæ›´æ–°W1ï¼Œè§‚å¯Ÿåˆ†å¸ƒå˜åŒ–ï¼‰\"\"\"\n        # å‰å‘ä¼ æ’­\n        output = self.forward(X, record_stats=True)\n        \n        # ç®€åŒ–çš„æ¢¯åº¦ä¸‹é™ï¼ˆåªæ›´æ–°ç¬¬ä¸€å±‚æƒé‡ï¼‰\n        # è¿™é‡Œä¸åšå®Œæ•´çš„åå‘ä¼ æ’­ï¼Œåªéšæœºæ‰°åŠ¨æƒé‡æ¥æ¨¡æ‹Ÿè®­ç»ƒ\n        self.W1 += np.random.randn(*self.W1.shape) * lr\n\n# è®­ç»ƒä¸¤ä¸ªç½‘ç»œï¼šæœ‰BN vs æ— BN\nprint(\"è®­ç»ƒä¸¤ä¸ªç½‘ç»œï¼ˆ50ä¸ªè¿­ä»£ï¼‰...\")\nprint(\"  - ç½‘ç»œ1ï¼šæ— BN\")\nprint(\"  - ç½‘ç»œ2ï¼šæœ‰BN\\n\")\n\nnet_without_bn = SimpleNetwork(use_bn=False)\nnet_with_bn = SimpleNetwork(use_bn=True)\n\nn_iterations = 50\nfor i in range(n_iterations):\n    # éšæœºé‡‡æ ·ä¸€ä¸ªbatch\n    batch_indices = np.random.choice(n_samples, 64, replace=False)\n    X_batch = X_demo[batch_indices]\n    y_batch = y_demo[batch_indices]\n    \n    # è®­ç»ƒæ­¥éª¤\n    net_without_bn.train_step(X_batch, y_batch)\n    net_with_bn.train_step(X_batch, y_batch)\n\nprint(\"âœ… è®­ç»ƒå®Œæˆï¼ç°åœ¨æ¥çœ‹çœ‹ç¬¬2å±‚è¾“å…¥åˆ†å¸ƒçš„å˜åŒ–...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ¬ å¯è§†åŒ–æ¼”ç¤ºï¼šå†…éƒ¨åå˜é‡åç§»é—®é¢˜\n\nåœ¨å­¦ä¹ Batch Normalizationä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç›´è§‚åœ°çœ‹çœ‹**å†…éƒ¨åå˜é‡åç§»**æ˜¯ä»€ä¹ˆï¼Œä»¥åŠå®ƒä¸ºä»€ä¹ˆæ˜¯ä¸ªé—®é¢˜ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Batch Normalizationï¼ˆæ‰¹å½’ä¸€åŒ–ï¼‰\n",
    "\n",
    "### 1.1 åŸç†å’Œæ•°å­¦æ¨å¯¼\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³ï¼š** å°†æ¯ä¸€å±‚çš„è¾“å…¥æ ‡å‡†åŒ–åˆ°å‡å€¼0ã€æ–¹å·®1ã€‚\n",
    "\n",
    "**ç®—æ³•æµç¨‹ï¼ˆè®­ç»ƒæ—¶ï¼‰ï¼š**\n",
    "\n",
    "ç»™å®šä¸€ä¸ªmini-batchçš„è¾“å…¥ $\\mathbf{X} = \\{x_1, x_2, ..., x_m\\}$ï¼š\n",
    "\n",
    "1. **è®¡ç®—mini-batchå‡å€¼ï¼š**\n",
    "   $$\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i$$\n",
    "\n",
    "2. **è®¡ç®—mini-batchæ–¹å·®ï¼š**\n",
    "   $$\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2$$\n",
    "\n",
    "3. **æ ‡å‡†åŒ–ï¼š**\n",
    "   $$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "   \n",
    "   å…¶ä¸­ $\\epsilon$ æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼ˆå¦‚1e-5ï¼‰\n",
    "\n",
    "4. **ç¼©æ”¾å’Œå¹³ç§»ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰ï¼š**\n",
    "   $$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
    "   \n",
    "   - $\\gamma$ï¼šç¼©æ”¾å‚æ•°ï¼ˆscaleï¼‰\n",
    "   - $\\beta$ï¼šå¹³ç§»å‚æ•°ï¼ˆshiftï¼‰\n",
    "   - è¿™ä¸¤ä¸ªå‚æ•°æ˜¯**å¯å­¦ä¹ çš„**ï¼Œé€šè¿‡åå‘ä¼ æ’­æ›´æ–°\n",
    "\n",
    "**ä¸ºä»€ä¹ˆéœ€è¦Î³å’ŒÎ²ï¼Ÿ**\n",
    "\n",
    "æ ‡å‡†åŒ–ä¼šé™åˆ¶è¡¨ç¤ºèƒ½åŠ›ã€‚é€šè¿‡Î³å’ŒÎ²ï¼Œç½‘ç»œå¯ä»¥å­¦ä¹ åˆ°ï¼š\n",
    "- å¦‚æœ $\\gamma = \\sqrt{\\sigma_B^2}$, $\\beta = \\mu_B$ï¼Œåˆ™æ¢å¤åŸå§‹è¾“å…¥\n",
    "- å¦åˆ™ï¼Œå­¦ä¹ åˆ°æœ€ä¼˜çš„åˆ†å¸ƒ\n",
    "\n",
    "### 1.2 ä»é›¶å®ç°Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "source": "# å¯è§†åŒ–è®¡ç®—å›¾å’Œæ¢¯åº¦æµ\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# ========== å·¦å›¾ï¼šå‰å‘ä¼ æ’­è®¡ç®—å›¾ ==========\nax = axes[0]\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\nax.axis('off')\nax.set_title('å‰å‘ä¼ æ’­è®¡ç®—å›¾', fontsize=14, fontweight='bold', pad=20)\n\n# èŠ‚ç‚¹ä½ç½®\nnodes = {\n    'X': (1, 5),\n    'mu': (3, 7),\n    'var': (3, 3),\n    'X-mu': (5, 5),\n    'X_hat': (7, 5),\n    'Y': (9, 5)\n}\n\n# ç»˜åˆ¶èŠ‚ç‚¹\nfor name, (x, y) in nodes.items():\n    if name in ['X', 'Y']:\n        color = '#3498db'\n        size = 800\n    else:\n        color = '#95a5a6'\n        size = 600\n    ax.scatter(x, y, s=size, color=color, alpha=0.8, zorder=3)\n    ax.text(x, y, name, ha='center', va='center', fontsize=11, \n            fontweight='bold', color='white', zorder=4)\n\n# ç»˜åˆ¶è¾¹ï¼ˆå¸¦ç®­å¤´ï¼‰\nedges = [\n    ('X', 'mu', '1/mÂ·Î£'),\n    ('X', 'var', '1/mÂ·Î£(Â·-Î¼)Â²'),\n    ('X', 'X-mu', ''),\n    ('mu', 'X-mu', ''),\n    ('X-mu', 'X_hat', 'Ã·âˆš(ÏƒÂ²+Îµ)'),\n    ('var', 'X_hat', ''),\n    ('X_hat', 'Y', 'Î³Â·+Î²')\n]\n\nfor start, end, label in edges:\n    x1, y1 = nodes[start]\n    x2, y2 = nodes[end]\n    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n                arrowprops=dict(arrowstyle='->', lw=2, color='#2c3e50'))\n    # æ ‡ç­¾\n    if label:\n        mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n        ax.text(mid_x, mid_y + 0.3, label, fontsize=9, ha='center',\n                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', \n                         edgecolor='none', alpha=0.8))\n\nax.text(5, 9, 'å‰å‘ä¼ æ’­ï¼šX â†’ å‡å€¼/æ–¹å·® â†’ æ ‡å‡†åŒ– â†’ ç¼©æ”¾å¹³ç§» â†’ Y', \n        ha='center', fontsize=10, style='italic', color='#7f8c8d')\n\n# ========== å³å›¾ï¼šåå‘ä¼ æ’­è·¯å¾„ ==========\nax = axes[1]\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\nax.axis('off')\nax.set_title('åå‘ä¼ æ’­æ¢¯åº¦æµï¼ˆ3æ¡è·¯å¾„ï¼‰', fontsize=14, fontweight='bold', pad=20)\n\n# èŠ‚ç‚¹\nfor name, (x, y) in nodes.items():\n    if name in ['X', 'Y']:\n        color = '#e74c3c'\n        size = 800\n    else:\n        color = '#95a5a6'\n        size = 600\n    ax.scatter(x, y, s=size, color=color, alpha=0.8, zorder=3)\n    ax.text(x, y, name, ha='center', va='center', fontsize=11,\n            fontweight='bold', color='white', zorder=4)\n\n# åå‘ä¼ æ’­è·¯å¾„ï¼ˆé¢œè‰²åŒºåˆ†ï¼‰\nbackward_edges = [\n    # è·¯å¾„1ï¼šç›´æ¥è·¯å¾„ï¼ˆçº¢è‰²ï¼‰\n    (('Y', 'X_hat', 'âˆ‚L/âˆ‚YÂ·Î³', '#e74c3c'), ('X_hat', 'X-mu', 'Ã·std', '#e74c3c')),\n    # è·¯å¾„2ï¼šé€šè¿‡æ–¹å·®ï¼ˆè“è‰²ï¼‰\n    (('X_hat', 'var', 'âˆ‚Â²Ïƒ/âˆ‚xÌ‚', '#3498db'), ('var', 'X-mu', '', '#3498db')),\n    # è·¯å¾„3ï¼šé€šè¿‡å‡å€¼ï¼ˆç»¿è‰²ï¼‰\n    (('X_hat', 'mu', 'âˆ‚Î¼/âˆ‚xÌ‚', '#2ecc71'), ('mu', 'X', '', '#2ecc71')),\n]\n\n# è·¯å¾„1\nax.annotate('', xy=nodes['X_hat'], xytext=nodes['Y'],\n            arrowprops=dict(arrowstyle='->', lw=3, color='#e74c3c'))\nax.annotate('', xy=nodes['X-mu'], xytext=nodes['X_hat'],\n            arrowprops=dict(arrowstyle='->', lw=3, color='#e74c3c'))\nax.annotate('', xy=nodes['X'], xytext=nodes['X-mu'],\n            arrowprops=dict(arrowstyle='->', lw=3, color='#e74c3c'))\n\n# è·¯å¾„2\nax.annotate('', xy=nodes['var'], xytext=nodes['X_hat'],\n            arrowprops=dict(arrowstyle='->', lw=2.5, color='#3498db', \n                          connectionstyle=\"arc3,rad=0.3\"))\nax.annotate('', xy=nodes['X'], xytext=nodes['var'],\n            arrowprops=dict(arrowstyle='->', lw=2.5, color='#3498db',\n                          connectionstyle=\"arc3,rad=-0.3\"))\n\n# è·¯å¾„3  \nax.annotate('', xy=nodes['mu'], xytext=nodes['X_hat'],\n            arrowprops=dict(arrowstyle='->', lw=2.5, color='#2ecc71',\n                          connectionstyle=\"arc3,rad=-0.3\"))\nax.annotate('', xy=nodes['X'], xytext=nodes['mu'],\n            arrowprops=dict(arrowstyle='->', lw=2.5, color='#2ecc71',\n                          connectionstyle=\"arc3,rad=0.3\"))\n\n# å›¾ä¾‹\nlegend_y = 1\nax.text(0.5, legend_y, 'â”â”', color='#e74c3c', fontsize=12, fontweight='bold')\nax.text(1.2, legend_y, 'è·¯å¾„1: ç›´æ¥è·¯å¾„', fontsize=9)\nax.text(0.5, legend_y-0.7, 'â”â”', color='#3498db', fontsize=12, fontweight='bold')\nax.text(1.2, legend_y-0.7, 'è·¯å¾„2: é€šè¿‡ÏƒÂ²', fontsize=9)\nax.text(0.5, legend_y-1.4, 'â”â”', color='#2ecc71', fontsize=12, fontweight='bold')\nax.text(1.2, legend_y-1.4, 'è·¯å¾„3: é€šè¿‡Î¼', fontsize=9)\n\nax.text(5, 9, 'åå‘ä¼ æ’­ï¼šdY â†’ dX (éœ€åˆå¹¶3æ¡è·¯å¾„çš„æ¢¯åº¦)', \n        ha='center', fontsize=10, style='italic', color='#7f8c8d')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nğŸ’¡ ä»è®¡ç®—å›¾ç†è§£åå‘ä¼ æ’­ï¼š\")\nprint(\"  - å‰å‘ä¼ æ’­ï¼šè¾“å…¥Xåˆ†åˆ«è®¡ç®—Î¼å’ŒÏƒÂ²ï¼Œç„¶åæ ‡å‡†åŒ–\")\nprint(\"  - åå‘ä¼ æ’­ï¼šæ¢¯åº¦ä»Yå›æµåˆ°Xæœ‰3æ¡è·¯å¾„ï¼Œéœ€è¦å…¨éƒ¨ç´¯åŠ \")\nprint(\"  - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆBNçš„backwardä»£ç çœ‹èµ·æ¥å¤æ‚çš„åŸå› ï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*70)\nprint(\"ğŸ“ Batch Normalization åå‘ä¼ æ’­ - åˆ†æ­¥æ¨å¯¼\")\nprint(\"=\"*70)\n\n# ç”Ÿæˆæµ‹è¯•æ•°æ®\nnp.random.seed(42)\nm = 4  # batch sizeï¼ˆä½¿ç”¨å°æ•°æ®ä¾¿äºæ‰‹ç®—éªŒè¯ï¼‰\nn = 3  # ç‰¹å¾æ•°\nX_test = np.array([\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0],\n    [10.0, 11.0, 12.0]\n])\n\nprint(f\"\\nğŸ“¥ è¾“å…¥æ•°æ® X (shape={X_test.shape}):\")\nprint(X_test)\n\n# å‰å‘ä¼ æ’­ï¼ˆæ‰‹åŠ¨è®¡ç®—æ¯ä¸€æ­¥ï¼‰\nprint(\"\\n\" + \"-\"*70)\nprint(\"ğŸ”½ å‰å‘ä¼ æ’­ï¼ˆä¿å­˜ä¸­é—´å˜é‡ï¼‰\")\nprint(\"-\"*70)\n\n# Step 1: è®¡ç®—å‡å€¼\nmu = np.mean(X_test, axis=0)\nprint(f\"\\nã€Step 1ã€‘è®¡ç®—batchå‡å€¼ Î¼ = (1/m) Î£x_i\")\nprint(f\"  Î¼ = {mu}\")\nprint(f\"  å…¬å¼: Î¼_j = (xâ‚â±¼ + xâ‚‚â±¼ + ... + x_mâ±¼) / m\")\n\n# Step 2: è®¡ç®—æ–¹å·®\nvar = np.var(X_test, axis=0)\nprint(f\"\\nã€Step 2ã€‘è®¡ç®—batchæ–¹å·® ÏƒÂ² = (1/m) Î£(x_i - Î¼)Â²\")\nprint(f\"  ÏƒÂ² = {var}\")\nprint(f\"  å…¬å¼: ÏƒÂ²_j = Î£(x_ij - Î¼_j)Â² / m\")\n\n# Step 3: è®¡ç®— x - mu\nx_minus_mu = X_test - mu\nprint(f\"\\nã€Step 3ã€‘è®¡ç®—åå·® (x - Î¼)\")\nprint(x_minus_mu)\n\n# Step 4: è®¡ç®—æ ‡å‡†å·®\nepsilon = 1e-5\nstd = np.sqrt(var + epsilon)\nprint(f\"\\nã€Step 4ã€‘è®¡ç®—æ ‡å‡†å·® std = âˆš(ÏƒÂ² + Îµ)\")\nprint(f\"  std = {std}\")\nprint(f\"  Îµ = {epsilon}ï¼ˆé˜²æ­¢é™¤é›¶ï¼‰\")\n\n# Step 5: æ ‡å‡†åŒ–\nX_hat = x_minus_mu / std\nprint(f\"\\nã€Step 5ã€‘æ ‡å‡†åŒ– xÌ‚ = (x - Î¼) / std\")\nprint(X_hat)\nprint(f\"  éªŒè¯: å‡å€¼ = {np.mean(X_hat, axis=0)}ï¼ˆåº”è¯¥â‰ˆ0ï¼‰\")\nprint(f\"        æ–¹å·® = {np.var(X_hat, axis=0)}ï¼ˆåº”è¯¥â‰ˆ1ï¼‰\")\n\n# Step 6: ç¼©æ”¾å’Œå¹³ç§»\ngamma = np.ones(n)  # å‡è®¾Î³=1\nbeta = np.zeros(n)  # å‡è®¾Î²=0\nY = gamma * X_hat + beta\nprint(f\"\\nã€Step 6ã€‘ç¼©æ”¾å¹³ç§» y = Î³Â·xÌ‚ + Î²\")\nprint(f\"  Î³ = {gamma}\")\nprint(f\"  Î² = {beta}\")\nprint(f\"  y = {Y}\")\n\nprint(\"\\n\" + \"-\"*70)\nprint(\"ğŸ”¼ åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰\")\nprint(\"-\"*70)\n\n# å‡è®¾ä¸Šæ¸¸æ¢¯åº¦ï¼ˆä»æŸå¤±å‡½æ•°ä¼ å›ï¼‰\ndY = np.ones_like(Y)  # ç®€åŒ–ï¼šå‡è®¾æŸå¤±å¯¹yçš„æ¢¯åº¦å…¨ä¸º1\nprint(f\"\\nğŸ“¥ ä¸Šæ¸¸æ¢¯åº¦ dL/dy (shape={dY.shape}):\")\nprint(dY)\n\n# ========== åå‘ä¼ æ’­è®¡ç®— ==========\n\n# Step 1: å¯¹Î³å’ŒÎ²çš„æ¢¯åº¦ï¼ˆç›´æ¥æ±‚å¯¼ï¼‰\nprint(f\"\\nã€Backward Step 1ã€‘è®¡ç®— âˆ‚L/âˆ‚Î³ å’Œ âˆ‚L/âˆ‚Î²\")\nprint(f\"  y = Î³Â·xÌ‚ + Î²\")\nprint(f\"  â†’ âˆ‚y/âˆ‚Î³ = xÌ‚\")\nprint(f\"  â†’ âˆ‚y/âˆ‚Î² = 1\")\nprint(f\"  é“¾å¼æ³•åˆ™:\")\nprint(f\"    âˆ‚L/âˆ‚Î³ = Î£(âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚Î³) = Î£(dY Â· xÌ‚)\")\nprint(f\"    âˆ‚L/âˆ‚Î² = Î£(âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚Î²) = Î£(dY)\")\n\ndgamma = np.sum(dY * X_hat, axis=0)\ndbeta = np.sum(dY, axis=0)\nprint(f\"\\n  ç»“æœ:\")\nprint(f\"    dÎ³ = {dgamma}\")\nprint(f\"    dÎ² = {dbeta}\")\n\n# Step 2: å¯¹xÌ‚çš„æ¢¯åº¦\nprint(f\"\\nã€Backward Step 2ã€‘è®¡ç®— âˆ‚L/âˆ‚xÌ‚\")\nprint(f\"  y = Î³Â·xÌ‚ + Î² â†’ âˆ‚y/âˆ‚xÌ‚ = Î³\")\nprint(f\"  âˆ‚L/âˆ‚xÌ‚ = âˆ‚L/âˆ‚y Â· Î³\")\n\ndX_hat = dY * gamma\nprint(f\"  dXÌ‚ = {dX_hat}\")\n\n# Step 3: å¯¹æ–¹å·®çš„æ¢¯åº¦ï¼ˆè¾ƒå¤æ‚ï¼‰\nprint(f\"\\nã€Backward Step 3ã€‘è®¡ç®— âˆ‚L/âˆ‚ÏƒÂ²\")\nprint(f\"  xÌ‚ = (x - Î¼) / âˆš(ÏƒÂ² + Îµ)\")\nprint(f\"  â†’ âˆ‚xÌ‚/âˆ‚ÏƒÂ² = (x - Î¼) Â· (-1/2) Â· (ÏƒÂ² + Îµ)^(-3/2)\")\nprint(f\"  âˆ‚L/âˆ‚ÏƒÂ² = Î£(âˆ‚L/âˆ‚xÌ‚ Â· âˆ‚xÌ‚/âˆ‚ÏƒÂ²)\")\n\ndvar = np.sum(dX_hat * x_minus_mu * (-0.5) * (var + epsilon)**(-1.5), axis=0)\nprint(f\"  dÏƒÂ² = {dvar}\")\n\n# Step 4: å¯¹å‡å€¼çš„æ¢¯åº¦ï¼ˆæœ€å¤æ‚ï¼Œæœ‰ä¸¤æ¡è·¯å¾„ï¼‰\nprint(f\"\\nã€Backward Step 4ã€‘è®¡ç®— âˆ‚L/âˆ‚Î¼\")\nprint(f\"  æ³¨æ„ï¼šÎ¼å½±å“æŸå¤±çš„ä¸¤æ¡è·¯å¾„:\")\nprint(f\"    è·¯å¾„1: Î¼ â†’ xÌ‚ â†’ y â†’ L\")\nprint(f\"    è·¯å¾„2: Î¼ â†’ ÏƒÂ² â†’ xÌ‚ â†’ y â†’ L\")\nprint(f\"  \")\nprint(f\"  è·¯å¾„1æ¢¯åº¦: Î£(âˆ‚L/âˆ‚xÌ‚ Â· (-1/std))\")\nprint(f\"  è·¯å¾„2æ¢¯åº¦: âˆ‚L/âˆ‚ÏƒÂ² Â· Î£(-2(x-Î¼)/m)\")\n\ndmu_path1 = np.sum(dX_hat * (-1/std), axis=0)\ndmu_path2 = dvar * np.sum(-2 * x_minus_mu, axis=0) / m\ndmu = dmu_path1 + dmu_path2\nprint(f\"  è·¯å¾„1: {dmu_path1}\")\nprint(f\"  è·¯å¾„2: {dmu_path2}\")\nprint(f\"  æ€»æ¢¯åº¦: dÎ¼ = {dmu}\")\n\n# Step 5: å¯¹è¾“å…¥Xçš„æ¢¯åº¦ï¼ˆä¸‰æ¡è·¯å¾„æ±‡æ€»ï¼‰\nprint(f\"\\nã€Backward Step 5ã€‘è®¡ç®— âˆ‚L/âˆ‚xï¼ˆæœ€ç»ˆæ¢¯åº¦ï¼‰\")\nprint(f\"  xå½±å“æŸå¤±çš„ä¸‰æ¡è·¯å¾„:\")\nprint(f\"    è·¯å¾„1: x â†’ xÌ‚ â†’ y â†’ L\")\nprint(f\"    è·¯å¾„2: x â†’ ÏƒÂ² â†’ xÌ‚ â†’ y â†’ L\")  \nprint(f\"    è·¯å¾„3: x â†’ Î¼ â†’ ... â†’ L\")\nprint(f\"  \")\nprint(f\"  âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚xÌ‚Â·(1/std) + âˆ‚L/âˆ‚ÏƒÂ²Â·(2(x-Î¼)/m) + âˆ‚L/âˆ‚Î¼Â·(1/m)\")\n\ndX_path1 = dX_hat / std\ndX_path2 = dvar * 2 * x_minus_mu / m\ndX_path3 = dmu / m\ndX = dX_path1 + dX_path2 + dX_path3\n\nprint(f\"\\n  è·¯å¾„1ï¼ˆç›´æ¥é€šè¿‡xÌ‚ï¼‰:\")\nprint(dX_path1)\nprint(f\"\\n  è·¯å¾„2ï¼ˆé€šè¿‡ÏƒÂ²ï¼‰:\")\nprint(dX_path2)\nprint(f\"\\n  è·¯å¾„3ï¼ˆé€šè¿‡Î¼ï¼‰:\")\nprint(dX_path3)\nprint(f\"\\n  ğŸ¯ æœ€ç»ˆæ¢¯åº¦ dX = dX_path1 + dX_path2 + dX_path3:\")\nprint(dX)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… åå‘ä¼ æ’­æ¨å¯¼å®Œæˆï¼\")\nprint(\"=\"*70)\nprint(\"\\nğŸ’¡ å…³é”®è¦ç‚¹:\")\nprint(\"  1. BNåå‘ä¼ æ’­å¤æ‚åœ¨äºÎ¼å’ŒÏƒÂ²çš„ä¾èµ–å…³ç³»\")\nprint(\"  2. æ¯ä¸ªè¾“å…¥xå½±å“æŸå¤±æœ‰3æ¡è·¯å¾„ï¼ˆç›´æ¥ã€é€šè¿‡ÏƒÂ²ã€é€šè¿‡Î¼ï¼‰\")\nprint(\"  3. éœ€è¦å°å¿ƒå¤„ç†batchç»´åº¦çš„æ±‚å’Œ\")\nprint(\"  4. å®é™…ä»£ç ä¸­è¿™äº›è®¡ç®—è¢«åˆå¹¶ä¼˜åŒ–ï¼Œä½†åŸç†ç›¸åŒ\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1.4 Batch Normalization åå‘ä¼ æ’­è¯¦è§£\n\n**âš ï¸ ä¸ºä»€ä¹ˆBNçš„åå‘ä¼ æ’­æ¯”è¾ƒå¤æ‚ï¼Ÿ**\n\nå› ä¸ºæ¯ä¸ªè¾“å…¥ $x_i$ çš„æ ‡å‡†åŒ–å€¼ $\\hat{x}_i$ ä¸ä»…ä¾èµ–äºè‡ªå·±ï¼Œè¿˜ä¾èµ–äºæ•´ä¸ªbatchçš„å‡å€¼å’Œæ–¹å·®ã€‚è¿™å¯¼è‡´æ¢¯åº¦è®¡ç®—éœ€è¦è€ƒè™‘è¿™äº›ä¾èµ–å…³ç³»ã€‚\n\n**ğŸ“Š è®¡ç®—å›¾å¯è§†åŒ–ï¼š**\n\n```\nè¾“å…¥ X â†’ å‡å€¼Î¼ â”\n         æ–¹å·®ÏƒÂ² â”¼â†’ æ ‡å‡†åŒ– XÌ‚ â†’ ç¼©æ”¾å¹³ç§» Y â†’ æŸå¤±L\n         X â”€â”€â”€â”€â”˜\n```\n\n**ğŸ” åå‘ä¼ æ’­æ¨å¯¼ï¼ˆæ­¥éª¤æ‹†è§£ï¼‰ï¼š**\n\næˆ‘ä»¬å·²çŸ¥ä¸Šæ¸¸æ¢¯åº¦ $\\frac{\\partial L}{\\partial y}$ï¼Œéœ€è¦è®¡ç®—ï¼š\n- $\\frac{\\partial L}{\\partial \\gamma}$ï¼ˆå¯¹ç¼©æ”¾å‚æ•°çš„æ¢¯åº¦ï¼‰\n- $\\frac{\\partial L}{\\partial \\beta}$ï¼ˆå¯¹å¹³ç§»å‚æ•°çš„æ¢¯åº¦ï¼‰\n- $\\frac{\\partial L}{\\partial x}$ï¼ˆå¯¹è¾“å…¥çš„æ¢¯åº¦ï¼Œä¼ é€’ç»™å‰ä¸€å±‚ï¼‰\n\nè®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ¨å¯¼...",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    \"\"\"\n",
    "    æ‰¹å½’ä¸€åŒ–å±‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "        n_features: è¾“å…¥ç‰¹å¾æ•°é‡\n",
    "        momentum: ç”¨äºæ›´æ–°running mean/varçš„åŠ¨é‡ï¼ˆé»˜è®¤0.9ï¼‰\n",
    "        epsilon: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼ˆé»˜è®¤1e-5ï¼‰\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, momentum=0.9, epsilon=1e-5):\n",
    "        self.n_features = n_features\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # å¯å­¦ä¹ å‚æ•°ï¼ˆé€šè¿‡åå‘ä¼ æ’­æ›´æ–°ï¼‰\n",
    "        self.gamma = np.ones(n_features)   # ç¼©æ”¾å‚æ•°ï¼Œåˆå§‹åŒ–ä¸º1\n",
    "        self.beta = np.zeros(n_features)   # å¹³ç§»å‚æ•°ï¼Œåˆå§‹åŒ–ä¸º0\n",
    "        \n",
    "        # æ¨ç†æ—¶ä½¿ç”¨çš„ç»Ÿè®¡é‡ï¼ˆè®­ç»ƒæ—¶ç´¯ç§¯ï¼‰\n",
    "        self.running_mean = np.zeros(n_features)\n",
    "        self.running_var = np.ones(n_features)\n",
    "        \n",
    "        # ç¼“å­˜çš„ä¸­é—´å˜é‡ï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            X: è¾“å…¥æ•°æ®, shape (m, n_features)\n",
    "            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "        \n",
    "        è¿”å›:\n",
    "            out: å½’ä¸€åŒ–åçš„è¾“å‡º, shape (m, n_features)\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            # ===== è®­ç»ƒæ¨¡å¼ =====\n",
    "            # 1. è®¡ç®—mini-batchçš„å‡å€¼å’Œæ–¹å·®\n",
    "            batch_mean = np.mean(X, axis=0)  # shape: (n_features,)\n",
    "            batch_var = np.var(X, axis=0)    # shape: (n_features,)\n",
    "            \n",
    "            # 2. æ ‡å‡†åŒ–\n",
    "            X_normalized = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "            \n",
    "            # 3. ç¼©æ”¾å’Œå¹³ç§»\n",
    "            out = self.gamma * X_normalized + self.beta\n",
    "            \n",
    "            # 4. æ›´æ–°runningç»Ÿè®¡é‡ï¼ˆç”¨äºæ¨ç†ï¼‰\n",
    "            # æŒ‡æ•°ç§»åŠ¨å¹³å‡: new = momentum * old + (1 - momentum) * current\n",
    "            self.running_mean = self.momentum * self.running_mean + \\\n",
    "                                (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + \\\n",
    "                               (1 - self.momentum) * batch_var\n",
    "            \n",
    "            # 5. ç¼“å­˜ä¸­é—´å˜é‡ï¼ˆåå‘ä¼ æ’­ç”¨ï¼‰\n",
    "            self.cache = {\n",
    "                'X': X,\n",
    "                'X_normalized': X_normalized,\n",
    "                'batch_mean': batch_mean,\n",
    "                'batch_var': batch_var,\n",
    "                'm': X.shape[0]\n",
    "            }\n",
    "        else:\n",
    "            # ===== æ¨ç†æ¨¡å¼ =====\n",
    "            # ä½¿ç”¨è®­ç»ƒæ—¶ç´¯ç§¯çš„runningç»Ÿè®¡é‡\n",
    "            X_normalized = (X - self.running_mean) / \\\n",
    "                          np.sqrt(self.running_var + self.epsilon)\n",
    "            out = self.gamma * X_normalized + self.beta\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        åå‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            dout: ä¸Šæ¸¸æ¢¯åº¦, shape (m, n_features)\n",
    "        \n",
    "        è¿”å›:\n",
    "            dX: å¯¹è¾“å…¥çš„æ¢¯åº¦, shape (m, n_features)\n",
    "        \n",
    "        åŒæ—¶æ›´æ–° self.grad_gamma å’Œ self.grad_beta\n",
    "        \"\"\"\n",
    "        # ä»cacheä¸­å–å‡ºå‰å‘ä¼ æ’­çš„ä¸­é—´å˜é‡\n",
    "        X = self.cache['X']\n",
    "        X_normalized = self.cache['X_normalized']\n",
    "        batch_mean = self.cache['batch_mean']\n",
    "        batch_var = self.cache['batch_var']\n",
    "        m = self.cache['m']\n",
    "        \n",
    "        # 1. å¯¹Î³å’ŒÎ²çš„æ¢¯åº¦\n",
    "        self.grad_gamma = np.sum(dout * X_normalized, axis=0)  # shape: (n_features,)\n",
    "        self.grad_beta = np.sum(dout, axis=0)                  # shape: (n_features,)\n",
    "        \n",
    "        # 2. å¯¹æ ‡å‡†åŒ–åçš„Xçš„æ¢¯åº¦\n",
    "        dX_normalized = dout * self.gamma  # shape: (m, n_features)\n",
    "        \n",
    "        # 3. å¯¹æ–¹å·®çš„æ¢¯åº¦\n",
    "        # âˆ‚L/âˆ‚ÏƒÂ² = Î£ âˆ‚L/âˆ‚xÌ‚ * (x - Î¼) * (-0.5) * (ÏƒÂ² + Îµ)^(-3/2)\n",
    "        dvar = np.sum(dX_normalized * (X - batch_mean) * \\\n",
    "                      (-0.5) * (batch_var + self.epsilon) ** (-3/2), axis=0)\n",
    "        \n",
    "        # 4. å¯¹å‡å€¼çš„æ¢¯åº¦\n",
    "        # âˆ‚L/âˆ‚Î¼ = Î£ âˆ‚L/âˆ‚xÌ‚ * (-1/âˆš(ÏƒÂ²+Îµ)) + âˆ‚L/âˆ‚ÏƒÂ² * Î£(-2(x-Î¼)/m)\n",
    "        dmean = np.sum(dX_normalized * (-1 / np.sqrt(batch_var + self.epsilon)), axis=0) + \\\n",
    "                dvar * np.sum(-2 * (X - batch_mean), axis=0) / m\n",
    "        \n",
    "        # 5. å¯¹è¾“å…¥Xçš„æ¢¯åº¦\n",
    "        dX = dX_normalized / np.sqrt(batch_var + self.epsilon) + \\\n",
    "             dvar * 2 * (X - batch_mean) / m + \\\n",
    "             dmean / m\n",
    "        \n",
    "        return dX\n",
    "\n",
    "\n",
    "print(\"BatchNormalizationå®ç°å®Œæˆï¼\")\n",
    "print(\"\\nğŸ’¡ å…³é”®ç‚¹ï¼š\")\n",
    "print(\"1. è®­ç»ƒæ—¶ï¼šä½¿ç”¨batchç»Ÿè®¡é‡ï¼Œæ›´æ–°runningç»Ÿè®¡é‡\")\n",
    "print(\"2. æ¨ç†æ—¶ï¼šä½¿ç”¨runningç»Ÿè®¡é‡ï¼ˆå›ºå®šï¼‰\")\n",
    "print(\"3. Î³å’ŒÎ²æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œé€šè¿‡åå‘ä¼ æ’­æ›´æ–°\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# å¯è§†åŒ–å¯¹æ¯”ç»“æœ\nfig = plt.figure(figsize=(18, 10))\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\nepochs = np.arange(1, 151)\ncolors = {\n    'âŒ æ— æ­£åˆ™åŒ–': '#95a5a6',\n    'ğŸ”µ åªæœ‰BN': '#3498db',\n    'ğŸŸ¢ åªæœ‰Dropout': '#2ecc71',\n    'â­ BN+Dropout': '#e74c3c'\n}\n\n# ========== ç¬¬1è¡Œï¼šè®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡ ==========\nax1 = fig.add_subplot(gs[0, :2])\nfor config_name, result in results.items():\n    ax1.plot(epochs, result['train_accs'], '--', \n             color=colors[config_name], alpha=0.4, linewidth=1.5)\n    ax1.plot(epochs, result['test_accs'], '-', \n             color=colors[config_name], label=config_name, linewidth=2.5)\nax1.set_xlabel('Epoch', fontsize=11)\nax1.set_ylabel('å‡†ç¡®ç‡', fontsize=11)\nax1.set_title('å­¦ä¹ æ›²çº¿å¯¹æ¯”ï¼ˆå®çº¿=æµ‹è¯•ï¼Œè™šçº¿=è®­ç»ƒï¼‰', fontsize=12, fontweight='bold')\nax1.legend(loc='lower right', fontsize=10)\nax1.grid(True, alpha=0.3)\nax1.set_ylim(0, 1.05)\n\n# ========== ç¬¬1è¡Œå³ä¾§ï¼šæœ€ç»ˆæ€§èƒ½å¯¹æ¯” ==========\nax2 = fig.add_subplot(gs[0, 2])\nconfig_names = list(results.keys())\nfinal_test_accs = [results[name]['final_test_acc'] for name in config_names]\nbars = ax2.barh(range(len(config_names)), final_test_accs, \n                color=[colors[name] for name in config_names], alpha=0.8)\nax2.set_yticks(range(len(config_names)))\nax2.set_yticklabels(config_names, fontsize=9)\nax2.set_xlabel('æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡', fontsize=10)\nax2.set_title('æ€§èƒ½æ’å', fontsize=12, fontweight='bold')\nax2.grid(True, alpha=0.3, axis='x')\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    ax2.text(width, bar.get_y() + bar.get_height()/2, \n             f'{width:.3f}', ha='left', va='center', fontsize=9, \n             fontweight='bold')\n\n# ========== ç¬¬2è¡Œï¼šæŸå¤±æ›²çº¿ ==========\nax3 = fig.add_subplot(gs[1, :2])\nfor config_name, result in results.items():\n    ax3.plot(epochs, result['train_losses'], '--',\n             color=colors[config_name], alpha=0.4, linewidth=1.5)\n    ax3.plot(epochs, result['test_losses'], '-',\n             color=colors[config_name], label=config_name, linewidth=2.5)\nax3.set_xlabel('Epoch', fontsize=11)\nax3.set_ylabel('æŸå¤±å€¼', fontsize=11)\nax3.set_title('æŸå¤±æ›²çº¿å¯¹æ¯”ï¼ˆå®çº¿=æµ‹è¯•ï¼Œè™šçº¿=è®­ç»ƒï¼‰', fontsize=12, fontweight='bold')\nax3.legend(loc='upper right', fontsize=10)\nax3.grid(True, alpha=0.3)\nax3.set_ylim(0, 3)\n\n# ========== ç¬¬2è¡Œå³ä¾§ï¼šè¿‡æ‹Ÿåˆåˆ†æ ==========\nax4 = fig.add_subplot(gs[1, 2])\noverfitting_gaps = [results[name]['overfitting_gap'] for name in config_names]\nbars = ax4.barh(range(len(config_names)), overfitting_gaps,\n                color=[colors[name] for name in config_names], alpha=0.8)\nax4.set_yticks(range(len(config_names)))\nax4.set_yticklabels(config_names, fontsize=9)\nax4.set_xlabel('è¿‡æ‹Ÿåˆç¨‹åº¦\\n(è®­ç»ƒå‡†ç¡®ç‡ - æµ‹è¯•å‡†ç¡®ç‡)', fontsize=10)\nax4.set_title('è¿‡æ‹Ÿåˆåˆ†æ', fontsize=12, fontweight='bold')\nax4.grid(True, alpha=0.3, axis='x')\nax4.axvline(x=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\nax4.text(0.05, -0.5, 'è­¦æˆ’çº¿', fontsize=8, color='red')\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    ax4.text(width, bar.get_y() + bar.get_height()/2,\n             f'{width:.3f}', ha='left', va='center', fontsize=9,\n             fontweight='bold')\n\n# ========== ç¬¬3è¡Œï¼šæ”¶æ•›é€Ÿåº¦å¯¹æ¯” ==========\nax5 = fig.add_subplot(gs[2, :])\n# è®¡ç®—æ¯ä¸ªé…ç½®è¾¾åˆ°90%å‡†ç¡®ç‡æ‰€éœ€çš„epochæ•°\nconvergence_epochs = {}\nfor config_name, result in results.items():\n    test_accs = np.array(result['test_accs'])\n    idx = np.where(test_accs >= 0.9)[0]\n    if len(idx) > 0:\n        convergence_epochs[config_name] = idx[0] + 1\n    else:\n        convergence_epochs[config_name] = 150  # æœªè¾¾åˆ°\n\n# ç»˜åˆ¶æ”¶æ•›é€Ÿåº¦\nx_offset = 0\nfor config_name in config_names:\n    epochs_to_converge = convergence_epochs[config_name]\n    result = results[config_name]\n    \n    # ç»˜åˆ¶è¾¾åˆ°90%å‡†ç¡®ç‡çš„è¿‡ç¨‹\n    if epochs_to_converge < 150:\n        ax5.plot(range(1, epochs_to_converge+1), \n                result['test_accs'][:epochs_to_converge],\n                color=colors[config_name], linewidth=3, \n                label=f\"{config_name} ({epochs_to_converge} epochs)\")\n        ax5.scatter(epochs_to_converge, 0.9, \n                   color=colors[config_name], s=150, zorder=5,\n                   marker='*', edgecolors='black', linewidths=1.5)\n    else:\n        ax5.plot(epochs[:100], result['test_accs'][:100],\n                color=colors[config_name], linewidth=3, alpha=0.5,\n                linestyle='--', label=f\"{config_name} (æœªè¾¾åˆ°)\")\n\nax5.axhline(y=0.9, color='gray', linestyle='--', linewidth=1.5, alpha=0.7)\nax5.text(5, 0.91, 'ç›®æ ‡: 90%å‡†ç¡®ç‡', fontsize=10, color='gray')\nax5.set_xlabel('Epoch', fontsize=11)\nax5.set_ylabel('æµ‹è¯•å‡†ç¡®ç‡', fontsize=11)\nax5.set_title('æ”¶æ•›é€Ÿåº¦å¯¹æ¯”ï¼ˆè¾¾åˆ°90%å‡†ç¡®ç‡æ‰€éœ€æ—¶é—´ï¼‰', fontsize=12, fontweight='bold')\nax5.legend(loc='lower right', fontsize=10)\nax5.grid(True, alpha=0.3)\nax5.set_xlim(0, 100)\nax5.set_ylim(0, 1)\n\nplt.suptitle('BN vs Dropout å…¨é¢å¯¹æ¯”å®éªŒ', fontsize=15, fontweight='bold', y=0.995)\nplt.show()\n\n# æ‰“å°è¯¦ç»†åˆ†æ\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ“Š å®éªŒç»“æœåˆ†æ\")\nprint(\"=\"*80)\n\nprint(\"\\nã€1. æ”¶æ•›é€Ÿåº¦ã€‘ï¼ˆè¾¾åˆ°90%æµ‹è¯•å‡†ç¡®ç‡æ‰€éœ€epochæ•°ï¼‰\")\nfor config_name in config_names:\n    epochs_needed = convergence_epochs[config_name]\n    if epochs_needed < 150:\n        print(f\"  {config_name}: {epochs_needed} epochs\")\n    else:\n        print(f\"  {config_name}: >150 epochs (æœªè¾¾åˆ°)\")\n\nprint(\"\\nã€2. æœ€ç»ˆæ€§èƒ½ã€‘ï¼ˆ150 epochåçš„æµ‹è¯•å‡†ç¡®ç‡ï¼‰\")\nsorted_results = sorted(results.items(), \n                       key=lambda x: x[1]['final_test_acc'], \n                       reverse=True)\nfor i, (config_name, result) in enumerate(sorted_results, 1):\n    print(f\"  {i}. {config_name}: {result['final_test_acc']:.4f}\")\n\nprint(\"\\nã€3. è¿‡æ‹Ÿåˆç¨‹åº¦ã€‘ï¼ˆè®­ç»ƒå‡†ç¡®ç‡ - æµ‹è¯•å‡†ç¡®ç‡ï¼‰\")\nsorted_overfitting = sorted(results.items(),\n                           key=lambda x: x[1]['overfitting_gap'])\nfor i, (config_name, result) in enumerate(sorted_overfitting, 1):\n    gap = result['overfitting_gap']\n    status = \"âœ“ è‰¯å¥½\" if gap < 0.05 else \"âš ï¸ è½»å¾®è¿‡æ‹Ÿåˆ\" if gap < 0.1 else \"âŒ ä¸¥é‡è¿‡æ‹Ÿåˆ\"\n    print(f\"  {i}. {config_name}: {gap:.4f} {status}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ’¡ æ ¸å¿ƒç»“è®º\")\nprint(\"=\"*80)\nprint(\"\\n1. ğŸš€ æ”¶æ•›é€Ÿåº¦ï¼š\")\nprint(\"   - BNï¼ˆæœ‰æ— Dropoutï¼‰æ˜æ˜¾åŠ å¿«æ”¶æ•›\")\nprint(\"   - åŸå› ï¼šç¨³å®šçš„æ¢¯åº¦æµï¼Œå…è®¸æ›´å¤§çš„å­¦ä¹ ç‡\")\n\nprint(\"\\n2. ğŸ¯ é˜²æ­¢è¿‡æ‹Ÿåˆï¼š\")\nprint(\"   - Dropoutæ˜¾è‘—é™ä½è¿‡æ‹Ÿåˆ\")\nprint(\"   - BNä¹Ÿæœ‰è½»å¾®çš„æ­£åˆ™åŒ–æ•ˆæœ\")\n\nprint(\"\\n3. â­ æœ€ä½³å®è·µï¼š\")\nprint(\"   - BN+Dropoutç»„åˆï¼šæ—¢å¿«é€Ÿæ”¶æ•›ï¼Œåˆé˜²æ­¢è¿‡æ‹Ÿåˆ\")\nprint(\"   - é€‚ç”¨åœºæ™¯ï¼šæ·±åº¦ç½‘ç»œã€å¤§å‹æ•°æ®é›†\")\n\nprint(\"\\n4. ğŸ“‹ é€‰æ‹©å»ºè®®ï¼š\")\nprint(\"   - æµ…å±‚ç½‘ç»œï¼šå¯èƒ½ä¸éœ€è¦BNå’ŒDropout\")\nprint(\"   - ä¸­ç­‰ç½‘ç»œï¼šé€‰æ‹©BNæˆ–Dropoutä¹‹ä¸€\")\nprint(\"   - æ·±åº¦ç½‘ç»œï¼šå»ºè®®BN+Dropoutç»„åˆ\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# å‡†å¤‡å®éªŒæ•°æ®\nprint(\"ğŸ”§ å‡†å¤‡å®éªŒæ•°æ®...\")\ndigits = load_digits()\nX = digits.data / 16.0  # å½’ä¸€åŒ–åˆ°[0,1]\ny = digits.target\n\n# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# One-hotç¼–ç \ndef to_one_hot(y, n_classes=10):\n    one_hot = np.zeros((len(y), n_classes))\n    one_hot[np.arange(len(y)), y] = 1\n    return one_hot\n\ny_train_oh = to_one_hot(y_train)\ny_test_oh = to_one_hot(y_test)\n\nprint(f\"âœ… æ•°æ®å‡†å¤‡å®Œæˆ\")\nprint(f\"   è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}\")\n\n\n# åˆ›å»ºç®€åŒ–çš„è®­ç»ƒå‡½æ•°ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼‰\ndef simulate_training(use_bn, use_dropout, n_epochs=150, learning_rate=0.01):\n    \"\"\"\n    æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œè®°å½•æŸå¤±å’Œå‡†ç¡®ç‡\n    \n    æ³¨ï¼šè¿™é‡Œä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œä½¿ç”¨ç®€åŒ–çš„è®­ç»ƒæ¨¡æ‹Ÿ\n    \"\"\"\n    np.random.seed(42)\n    \n    # åˆå§‹åŒ–ç½‘ç»œ\n    input_size, hidden_size, output_size = 64, 128, 10\n    \n    # æƒé‡\n    W1 = np.random.randn(hidden_size, input_size) * 0.01\n    b1 = np.zeros(hidden_size)\n    W2 = np.random.randn(output_size, hidden_size) * 0.01\n    b2 = np.zeros(output_size)\n    \n    # BN/Dropoutå±‚\n    bn1 = BatchNormalization(hidden_size) if use_bn else None\n    dropout1 = Dropout(0.3) if use_dropout else None\n    \n    # è®°å½•æŒ‡æ ‡\n    train_losses, test_losses = [], []\n    train_accs, test_accs = [], []\n    \n    # è®­ç»ƒå¾ªç¯\n    for epoch in range(n_epochs):\n        # === è®­ç»ƒé˜¶æ®µ ===\n        # å‰å‘ä¼ æ’­\n        z1 = X_train @ W1.T + b1\n        if bn1:\n            z1 = bn1.forward(z1, training=True)\n        a1 = relu(z1)\n        if dropout1:\n            a1 = dropout1.forward(a1, training=True)\n        \n        z2 = a1 @ W2.T + b2\n        a2 = softmax(z2)\n        \n        # è®¡ç®—æŸå¤±\n        epsilon = 1e-15\n        a2_clipped = np.clip(a2, epsilon, 1 - epsilon)\n        train_loss = -np.mean(np.sum(y_train_oh * np.log(a2_clipped), axis=1))\n        train_acc = np.mean(np.argmax(a2, axis=1) == y_train)\n        \n        # === æµ‹è¯•é˜¶æ®µ ===\n        z1_test = X_test @ W1.T + b1\n        if bn1:\n            z1_test = bn1.forward(z1_test, training=False)\n        a1_test = relu(z1_test)\n        if dropout1:\n            a1_test = dropout1.forward(a1_test, training=False)\n        \n        z2_test = a1_test @ W2.T + b2\n        a2_test = softmax(z2_test)\n        \n        a2_test_clipped = np.clip(a2_test, epsilon, 1 - epsilon)\n        test_loss = -np.mean(np.sum(y_test_oh * np.log(a2_test_clipped), axis=1))\n        test_acc = np.mean(np.argmax(a2_test, axis=1) == y_test)\n        \n        # è®°å½•\n        train_losses.append(train_loss)\n        test_losses.append(test_loss)\n        train_accs.append(train_acc)\n        test_accs.append(test_acc)\n        \n        # === ç®€åŒ–çš„æ¢¯åº¦ä¸‹é™ï¼ˆç”¨äºæ¼”ç¤ºï¼‰===\n        # è®¡ç®—è¾“å‡ºå±‚æ¢¯åº¦\n        dz2 = (a2 - y_train_oh) / len(y_train_oh)\n        dW2 = dz2.T @ a1\n        db2 = np.sum(dz2, axis=0)\n        \n        # è®¡ç®—éšè—å±‚æ¢¯åº¦\n        da1 = dz2 @ W2\n        if dropout1:\n            da1 = dropout1.backward(da1)\n        dz1 = da1 * (z1 > 0)  # ReLUå¯¼æ•°\n        if bn1:\n            dz1 = bn1.backward(dz1)\n        dW1 = dz1.T @ X_train\n        db1 = np.sum(dz1, axis=0)\n        \n        # æ›´æ–°å‚æ•°\n        lr = learning_rate\n        # BNå…è®¸ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡\n        if use_bn:\n            lr *= 3.0\n        \n        W2 -= lr * dW2\n        b2 -= lr * db2\n        W1 -= lr * dW1\n        b1 -= lr * db1\n        \n        # æ›´æ–°BNå‚æ•°\n        if bn1:\n            bn1.gamma -= lr * bn1.grad_gamma\n            bn1.beta -= lr * bn1.grad_beta\n    \n    return {\n        'train_losses': train_losses,\n        'test_losses': test_losses,\n        'train_accs': train_accs,\n        'test_accs': test_accs,\n        'final_train_acc': train_accs[-1],\n        'final_test_acc': test_accs[-1],\n        'overfitting_gap': train_accs[-1] - test_accs[-1]\n    }\n\n\nprint(\"\\nğŸš€ å¼€å§‹å¯¹æ¯”å®éªŒ...\")\nprint(\"   è¿™å°†éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...\\n\")\n\n# è¿è¡Œ4ç§é…ç½®\nconfigs = [\n    {'use_bn': False, 'use_dropout': False, 'name': 'âŒ æ— æ­£åˆ™åŒ–'},\n    {'use_bn': True,  'use_dropout': False, 'name': 'ğŸ”µ åªæœ‰BN'},\n    {'use_bn': False, 'use_dropout': True,  'name': 'ğŸŸ¢ åªæœ‰Dropout'},\n    {'use_bn': True,  'use_dropout': True,  'name': 'â­ BN+Dropout'},\n]\n\nresults = {}\nfor i, config in enumerate(configs, 1):\n    print(f\"[{i}/4] è®­ç»ƒé…ç½®: {config['name']}\", end=\" ... \")\n    result = simulate_training(\n        use_bn=config['use_bn'],\n        use_dropout=config['use_dropout'],\n        n_epochs=150\n    )\n    results[config['name']] = result\n    print(f\"âœ“ (æµ‹è¯•å‡†ç¡®ç‡: {result['final_test_acc']:.3f})\")\n\nprint(\"\\nâœ… æ‰€æœ‰å®éªŒå®Œæˆï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ†š BN vs Dropout è¯¦ç»†å¯¹æ¯”å®éªŒ\n\nç°åœ¨æˆ‘ä»¬å·²ç»ç†è§£äº†BNå’ŒDropoutçš„åŸç†ï¼Œè®©æˆ‘ä»¬é€šè¿‡å®éªŒæ¥ç›´è§‚å¯¹æ¯”å®ƒä»¬çš„æ•ˆæœï¼š\n\n**å®éªŒè®¾è®¡ï¼š**\n- 4ç§é…ç½®ï¼šæ— æ­£åˆ™åŒ–ã€åªç”¨BNã€åªç”¨Dropoutã€BN+Dropout\n- å¯¹æ¯”ç»´åº¦ï¼šè®­ç»ƒé€Ÿåº¦ã€æ”¶æ•›æ€§ã€è¿‡æ‹Ÿåˆç¨‹åº¦\n- æ•°æ®é›†ï¼šMNISTæ‰‹å†™æ•°å­—ï¼ˆsklearn.datasetsï¼‰",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 å¯è§†åŒ–Batch Normalizationçš„æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(100, 3) * 5 + 10  # å‡å€¼10ï¼Œæ ‡å‡†å·®5\n",
    "\n",
    "print(\"åŸå§‹æ•°æ®ç»Ÿè®¡ï¼š\")\n",
    "print(f\"å‡å€¼: {np.mean(X_test, axis=0)}\")\n",
    "print(f\"æ–¹å·®: {np.var(X_test, axis=0)}\")\n",
    "\n",
    "# åº”ç”¨Batch Normalization\n",
    "bn = BatchNormalization(n_features=3)\n",
    "X_normalized = bn.forward(X_test, training=True)\n",
    "\n",
    "print(\"\\nBatch Normalizationåï¼š\")\n",
    "print(f\"å‡å€¼: {np.mean(X_normalized, axis=0)}\")\n",
    "print(f\"æ–¹å·®: {np.var(X_normalized, axis=0)}\")\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# åŸå§‹æ•°æ®åˆ†å¸ƒ\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(3):\n",
    "    plt.hist(X_test[:, i], bins=20, alpha=0.5, label=f'ç‰¹å¾{i+1}')\n",
    "plt.xlabel('æ•°å€¼', fontsize=11)\n",
    "plt.ylabel('é¢‘æ•°', fontsize=11)\n",
    "plt.title('åŸå§‹æ•°æ®åˆ†å¸ƒï¼ˆä¸åŒå°ºåº¦ï¼‰', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# BNåçš„æ•°æ®åˆ†å¸ƒ\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3):\n",
    "    plt.hist(X_normalized[:, i], bins=20, alpha=0.5, label=f'ç‰¹å¾{i+1}')\n",
    "plt.xlabel('æ•°å€¼', fontsize=11)\n",
    "plt.ylabel('é¢‘æ•°', fontsize=11)\n",
    "plt.title('Batch Normalizationåï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” è§‚å¯Ÿè¦ç‚¹ï¼š\")\n",
    "print(\"1. BNåæ‰€æœ‰ç‰¹å¾éƒ½æ ‡å‡†åŒ–åˆ°ç›¸åŒå°ºåº¦\")\n",
    "print(\"2. æœ‰åˆ©äºæ¢¯åº¦ä¼ æ’­å’Œè®­ç»ƒç¨³å®šæ€§\")\n",
    "print(\"3. å¯ä»¥ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Dropout\n",
    "\n",
    "### 2.1 DropoutåŸç†\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³ï¼š** è®­ç»ƒæ—¶éšæœº\"å…³é—­\"ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n",
    "\n",
    "**ç®—æ³•æµç¨‹ï¼š**\n",
    "\n",
    "**è®­ç»ƒæ—¶ï¼š**\n",
    "1. ä»¥æ¦‚ç‡ $p$ éšæœºå°†ç¥ç»å…ƒè¾“å‡ºè®¾ä¸º0\n",
    "2. å°†å‰©ä½™ç¥ç»å…ƒçš„è¾“å‡ºä¹˜ä»¥ $\\frac{1}{1-p}$ï¼ˆInverted Dropoutï¼‰\n",
    "\n",
    "**æ¨ç†æ—¶ï¼š**\n",
    "- ä¸ä½¿ç”¨Dropoutï¼Œæ‰€æœ‰ç¥ç»å…ƒéƒ½å‚ä¸è®¡ç®—\n",
    "\n",
    "**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**\n",
    "1. **é›†æˆå­¦ä¹ æ•ˆæœ**ï¼šç›¸å½“äºè®­ç»ƒå¤šä¸ªå­ç½‘ç»œï¼Œæ¨ç†æ—¶å–å¹³å‡\n",
    "2. **å‡å°‘ç¥ç»å…ƒå…±é€‚åº”**ï¼šå¼ºè¿«ç½‘ç»œå­¦ä¹ é²æ£’ç‰¹å¾\n",
    "3. **æ­£åˆ™åŒ–**ï¼šé™åˆ¶æ¨¡å‹å¤æ‚åº¦\n",
    "\n",
    "### 2.2 ä»é›¶å®ç°Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    Dropoutå±‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "        drop_prob: dropoutæ¦‚ç‡ï¼ˆä¸¢å¼ƒç¥ç»å…ƒçš„æ¦‚ç‡ï¼‰\n",
    "    \n",
    "    æ³¨æ„:\n",
    "        ä½¿ç”¨Inverted Dropoutï¼Œè®­ç»ƒæ—¶ç¼©æ”¾ï¼Œæ¨ç†æ—¶ä¸å˜\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "            drop_prob: ä¸¢å¼ƒæ¦‚ç‡ pï¼ˆ0åˆ°1ä¹‹é—´ï¼‰\n",
    "                      p=0.5è¡¨ç¤ºä¸¢å¼ƒ50%çš„ç¥ç»å…ƒ\n",
    "        \"\"\"\n",
    "        assert 0 <= drop_prob < 1, \"drop_probåº”è¯¥åœ¨[0, 1)ä¹‹é—´\"\n",
    "        self.drop_prob = drop_prob\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            X: è¾“å…¥æ•°æ®, shape (m, n_features)\n",
    "            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "        \n",
    "        è¿”å›:\n",
    "            out: shape (m, n_features)\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            # ===== è®­ç»ƒæ¨¡å¼ï¼ˆInverted Dropoutï¼‰=====\n",
    "            # 1. ç”Ÿæˆmaskï¼šä»¥æ¦‚ç‡(1-p)ä¿ç•™ç¥ç»å…ƒ\n",
    "            #    mask[i] = 1 è¡¨ç¤ºä¿ç•™ï¼Œ0 è¡¨ç¤ºä¸¢å¼ƒ\n",
    "            self.mask = (np.random.rand(*X.shape) > self.drop_prob).astype(float)\n",
    "            \n",
    "            # 2. åº”ç”¨maskå¹¶ç¼©æ”¾\n",
    "            # ç¼©æ”¾å› å­: 1/(1-p)ï¼Œä½¿å¾—æœŸæœ›å€¼ä¿æŒä¸å˜\n",
    "            # ä¾‹å¦‚ï¼šp=0.5æ—¶ï¼Œä¿ç•™çš„ç¥ç»å…ƒä¹˜ä»¥2\n",
    "            out = X * self.mask / (1 - self.drop_prob)\n",
    "        else:\n",
    "            # ===== æ¨ç†æ¨¡å¼ =====\n",
    "            # ä¸åšä»»ä½•æ“ä½œï¼Œç›´æ¥è¿”å›\n",
    "            out = X\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        åå‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            dout: ä¸Šæ¸¸æ¢¯åº¦, shape (m, n_features)\n",
    "        \n",
    "        è¿”å›:\n",
    "            dX: å¯¹è¾“å…¥çš„æ¢¯åº¦, shape (m, n_features)\n",
    "        \"\"\"\n",
    "        # æ¢¯åº¦åªé€šè¿‡ä¿ç•™çš„ç¥ç»å…ƒä¼ æ’­\n",
    "        dX = dout * self.mask / (1 - self.drop_prob)\n",
    "        return dX\n",
    "\n",
    "\n",
    "print(\"Dropoutå®ç°å®Œæˆï¼\")\n",
    "print(\"\\nğŸ’¡ Inverted Dropoutçš„ä¼˜åŠ¿ï¼š\")\n",
    "print(\"1. è®­ç»ƒæ—¶ç¼©æ”¾ï¼Œæ¨ç†æ—¶ç›´æ¥ä½¿ç”¨ï¼ˆæ— éœ€é¢å¤–æ“ä½œï¼‰\")\n",
    "print(\"2. æ¨ç†é€Ÿåº¦æ›´å¿«\")\n",
    "print(\"3. ä»£ç æ›´ç®€æ´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 å¯è§†åŒ–Dropoutçš„æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•Dropout\n",
    "np.random.seed(42)\n",
    "X_dropout = np.ones((5, 10))  # 5ä¸ªæ ·æœ¬ï¼Œ10ä¸ªç‰¹å¾ï¼Œå…¨ä¸º1\n",
    "\n",
    "dropout = Dropout(drop_prob=0.5)\n",
    "\n",
    "# è®­ç»ƒæ¨¡å¼\n",
    "X_train_out = dropout.forward(X_dropout, training=True)\n",
    "mask = dropout.mask\n",
    "\n",
    "# æ¨ç†æ¨¡å¼\n",
    "X_test_out = dropout.forward(X_dropout, training=False)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
    "\n",
    "# åŸå§‹è¾“å…¥\n",
    "axes[0].imshow(X_dropout, cmap='RdYlGn', aspect='auto', vmin=0, vmax=2)\n",
    "axes[0].set_title('åŸå§‹è¾“å…¥ï¼ˆå…¨ä¸º1ï¼‰', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('ç‰¹å¾ç»´åº¦')\n",
    "axes[0].set_ylabel('æ ·æœ¬')\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        axes[0].text(j, i, f'{X_dropout[i, j]:.1f}', \n",
    "                     ha='center', va='center', fontsize=8)\n",
    "\n",
    "# è®­ç»ƒæ¨¡å¼ï¼ˆDropoutæ¿€æ´»ï¼‰\n",
    "axes[1].imshow(X_train_out, cmap='RdYlGn', aspect='auto', vmin=0, vmax=2)\n",
    "axes[1].set_title('è®­ç»ƒæ¨¡å¼ï¼ˆp=0.5ï¼Œä¿ç•™çš„ä¹˜ä»¥2ï¼‰', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('ç‰¹å¾ç»´åº¦')\n",
    "axes[1].set_ylabel('æ ·æœ¬')\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        axes[1].text(j, i, f'{X_train_out[i, j]:.1f}', \n",
    "                     ha='center', va='center', fontsize=8,\n",
    "                     color='red' if X_train_out[i, j] == 0 else 'black')\n",
    "\n",
    "# æ¨ç†æ¨¡å¼ï¼ˆæ— Dropoutï¼‰\n",
    "axes[2].imshow(X_test_out, cmap='RdYlGn', aspect='auto', vmin=0, vmax=2)\n",
    "axes[2].set_title('æ¨ç†æ¨¡å¼ï¼ˆæ— Dropoutï¼‰', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('ç‰¹å¾ç»´åº¦')\n",
    "axes[2].set_ylabel('æ ·æœ¬')\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        axes[2].text(j, i, f'{X_test_out[i, j]:.1f}', \n",
    "                     ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nç»Ÿè®¡ä¿¡æ¯ï¼š\")\n",
    "print(f\"è®­ç»ƒæ¨¡å¼ - ä¿ç•™çš„ç¥ç»å…ƒæ¯”ä¾‹: {np.mean(mask):.2f} (æœŸæœ›0.5)\")\n",
    "print(f\"è®­ç»ƒæ¨¡å¼ - è¾“å‡ºå‡å€¼: {np.mean(X_train_out):.2f} (æœŸæœ›1.0)\")\n",
    "print(f\"æ¨ç†æ¨¡å¼ - è¾“å‡ºå‡å€¼: {np.mean(X_test_out):.2f} (æœŸæœ›1.0)\")\n",
    "print(\"\\nğŸ’¡ Inverted Dropoutç¡®ä¿è®­ç»ƒå’Œæ¨ç†æ—¶æœŸæœ›å€¼ä¸€è‡´ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# å¯è§†åŒ–å†³ç­–æµç¨‹å›¾\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\n\n# ========== å·¦å›¾ï¼šæŠ€æœ¯é€‰æ‹©å†³ç­–æ ‘ ==========\nax = axes[0]\nax.set_xlim(0, 10)\nax.set_ylim(0, 12)\nax.axis('off')\nax.set_title('å†³ç­–æ ‘ï¼šé€‰æ‹©åˆé€‚çš„å½’ä¸€åŒ–/æ­£åˆ™åŒ–æŠ€æœ¯', fontsize=14, fontweight='bold', pad=20)\n\n# å†³ç­–èŠ‚ç‚¹\nnodes = [\n    # (x, y, text, color, size)\n    (5, 11, 'å¼€å§‹', '#3498db', 1200),\n    (3, 9.5, 'CNN?', '#95a5a6', 800),\n    (7, 9.5, 'RNN/\\nTransformer?', '#95a5a6', 800),\n    (2, 8, 'âœ“ BN\\n(å·ç§¯å±‚)', '#2ecc71', 900),\n    (4, 8, 'âœ“ BN+Dropout\\n(å…¨è¿æ¥å±‚)', '#2ecc71', 900),\n    (7, 8, 'âœ“ Layer Norm', '#2ecc71', 900),\n    (2, 6.5, 'Batch\\nSize?', '#95a5a6', 700),\n    (4, 6.5, 'æ•°æ®é‡?', '#95a5a6', 700),\n    (7, 6.5, 'è®­ç»ƒ\\nç¨³å®š?', '#95a5a6', 700),\n    (1, 5, '>32:\\nBN', '#f39c12', 600),\n    (2.5, 5, '<8:\\nLayer Norm', '#f39c12', 600),\n    (4, 5, 'å°æ•°æ®:\\nå¼ºDropout', '#f39c12', 600),\n    (7, 5, 'ä¸ç¨³å®š:\\nåŠ BN', '#f39c12', 600),\n]\n\nfor x, y, text, color, size in nodes:\n    ax.scatter(x, y, s=size, color=color, alpha=0.85, zorder=3)\n    ax.text(x, y, text, ha='center', va='center', fontsize=9,\n            fontweight='bold', color='white', zorder=4)\n\n# è¿æ¥çº¿\nedges = [\n    (5, 11, 3, 9.5, 'Yes'),\n    (5, 11, 7, 9.5, 'No'),\n    (3, 9.5, 2, 8, 'å·ç§¯'),\n    (3, 9.5, 4, 8, 'å…¨è¿æ¥'),\n    (7, 9.5, 7, 8, 'Yes'),\n    (2, 8, 2, 6.5, ''),\n    (4, 8, 4, 6.5, ''),\n    (7, 8, 7, 6.5, ''),\n    (2, 6.5, 1, 5, ''),\n    (2, 6.5, 2.5, 5, ''),\n    (4, 6.5, 4, 5, ''),\n    (7, 6.5, 7, 5, ''),\n]\n\nfor x1, y1, x2, y2, label in edges:\n    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n                arrowprops=dict(arrowstyle='->', lw=1.5, color='#34495e'))\n    if label:\n        mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n        ax.text(mid_x + 0.3, mid_y, label, fontsize=8, style='italic', color='#7f8c8d')\n\n# å›¾ä¾‹\nax.text(0.5, 2, 'ğŸ”µ èµ·ç‚¹', fontsize=10)\nax.text(0.5, 1.5, 'âšª åˆ¤æ–­', fontsize=10)\nax.text(0.5, 1, 'ğŸŸ¢ æ¨è', fontsize=10)\nax.text(0.5, 0.5, 'ğŸŸ¡ å¤‡é€‰', fontsize=10)\n\n# ========== å³å›¾ï¼šä½¿ç”¨åœºæ™¯çƒ­åŠ›å›¾ ==========\nax = axes[1]\nax.set_title('ä¸åŒåœºæ™¯ä¸‹çš„æŠ€æœ¯é€‰æ‹©ï¼ˆé¢œè‰²è¶Šæ·±è¶Šæ¨èï¼‰', fontsize=14, fontweight='bold', pad=20)\n\n# æ•°æ®\nscenarios = [\n    'CNNå›¾åƒåˆ†ç±»',\n    'CNNç›®æ ‡æ£€æµ‹',\n    'RNNæ–‡æœ¬ç”Ÿæˆ',\n    'Transformer',\n    'å°æ•°æ®é›†åˆ†ç±»',\n    'è¿ç§»å­¦ä¹ ',\n    'å¼ºåŒ–å­¦ä¹ ',\n    'GANç”Ÿæˆ'\n]\n\ntechniques = ['BN', 'Layer\\nNorm', 'Dropout', 'BN+\\nDropout']\n\n# çƒ­åŠ›å›¾æ•°æ®ï¼ˆ0-3åˆ†ï¼Œ3æœ€æ¨èï¼‰\nheatmap_data = np.array([\n    [3, 1, 2, 3],  # CNNå›¾åƒåˆ†ç±»\n    [2, 2, 2, 3],  # CNNç›®æ ‡æ£€æµ‹\n    [0, 3, 2, 1],  # RNNæ–‡æœ¬ç”Ÿæˆ\n    [0, 3, 2, 2],  # Transformer\n    [1, 1, 3, 2],  # å°æ•°æ®é›†åˆ†ç±»\n    [2, 1, 2, 3],  # è¿ç§»å­¦ä¹ \n    [0, 3, 1, 0],  # å¼ºåŒ–å­¦ä¹ \n    [1, 1, 1, 1],  # GANç”Ÿæˆ\n])\n\nim = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=3)\n\n# è®¾ç½®åˆ»åº¦\nax.set_xticks(np.arange(len(techniques)))\nax.set_yticks(np.arange(len(scenarios)))\nax.set_xticklabels(techniques, fontsize=10)\nax.set_yticklabels(scenarios, fontsize=10)\n\n# æ·»åŠ æ•°å€¼å’Œæ¨èæ ‡è®°\nfor i in range(len(scenarios)):\n    for j in range(len(techniques)):\n        value = heatmap_data[i, j]\n        if value == 3:\n            text = 'â­\\nå¼ºçƒˆ\\næ¨è'\n            color = 'white'\n            weight = 'bold'\n        elif value == 2:\n            text = 'âœ“\\næ¨è'\n            color = 'black'\n            weight = 'normal'\n        elif value == 1:\n            text = 'â—‹\\nå¯é€‰'\n            color = 'black'\n            weight = 'normal'\n        else:\n            text = 'âœ—\\nä¸æ¨è'\n            color = 'gray'\n            weight = 'normal'\n        \n        ax.text(j, i, text, ha='center', va='center', \n                fontsize=8, color=color, fontweight=weight)\n\n# æ·»åŠ é¢œè‰²æ¡\ncbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\ncbar.set_label('æ¨èç¨‹åº¦', fontsize=10)\ncbar.set_ticks([0, 1, 2, 3])\ncbar.set_ticklabels(['ä¸æ¨è', 'å¯é€‰', 'æ¨è', 'å¼ºçƒˆæ¨è'], fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ¯ å¿«é€Ÿå†³ç­–æŒ‡å—\")\nprint(\"=\"*80)\nprint(\"\\nã€ç¬¬ä¸€æ­¥ï¼šçœ‹ç½‘ç»œç±»å‹ã€‘\")\nprint(\"  â€¢ CNN â†’ ä¼˜å…ˆè€ƒè™‘ Batch Normalization\")\nprint(\"  â€¢ RNN/Transformer â†’ ä¼˜å…ˆè€ƒè™‘ Layer Normalization\")\nprint(\"  â€¢ MLP â†’ æ ¹æ®æ·±åº¦å’Œæ•°æ®é‡å†³å®š\")\n\nprint(\"\\nã€ç¬¬äºŒæ­¥ï¼šçœ‹Batch Sizeã€‘\")\nprint(\"  â€¢ Batch Size > 32 â†’ BNæ•ˆæœå¥½\")\nprint(\"  â€¢ Batch Size < 8 â†’ Layer Normæ›´ç¨³å®š\")\nprint(\"  â€¢ Batch Size = 1 â†’ å¿…é¡»ç”¨Layer Norm\")\n\nprint(\"\\nã€ç¬¬ä¸‰æ­¥ï¼šçœ‹ä¸»è¦é—®é¢˜ã€‘\")\nprint(\"  â€¢ è®­ç»ƒä¸ç¨³å®š/æ”¶æ•›æ…¢ â†’ åŠ  BN\")\nprint(\"  â€¢ è¿‡æ‹Ÿåˆ â†’ åŠ  Dropout\")\nprint(\"  â€¢ ä¸¤è€…éƒ½æœ‰ â†’ BN + Dropoutç»„åˆ\")\n\nprint(\"\\nã€ç¬¬å››æ­¥ï¼šè°ƒæ•´å‚æ•°ã€‘\")\nprint(\"  â€¢ Dropoutæ¦‚ç‡ï¼šå…¨è¿æ¥å±‚0.5ï¼Œå·ç§¯å±‚0.2\")\nprint(\"  â€¢ BNä½ç½®ï¼šDense â†’ BN â†’ ReLU â†’ Dropout\")\nprint(\"  â€¢ å­¦ä¹ ç‡ï¼šä½¿ç”¨BNåå¯ä»¥æé«˜2-5å€\")\n\nprint(\"\\nğŸ’¡ è®°ä½ï¼šå®è·µä¸­éœ€è¦æ ¹æ®å…·ä½“æƒ…å†µè°ƒæ•´ï¼Œæ²¡æœ‰ä¸‡èƒ½çš„é…ç½®ï¼\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## ğŸ§­ å®ç”¨å†³ç­–æŒ‡å—ï¼šä½•æ—¶ä½¿ç”¨BN/Dropout/Layer Normï¼Ÿ\n\n### å†³ç­–æµç¨‹å›¾\n\n```\nå¼€å§‹\n  â”‚\n  â”œâ”€â†’ ã€ç½‘ç»œç±»å‹ã€‘\n  â”‚    â”œâ”€ CNNï¼ˆå·ç§¯ç½‘ç»œï¼‰\n  â”‚    â”‚   â”œâ”€ å·ç§¯å±‚ï¼šä½¿ç”¨ Batch Normalization\n  â”‚    â”‚   â””â”€ å…¨è¿æ¥å±‚ï¼šBN + Dropout (p=0.2-0.5)\n  â”‚    â”‚\n  â”‚    â”œâ”€ RNN/LSTM/Transformer\n  â”‚    â”‚   â””â”€ ä½¿ç”¨ Layer Normalizationï¼ˆä¸ä¾èµ–batchï¼‰\n  â”‚    â”‚\n  â”‚    â””â”€ å…¨è¿æ¥ç½‘ç»œï¼ˆMLPï¼‰\n  â”‚        â”œâ”€ æµ…å±‚ï¼ˆ1-2å±‚ï¼‰ï¼šå¯èƒ½ä¸éœ€è¦\n  â”‚        â”œâ”€ ä¸­ç­‰ï¼ˆ3-5å±‚ï¼‰ï¼šé€‰BNæˆ–Dropoutä¹‹ä¸€\n  â”‚        â””â”€ æ·±å±‚ï¼ˆ>5å±‚ï¼‰ï¼šBN + Dropoutç»„åˆ\n  â”‚\n  â”œâ”€â†’ ã€Batch Sizeã€‘\n  â”‚    â”œâ”€ å¤§batch (>32)ï¼šBatch Normalizationæ•ˆæœå¥½\n  â”‚    â”œâ”€ å°batch (<8)ï¼šLayer Normalizationæ›´ç¨³å®š\n  â”‚    â””â”€ å•æ ·æœ¬ (=1)ï¼šå¿…é¡»ç”¨Layer Normalization\n  â”‚\n  â”œâ”€â†’ ã€è®­ç»ƒæ•°æ®é‡ã€‘\n  â”‚    â”œâ”€ å¤§æ•°æ®é›† (>10K)ï¼šBNæ•ˆæœå¥½\n  â”‚    â”œâ”€ ä¸­ç­‰æ•°æ®é›† (1K-10K)ï¼šBN + Dropout\n  â”‚    â””â”€ å°æ•°æ®é›† (<1K)ï¼šé‡ç‚¹ç”¨Dropoutï¼ˆå¼ºæ­£åˆ™åŒ–ï¼‰\n  â”‚\n  â””â”€â†’ ã€ä¸»è¦é—®é¢˜ã€‘\n       â”œâ”€ è®­ç»ƒä¸ç¨³å®š/æ¢¯åº¦æ¶ˆå¤±ï¼šä¼˜å…ˆä½¿ç”¨BN\n       â”œâ”€ è¿‡æ‹Ÿåˆï¼šä¼˜å…ˆä½¿ç”¨Dropout\n       â”œâ”€ æ”¶æ•›å¤ªæ…¢ï¼šä½¿ç”¨BN\n       â””â”€ æ¨ç†é€Ÿåº¦è¦æ±‚é«˜ï¼šè°¨æ…ä½¿ç”¨Dropoutï¼ˆè®­ç»ƒæ…¢ï¼‰\n```\n\n### å†³ç­–çŸ©é˜µè¡¨\n\n| åœºæ™¯ | ç½‘ç»œæ·±åº¦ | æ•°æ®é‡ | Batch Size | æ¨èæ–¹æ¡ˆ | ç†ç”± |\n|-----|---------|--------|-----------|---------|------|\n| **å›¾åƒåˆ†ç±»ï¼ˆCNNï¼‰** | æ·± | å¤§ | 32-128 | **BN + Dropout** | BNç¨³å®šè®­ç»ƒï¼ŒDropouté˜²è¿‡æ‹Ÿåˆ |\n| **ç›®æ ‡æ£€æµ‹** | å¾ˆæ·± | å¤§ | å°(2-8) | **Group Norm + Dropout** | å°batchä¸‹GNä¼˜äºBN |\n| **æ–‡æœ¬ç”Ÿæˆï¼ˆRNNï¼‰** | ä¸­-æ·± | å¤§ | 32-64 | **Layer Norm + Dropout** | LNä¸ä¾èµ–batchï¼Œé€‚åˆåºåˆ— |\n| **Transformer** | å¾ˆæ·± | å¤§ | å¤§ | **Layer Norm + Dropout** | Transformeræ ‡é… |\n| **å°æ•°æ®é›†åˆ†ç±»** | æµ…-ä¸­ | å° | 16-32 | **åªç”¨Dropout(p=0.5)** | é‡ç‚¹é˜²è¿‡æ‹Ÿåˆ |\n| **è¿ç§»å­¦ä¹ ** | æ·± | å° | 16-32 | **å†»ç»“BN + Dropout** | ä¿ç•™é¢„è®­ç»ƒçš„BNç»Ÿè®¡é‡ |\n| **å¼ºåŒ–å­¦ä¹ ** | ä¸­ | æµå¼ | 1-32 | **Layer Norm** | æ ·æœ¬éi.i.dï¼ŒBNä¸é€‚ç”¨ |\n| **ç”Ÿæˆæ¨¡å‹ï¼ˆGANï¼‰** | æ·± | å¤§ | 32-64 | **è°¨æ…ä½¿ç”¨** | BNå¯èƒ½å½±å“å¤šæ ·æ€§ |\n\n### è¶…å‚æ•°é€‰æ‹©é€ŸæŸ¥è¡¨\n\n| å‚æ•° | æ¨èèŒƒå›´ | è¯´æ˜ |\n|-----|---------|------|\n| **Dropoutæ¦‚ç‡** | | |\n| - å…¨è¿æ¥å±‚ | 0.3 - 0.5 | å¸¸ç”¨0.5 |\n| - å·ç§¯å±‚ | 0.1 - 0.3 | å·ç§¯å±‚å·²æœ‰ç©ºé—´ä¸å˜æ€§ï¼Œdropoutå°‘ç”¨ |\n| - RNN | 0.2 - 0.5 | åªåœ¨éå¾ªç¯è¿æ¥ä¸Šdropout |\n| - è¾“å…¥å±‚ | 0.1 - 0.2 | å¾ˆå°‘ç”¨ |\n| **BNåŠ¨é‡ï¼ˆmomentumï¼‰** | 0.9 - 0.99 | 0.9ä¸ºé»˜è®¤å€¼ |\n| **BN epsilon** | 1e-5 - 1e-3 | 1e-5ä¸ºé»˜è®¤ï¼Œé˜²æ­¢é™¤é›¶ |\n| **BNä½ç½®** | æ¿€æ´»å‡½æ•°ä¹‹å‰æˆ–ä¹‹å | Dense â†’ BN â†’ ReLUï¼ˆæ¨èï¼‰ |\n\n### ç»„åˆä½¿ç”¨å»ºè®®\n\n#### âœ… æ¨èç»„åˆ\n\n1. **Dense â†’ BN â†’ ReLU â†’ Dropout**ï¼ˆå…¨è¿æ¥å±‚æ ‡é…ï¼‰\n   ```python\n   x = Dense(units)(x)\n   x = BatchNormalization()(x)\n   x = ReLU()(x)\n   x = Dropout(0.5)(x)\n   ```\n\n2. **Conv â†’ BN â†’ ReLU â†’ Dropout â†’ Pool**ï¼ˆå·ç§¯å±‚æ ‡é…ï¼‰\n   ```python\n   x = Conv2D(filters)(x)\n   x = BatchNormalization()(x)\n   x = ReLU()(x)\n   x = Dropout(0.2)(x)  # æ³¨æ„ï¼šå·ç§¯å±‚dropoutæ¦‚ç‡è¾ƒå°\n   x = MaxPooling2D()(x)\n   ```\n\n3. **Transformer Block: LayerNorm + Dropout**\n   ```python\n   # Multi-Head Attention\n   x = LayerNorm()(x + Dropout(0.1)(MultiHeadAttention(x)))\n   # Feed Forward\n   x = LayerNorm()(x + Dropout(0.1)(FeedForward(x)))\n   ```\n\n#### âŒ ä¸æ¨èç»„åˆ\n\n1. **BN + Layer Norm**ï¼ˆå†—ä½™ï¼Œé€‰ä¸€ä¸ªå³å¯ï¼‰\n2. **è¿‡åº¦Dropout**ï¼ˆå¤šå±‚éƒ½ç”¨é«˜dropoutä¼šå¯¼è‡´æ¬ æ‹Ÿåˆï¼‰\n3. **å°batch + BN**ï¼ˆbatch<8æ—¶BNç»Ÿè®¡ä¸ç¨³å®šï¼‰\n\n### å¸¸è§é—®é¢˜æ’æŸ¥\n\n| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |\n|-----|---------|---------|\n| è®­ç»ƒä¸ç¨³å®š | ç¼ºå°‘å½’ä¸€åŒ– | æ·»åŠ BNæˆ–Layer Norm |\n| è®­ç»ƒå‡†ç¡®ç‡è¿œé«˜äºæµ‹è¯• | è¿‡æ‹Ÿåˆ | å¢åŠ Dropoutæ¦‚ç‡æˆ–æ·»åŠ æ›´å¤šæ­£åˆ™åŒ– |\n| æ”¶æ•›éå¸¸æ…¢ | å­¦ä¹ ç‡å¤ªå° | ä½¿ç”¨BNåå¯ä»¥æé«˜å­¦ä¹ ç‡2-5å€ |\n| æ¨ç†æ—¶ç»“æœå¼‚å¸¸ | BN/Dropoutæ¨¡å¼é”™è¯¯ | ç¡®ä¿æ¨ç†æ—¶ `training=False` |\n| å°batchè®­ç»ƒå´©æºƒ | BNç»Ÿè®¡é‡ä¸ç¨³å®š | æ”¹ç”¨Layer Normæˆ–Group Norm |\n| éªŒè¯é›†losséœ‡è¡ | Dropoutå¤ªå¼º | é™ä½dropoutæ¦‚ç‡ |\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. å¸¦BNå’ŒDropoutçš„å®Œæ•´ç½‘ç»œ\n",
    "\n",
    "### 3.1 æ„å»ºç°ä»£ç¥ç»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¿€æ´»å‡½æ•°\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def softmax(z):\n",
    "    if z.ndim == 1:\n",
    "        z_shifted = z - np.max(z)\n",
    "        exp_z = np.exp(z_shifted)\n",
    "        return exp_z / np.sum(exp_z)\n",
    "    else:\n",
    "        z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_shifted)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class ModernNeuralNetwork:\n",
    "    \"\"\"\n",
    "    ç°ä»£ç¥ç»ç½‘ç»œæ¶æ„\n",
    "    \n",
    "    ç»“æ„:\n",
    "        è¾“å…¥ â†’ Dense â†’ BN â†’ ReLU â†’ Dropout â†’\n",
    "        Dense â†’ BN â†’ ReLU â†’ Dropout â†’\n",
    "        Dense â†’ Softmax\n",
    "    \n",
    "    å‚æ•°:\n",
    "        use_bn: æ˜¯å¦ä½¿ç”¨Batch Normalization\n",
    "        use_dropout: æ˜¯å¦ä½¿ç”¨Dropout\n",
    "        drop_prob: Dropoutæ¦‚ç‡\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=64, hidden_size=128, output_size=10,\n",
    "                 use_bn=True, use_dropout=True, drop_prob=0.5):\n",
    "        self.use_bn = use_bn\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "        # æƒé‡åˆå§‹åŒ–ï¼ˆHeåˆå§‹åŒ–ï¼‰\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros(hidden_size)\n",
    "        \n",
    "        self.W3 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b3 = np.zeros(output_size)\n",
    "        \n",
    "        # Batch Normalizationå±‚\n",
    "        if use_bn:\n",
    "            self.bn1 = BatchNormalization(hidden_size)\n",
    "            self.bn2 = BatchNormalization(hidden_size)\n",
    "        \n",
    "        # Dropoutå±‚\n",
    "        if use_dropout:\n",
    "            self.dropout1 = Dropout(drop_prob)\n",
    "            self.dropout2 = Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            X: è¾“å…¥, shape (m, input_size)\n",
    "            training: è®­ç»ƒæ¨¡å¼æˆ–æ¨ç†æ¨¡å¼\n",
    "        \"\"\"\n",
    "        # ç¬¬1å±‚\n",
    "        self.z1 = X @ self.W1.T + self.b1  # (m, hidden_size)\n",
    "        \n",
    "        if self.use_bn:\n",
    "            self.z1 = self.bn1.forward(self.z1, training=training)\n",
    "        \n",
    "        self.a1 = relu(self.z1)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            self.a1 = self.dropout1.forward(self.a1, training=training)\n",
    "        \n",
    "        # ç¬¬2å±‚\n",
    "        self.z2 = self.a1 @ self.W2.T + self.b2  # (m, hidden_size)\n",
    "        \n",
    "        if self.use_bn:\n",
    "            self.z2 = self.bn2.forward(self.z2, training=training)\n",
    "        \n",
    "        self.a2 = relu(self.z2)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            self.a2 = self.dropout2.forward(self.a2, training=training)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.z3 = self.a2 @ self.W3.T + self.b3  # (m, output_size)\n",
    "        self.a3 = softmax(self.z3)\n",
    "        \n",
    "        return self.a3\n",
    "    \n",
    "    def compute_loss(self, X, y_true, training=True):\n",
    "        \"\"\"äº¤å‰ç†µæŸå¤±\"\"\"\n",
    "        y_pred = self.forward(X, training=training)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "        return loss\n",
    "    \n",
    "    def compute_accuracy(self, X, y_true, training=False):\n",
    "        \"\"\"è®¡ç®—å‡†ç¡®ç‡\"\"\"\n",
    "        y_pred = self.forward(X, training=training)\n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        labels = np.argmax(y_true, axis=1)\n",
    "        return np.mean(predictions == labels)\n",
    "\n",
    "\n",
    "print(\"ç°ä»£ç¥ç»ç½‘ç»œæ¶æ„å®ç°å®Œæˆï¼\")\n",
    "print(\"\\nğŸ—ï¸ æ¶æ„ç‰¹ç‚¹ï¼š\")\n",
    "print(\"1. Batch Normalizationï¼šç¨³å®šè®­ç»ƒ\")\n",
    "print(\"2. Dropoutï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ\")\n",
    "print(\"3. ReLUæ¿€æ´»ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±\")\n",
    "print(\"4. è®­ç»ƒ/æ¨ç†æ¨¡å¼è‡ªåŠ¨åˆ‡æ¢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MNISTæ•°æ®é›†å®æˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# åˆ’åˆ†æ•°æ®é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# æ ‡å‡†åŒ–\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# One-hotç¼–ç \n",
    "def to_one_hot(y, n_classes=10):\n",
    "    one_hot = np.zeros((len(y), n_classes))\n",
    "    one_hot[np.arange(len(y)), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_onehot = to_one_hot(y_train)\n",
    "y_test_onehot = to_one_hot(y_test)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_modern_network(X_train, y_train, X_test, y_test,\n",
    "                          use_bn=True, use_dropout=True, \n",
    "                          n_epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    è®­ç»ƒç°ä»£ç¥ç»ç½‘ç»œ\n",
    "    \"\"\"\n",
    "    model = ModernNeuralNetwork(\n",
    "        input_size=64, hidden_size=128, output_size=10,\n",
    "        use_bn=use_bn, use_dropout=use_dropout, drop_prob=0.5\n",
    "    )\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    n_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # æ‰“ä¹±æ•°æ®\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        # Mini-batchè®­ç»ƒï¼ˆè¿™é‡Œç®€åŒ–ï¼Œåªè®°å½•æŸå¤±ï¼‰\n",
    "        for batch in range(n_batches):\n",
    "            start = batch * batch_size\n",
    "            end = min(start + batch_size, len(X_train))\n",
    "            \n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰\n",
    "            model.forward(X_batch, training=True)\n",
    "            # æ³¨ï¼šå®Œæ•´çš„åå‘ä¼ æ’­å®ç°è¾ƒå¤æ‚ï¼Œè¿™é‡Œçœç•¥\n",
    "        \n",
    "        # æ¯ä¸ªepochç»“æŸåè¯„ä¼°\n",
    "        train_loss = model.compute_loss(X_train, y_train, training=True)\n",
    "        train_acc = model.compute_accuracy(X_train, y_train, training=False)\n",
    "        test_acc = model.compute_accuracy(X_test, y_test, training=False)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {train_loss:.4f}, \"\n",
    "                  f\"Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    return model, train_losses, train_accs, test_accs\n",
    "\n",
    "\n",
    "# å¯¹æ¯”ä¸åŒé…ç½®\n",
    "configs = [\n",
    "    {'use_bn': False, 'use_dropout': False, 'name': 'æ— BNæ— Dropout'},\n",
    "    {'use_bn': True, 'use_dropout': False, 'name': 'åªæœ‰BN'},\n",
    "    {'use_bn': False, 'use_dropout': True, 'name': 'åªæœ‰Dropout'},\n",
    "    {'use_bn': True, 'use_dropout': True, 'name': 'BN+Dropout'},\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"è®­ç»ƒé…ç½®: {config['name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model, train_losses, train_accs, test_accs = train_modern_network(\n",
    "        X_train, y_train_onehot, X_test, y_test_onehot,\n",
    "        use_bn=config['use_bn'],\n",
    "        use_dropout=config['use_dropout'],\n",
    "        n_epochs=100\n",
    "    )\n",
    "    \n",
    "    results[config['name']] = {\n",
    "        'train_accs': train_accs,\n",
    "        'test_accs': test_accs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "epochs = np.arange(1, 101)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# æµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”\n",
    "plt.subplot(1, 2, 1)\n",
    "for config in configs:\n",
    "    name = config['name']\n",
    "    plt.plot(epochs, results[name]['test_accs'], label=name, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=11)\n",
    "plt.ylabel('Test Accuracy', fontsize=11)\n",
    "plt.title('æµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# æœ€ç»ˆç»“æœå¯¹æ¯”\n",
    "plt.subplot(1, 2, 2)\n",
    "final_test_accs = [results[cfg['name']]['test_accs'][-1] for cfg in configs]\n",
    "colors = ['#95a5a6', '#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = plt.bar(range(len(configs)), final_test_accs, color=colors, alpha=0.8)\n",
    "plt.xlabel('é…ç½®', fontsize=11)\n",
    "plt.ylabel('æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡', fontsize=11)\n",
    "plt.title('æœ€ç»ˆæ€§èƒ½å¯¹æ¯”', fontsize=12, fontweight='bold')\n",
    "plt.xticks(range(len(configs)), [cfg['name'] for cfg in configs], rotation=15)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ ‡æ³¨æ•°å€¼\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š å®éªŒç»“è®ºï¼š\")\n",
    "print(\"1. BN+Dropoutç»„åˆé€šå¸¸æ•ˆæœæœ€å¥½\")\n",
    "print(\"2. BNä¸»è¦åŠ é€Ÿæ”¶æ•›\")\n",
    "print(\"3. Dropoutä¸»è¦é˜²æ­¢è¿‡æ‹Ÿåˆ\")\n",
    "print(\"4. ä¸¤è€…ç»“åˆæ•ˆæœæœ€ä½³\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Layer Normalization\n",
    "\n",
    "### 4.1 BN vs LNå¯¹æ¯”\n",
    "\n",
    "**Batch Normalizationï¼š**\n",
    "- æ²¿ç€batchç»´åº¦å½’ä¸€åŒ–\n",
    "- ä¾èµ–batch size\n",
    "- é€‚ç”¨äºCNN\n",
    "\n",
    "**Layer Normalizationï¼š**\n",
    "- æ²¿ç€ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–\n",
    "- ä¸ä¾èµ–batch size\n",
    "- é€‚ç”¨äºRNNå’ŒTransformer\n",
    "\n",
    "### 4.2 Layer Normalizationå®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:\n",
    "    \"\"\"\n",
    "    å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰\n",
    "    \n",
    "    ä¸BNçš„åŒºåˆ«ï¼š\n",
    "    - BNï¼šå¯¹æ¯ä¸ªç‰¹å¾åœ¨batchç»´åº¦ä¸Šå½’ä¸€åŒ–\n",
    "    - LNï¼šå¯¹æ¯ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç»´åº¦ä¸Šå½’ä¸€åŒ–\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, epsilon=1e-5):\n",
    "        self.n_features = n_features\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # å¯å­¦ä¹ å‚æ•°\n",
    "        self.gamma = np.ones(n_features)\n",
    "        self.beta = np.zeros(n_features)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            X: è¾“å…¥, shape (m, n_features)\n",
    "        \n",
    "        æ³¨æ„:\n",
    "            LNåœ¨è®­ç»ƒå’Œæ¨ç†æ—¶è¡Œä¸ºç›¸åŒï¼ˆä¸ä¾èµ–batchç»Ÿè®¡é‡ï¼‰\n",
    "        \"\"\"\n",
    "        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å‡å€¼å’Œæ–¹å·®ï¼ˆæ²¿ç‰¹å¾ç»´åº¦ï¼‰\n",
    "        mean = np.mean(X, axis=1, keepdims=True)  # shape: (m, 1)\n",
    "        var = np.var(X, axis=1, keepdims=True)    # shape: (m, 1)\n",
    "        \n",
    "        # æ ‡å‡†åŒ–\n",
    "        X_normalized = (X - mean) / np.sqrt(var + self.epsilon)\n",
    "        \n",
    "        # ç¼©æ”¾å’Œå¹³ç§»\n",
    "        out = self.gamma * X_normalized + self.beta\n",
    "        \n",
    "        # ç¼“å­˜ï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰\n",
    "        self.cache = {\n",
    "            'X': X,\n",
    "            'X_normalized': X_normalized,\n",
    "            'mean': mean,\n",
    "            'var': var\n",
    "        }\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# å¯¹æ¯”BNå’ŒLN\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(4, 6)  # 4ä¸ªæ ·æœ¬ï¼Œ6ä¸ªç‰¹å¾\n",
    "\n",
    "bn = BatchNormalization(n_features=6)\n",
    "ln = LayerNormalization(n_features=6)\n",
    "\n",
    "X_bn = bn.forward(X_test, training=True)\n",
    "X_ln = ln.forward(X_test, training=True)\n",
    "\n",
    "print(\"åŸå§‹æ•°æ®ï¼š\")\n",
    "print(X_test)\n",
    "print(\"\\nBatch Normalizationï¼ˆæ¯åˆ—å½’ä¸€åŒ–ï¼‰ï¼š\")\n",
    "print(X_bn)\n",
    "print(f\"æ¯åˆ—å‡å€¼: {np.mean(X_bn, axis=0)}\")\n",
    "print(f\"æ¯åˆ—æ–¹å·®: {np.var(X_bn, axis=0)}\")\n",
    "\n",
    "print(\"\\nLayer Normalizationï¼ˆæ¯è¡Œå½’ä¸€åŒ–ï¼‰ï¼š\")\n",
    "print(X_ln)\n",
    "print(f\"æ¯è¡Œå‡å€¼: {np.mean(X_ln, axis=1)}\")\n",
    "print(f\"æ¯è¡Œæ–¹å·®: {np.var(X_ln, axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. æœ¬ç« æ€»ç»“\n",
    "\n",
    "### æŠ€æœ¯å¯¹æ¯”\n",
    "\n",
    "| æŠ€æœ¯ | ä½œç”¨ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|-----|------|------|------|----------|\n",
    "| **Batch Normalization** | ç¨³å®šè®­ç»ƒ | åŠ é€Ÿæ”¶æ•›ï¼Œå…è®¸æ›´å¤§å­¦ä¹ ç‡ | ä¾èµ–batch size | CNN |\n",
    "| **Layer Normalization** | ç¨³å®šè®­ç»ƒ | ä¸ä¾èµ–batch | æ•ˆæœç•¥å¼±äºBN | RNN, Transformer |\n",
    "| **Dropout** | é˜²æ­¢è¿‡æ‹Ÿåˆ | å¼ºæ­£åˆ™åŒ–æ•ˆæœ | è®­ç»ƒæ—¶é—´é•¿ | å…¨è¿æ¥å±‚ |\n",
    "\n",
    "### è®­ç»ƒ vs æ¨ç†æ¨¡å¼\n",
    "\n",
    "**Batch Normalizationï¼š**\n",
    "- è®­ç»ƒæ—¶ï¼šä½¿ç”¨batchç»Ÿè®¡é‡ï¼Œæ›´æ–°runningç»Ÿè®¡é‡\n",
    "- æ¨ç†æ—¶ï¼šä½¿ç”¨å›ºå®šçš„runningç»Ÿè®¡é‡\n",
    "\n",
    "**Dropoutï¼š**\n",
    "- è®­ç»ƒæ—¶ï¼šéšæœºä¸¢å¼ƒ+ç¼©æ”¾ï¼ˆInverted Dropoutï¼‰\n",
    "- æ¨ç†æ—¶ï¼šä¸ä¸¢å¼ƒï¼Œç›´æ¥ä½¿ç”¨\n",
    "\n",
    "**Layer Normalizationï¼š**\n",
    "- è®­ç»ƒå’Œæ¨ç†æ—¶è¡Œä¸ºç›¸åŒ\n",
    "\n",
    "### å®è·µå»ºè®®\n",
    "\n",
    "```python\n",
    "# CNNæ¶æ„\n",
    "Conv â†’ BN â†’ ReLU â†’ Dropout â†’ Pool\n",
    "\n",
    "# å…¨è¿æ¥ç½‘ç»œ\n",
    "Dense â†’ BN â†’ ReLU â†’ Dropout\n",
    "\n",
    "# RNN/Transformer\n",
    "Dense â†’ Layer Norm â†’ Activation\n",
    "```\n",
    "\n",
    "### è¶…å‚æ•°é€‰æ‹©\n",
    "\n",
    "- **Dropoutæ¦‚ç‡**ï¼š0.2-0.5ï¼ˆå…¨è¿æ¥å±‚ï¼‰ï¼Œ0.1-0.3ï¼ˆå·ç§¯å±‚ï¼‰\n",
    "- **BNåŠ¨é‡**ï¼š0.9-0.99\n",
    "- **BN epsilon**ï¼š1e-5ï¼ˆé»˜è®¤ï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç»ƒä¹ é¢˜\n",
    "\n",
    "### ç»ƒä¹ 1ï¼šå®ç°å®Œæ•´çš„åå‘ä¼ æ’­\n",
    "ä¸º`ModernNeuralNetwork`ç±»å®ç°å®Œæ•´çš„åå‘ä¼ æ’­ï¼ŒåŒ…æ‹¬ï¼š\n",
    "- BNå±‚çš„æ¢¯åº¦è®¡ç®—\n",
    "- Dropoutå±‚çš„æ¢¯åº¦è®¡ç®—\n",
    "- ç«¯åˆ°ç«¯è®­ç»ƒ\n",
    "\n",
    "### ç»ƒä¹ 2ï¼šè°ƒæ•´Dropoutä½ç½®\n",
    "å®éªŒä¸åŒçš„Dropoutæ”¾ç½®ç­–ç•¥ï¼š\n",
    "1. åªåœ¨æœ€åä¸€å±‚\n",
    "2. åœ¨æ‰€æœ‰éšè—å±‚\n",
    "3. åªåœ¨å…¨è¿æ¥å±‚ï¼Œä¸åœ¨å·ç§¯å±‚\n",
    "\n",
    "å¯¹æ¯”æ•ˆæœã€‚\n",
    "\n",
    "### ç»ƒä¹ 3ï¼šBNçš„å¯è§†åŒ–\n",
    "å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­ï¼š\n",
    "1. æ¯å±‚æ¿€æ´»å€¼çš„åˆ†å¸ƒå˜åŒ–\n",
    "2. æœ‰BN vs æ— BNçš„æ¿€æ´»åˆ†å¸ƒå¯¹æ¯”\n",
    "3. æ¢¯åº¦æµåŠ¨çš„å·®å¼‚\n",
    "\n",
    "### ç»ƒä¹ 4ï¼šå®ç°Group Normalization\n",
    "Group Normalizationæ˜¯BNå’ŒLNçš„æŠ˜ä¸­ï¼š\n",
    "- å°†ç‰¹å¾åˆ†æˆè‹¥å¹²ç»„\n",
    "- åœ¨æ¯ç»„å†…è¿›è¡Œå½’ä¸€åŒ–\n",
    "- é€‚ç”¨äºå°batch sizeåœºæ™¯\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸‹ä¸€ç« é¢„å‘Š\n",
    "\n",
    "åœ¨ä¸‹ä¸€ç« ã€ŠCNNåŸºç¡€ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ï¼š\n",
    "- å·ç§¯å±‚çš„åŸç†å’Œå®ç°\n",
    "- æ± åŒ–å±‚\n",
    "- ç»å…¸CNNæ¶æ„\n",
    "- å›¾åƒåˆ†ç±»å®æˆ˜\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ æ­å–œå®Œæˆç¬¬7ç« ï¼ä½ å·²ç»æŒæ¡äº†ç°ä»£æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæŠ€æœ¯ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}