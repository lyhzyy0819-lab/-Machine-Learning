"""
============================================================================
ç»ƒä¹ 5ï¼šç¥ç»ç½‘ç»œå›å½’å®æˆ˜
============================================================================

ğŸ“š é—®é¢˜èƒŒæ™¯ï¼š
    åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå›å½’é—®é¢˜å’Œåˆ†ç±»é—®é¢˜åŒæ ·é‡è¦ã€‚
    æœ¬ç»ƒä¹ å°†å¸®åŠ©ä½ æ·±å…¥ç†è§£ç¥ç»ç½‘ç»œå›å½’çš„å„ä¸ªæ–¹é¢ã€‚

ğŸ¯ å­¦ä¹ ç›®æ ‡ï¼š
    1. å®ç° Huber Loss åŠå…¶æ¢¯åº¦
    2. å®Œæˆæˆ¿ä»·é¢„æµ‹å®Œæ•´æµç¨‹
    3. ç†è§£ç‰¹å¾é‡è¦æ€§åˆ†æ
    4. æŒæ¡è¶…å‚æ•°å¯¹å›å½’æ€§èƒ½çš„å½±å“

============================================================================
"""

# ============================================================================
# ç¬¬1éƒ¨åˆ†ï¼šå¯¼å…¥åº“å’Œç¯å¢ƒé…ç½®
# ============================================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'STHeiti']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 70)
print("ç»ƒä¹ 5ï¼šç¥ç»ç½‘ç»œå›å½’å®æˆ˜")
print("=" * 70)


# ============================================================================
# ç¬¬2éƒ¨åˆ†ï¼šç»ƒä¹ 1 - å®ç° Huber Loss
# ============================================================================
"""
ğŸ“– Huber Loss åŸç†

Huber Loss æ˜¯ MSE å’Œ MAE çš„ç»“åˆï¼š
- å½“ |error| <= delta: L = 0.5 * error^2 (åƒ MSE)
- å½“ |error| > delta:  L = delta * (|error| - 0.5 * delta) (åƒ MAE)

ä¼˜ç‚¹ï¼š
- å°è¯¯å·®æ—¶ï¼šå…‰æ»‘ã€å¯å¾®ã€æ”¶æ•›å¿«
- å¤§è¯¯å·®æ—¶ï¼šå¯¹å¼‚å¸¸å€¼é²æ£’

æ¢¯åº¦ï¼š
- å½“ |error| <= delta: dL/d_pred = -error (åƒ MSE)
- å½“ |error| > delta:  dL/d_pred = -delta * sign(error) (åƒ MAE)
"""

print("\n" + "="*70)
print("ç»ƒä¹ 1ï¼šå®ç° Huber Loss åŠå…¶æ¢¯åº¦")
print("="*70)


def huber_loss(y_true, y_pred, delta=1.0):
    """
    Huber Loss æŸå¤±å‡½æ•°

    ğŸ“ æ•°å­¦å…¬å¼ï¼š
        å½“ |y - Å·| <= Î´: L = 0.5 * (y - Å·)Â²
        å½“ |y - Å·| > Î´:  L = Î´ * (|y - Å·| - 0.5 * Î´)

    å‚æ•°:
        y_true: çœŸå®å€¼, shape (n_samples,) æˆ– (n_samples, 1)
        y_pred: é¢„æµ‹å€¼, shape (n_samples,) æˆ– (n_samples, 1)
        delta: åˆ‡æ¢é˜ˆå€¼ï¼Œæ§åˆ¶ MSE å’Œ MAE çš„åˆ‡æ¢ç‚¹
               é»˜è®¤ 1.0

    è¿”å›:
        loss: æ ‡é‡ï¼ŒHuber æŸå¤±å€¼

    ğŸ’¡ æç¤ºï¼š
        - ä½¿ç”¨ np.where() æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒçš„è®¡ç®—æ–¹å¼
        - æ³¨æ„å¤„ç† y_true å’Œ y_pred çš„å½¢çŠ¶
    """
    # ===== ä½ çš„ä»£ç  =====
    # ç¬¬1æ­¥ï¼šè®¡ç®—é¢„æµ‹è¯¯å·®
    error = y_true.flatten() - y_pred.flatten()

    # ç¬¬2æ­¥ï¼šè®¡ç®—ç»å¯¹è¯¯å·®
    abs_error = np.abs(error)

    # ç¬¬3æ­¥ï¼šæ ¹æ®è¯¯å·®å¤§å°é€‰æ‹©è®¡ç®—æ–¹å¼
    # å°è¯¯å·®ï¼ˆ|e| <= deltaï¼‰ï¼šä½¿ç”¨ MSE å½¢å¼ 0.5 * e^2
    # å¤§è¯¯å·®ï¼ˆ|e| > deltaï¼‰ï¼šä½¿ç”¨çº¿æ€§å½¢å¼ delta * (|e| - 0.5 * delta)
    quadratic = 0.5 * error ** 2
    linear = delta * (abs_error - 0.5 * delta)

    # ç¬¬4æ­¥ï¼šæ ¹æ®æ¡ä»¶é€‰æ‹©
    loss = np.where(abs_error <= delta, quadratic, linear)

    # ç¬¬5æ­¥ï¼šè¿”å›å¹³å‡æŸå¤±
    return np.mean(loss)
    # ===== ä»£ç ç»“æŸ =====


def huber_loss_gradient(y_true, y_pred, delta=1.0):
    """
    Huber Loss å¯¹é¢„æµ‹å€¼çš„æ¢¯åº¦

    ğŸ“ æ¢¯åº¦å…¬å¼ï¼š
        å½“ |y - Å·| <= Î´: âˆ‚L/âˆ‚Å· = -(y - Å·) = (Å· - y)
        å½“ |y - Å·| > Î´:  âˆ‚L/âˆ‚Å· = -Î´ * sign(y - Å·) = Î´ * sign(Å· - y)

    å‚æ•°:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        delta: åˆ‡æ¢é˜ˆå€¼

    è¿”å›:
        gradient: æ¢¯åº¦, shape ä¸ y_pred ç›¸åŒ

    ğŸ’¡ æç¤ºï¼š
        - æ¢¯åº¦è¡¨ç¤ºæŸå¤±å¯¹é¢„æµ‹å€¼çš„å¯¼æ•°
        - æ³¨æ„ sign å‡½æ•°ï¼šnp.sign()
    """
    # ===== ä½ çš„ä»£ç  =====
    # ç¬¬1æ­¥ï¼šè®¡ç®—è¯¯å·®ï¼ˆæ³¨æ„æ–¹å‘ï¼špred - trueï¼‰
    error = y_pred.flatten() - y_true.flatten()

    # ç¬¬2æ­¥ï¼šè®¡ç®—ç»å¯¹è¯¯å·®
    abs_error = np.abs(error)

    # ç¬¬3æ­¥ï¼šæ ¹æ®è¯¯å·®å¤§å°è®¡ç®—æ¢¯åº¦
    # å°è¯¯å·®ï¼šæ¢¯åº¦ = errorï¼ˆMSE çš„æ¢¯åº¦ï¼‰
    # å¤§è¯¯å·®ï¼šæ¢¯åº¦ = delta * sign(error)ï¼ˆMAE çš„æ¢¯åº¦ï¼Œè¢« delta æˆªæ–­ï¼‰
    grad = np.where(abs_error <= delta, error, delta * np.sign(error))

    # ç¬¬4æ­¥ï¼šè¿”å›å¹³å‡æ¢¯åº¦
    return grad.reshape(y_pred.shape) / len(y_true)
    # ===== ä»£ç ç»“æŸ =====


# æµ‹è¯• Huber Loss å®ç°
print("\næµ‹è¯• Huber Loss å®ç°:")
y_true_test = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
y_pred_test = np.array([1.1, 2.5, 2.5, 4.2, 10.0])  # æœ€åä¸€ä¸ªæ˜¯å¼‚å¸¸é¢„æµ‹

print(f"çœŸå®å€¼: {y_true_test}")
print(f"é¢„æµ‹å€¼: {y_pred_test}")
print(f"è¯¯å·®:   {y_true_test - y_pred_test}")

mse = np.mean((y_true_test - y_pred_test) ** 2)
mae = np.mean(np.abs(y_true_test - y_pred_test))
huber = huber_loss(y_true_test, y_pred_test, delta=1.0)

print(f"\nMSE Loss:   {mse:.4f}")
print(f"MAE Loss:   {mae:.4f}")
print(f"Huber Loss: {huber:.4f}")
print("\nâœ“ å¦‚æœ Huber ä»‹äº MSE å’Œ MAE ä¹‹é—´ï¼ˆä½†æ›´æ¥è¿‘ MAEï¼‰ï¼Œåˆ™å®ç°æ­£ç¡®ï¼")


# ============================================================================
# ç¬¬3éƒ¨åˆ†ï¼šç»ƒä¹ 2 - å®Œæ•´çš„ MLP å›å½’
# ============================================================================

print("\n" + "="*70)
print("ç»ƒä¹ 2ï¼šå®Œæ•´çš„æˆ¿ä»·é¢„æµ‹ MLP")
print("="*70)


class MLPRegressorWithHuber:
    """
    æ”¯æŒå¤šç§æŸå¤±å‡½æ•°çš„ MLP å›å½’å™¨

    ğŸ’¡ ä¸åˆ†ç±»ç½‘ç»œçš„åŒºåˆ«ï¼š
    1. è¾“å‡ºå±‚æ— æ¿€æ´»å‡½æ•°ï¼ˆçº¿æ€§è¾“å‡ºï¼‰
    2. æ”¯æŒ MSEã€MAEã€Huber ä¸‰ç§æŸå¤±
    3. ä½¿ç”¨ RÂ²ã€RMSE è¯„ä¼°
    """

    def __init__(self, layer_sizes, loss_type='mse', huber_delta=1.0):
        """
        åˆå§‹åŒ–ç½‘ç»œ

        å‚æ•°:
            layer_sizes: å„å±‚ç¥ç»å…ƒæ•°é‡åˆ—è¡¨ï¼Œå¦‚ [8, 64, 32, 1]
            loss_type: æŸå¤±å‡½æ•°ç±»å‹ 'mse', 'mae', æˆ– 'huber'
            huber_delta: Huber æŸå¤±çš„ delta å‚æ•°
        """
        self.layer_sizes = layer_sizes
        self.loss_type = loss_type
        self.huber_delta = huber_delta
        self.n_layers = len(layer_sizes)

        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        self.weights = []
        self.biases = []

        # ä½¿ç”¨ He åˆå§‹åŒ–
        for i in range(self.n_layers - 1):
            # æƒé‡ shape: (å½“å‰å±‚, ä¸‹ä¸€å±‚)
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)

        # è®°å½•è®­ç»ƒå†å²
        self.train_losses = []
        self.val_losses = []

        print(f"ç½‘ç»œç»“æ„: {' -> '.join(map(str, layer_sizes))}")
        print(f"æŸå¤±å‡½æ•°: {loss_type.upper()}")

    def relu(self, z):
        """ReLU æ¿€æ´»å‡½æ•°"""
        return np.maximum(0, z)

    def relu_derivative(self, z):
        """ReLU å¯¼æ•°"""
        return (z > 0).astype(float)

    def forward(self, X):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            X: è¾“å…¥, shape (n_samples, n_features)

        è¿”å›:
            output: é¢„æµ‹å€¼, shape (n_samples, 1)
        """
        self.activations = [X]  # ä¿å­˜å„å±‚æ¿€æ´»å€¼
        self.z_values = []       # ä¿å­˜å„å±‚çº¿æ€§è¾“å‡º

        current = X

        # éšè—å±‚ï¼šä½¿ç”¨ ReLU
        for i in range(self.n_layers - 2):
            z = current @ self.weights[i] + self.biases[i]
            self.z_values.append(z)
            a = self.relu(z)
            self.activations.append(a)
            current = a

        # è¾“å‡ºå±‚ï¼šæ— æ¿€æ´»å‡½æ•°ï¼ˆçº¿æ€§ï¼‰
        z_out = current @ self.weights[-1] + self.biases[-1]
        self.z_values.append(z_out)
        self.activations.append(z_out)  # è¾“å‡ºå±‚çš„æ¿€æ´»å°±æ˜¯çº¿æ€§è¾“å‡º

        return z_out

    def compute_loss(self, y_true, y_pred):
        """
        è®¡ç®—æŸå¤±

        æ ¹æ® self.loss_type é€‰æ‹©ä¸åŒçš„æŸå¤±å‡½æ•°
        """
        y_true = y_true.flatten()
        y_pred = y_pred.flatten()

        if self.loss_type == 'mse':
            return np.mean((y_true - y_pred) ** 2)
        elif self.loss_type == 'mae':
            return np.mean(np.abs(y_true - y_pred))
        elif self.loss_type == 'huber':
            return huber_loss(y_true, y_pred, self.huber_delta)
        else:
            raise ValueError(f"æœªçŸ¥çš„æŸå¤±ç±»å‹: {self.loss_type}")

    def compute_output_gradient(self, y_true, y_pred):
        """
        è®¡ç®—è¾“å‡ºå±‚æ¢¯åº¦ dL/dz_out

        ä¸åŒæŸå¤±å‡½æ•°æœ‰ä¸åŒçš„æ¢¯åº¦ï¼š
        - MSE: (y_pred - y_true) / n
        - MAE: sign(y_pred - y_true) / n
        - Huber: ç»“åˆ MSE å’Œ MAE
        """
        n = len(y_true)
        y_true = y_true.flatten()
        y_pred = y_pred.flatten()

        if self.loss_type == 'mse':
            # MSE æ¢¯åº¦: 2 * (pred - true) / nï¼Œç®€åŒ–ä¸º (pred - true) / n
            grad = (y_pred - y_true) / n
        elif self.loss_type == 'mae':
            # MAE æ¢¯åº¦: sign(pred - true) / n
            grad = np.sign(y_pred - y_true) / n
        elif self.loss_type == 'huber':
            # Huber æ¢¯åº¦
            grad = huber_loss_gradient(y_true, y_pred, self.huber_delta).flatten()

        return grad.reshape(-1, 1)

    def backward(self, y_true, learning_rate=0.01):
        """
        åå‘ä¼ æ’­ + å‚æ•°æ›´æ–°
        """
        n_samples = y_true.shape[0]

        # è¾“å‡ºå±‚æ¢¯åº¦
        dz = self.compute_output_gradient(y_true, self.activations[-1])

        # ä»åå‘å‰è®¡ç®—æ¢¯åº¦
        for i in range(self.n_layers - 2, -1, -1):
            # è®¡ç®—æƒé‡å’Œåç½®æ¢¯åº¦
            dW = self.activations[i].T @ dz
            db = np.sum(dz, axis=0, keepdims=True)

            # è®¡ç®—ä¼ é€’ç»™å‰ä¸€å±‚çš„æ¢¯åº¦
            if i > 0:
                da = dz @ self.weights[i].T
                dz = da * self.relu_derivative(self.z_values[i-1])

            # æ›´æ–°å‚æ•°
            self.weights[i] -= learning_rate * dW
            self.biases[i] -= learning_rate * db

    def fit(self, X_train, y_train, X_val=None, y_val=None,
            epochs=100, learning_rate=0.01, batch_size=32, verbose=True):
        """
        è®­ç»ƒæ¨¡å‹
        """
        n_samples = X_train.shape[0]
        y_train = y_train.reshape(-1, 1)
        if y_val is not None:
            y_val = y_val.reshape(-1, 1)

        self.train_losses = []
        self.val_losses = []

        for epoch in range(epochs):
            # æ‰“ä¹±æ•°æ®
            indices = np.random.permutation(n_samples)
            X_shuffled = X_train[indices]
            y_shuffled = y_train[indices]

            # å°æ‰¹é‡è®­ç»ƒ
            for i in range(0, n_samples, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]

                # å‰å‘ + åå‘
                self.forward(X_batch)
                self.backward(y_batch, learning_rate)

            # è®°å½•æŸå¤±
            train_pred = self.forward(X_train)
            train_loss = self.compute_loss(y_train, train_pred)
            self.train_losses.append(train_loss)

            if X_val is not None:
                val_pred = self.forward(X_val)
                val_loss = self.compute_loss(y_val, val_pred)
                self.val_losses.append(val_loss)

            if verbose and (epoch + 1) % 50 == 0:
                msg = f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}"
                if X_val is not None:
                    msg += f" - Val Loss: {val_loss:.4f}"
                print(msg)

    def predict(self, X):
        """é¢„æµ‹"""
        return self.forward(X)

    def score(self, X, y_true):
        """è®¡ç®— RÂ² åˆ†æ•°"""
        y_pred = self.predict(X)
        ss_res = np.sum((y_true.flatten() - y_pred.flatten()) ** 2)
        ss_tot = np.sum((y_true.flatten() - np.mean(y_true)) ** 2)
        return 1 - (ss_res / ss_tot)


# åŠ è½½æ•°æ®
print("\nåŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†...")
housing = fetch_california_housing()
X, y = housing.data, housing.target

# æ•°æ®åˆ’åˆ†
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

# æ ‡å‡†åŒ–
scaler_X = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()

print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
print(f"éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")


# ============================================================================
# ç¬¬4éƒ¨åˆ†ï¼šå¯¹æ¯”ä¸åŒæŸå¤±å‡½æ•°
# ============================================================================

print("\n" + "="*70)
print("å¯¹æ¯”ä¸‰ç§æŸå¤±å‡½æ•°çš„æ•ˆæœ")
print("="*70)

results = {}

for loss_type in ['mse', 'mae', 'huber']:
    print(f"\n--- è®­ç»ƒ {loss_type.upper()} æ¨¡å‹ ---")

    model = MLPRegressorWithHuber(
        layer_sizes=[8, 64, 32, 1],
        loss_type=loss_type,
        huber_delta=1.0
    )

    model.fit(
        X_train_scaled, y_train_scaled,
        X_val=X_val_scaled, y_val=y_val_scaled,
        epochs=200,
        learning_rate=0.01,
        batch_size=64,
        verbose=True
    )

    # è¯„ä¼°
    y_pred_scaled = model.predict(X_test_scaled)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)

    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    results[loss_type] = {
        'model': model,
        'r2': r2,
        'rmse': rmse,
        'mae': mae,
        'train_losses': model.train_losses,
        'val_losses': model.val_losses
    }

    print(f"RÂ² = {r2:.4f}, RMSE = {rmse:.4f}, MAE = {mae:.4f}")


# ============================================================================
# ç¬¬5éƒ¨åˆ†ï¼šå¯è§†åŒ–ç»“æœ
# ============================================================================

print("\n" + "="*70)
print("å¯è§†åŒ–ç»“æœ")
print("="*70)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ----- å›¾1ï¼šå­¦ä¹ æ›²çº¿å¯¹æ¯” -----
ax1 = axes[0, 0]
colors = {'mse': 'blue', 'mae': 'red', 'huber': 'green'}
for loss_type, data in results.items():
    epochs = range(1, len(data['train_losses']) + 1)
    ax1.plot(epochs, data['val_losses'], color=colors[loss_type],
             linewidth=2, label=f'{loss_type.upper()} éªŒè¯æŸå¤±')

ax1.set_xlabel('Epoch', fontsize=11)
ax1.set_ylabel('Loss', fontsize=11)
ax1.set_title('éªŒè¯æŸå¤±å­¦ä¹ æ›²çº¿å¯¹æ¯”', fontsize=12, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# ----- å›¾2ï¼šè¯„ä¼°æŒ‡æ ‡å¯¹æ¯” -----
ax2 = axes[0, 1]
metrics = ['RÂ²', 'RMSE', 'MAE']
x_pos = np.arange(len(metrics))
width = 0.25

for i, (loss_type, data) in enumerate(results.items()):
    values = [data['r2'], data['rmse'], data['mae']]
    ax2.bar(x_pos + i*width, values, width, label=loss_type.upper(), color=colors[loss_type])

ax2.set_xticks(x_pos + width)
ax2.set_xticklabels(metrics)
ax2.set_ylabel('æ•°å€¼', fontsize=11)
ax2.set_title('è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”', fontsize=12, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

# ----- å›¾3ï¼šé¢„æµ‹å€¼ vs çœŸå®å€¼ï¼ˆæœ€ä½³æ¨¡å‹ï¼‰-----
ax3 = axes[1, 0]
best_loss = max(results.keys(), key=lambda k: results[k]['r2'])
best_model = results[best_loss]['model']
y_pred_scaled = best_model.predict(X_test_scaled)
y_pred_best = scaler_y.inverse_transform(y_pred_scaled)

ax3.scatter(y_test, y_pred_best, alpha=0.3, s=10, c='blue')
min_val, max_val = min(y_test.min(), y_pred_best.min()), max(y_test.max(), y_pred_best.max())
ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹')
ax3.set_xlabel('çœŸå®æˆ¿ä»·', fontsize=11)
ax3.set_ylabel('é¢„æµ‹æˆ¿ä»·', fontsize=11)
ax3.set_title(f'æœ€ä½³æ¨¡å‹ ({best_loss.upper()}) é¢„æµ‹ vs çœŸå®', fontsize=12, fontweight='bold')
ax3.legend()
ax3.grid(True, alpha=0.3)

# ----- å›¾4ï¼šæ®‹å·®åˆ†å¸ƒ -----
ax4 = axes[1, 1]
residuals = y_test - y_pred_best.flatten()
ax4.hist(residuals, bins=50, color='steelblue', edgecolor='white', alpha=0.7)
ax4.axvline(x=0, color='red', linestyle='--', linewidth=2, label='é›¶è¯¯å·®')
ax4.axvline(x=np.mean(residuals), color='green', linestyle='-', linewidth=2,
            label=f'å‡å€¼={np.mean(residuals):.3f}')
ax4.set_xlabel('æ®‹å·® (çœŸå® - é¢„æµ‹)', fontsize=11)
ax4.set_ylabel('é¢‘æ•°', fontsize=11)
ax4.set_title('æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('/Users/lyh/Desktop/ Machine Learning/neural_networks/regression_results.png',
            dpi=150, bbox_inches='tight')
plt.show()


# ============================================================================
# ç¬¬6éƒ¨åˆ†ï¼šç»ƒä¹ 3 - ç‰¹å¾é‡è¦æ€§åˆ†æ
# ============================================================================

print("\n" + "="*70)
print("ç»ƒä¹ 3ï¼šç‰¹å¾é‡è¦æ€§åˆ†æ")
print("="*70)

"""
ğŸ“– ç‰¹å¾é‡è¦æ€§åˆ†ææ–¹æ³•

æ–¹æ³•1ï¼šåŸºäºæƒé‡çš„åˆ†æ
    - æŸ¥çœ‹ç¬¬ä¸€å±‚æƒé‡çš„ç»å¯¹å€¼å¤§å°
    - æƒé‡è¶Šå¤§ï¼Œç‰¹å¾å½±å“è¶Šå¤§

æ–¹æ³•2ï¼šç½®æ¢é‡è¦æ€§ï¼ˆPermutation Importanceï¼‰
    - æ‰“ä¹±æŸä¸ªç‰¹å¾çš„å€¼
    - è§‚å¯Ÿæ¨¡å‹æ€§èƒ½ä¸‹é™ç¨‹åº¦
    - ä¸‹é™è¶Šå¤šï¼Œç‰¹å¾è¶Šé‡è¦
"""

# ä½¿ç”¨ç½®æ¢é‡è¦æ€§
print("\nä½¿ç”¨ç½®æ¢é‡è¦æ€§åˆ†æç‰¹å¾...")

feature_names = housing.feature_names
importance_scores = []

# åŸºå‡†æ€§èƒ½
baseline_pred = best_model.predict(X_test_scaled)
baseline_r2 = r2_score(y_test, scaler_y.inverse_transform(baseline_pred))

print(f"åŸºå‡† RÂ²: {baseline_r2:.4f}\n")

for i, name in enumerate(feature_names):
    # å¤åˆ¶æµ‹è¯•æ•°æ®
    X_permuted = X_test_scaled.copy()

    # æ‰“ä¹±ç¬¬ i ä¸ªç‰¹å¾
    np.random.shuffle(X_permuted[:, i])

    # è®¡ç®—æ‰“ä¹±åçš„æ€§èƒ½
    permuted_pred = best_model.predict(X_permuted)
    permuted_r2 = r2_score(y_test, scaler_y.inverse_transform(permuted_pred))

    # é‡è¦æ€§ = æ€§èƒ½ä¸‹é™ç¨‹åº¦
    importance = baseline_r2 - permuted_r2
    importance_scores.append(importance)

    print(f"  {name:<15}: æ‰“ä¹±å RÂ² = {permuted_r2:.4f}, ä¸‹é™ = {importance:.4f}")

# å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
plt.figure(figsize=(10, 6))
sorted_idx = np.argsort(importance_scores)
plt.barh(range(len(feature_names)), np.array(importance_scores)[sorted_idx], color='steelblue')
plt.yticks(range(len(feature_names)), np.array(feature_names)[sorted_idx])
plt.xlabel('é‡è¦æ€§ (RÂ² ä¸‹é™)', fontsize=11)
plt.title('ç‰¹å¾é‡è¦æ€§åˆ†æ (ç½®æ¢é‡è¦æ€§)', fontsize=12, fontweight='bold')
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('/Users/lyh/Desktop/ Machine Learning/neural_networks/feature_importance.png',
            dpi=150, bbox_inches='tight')
plt.show()

print("\nğŸ’¡ ç‰¹å¾é‡è¦æ€§è§£è¯»:")
most_important = feature_names[np.argmax(importance_scores)]
print(f"   æœ€é‡è¦çš„ç‰¹å¾: {most_important}")
print("   è¿™é€šå¸¸ä¸æˆ¿ä»·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼")


# ============================================================================
# ç¬¬7éƒ¨åˆ†ï¼šæ€»ç»“
# ============================================================================

print("\n" + "="*70)
print("ç»ƒä¹ æ€»ç»“")
print("="*70)

print("""
âœ… å®Œæˆçš„å†…å®¹:

1. Huber Loss å®ç°
   - ç†è§£äº† Huber Loss çš„æ•°å­¦åŸç†
   - å®ç°äº†æŸå¤±å‡½æ•°å’Œæ¢¯åº¦è®¡ç®—

2. å®Œæ•´çš„ MLP å›å½’
   - æ”¯æŒ MSEã€MAEã€Huber ä¸‰ç§æŸå¤±
   - ä½¿ç”¨åŠ å·æˆ¿ä»·æ•°æ®é›†è®­ç»ƒ

3. æŸå¤±å‡½æ•°å¯¹æ¯”
   - MSE: æ”¶æ•›å¿«ï¼Œä½†å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
   - MAE: å¯¹å¼‚å¸¸å€¼é²æ£’ï¼Œä½†å¯èƒ½æ”¶æ•›ä¸ç¨³å®š
   - Huber: ç»“åˆä¸¤è€…ä¼˜ç‚¹

4. ç‰¹å¾é‡è¦æ€§åˆ†æ
   - ä½¿ç”¨ç½®æ¢é‡è¦æ€§æ–¹æ³•
   - è¯†åˆ«äº†æœ€é‡è¦çš„ç‰¹å¾

ğŸ“Š æœ€ç»ˆç»“æœ:
""")

for loss_type, data in results.items():
    print(f"   {loss_type.upper()}: RÂ² = {data['r2']:.4f}, RMSE = {data['rmse']:.4f}")

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_loss.upper()} (RÂ² = {results[best_loss]['r2']:.4f})")
print("\n" + "="*70)
