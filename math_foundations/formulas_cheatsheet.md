# æœºå™¨å­¦ä¹ æ ¸å¿ƒå…¬å¼é€ŸæŸ¥è¡¨

> æ¯ä¸ªå…¬å¼éƒ½é…æœ‰ï¼šé€šä¿—è§£é‡Š + ä»£ç å®ç° + å®é™…ä¾‹å­

---

## ğŸ“š ç›®å½•

- [çº¿æ€§ä»£æ•°å…¬å¼](#çº¿æ€§ä»£æ•°å…¬å¼)
- [å¾®ç§¯åˆ†å…¬å¼](#å¾®ç§¯åˆ†å…¬å¼)
- [æ¦‚ç‡ç»Ÿè®¡å…¬å¼](#æ¦‚ç‡ç»Ÿè®¡å…¬å¼)
- [æœºå™¨å­¦ä¹ ç®—æ³•å…¬å¼](#æœºå™¨å­¦ä¹ ç®—æ³•å…¬å¼)

---

## çº¿æ€§ä»£æ•°å…¬å¼

### 1. å‘é‡ç‚¹ç§¯

**å…¬å¼ï¼š**
$$\vec{a} \cdot \vec{b} = \sum_{i=1}^{n} a_i b_i = a_1b_1 + a_2b_2 + ... + a_nb_n$$

**é€šä¿—è§£é‡Šï¼š**
å¯¹åº”ä½ç½®ç›¸ä¹˜å†æ±‚å’Œ

**ä»£ç ï¼š**
```python
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# æ–¹æ³•1ï¼šç›´æ¥ç”¨@
dot_product = a @ b

# æ–¹æ³•2ï¼šnp.dot
dot_product = np.dot(a, b)

# æ–¹æ³•3ï¼šæ‰‹åŠ¨è®¡ç®—
dot_product = np.sum(a * b)

print(dot_product)  # 32
```

**MLåº”ç”¨ï¼š**
- è®¡ç®—ç›¸ä¼¼åº¦
- ç¥ç»ç½‘ç»œä¸­çš„åŠ æƒæ±‚å’Œ

---

### 2. çŸ©é˜µä¹˜æ³•

**å…¬å¼ï¼š**
$$C_{ij} = \sum_{k=1}^{m} A_{ik} B_{kj}$$

**é€šä¿—è§£é‡Šï¼š**
Cçš„ç¬¬iè¡Œç¬¬jåˆ— = Açš„ç¬¬iè¡Œ ç‚¹ä¹˜ Bçš„ç¬¬jåˆ—

**ä»£ç ï¼š**
```python
A = np.array([[1, 2],
              [3, 4]])  # 2x2

B = np.array([[5, 6],
              [7, 8]])  # 2x2

# çŸ©é˜µä¹˜æ³•
C = A @ B  # æˆ– np.matmul(A, B) æˆ– np.dot(A, B)

print(C)
# [[19 22]
#  [43 50]]

# éªŒè¯C[0,0] = 1*5 + 2*7 = 19 âœ“
```

**MLåº”ç”¨ï¼š**
- ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­
- çº¿æ€§å˜æ¢

---

### 3. å‘é‡çš„æ¨¡ï¼ˆé•¿åº¦ï¼‰

**å…¬å¼ï¼š**
$$\|\vec{v}\| = \sqrt{v_1^2 + v_2^2 + ... + v_n^2}$$

**é€šä¿—è§£é‡Šï¼š**
å‘é‡çš„é•¿åº¦

**ä»£ç ï¼š**
```python
v = np.array([3, 4])

# æ–¹æ³•1ï¼šnp.linalg.norm
length = np.linalg.norm(v)

# æ–¹æ³•2ï¼šæ‰‹åŠ¨è®¡ç®—
length = np.sqrt(np.sum(v**2))

print(length)  # 5.0
```

**MLåº”ç”¨ï¼š**
- å½’ä¸€åŒ–
- è·ç¦»è®¡ç®—
- æ­£åˆ™åŒ–

---

### 4. ä½™å¼¦ç›¸ä¼¼åº¦

**å…¬å¼ï¼š**
$$\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \|\vec{b}\|}$$

**é€šä¿—è§£é‡Šï¼š**
ä¸¤ä¸ªå‘é‡å¤¹è§’çš„ä½™å¼¦å€¼ï¼Œè¡¡é‡æ–¹å‘çš„ç›¸ä¼¼åº¦

**ä»£ç ï¼š**
```python
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

sim = cosine_similarity(a, b)
print(f'ç›¸ä¼¼åº¦: {sim:.4f}')  # 0.9746
```

**MLåº”ç”¨ï¼š**
- æ¨èç³»ç»Ÿ
- æ–‡æœ¬ç›¸ä¼¼åº¦
- å›¾åƒæ£€ç´¢

---

### 5. çŸ©é˜µçš„è¿¹

**å…¬å¼ï¼š**
$$\text{tr}(A) = \sum_{i=1}^{n} A_{ii}$$

**é€šä¿—è§£é‡Šï¼š**
å¯¹è§’çº¿å…ƒç´ ä¹‹å’Œ

**ä»£ç ï¼š**
```python
A = np.array([[1, 2],
              [3, 4]])

trace = np.trace(A)  # æˆ– A.diagonal().sum()

print(trace)  # 5 (1 + 4)
```

---

## å¾®ç§¯åˆ†å…¬å¼

### 1. å¯¼æ•°å®šä¹‰

**å…¬å¼ï¼š**
$$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$$

**é€šä¿—è§£é‡Šï¼š**
å‡½æ•°åœ¨æŸç‚¹çš„æ–œç‡

**ä»£ç ï¼š**
```python
def numerical_derivative(f, x, h=1e-5):
    """æ•°å€¼è®¡ç®—å¯¼æ•°"""
    return (f(x + h) - f(x)) / h

# ä¾‹å­ï¼šf(x) = x^2ï¼Œå¯¼æ•°æ˜¯2x
f = lambda x: x**2

x = 3
derivative = numerical_derivative(f, x)
print(f'f(x)=xÂ²åœ¨x={x}å¤„çš„å¯¼æ•°: {derivative:.4f}')  # 6.0
print(f'è§£æè§£: {2*x}')  # 6
```

---

### 2. å¸¸è§å‡½æ•°çš„å¯¼æ•°

| å‡½æ•° | å¯¼æ•° | ä»£ç éªŒè¯ |
|------|------|----------|
| $f(x) = c$ | $f'(x) = 0$ | `derivative(lambda x: 5, 3) â‰ˆ 0` |
| $f(x) = x$ | $f'(x) = 1$ | `derivative(lambda x: x, 3) â‰ˆ 1` |
| $f(x) = x^n$ | $f'(x) = nx^{n-1}$ | `derivative(lambda x: x**3, 2) â‰ˆ 12` |
| $f(x) = e^x$ | $f'(x) = e^x$ | `derivative(np.exp, 1) â‰ˆ e` |
| $f(x) = \ln(x)$ | $f'(x) = 1/x$ | `derivative(np.log, 2) â‰ˆ 0.5` |
| $f(x) = \sin(x)$ | $f'(x) = \cos(x)$ | `derivative(np.sin, 0) â‰ˆ 1` |

**ä»£ç ç¤ºä¾‹ï¼š**
```python
# éªŒè¯xÂ³çš„å¯¼æ•°æ˜¯3xÂ²
f = lambda x: x**3
x = 2

numerical = numerical_derivative(f, x)
analytical = 3 * x**2

print(f'æ•°å€¼å¯¼æ•°: {numerical:.4f}')  # 12.0
print(f'è§£æå¯¼æ•°: {analytical}')     # 12
```

---

### 3. æ¢¯åº¦

**å…¬å¼ï¼š**
$$\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}$$

**é€šä¿—è§£é‡Šï¼š**
å¤šå˜é‡å‡½æ•°åœ¨å„ä¸ªæ–¹å‘ä¸Šçš„å¯¼æ•°ç»„æˆçš„å‘é‡

**ä»£ç ï¼š**
```python
def numerical_gradient(f, x, h=1e-5):
    """
    è®¡ç®—å¤šå˜é‡å‡½æ•°çš„æ•°å€¼æ¢¯åº¦
    """
    grad = np.zeros_like(x, dtype=float)

    for i in range(len(x)):
        x_plus_h = x.copy()
        x_plus_h[i] += h

        grad[i] = (f(x_plus_h) - f(x)) / h
    print(x_plus_h)

    return grad

# ä¾‹å­ï¼šf(x,y) = xÂ² + yÂ²
# æ¢¯åº¦ = [2x, 2y]
f = lambda x: x[0]**2 + x[1]**2

point = np.array([3.0, 4.0])
grad = numerical_gradient(f, point)

print(f'æ•°å€¼æ¢¯åº¦: {grad}')      # [6.0, 8.0]
print(f'è§£ææ¢¯åº¦: [6.0, 8.0]')  # [2*3, 2*4]
```

**MLåº”ç”¨ï¼š**
- æ¢¯åº¦ä¸‹é™ä¼˜åŒ–
- åå‘ä¼ æ’­

---

### 4. é“¾å¼æ³•åˆ™

**å…¬å¼ï¼š**
$$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$$

**é€šä¿—è§£é‡Šï¼š**
å¤åˆå‡½æ•°çš„å¯¼æ•° = å¤–å±‚å¯¼æ•° Ã— å†…å±‚å¯¼æ•°

**ä»£ç ï¼š**
```python
# ä¾‹å­ï¼šy = (xÂ² + 1)Â³
# ä»¤u = xÂ² + 1ï¼Œåˆ™y = uÂ³
# dy/dx = dy/du * du/dx = 3uÂ² * 2x

def f(x):
    """y = (xÂ² + 1)Â³"""
    return (x**2 + 1)**3

def df(x):
    """å¯¼æ•°ï¼ˆé“¾å¼æ³•åˆ™ï¼‰"""
    u = x**2 + 1
    return 3 * u**2 * 2 * x

x = 2
numerical = numerical_derivative(f, x)
analytical = df(x)

print(f'æ•°å€¼å¯¼æ•°: {numerical:.4f}')
print(f'è§£æå¯¼æ•°: {analytical}')
```

**MLåº”ç”¨ï¼š**
- åå‘ä¼ æ’­çš„æ ¸å¿ƒ
- è®¡ç®—å¤æ‚ç½‘ç»œçš„æ¢¯åº¦

---

### 5. æ¢¯åº¦ä¸‹é™æ›´æ–°è§„åˆ™

**å…¬å¼ï¼š**
$$\theta_{new} = \theta_{old} - \alpha \nabla L(\theta)$$

**é€šä¿—è§£é‡Šï¼š**
æ–°å‚æ•° = æ—§å‚æ•° - å­¦ä¹ ç‡ Ã— æ¢¯åº¦

**ä»£ç ï¼š**
```python
def gradient_descent(f, grad_f, theta_init, learning_rate=0.1, n_iter=100):
    """
    æ¢¯åº¦ä¸‹é™ä¼˜åŒ–
    """
    theta = theta_init.copy()
    history = [theta.copy()]

    for i in range(n_iter):
        grad = grad_f(theta)
        theta = theta - learning_rate * grad  # æ›´æ–°è§„åˆ™
        history.append(theta.copy())

    return theta, np.array(history)

# ä¾‹å­ï¼šæœ€å°åŒ–f(x,y) = (x-1)Â² + (y-2)Â²
f = lambda x: (x[0]-1)**2 + (x[1]-2)**2
grad_f = lambda x: np.array([2*(x[0]-1), 2*(x[1]-2)])

theta_init = np.array([0.0, 0.0])
theta_final, history = gradient_descent(f, grad_f, theta_init, 0.1, 50)

print(f'åˆå§‹ç‚¹: {theta_init}')
print(f'æœ€ç»ˆç‚¹: {theta_final}')
print(f'çœŸå®æœ€å°å€¼: [1.0, 2.0]')
```

---

## æ¦‚ç‡ç»Ÿè®¡å…¬å¼

### 1. æœŸæœ›ï¼ˆå‡å€¼ï¼‰

**å…¬å¼ï¼š**

ç¦»æ•£ï¼š$E[X] = \sum_{i} x_i p(x_i)$

è¿ç»­ï¼š$E[X] = \int x p(x) dx$

**é€šä¿—è§£é‡Šï¼š**
åŠ æƒå¹³å‡å€¼

**ä»£ç ï¼š**
```python
# ç¦»æ•£æƒ…å†µ
values = np.array([1, 2, 3, 4, 5])
probabilities = np.array([0.1, 0.2, 0.3, 0.25, 0.15])

expectation = np.sum(values * probabilities)
print(f'æœŸæœ›: {expectation}')  # 2.95

# æˆ–è€…ä»æ ·æœ¬ä¼°è®¡
samples = np.random.choice(values, size=10000, p=probabilities)
estimated_mean = np.mean(samples)
print(f'æ ·æœ¬å‡å€¼: {estimated_mean:.2f}')
```

---

### 2. æ–¹å·®

**å…¬å¼ï¼š**
$$\text{Var}(X) = E[(X - \mu)^2] = E[X^2] - (E[X])^2$$

**é€šä¿—è§£é‡Šï¼š**
æ•°æ®çš„ç¦»æ•£ç¨‹åº¦

**ä»£ç ï¼š**
```python
data = np.array([1, 2, 3, 4, 5])

# æ–¹æ³•1ï¼šä½¿ç”¨å…¬å¼
mean = np.mean(data)
variance = np.mean((data - mean)**2)

# æ–¹æ³•2ï¼šNumPyå‡½æ•°
variance = np.var(data)

# æ ‡å‡†å·®
std = np.std(data)

print(f'å‡å€¼: {mean}')
print(f'æ–¹å·®: {variance}')
print(f'æ ‡å‡†å·®: {std}')
```

---

### 3. åæ–¹å·®

**å…¬å¼ï¼š**
$$\text{Cov}(X,Y) = E[(X-E[X])(Y-E[Y])]$$

**é€šä¿—è§£é‡Šï¼š**
ä¸¤ä¸ªå˜é‡ä¸€èµ·å˜åŒ–çš„ç¨‹åº¦

**ä»£ç ï¼š**
```python
X = np.array([1, 2, 3, 4, 5])
Y = np.array([2, 4, 6, 8, 10])

# æ–¹æ³•1ï¼šæ‰‹åŠ¨è®¡ç®—
cov = np.mean((X - np.mean(X)) * (Y - np.mean(Y)))

# æ–¹æ³•2ï¼šNumPyå‡½æ•°
cov_matrix = np.cov(X, Y)
cov = cov_matrix[0, 1]

print(f'åæ–¹å·®: {cov}')
```

---

### 4. æ­£æ€åˆ†å¸ƒ

**å…¬å¼ï¼š**
$$p(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

**é€šä¿—è§£é‡Šï¼š**
é’Ÿå½¢æ›²çº¿åˆ†å¸ƒ

**ä»£ç ï¼š**
```python
from scipy import stats

# åˆ›å»ºæ­£æ€åˆ†å¸ƒå¯¹è±¡
mu = 0
sigma = 1
normal = stats.norm(mu, sigma)

# æ¦‚ç‡å¯†åº¦
x = 0
pdf = normal.pdf(x)
print(f'åœ¨x={x}çš„æ¦‚ç‡å¯†åº¦: {pdf:.4f}')

# ç”Ÿæˆéšæœºæ ·æœ¬
samples = normal.rvs(size=1000)

# æˆ–ç”¨NumPy
samples = np.random.normal(mu, sigma, 1000)

# éªŒè¯
print(f'æ ·æœ¬å‡å€¼: {np.mean(samples):.2f}')
print(f'æ ·æœ¬æ ‡å‡†å·®: {np.std(samples):.2f}')
```

---

### 5. è´å¶æ–¯å®šç†

**å…¬å¼ï¼š**
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

**é€šä¿—è§£é‡Šï¼š**
å·²çŸ¥Bå‘ç”Ÿï¼Œæ›´æ–°Açš„æ¦‚ç‡

**ä»£ç ç¤ºä¾‹ï¼š**
```python
# ä¾‹å­ï¼šç–¾ç—…æ£€æµ‹
# P(ç—…) = 0.01ï¼ˆæ‚£ç—…ç‡ï¼‰
# P(é˜³æ€§|ç—…) = 0.95ï¼ˆçœŸé˜³æ€§ç‡ï¼‰
# P(é˜³æ€§|å¥åº·) = 0.05ï¼ˆå‡é˜³æ€§ç‡ï¼‰
# æ±‚ï¼šæ£€æµ‹é˜³æ€§æ—¶ï¼ŒçœŸçš„æ‚£ç—…çš„æ¦‚ç‡ï¼Ÿ

P_disease = 0.01
P_positive_given_disease = 0.95
P_positive_given_healthy = 0.05
P_healthy = 1 - P_disease

# P(é˜³æ€§)
P_positive = (P_positive_given_disease * P_disease +
              P_positive_given_healthy * P_healthy)

# è´å¶æ–¯å®šç†
P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive

print(f'æ£€æµ‹é˜³æ€§æ—¶çœŸæ‚£ç—…çš„æ¦‚ç‡: {P_disease_given_positive:.2%}')
# åªæœ‰16%ï¼è¯´æ˜å‡é˜³æ€§å¾ˆå¸¸è§
```

---

## æœºå™¨å­¦ä¹ ç®—æ³•å…¬å¼

### 1. çº¿æ€§å›å½’

**å…¬å¼ï¼š**
$$\hat{y} = w^T x + b$$

**æŸå¤±å‡½æ•°ï¼ˆMSEï¼‰ï¼š**
$$L = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**æ­£è§„æ–¹ç¨‹ï¼š**
$$w = (X^TX)^{-1}X^Ty$$

**ä»£ç ï¼š**
```python
# ç”Ÿæˆæ•°æ®
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# æ–¹æ³•1ï¼šæ­£è§„æ–¹ç¨‹
X_b = np.c_[np.ones((100, 1)), X]  # æ·»åŠ åç½®é¡¹
theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

print(f'å‚æ•° (æ­£è§„æ–¹ç¨‹): {theta.ravel()}')

# æ–¹æ³•2ï¼šæ¢¯åº¦ä¸‹é™
theta = np.random.randn(2, 1)
learning_rate = 0.1
n_iterations = 1000

for iteration in range(n_iterations):
    gradients = 2/100 * X_b.T @ (X_b @ theta - y)
    theta = theta - learning_rate * gradients

print(f'å‚æ•° (æ¢¯åº¦ä¸‹é™): {theta.ravel()}')

# æ–¹æ³•3ï¼šsklearn
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
print(f'å‚æ•° (sklearn): [{model.intercept_[0]:.4f}, {model.coef_[0][0]:.4f}]')
```

---

### 2. é€»è¾‘å›å½’

**Sigmoidå‡½æ•°ï¼š**
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**é¢„æµ‹æ¦‚ç‡ï¼š**
$$P(y=1|x) = \sigma(w^T x + b)$$

**äº¤å‰ç†µæŸå¤±ï¼š**
$$L = -\frac{1}{n}\sum_{i=1}^{n}[y_i\log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

**ä»£ç ï¼š**
```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression_predict(X, theta):
    return sigmoid(X @ theta)

# Sigmoidç¤ºä¾‹
z = np.linspace(-10, 10, 100)
plt.plot(z, sigmoid(z))
plt.title('Sigmoidå‡½æ•°')
plt.xlabel('z')
plt.ylabel('Ïƒ(z)')
plt.grid(True)
```

---

### 3. Softmaxï¼ˆå¤šåˆ†ç±»ï¼‰

**å…¬å¼ï¼š**
$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

**é€šä¿—è§£é‡Šï¼š**
å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ

**ä»£ç ï¼š**
```python
def softmax(z):
    # æ•°å€¼ç¨³å®šç‰ˆæœ¬
    exp_z = np.exp(z - np.max(z))
    return exp_z / np.sum(exp_z)

# ä¾‹å­
logits = np.array([2.0, 1.0, 0.1])
probs = softmax(logits)

print(f'Logits: {logits}')
print(f'æ¦‚ç‡: {probs}')
print(f'æ¦‚ç‡å’Œ: {np.sum(probs):.4f}')  # åº”è¯¥ä¸º1
```

---

### 4. æ­£åˆ™åŒ–

**L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰ï¼š**
$$L = \text{MSE} + \lambda \sum_{j=1}^{n}|w_j|$$

**L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰ï¼š**
$$L = \text{MSE} + \lambda \sum_{j=1}^{n}w_j^2$$

**ä»£ç ï¼š**
```python
from sklearn.linear_model import Ridge, Lasso

# Ridgeå›å½’
ridge = Ridge(alpha=1.0)  # alphaå°±æ˜¯Î»
ridge.fit(X_train, y_train)

# Lassoå›å½’
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

print(f'Ridgeæƒé‡: {ridge.coef_}')
print(f'Lassoæƒé‡: {lasso.coef_}')
print(f'Lassoæœ‰{np.sum(lasso.coef_ == 0)}ä¸ªæƒé‡è¢«ç½®ä¸º0ï¼ˆç‰¹å¾é€‰æ‹©ï¼‰')
```

---

### 5. K-è¿‘é‚»è·ç¦»

**æ¬§æ°è·ç¦»ï¼š**
$$d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$$

**æ›¼å“ˆé¡¿è·ç¦»ï¼š**
$$d(x, y) = \sum_{i=1}^{n}|x_i - y_i|$$

**ä»£ç ï¼š**
```python
from scipy.spatial.distance import euclidean, cityblock

x = np.array([1, 2, 3])
y = np.array([4, 5, 6])

# æ¬§æ°è·ç¦»
dist_euclidean = euclidean(x, y)
# æˆ–
dist_euclidean = np.linalg.norm(x - y)

# æ›¼å“ˆé¡¿è·ç¦»
dist_manhattan = cityblock(x, y)
# æˆ–
dist_manhattan = np.sum(np.abs(x - y))

print(f'æ¬§æ°è·ç¦»: {dist_euclidean:.4f}')
print(f'æ›¼å“ˆé¡¿è·ç¦»: {dist_manhattan}')
```

---

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### 1. å…¬å¼è½¬ä»£ç çš„æ­¥éª¤
```
1. è¯†åˆ«ç¬¦å· â†’ æŸ¥math_symbols_guide.md
2. ç†è§£å…¬å¼ â†’ çœ‹æœ¬æ–‡æ¡£çš„"é€šä¿—è§£é‡Š"
3. å†™ä¼ªä»£ç  â†’ ç”¨è‡ªç„¶è¯­è¨€æè¿°æ­¥éª¤
4. å®ç°ä»£ç  â†’ ç”¨NumPyå®ç°
5. éªŒè¯ç»“æœ â†’ ç”¨ç®€å•ä¾‹å­æµ‹è¯•
```

### 2. è°ƒè¯•å…¬å¼å®ç°
```python
# æŠ€å·§1ï¼šæ‰“å°ä¸­é—´ç»“æœ
def my_function(x):
    step1 = x**2
    print(f'Step 1: {step1}')
    step2 = step1 + 1
    print(f'Step 2: {step2}')
    return step2

# æŠ€å·§2ï¼šç”¨ç®€å•ä¾‹å­éªŒè¯
# ç”¨æ‰‹ç®—èƒ½éªŒè¯çš„ç®€å•æ•°å­—
x = 2  # è€Œä¸æ˜¯å¤æ‚çš„æµ®ç‚¹æ•°

# æŠ€å·§3ï¼šå¯¹æ¯”åº“å‡½æ•°
# ä½ çš„å®ç° vs NumPy/SciPy/sklearn
```

### 3. å¸¸è§é”™è¯¯
```python
# âŒ é”™è¯¯ï¼šå¿˜è®°è½´
np.sum(X**2)  # æ‰€æœ‰å…ƒç´ æ±‚å’Œ

# âœ… æ­£ç¡®ï¼šæŒ‡å®šè½´
np.sum(X**2, axis=1)  # æŒ‰è¡Œæ±‚å’Œ

# âŒ é”™è¯¯ï¼šç»´åº¦ä¸åŒ¹é…
a = np.array([1,2,3])      # (3,)
b = np.array([[1],[2],[3]]) # (3,1)
a @ b  # é”™è¯¯ï¼

# âœ… æ­£ç¡®ï¼šè°ƒæ•´å½¢çŠ¶
a.reshape(-1, 1) @ b.T  # æˆ– a[:, None] @ b.T
```

---

## ğŸ”— æ‰©å±•é˜…è¯»

- **Matrix Cookbook**ï¼šçŸ©é˜µè¿ç®—å…¬å¼å¤§å…¨
- **Andrew Ngçš„MLè¯¾ç¨‹**ï¼šå…¬å¼è®²è§£æ¸…æ™°
- **Deep Learning Book**ï¼šæ·±åº¦å­¦ä¹ æ•°å­¦åŸºç¡€

---

**æŒæ¡è¿™äº›å…¬å¼ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•ä¸å†ç¥ç§˜ï¼** ğŸ¯
