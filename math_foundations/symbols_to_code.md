# æ•°å­¦ç¬¦å· â†’ Pythonä»£ç  å¯¹ç…§è¡¨

> çœ‹åˆ°å…¬å¼ç«‹åˆ»çŸ¥é“æ€ä¹ˆå†™ä»£ç ï¼

---

## ğŸ¯ å¿«é€ŸæŸ¥æ‰¾

é‡åˆ°æ•°å­¦å…¬å¼ï¼ŸæŒ‰`Ctrl+F`æœç´¢ç¬¦å·ï¼Œç«‹å³æ‰¾åˆ°å¯¹åº”ä»£ç ï¼

---

## åŸºç¡€è¿ç®—

| æ•°å­¦ç¬¦å· | å«ä¹‰ | Pythonä»£ç  | ä¾‹å­ |
|---------|------|-----------|------|
| $x + y$ | åŠ æ³• | `x + y` | `3 + 2 = 5` |
| $x - y$ | å‡æ³• | `x - y` | `5 - 2 = 3` |
| $x \times y$ | ä¹˜æ³• | `x * y` | `3 * 4 = 12` |
| $x / y$ | é™¤æ³• | `x / y` | `10 / 2 = 5.0` |
| $x^2$ | å¹³æ–¹ | `x**2` æˆ– `np.square(x)` | `3**2 = 9` |
| $x^n$ | næ¬¡æ–¹ | `x**n` æˆ– `np.power(x, n)` | `2**3 = 8` |
| $\sqrt{x}$ | å¹³æ–¹æ ¹ | `np.sqrt(x)` | `np.sqrt(9) = 3.0` |
| $\sqrt[n]{x}$ | næ¬¡æ ¹ | `x**(1/n)` | `8**(1/3) â‰ˆ 2.0` |
| $\|x\|$ | ç»å¯¹å€¼ | `abs(x)` æˆ– `np.abs(x)` | `abs(-5) = 5` |
| $e^x$ | æŒ‡æ•° | `np.exp(x)` | `np.exp(1) â‰ˆ 2.718` |
| $\ln(x)$ | è‡ªç„¶å¯¹æ•° | `np.log(x)` | `np.log(e) = 1` |
| $\log_{10}(x)$ | å¸¸ç”¨å¯¹æ•° | `np.log10(x)` | `np.log10(100) = 2` |

---

## å‘é‡è¿ç®—

### å‘é‡åŸºç¡€

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  | ä¾‹å­ |
|---------|------|----------|------|
| $\vec{v} = [v_1, v_2, v_3]$ | å‘é‡ | `v = np.array([v1, v2, v3])` | `v = np.array([1, 2, 3])` |
| $v_i$ | ç¬¬iä¸ªå…ƒç´  | `v[i]` | `v[0] = 1` |
| $\vec{v} + \vec{w}$ | å‘é‡åŠ æ³• | `v + w` | `[1,2] + [3,4] = [4,6]` |
| $c\vec{v}$ | æ ‡é‡ä¹˜æ³• | `c * v` | `2 * [1,2] = [2,4]` |

### å‘é‡ç‚¹ç§¯

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  |
|---------|------|----------|
| $\vec{a} \cdot \vec{b}$ | ç‚¹ç§¯/å†…ç§¯ | `np.dot(a, b)` æˆ– `a @ b` |
| $\langle a, b \rangle$ | å†…ç§¯ï¼ˆå¦ä¸€ç§å†™æ³•ï¼‰ | `np.dot(a, b)` |
| $\sum_{i=1}^{n} a_i b_i$ | ç‚¹ç§¯å®šä¹‰ | `np.sum(a * b)` |

**ä»£ç ç¤ºä¾‹ï¼š**
```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# ä¸‰ç§ç­‰ä»·å†™æ³•
dot1 = np.dot(a, b)      # 32
dot2 = a @ b             # 32
dot3 = np.sum(a * b)     # 32
```

### å‘é‡æ¨¡ï¼ˆé•¿åº¦ï¼‰

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  |
|---------|------|----------|
| $\|\vec{v}\|$ | å‘é‡çš„é•¿åº¦ | `np.linalg.norm(v)` |
| $\|\vec{v}\|_2$ | L2èŒƒæ•°ï¼ˆæ¬§æ°è·ç¦»ï¼‰ | `np.linalg.norm(v)` æˆ– `np.sqrt(np.sum(v**2))` |
| $\|\vec{v}\|_1$ | L1èŒƒæ•° | `np.linalg.norm(v, 1)` æˆ– `np.sum(np.abs(v))` |
| $\|\vec{v}\|_\infty$ | æ— ç©·èŒƒæ•° | `np.linalg.norm(v, np.inf)` æˆ– `np.max(np.abs(v))` |

**ä»£ç ç¤ºä¾‹ï¼š**
```python
v = np.array([3, 4])

# L2èŒƒæ•°ï¼ˆæ¬§æ°è·ç¦»ï¼‰
l2 = np.linalg.norm(v)           # 5.0
l2_manual = np.sqrt(np.sum(v**2)) # 5.0

# L1èŒƒæ•°
l1 = np.linalg.norm(v, 1)         # 7.0
l1_manual = np.sum(np.abs(v))     # 7.0
```

---

## çŸ©é˜µè¿ç®—

### çŸ©é˜µåŸºç¡€

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  | ä¾‹å­ |
|---------|------|----------|------|
| $A = \begin{bmatrix}a&b\\c&d\end{bmatrix}$ | çŸ©é˜µ | `A = np.array([[a,b],[c,d]])` | `A = np.array([[1,2],[3,4]])` |
| $A_{ij}$ | ç¬¬iè¡Œjåˆ—å…ƒç´  | `A[i, j]` | `A[0, 1] = 2` |
| $A^T$ | è½¬ç½® | `A.T` æˆ– `np.transpose(A)` | `A.T` |
| $A^{-1}$ | é€†çŸ©é˜µ | `np.linalg.inv(A)` | `np.linalg.inv(A)` |

**ä»£ç ç¤ºä¾‹ï¼š**
```python
A = np.array([[1, 2],
              [3, 4]])

# è½¬ç½®
AT = A.T
# [[1 3]
#  [2 4]]

# é€†çŸ©é˜µ
A_inv = np.linalg.inv(A)

# éªŒè¯ A @ A^(-1) = I
I = A @ A_inv  # å•ä½çŸ©é˜µ
```

### çŸ©é˜µä¹˜æ³•

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  |
|---------|------|----------|
| $AB$ | çŸ©é˜µä¹˜æ³• | `A @ B` æˆ– `np.matmul(A, B)` æˆ– `np.dot(A, B)` |
| $A \odot B$ | å¯¹åº”å…ƒç´ ç›¸ä¹˜ | `A * B` æˆ– `np.multiply(A, B)` |

**é‡è¦åŒºåˆ†ï¼š**
```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# çŸ©é˜µä¹˜æ³•
C = A @ B
# [[19 22]
#  [43 50]]

# å¯¹åº”å…ƒç´ ç›¸ä¹˜ï¼ˆä¸æ˜¯çŸ©é˜µä¹˜æ³•ï¼ï¼‰
D = A * B
# [[ 5 12]
#  [21 32]]
```

### ç‰¹æ®ŠçŸ©é˜µ

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  |
|---------|------|----------|
| $I_n$ | nÃ—nå•ä½çŸ©é˜µ | `np.eye(n)` æˆ– `np.identity(n)` |
| $\mathbf{0}_{m \times n}$ | mÃ—né›¶çŸ©é˜µ | `np.zeros((m, n))` |
| $\mathbf{1}_{m \times n}$ | mÃ—nå…¨1çŸ©é˜µ | `np.ones((m, n))` |
| $\text{diag}(v)$ | å¯¹è§’çŸ©é˜µ | `np.diag(v)` |

---

## æ±‚å’Œä¸è¿ä¹˜

### æ±‚å’Œç¬¦å· Î£

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  |
|---------|------|----------|
| $\sum_{i=1}^{n} x_i$ | æ±‚å’Œ | `np.sum(x)` |
| $\sum_{i=1}^{n} x_i^2$ | å¹³æ–¹å’Œ | `np.sum(x**2)` |
| $\sum_{i,j} A_{ij}$ | çŸ©é˜µæ‰€æœ‰å…ƒç´ æ±‚å’Œ | `np.sum(A)` |
| $\sum_{i} A_{ij}$ | æŒ‰åˆ—æ±‚å’Œ | `np.sum(A, axis=0)` |
| $\sum_{j} A_{ij}$ | æŒ‰è¡Œæ±‚å’Œ | `np.sum(A, axis=1)` |

**ä»£ç ç¤ºä¾‹ï¼š**
```python
x = np.array([1, 2, 3, 4, 5])

# Î£x_i
total = np.sum(x)  # 15

# Î£x_iÂ²
sum_squares = np.sum(x**2)  # 55

# çŸ©é˜µæ±‚å’Œ
A = np.array([[1, 2, 3],
              [4, 5, 6]])

total = np.sum(A)        # 21 (æ‰€æœ‰å…ƒç´ )
col_sum = np.sum(A, axis=0)  # [5, 7, 9] (æŒ‰åˆ—)
row_sum = np.sum(A, axis=1)  # [6, 15] (æŒ‰è¡Œ)
```

### è¿ä¹˜ç¬¦å· Î 

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  |
|---------|------|----------|
| $\prod_{i=1}^{n} x_i$ | è¿ä¹˜ | `np.prod(x)` |

**ä»£ç ç¤ºä¾‹ï¼š**
```python
x = np.array([1, 2, 3, 4])

# Î x_i = 1Ã—2Ã—3Ã—4
product = np.prod(x)  # 24
```

---

## å¾®ç§¯åˆ†ç¬¦å·

### å¯¼æ•°

| æ•°å­¦ç¬¦å· | å«ä¹‰ | æ•°å€¼è®¡ç®—ä»£ç  |
|---------|------|-------------|
| $\frac{df}{dx}$ | å¯¼æ•° | `(f(x+h) - f(x)) / h` |
| $f'(x)$ | ä¸€é˜¶å¯¼æ•° | `numerical_derivative(f, x)` |
| $\frac{\partial f}{\partial x}$ | åå¯¼æ•° | `(f(x+h, y) - f(x, y)) / h` |

**æ•°å€¼å¾®åˆ†ä»£ç ï¼š**
```python
def numerical_derivative(f, x, h=1e-5):
    """è®¡ç®—å¯¼æ•°"""
    return (f(x + h) - f(x)) / h

# ä¾‹å­
f = lambda x: x**2
derivative_at_3 = numerical_derivative(f, 3)  # â‰ˆ 6

# åå¯¼æ•°
def partial_derivative(f, x, i, h=1e-5):
    """è®¡ç®—å¯¹ç¬¬iä¸ªå˜é‡çš„åå¯¼æ•°"""
    x_plus_h = x.copy()
    x_plus_h[i] += h
    return (f(x_plus_h) - f(x)) / h

f = lambda x: x[0]**2 + x[1]**2
point = np.array([3.0, 4.0])
df_dx = partial_derivative(f, point, 0)  # â‰ˆ 6
df_dy = partial_derivative(f, point, 1)  # â‰ˆ 8
```

### æ¢¯åº¦

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  |
|---------|------|----------|
| $\nabla f$ | æ¢¯åº¦å‘é‡ | `numerical_gradient(f, x)` |
| $\nabla f = [\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ...]$ | æ¢¯åº¦å®šä¹‰ | è§ä¸‹æ–¹ä»£ç  |

**æ¢¯åº¦è®¡ç®—ä»£ç ï¼š**
```python
def numerical_gradient(f, x, h=1e-5):
    """è®¡ç®—æ¢¯åº¦"""
    grad = np.zeros_like(x, dtype=float)

    for i in range(len(x)):
        x_plus_h = x.copy()
        x_plus_h[i] += h
        grad[i] = (f(x_plus_h) - f(x)) / h

    return grad

# ä½¿ç”¨
f = lambda x: x[0]**2 + x[1]**2  # f(x,y) = xÂ² + yÂ²
point = np.array([3.0, 4.0])
grad = numerical_gradient(f, point)  # [6.0, 8.0]
```

---

## æ¦‚ç‡ç»Ÿè®¡ç¬¦å·

### æ¦‚ç‡

| æ•°å­¦ç¬¦å· | å«ä¹‰ | Pythonä»£ç  |
|---------|------|-----------|
| $P(A)$ | æ¦‚ç‡ | `count_A / total` |
| $P(A\|B)$ | æ¡ä»¶æ¦‚ç‡ | `P_AB / P_B` |
| $P(A, B)$ | è”åˆæ¦‚ç‡ | `P_A * P_B_given_A` |

### æœŸæœ›å’Œæ–¹å·®

| æ•°å­¦ç¬¦å· | å«ä¹‰ | NumPyä»£ç  |
|---------|------|----------|
| $E[X]$ æˆ– $\mu$ | æœŸæœ›/å‡å€¼ | `np.mean(X)` |
| $\text{Var}(X)$ æˆ– $\sigma^2$ | æ–¹å·® | `np.var(X)` |
| $\sigma$ | æ ‡å‡†å·® | `np.std(X)` |
| $\text{Cov}(X,Y)$ | åæ–¹å·® | `np.cov(X, Y)[0,1]` |

**ä»£ç ç¤ºä¾‹ï¼š**
```python
data = np.array([1, 2, 3, 4, 5])

# æœŸæœ›ï¼ˆå‡å€¼ï¼‰
mean = np.mean(data)  # 3.0

# æ–¹å·®
variance = np.var(data)  # 2.0

# æ ‡å‡†å·®
std = np.std(data)  # 1.414

# åæ–¹å·®
X = np.array([1, 2, 3, 4])
Y = np.array([2, 4, 6, 8])
cov_matrix = np.cov(X, Y)
covariance = cov_matrix[0, 1]
```

### åˆ†å¸ƒ

| æ•°å­¦ç¬¦å· | å«ä¹‰ | SciPyä»£ç  |
|---------|------|----------|
| $X \sim \mathcal{N}(\mu, \sigma^2)$ | æ­£æ€åˆ†å¸ƒ | `stats.norm(mu, sigma)` |
| $X \sim \text{Uniform}(a, b)$ | å‡åŒ€åˆ†å¸ƒ | `stats.uniform(a, b-a)` |
| $X \sim \text{Bernoulli}(p)$ | ä¼¯åŠªåˆ©åˆ†å¸ƒ | `stats.bernoulli(p)` |

**ä»£ç ç¤ºä¾‹ï¼š**
```python
from scipy import stats

# æ­£æ€åˆ†å¸ƒ N(0, 1)
normal = stats.norm(0, 1)
samples = normal.rvs(size=1000)  # ç”Ÿæˆæ ·æœ¬
pdf_value = normal.pdf(0)         # æ¦‚ç‡å¯†åº¦

# å‡åŒ€åˆ†å¸ƒ Uniform(0, 1)
uniform = stats.uniform(0, 1)
samples = uniform.rvs(size=1000)

# æˆ–ç”¨NumPy
samples = np.random.normal(0, 1, 1000)  # æ­£æ€åˆ†å¸ƒ
samples = np.random.uniform(0, 1, 1000) # å‡åŒ€åˆ†å¸ƒ
```

---

## æœºå™¨å­¦ä¹ å¸¸ç”¨å…¬å¼

### 1. çº¿æ€§å›å½’

**æ•°å­¦å…¬å¼ï¼š**
$$\hat{y} = w^T x + b = \sum_{i=1}^{n} w_i x_i + b$$

**ä»£ç ï¼š**
```python
# å‘é‡åŒ–ç‰ˆæœ¬
y_pred = X @ w + b

# æˆ–å¾ªç¯ç‰ˆæœ¬
y_pred = np.sum(w * x) + b

# sklearn
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)
```

### 2. å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰

**æ•°å­¦å…¬å¼ï¼š**
$$\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**ä»£ç ï¼š**
```python
# æ‰‹åŠ¨è®¡ç®—
mse = np.mean((y - y_pred)**2)

# sklearn
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y, y_pred)
```

### 3. æ¢¯åº¦ä¸‹é™

**æ•°å­¦å…¬å¼ï¼š**
$$\theta \leftarrow \theta - \alpha \nabla L(\theta)$$

**ä»£ç ï¼š**
```python
# å•æ¬¡æ›´æ–°
theta = theta - learning_rate * gradient

# å®Œæ•´è®­ç»ƒå¾ªç¯
for epoch in range(n_epochs):
    # è®¡ç®—æ¢¯åº¦
    gradient = compute_gradient(X, y, theta)

    # æ›´æ–°å‚æ•°
    theta = theta - learning_rate * gradient

    # è®¡ç®—æŸå¤±
    loss = compute_loss(X, y, theta)
```

### 4. Sigmoidå‡½æ•°

**æ•°å­¦å…¬å¼ï¼š**
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**ä»£ç ï¼š**
```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# å‘é‡åŒ–ç‰ˆæœ¬è‡ªåŠ¨å¤„ç†æ•°ç»„
z = np.array([-1, 0, 1])
s = sigmoid(z)  # [0.268, 0.5, 0.731]
```

### 5. Softmaxå‡½æ•°

**æ•°å­¦å…¬å¼ï¼š**
$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

**ä»£ç ï¼š**
```python
def softmax(z):
    # æ•°å€¼ç¨³å®šç‰ˆæœ¬
    exp_z = np.exp(z - np.max(z))
    return exp_z / np.sum(exp_z)

# ä½¿ç”¨
logits = np.array([2.0, 1.0, 0.1])
probs = softmax(logits)
# [0.659, 0.242, 0.099]
```

### 6. ä½™å¼¦ç›¸ä¼¼åº¦

**æ•°å­¦å…¬å¼ï¼š**
$$\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \|\vec{b}\|}$$

**ä»£ç ï¼š**
```python
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# æˆ–ä½¿ç”¨sklearn
from sklearn.metrics.pairwise import cosine_similarity
sim = cosine_similarity([a], [b])[0, 0]
```

---

## å¸¸è§æ¨¡å¼é€ŸæŸ¥

### æ¨¡å¼1ï¼šæŒ‰è¡Œ/åˆ—æ“ä½œ

```python
# æŒ‰è¡Œæ±‚å’Œ: Î£_j A_ij
row_sum = np.sum(A, axis=1)

# æŒ‰åˆ—æ±‚å’Œ: Î£_i A_ij
col_sum = np.sum(A, axis=0)

# è®°å¿†æ–¹æ³•ï¼šaxis=0æ²¿ç€ç¬¬0ç»´ï¼ˆè¡Œï¼‰æŠ˜å ï¼Œå¾—åˆ°åˆ—
#          axis=1æ²¿ç€ç¬¬1ç»´ï¼ˆåˆ—ï¼‰æŠ˜å ï¼Œå¾—åˆ°è¡Œ
```

### æ¨¡å¼2ï¼šå¹¿æ’­

```python
# æ•°å­¦: æ¯è¡Œå‡å»å‡å€¼
# X - mean(X, axis=0)

X_centered = X - np.mean(X, axis=0)

# æ•°å­¦: æ¯åˆ—é™¤ä»¥æ ‡å‡†å·®
# X / std(X, axis=1)

X_normalized = X / np.std(X, axis=1, keepdims=True)
```

### æ¨¡å¼3ï¼šæ¡ä»¶ç´¢å¼•

```python
# æ•°å­¦: {x | x > 0}
positive = X[X > 0]

# æ•°å­¦: Î£_{x_i > 0} x_i
sum_positive = np.sum(X[X > 0])

# æ•°å­¦: count({x | x > threshold})
count = np.sum(X > threshold)
```

---

## ğŸ”§ è°ƒè¯•æŠ€å·§

### æ£€æŸ¥å½¢çŠ¶
```python
print(f'X.shape = {X.shape}')
print(f'w.shape = {w.shape}')
print(f'y.shape = {y.shape}')

# é¢„æœŸ: (n_samples, n_features) @ (n_features, 1) = (n_samples, 1)
```

### æ£€æŸ¥æ•°å€¼èŒƒå›´
```python
print(f'min={X.min()}, max={X.max()}, mean={X.mean():.2f}')
```

### éªŒè¯å®ç°
```python
# ç”¨ç®€å•ä¾‹å­æ‰‹ç®—éªŒè¯
X_simple = np.array([[1, 2], [3, 4]])
# æ‰‹ç®—ç»“æœ...
# å¯¹æ¯”ä»£ç ç»“æœ
```

---

## ğŸ“ å¿«é€Ÿå‚è€ƒå¡

### NumPyæ ¸å¿ƒå‡½æ•°

| æ“ä½œ | å‡½æ•° |
|------|------|
| æ±‚å’Œ | `np.sum()` |
| å‡å€¼ | `np.mean()` |
| æœ€å¤§ | `np.max()` |
| æœ€å° | `np.min()` |
| ç‚¹ç§¯ | `np.dot()` æˆ– `@` |
| èŒƒæ•° | `np.linalg.norm()` |
| è½¬ç½® | `.T` |
| é€†çŸ©é˜µ | `np.linalg.inv()` |
| æŒ‡æ•° | `np.exp()` |
| å¯¹æ•° | `np.log()` |
| å¹³æ–¹æ ¹ | `np.sqrt()` |
| å¹³æ–¹ | `np.square()` æˆ– `**2` |
| ç»å¯¹å€¼ | `np.abs()` |

---

**çœ‹åˆ°å…¬å¼ä¸å†è¿·èŒ«ï¼Œç›´æ¥å†™ä»£ç ï¼** ğŸ’»
