{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”„ åå‘ä¼ æ’­ç®—æ³•çš„æ•°å­¦æ¨å¯¼\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ã€‚æœ¬æ•™ç¨‹å°†ä»æ•°å­¦å±‚é¢æ·±å…¥å‰–æè¿™ä¸ªç®—æ³•ã€‚\n",
    "\n",
    "**å­¦å®Œæœ¬æ•™ç¨‹ä½ å°†èƒ½å¤Ÿï¼š**\n",
    "\n",
    "1. âœ… å®Œæ•´æ¨å¯¼å•å±‚ç½‘ç»œçš„åå‘ä¼ æ’­å…¬å¼\n",
    "2. âœ… ç†è§£å¤šå±‚ç½‘ç»œä¸­è¯¯å·®çš„é€å±‚åå‘ä¼ æ’­æœºåˆ¶\n",
    "3. âœ… æŒæ¡é“¾å¼æ³•åˆ™åœ¨æ·±åº¦ç½‘ç»œä¸­çš„åº”ç”¨\n",
    "4. âœ… ç†è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸çš„æ•°å­¦æ ¹æº\n",
    "5. âœ… ä»é›¶å®ç°å¤šå±‚ç½‘ç»œçš„å®Œæ•´å‰å‘å’Œåå‘ä¼ æ’­\n",
    "6. âœ… ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†éªŒè¯æ‰‹åŠ¨æ¨å¯¼çš„æ­£ç¡®æ€§\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å…ˆä¿®çŸ¥è¯†\n",
    "\n",
    "- çŸ©é˜µå¾®åˆ†ä¸æ¢¯åº¦è®¡ç®— â†’ `05_matrix_calculus.ipynb`\n",
    "- é“¾å¼æ³•åˆ™ â†’ `02_calculus.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– ç›®å½•\n",
    "\n",
    "1. **å•å±‚ç½‘ç»œçš„åå‘ä¼ æ’­**\n",
    "   - å‰å‘ä¼ æ’­ï¼š$\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}$, $\\mathbf{a} = \\sigma(\\mathbf{z})$\n",
    "   - æŸå¤±å‡½æ•°ï¼š$L = \\text{loss}(\\mathbf{a}, \\mathbf{y})$\n",
    "   - åå‘ä¼ æ’­æ¨å¯¼ï¼š$\\frac{\\partial L}{\\partial W}$, $\\frac{\\partial L}{\\partial \\mathbf{b}}$, $\\frac{\\partial L}{\\partial \\mathbf{x}}$\n",
    "   - å¸¸è§æ¿€æ´»å‡½æ•°çš„å¯¼æ•°\n",
    "\n",
    "2. **å¤šå±‚ç½‘ç»œçš„é“¾å¼æ³•åˆ™**\n",
    "   - ä¸¤å±‚ç½‘ç»œçš„å®Œæ•´æ¨å¯¼\n",
    "   - è¯¯å·®çš„åå‘ä¼ æ’­æœºåˆ¶\n",
    "   - æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸çš„æ•°å­¦æ ¹æº\n",
    "   - æ®‹å·®è¿æ¥çš„æ•°å­¦è§£é‡Š\n",
    "\n",
    "3. **å‘é‡åŒ–å®ç°**\n",
    "   - æ‰¹é‡æ•°æ®çš„åå‘ä¼ æ’­\n",
    "   - å¹¿æ’­æœºåˆ¶çš„åº”ç”¨\n",
    "   - ä»é›¶å®ç°å¤šå±‚ç½‘ç»œ\n",
    "   - ä¸è‡ªåŠ¨å¾®åˆ†å¯¹æ¯”éªŒè¯\n",
    "\n",
    "**é¢„è®¡å­¦ä¹ æ—¶é—´ï¼š** 4-5 å°æ—¶\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®æ‰“å°ç²¾åº¦\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒé…ç½®å®Œæˆï¼\")\n",
    "print(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1ï¸âƒ£ å•å±‚ç½‘ç»œçš„åå‘ä¼ æ’­\n",
    "\n",
    "## 1.1 ç½‘ç»œç»“æ„å®šä¹‰\n",
    "\n",
    "è€ƒè™‘æœ€ç®€å•çš„å•å±‚ç¥ç»ç½‘ç»œï¼š\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z} &= W\\mathbf{x} + \\mathbf{b} \\quad &\\text{(çº¿æ€§å˜æ¢)} \\\\\n",
    "\\mathbf{a} &= \\sigma(\\mathbf{z}) \\quad &\\text{(æ¿€æ´»å‡½æ•°)} \\\\\n",
    "L &= \\text{Loss}(\\mathbf{a}, \\mathbf{y}) \\quad &\\text{(æŸå¤±å‡½æ•°)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**å½¢çŠ¶çº¦å®šï¼š**\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^{n}$ - è¾“å…¥å‘é‡ï¼ˆ$n$ ä¸ªç‰¹å¾ï¼‰\n",
    "- $W \\in \\mathbb{R}^{m \\times n}$ - æƒé‡çŸ©é˜µ\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^{m}$ - åç½®å‘é‡\n",
    "- $\\mathbf{z} \\in \\mathbb{R}^{m}$ - çº¿æ€§è¾“å‡ºï¼ˆæ¿€æ´»å‰ï¼‰\n",
    "- $\\mathbf{a} \\in \\mathbb{R}^{m}$ - æ¿€æ´»è¾“å‡ºï¼ˆæ¿€æ´»åï¼‰\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{m}$ - ç›®æ ‡è¾“å‡º\n",
    "- $L \\in \\mathbb{R}$ - æŸå¤±ï¼ˆæ ‡é‡ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 è®¡ç®—å›¾å¯è§†åŒ–\n",
    "\n",
    "```\n",
    "è¾“å…¥       çº¿æ€§å±‚          æ¿€æ´»å‡½æ•°        æŸå¤±å‡½æ•°\n",
    " x  â”€â”€â†’  [W, b]  â”€â”€â†’  z  â”€â”€â†’  Ïƒ  â”€â”€â†’  a  â”€â”€â†’  Loss  â”€â”€â†’  L\n",
    "                                               â†‘\n",
    "                                               y (ç›®æ ‡)\n",
    "```\n",
    "\n",
    "**åå‘ä¼ æ’­æ–¹å‘ï¼š**\n",
    "\n",
    "```\n",
    "âˆ‚L/âˆ‚x â†â”€ âˆ‚L/âˆ‚W â†â”€ âˆ‚L/âˆ‚z â†â”€ âˆ‚L/âˆ‚a â†â”€ âˆ‚L/âˆ‚L = 1\n",
    "         âˆ‚L/âˆ‚b â†â”€â”˜\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 å‰å‘ä¼ æ’­ï¼šé€æ­¥è®¡ç®—\n",
    "\n",
    "### ğŸ”¢ æ•°å€¼ç¤ºä¾‹\n",
    "\n",
    "è®¾å®šå…·ä½“æ•°å€¼æ¥å¸®åŠ©ç†è§£ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ¿€æ´»å‡½æ•°å’Œå¯¼æ•°\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid æ¿€æ´»å‡½æ•°: Ïƒ(z) = 1 / (1 + exp(-z))\n",
    "    \n",
    "    æ€§è´¨:\n",
    "    - è¾“å‡ºèŒƒå›´: (0, 1)\n",
    "    - å¯å¾®åˆ†\n",
    "    - é¥±å’Œæ€§: zå¾ˆå¤§æˆ–å¾ˆå°æ—¶å¯¼æ•°æ¥è¿‘0\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"\n",
    "    Sigmoid å¯¼æ•°: Ïƒ'(z) = Ïƒ(z) * (1 - Ïƒ(z))\n",
    "    \n",
    "    æ¨å¯¼:\n",
    "    Ïƒ(z) = 1 / (1 + e^(-z))\n",
    "    \n",
    "    ä»¤ u = 1 + e^(-z), åˆ™ Ïƒ = 1/u\n",
    "    dÏƒ/dz = -1/uÂ² * du/dz\n",
    "          = -1/uÂ² * (-e^(-z))\n",
    "          = e^(-z) / (1 + e^(-z))Â²\n",
    "    \n",
    "    åŒ–ç®€:\n",
    "    = [1 / (1 + e^(-z))] * [e^(-z) / (1 + e^(-z))]\n",
    "    = Ïƒ(z) * [1 - Ïƒ(z)]  â† å¸¸ç”¨å½¢å¼\n",
    "    \"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# å‰å‘ä¼ æ’­ç¤ºä¾‹\n",
    "print(\"=\" * 60)\n",
    "print(\"å•å±‚ç½‘ç»œå‰å‘ä¼ æ’­ç¤ºä¾‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å®šä¹‰å‚æ•°\n",
    "n = 3  # è¾“å…¥ç»´åº¦\n",
    "m = 2  # è¾“å‡ºç»´åº¦\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0])           # (3,) è¾“å…¥\n",
    "W = np.array([[0.1, 0.2, 0.3],          # (2, 3) æƒé‡\n",
    "              [0.4, 0.5, 0.6]])\n",
    "b = np.array([0.1, 0.2])                # (2,) åç½®\n",
    "y = np.array([1.0, 0.0])                # (2,) ç›®æ ‡\n",
    "\n",
    "print(f\"\\nè¾“å…¥ x: {x}\")\n",
    "print(f\"æƒé‡ W:\\n{W}\")\n",
    "print(f\"åç½® b: {b}\")\n",
    "print(f\"ç›®æ ‡ y: {y}\")\n",
    "\n",
    "# Step 1: çº¿æ€§å˜æ¢ z = Wx + b\n",
    "z = np.dot(W, x) + b\n",
    "print(f\"\\n[å‰å‘] z = Wx + b = {z}\")\n",
    "print(f\"  è®¡ç®—: [{W[0] @ x + b[0]:.4f}, {W[1] @ x + b[1]:.4f}]\")\n",
    "\n",
    "# Step 2: æ¿€æ´»å‡½æ•° a = Ïƒ(z)\n",
    "a = sigmoid(z)\n",
    "print(f\"\\n[å‰å‘] a = Ïƒ(z) = {a}\")\n",
    "\n",
    "# Step 3: æŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨ MSEï¼‰\n",
    "# L = 0.5 * ||a - y||Â²\n",
    "loss = 0.5 * np.sum((a - y) ** 2)\n",
    "print(f\"\\n[å‰å‘] L = 0.5 * ||a - y||Â² = {loss:.6f}\")\n",
    "print(f\"  è¯¯å·® (a - y) = {a - y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 åå‘ä¼ æ’­ï¼šå®Œæ•´æ¨å¯¼\n",
    "\n",
    "### ğŸ¯ ç›®æ ‡\n",
    "\n",
    "è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W}, \\quad \\frac{\\partial L}{\\partial \\mathbf{b}}, \\quad \\frac{\\partial L}{\\partial \\mathbf{x}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ æ¨å¯¼æ­¥éª¤\n",
    "\n",
    "#### Step 1: $\\frac{\\partial L}{\\partial \\mathbf{a}}$ - æŸå¤±å¯¹æ¿€æ´»å€¼çš„æ¢¯åº¦\n",
    "\n",
    "ä½¿ç”¨ MSE æŸå¤±ï¼š$L = \\frac{1}{2} \\sum_{i=1}^{m} (a_i - y_i)^2$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_i} = a_i - y_i \\quad \\Rightarrow \\quad \\frac{\\partial L}{\\partial \\mathbf{a}} = \\mathbf{a} - \\mathbf{y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: $\\frac{\\partial L}{\\partial \\mathbf{z}}$ - æŸå¤±å¯¹çº¿æ€§è¾“å‡ºçš„æ¢¯åº¦\n",
    "\n",
    "ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{z}} = \\frac{\\partial L}{\\partial \\mathbf{a}} \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}}\n",
    "$$\n",
    "\n",
    "å› ä¸º $\\mathbf{a} = \\sigma(\\mathbf{z})$ æ˜¯é€å…ƒç´ æ“ä½œï¼ŒJacobian æ˜¯å¯¹è§’çŸ©é˜µï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}} = \\text{diag}(\\sigma'(z_1), \\sigma'(z_2), \\ldots, \\sigma'(z_m))\n",
    "$$\n",
    "\n",
    "çŸ©é˜µ-å‘é‡ä¹˜æ³•é€€åŒ–ä¸º**é€å…ƒç´ ä¹˜æ³•**ï¼ˆHadamard product $\\odot$ï¼‰ï¼š\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{\\partial L}{\\partial \\mathbf{z}} = (\\mathbf{a} - \\mathbf{y}) \\odot \\sigma'(\\mathbf{z}) = \\delta}\n",
    "$$\n",
    "\n",
    "æˆ‘ä»¬å°† $\\delta$ ç§°ä¸º**è¯¯å·®å‘é‡**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: $\\frac{\\partial L}{\\partial W}$ - æŸå¤±å¯¹æƒé‡çš„æ¢¯åº¦\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial W}\n",
    "$$\n",
    "\n",
    "å› ä¸º $z_i = \\sum_{j=1}^{n} W_{ij} x_j + b_i$ï¼Œæ‰€ä»¥ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_i}{\\partial W_{ij}} = x_j\n",
    "$$\n",
    "\n",
    "å› æ­¤ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{ij}} = \\delta_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "çŸ©é˜µå½¢å¼ï¼š\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{\\partial L}{\\partial W} = \\delta \\mathbf{x}^T}\n",
    "$$\n",
    "\n",
    "**å½¢çŠ¶éªŒè¯ï¼š**\n",
    "- $\\delta \\in \\mathbb{R}^{m}$ (åˆ—å‘é‡)\n",
    "- $\\mathbf{x}^T \\in \\mathbb{R}^{1 \\times n}$ (è¡Œå‘é‡)\n",
    "- $\\delta \\mathbf{x}^T \\in \\mathbb{R}^{m \\times n}$ âœ… ä¸ $W$ å½¢çŠ¶ä¸€è‡´\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: $\\frac{\\partial L}{\\partial \\mathbf{b}}$ - æŸå¤±å¯¹åç½®çš„æ¢¯åº¦\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}}\n",
    "$$\n",
    "\n",
    "å› ä¸º $\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}$ï¼Œæ‰€ä»¥ $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}} = I$ (å•ä½çŸ©é˜µ)ã€‚\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{\\partial L}{\\partial \\mathbf{b}} = \\delta}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5: $\\frac{\\partial L}{\\partial \\mathbf{x}}$ - æŸå¤±å¯¹è¾“å…¥çš„æ¢¯åº¦ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¦‚æœè¦å°†æ¢¯åº¦ç»§ç»­åå‘ä¼ æ’­åˆ°ä¸Šä¸€å±‚ï¼Œéœ€è¦è®¡ç®—ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\n",
    "$$\n",
    "\n",
    "å› ä¸º $\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}$ï¼Œæ‰€ä»¥ $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} = W$ã€‚\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{\\partial L}{\\partial \\mathbf{x}} = W^T \\delta}\n",
    "$$\n",
    "\n",
    "**å½¢çŠ¶éªŒè¯ï¼š**\n",
    "- $W^T \\in \\mathbb{R}^{n \\times m}$\n",
    "- $\\delta \\in \\mathbb{R}^{m}$\n",
    "- $W^T \\delta \\in \\mathbb{R}^{n}$ âœ… ä¸ $\\mathbf{x}$ å½¢çŠ¶ä¸€è‡´\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åå‘ä¼ æ’­è®¡ç®—ï¼ˆä½¿ç”¨å‰é¢çš„æ•°æ®ï¼‰\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"å•å±‚ç½‘ç»œåå‘ä¼ æ’­è®¡ç®—\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: âˆ‚L/âˆ‚a = a - y\n",
    "dL_da = a - y\n",
    "print(f\"\\n[åå‘] âˆ‚L/âˆ‚a = a - y = {dL_da}\")\n",
    "\n",
    "# Step 2: âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a âŠ™ Ïƒ'(z)\n",
    "dL_dz = dL_da * sigmoid_derivative(z)  # é€å…ƒç´ ä¹˜æ³•\n",
    "print(f\"\\n[åå‘] âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a âŠ™ Ïƒ'(z)\")\n",
    "print(f\"  Ïƒ'(z) = {sigmoid_derivative(z)}\")\n",
    "print(f\"  Î´ = âˆ‚L/âˆ‚z = {dL_dz}\")\n",
    "\n",
    "# Step 3: âˆ‚L/âˆ‚W = Î´ x^T\n",
    "dL_dW = np.outer(dL_dz, x)  # å¤–ç§¯\n",
    "print(f\"\\n[åå‘] âˆ‚L/âˆ‚W = Î´ x^T\")\n",
    "print(f\"  å½¢çŠ¶: ({dL_dz.shape[0]},) âŠ— ({x.shape[0]},) = ({dL_dW.shape[0]}, {dL_dW.shape[1]})\")\n",
    "print(f\"  âˆ‚L/âˆ‚W =\\n{dL_dW}\")\n",
    "\n",
    "# Step 4: âˆ‚L/âˆ‚b = Î´\n",
    "dL_db = dL_dz\n",
    "print(f\"\\n[åå‘] âˆ‚L/âˆ‚b = Î´ = {dL_db}\")\n",
    "\n",
    "# Step 5: âˆ‚L/âˆ‚x = W^T Î´ (å¯é€‰)\n",
    "dL_dx = np.dot(W.T, dL_dz)\n",
    "print(f\"\\n[åå‘] âˆ‚L/âˆ‚x = W^T Î´\")\n",
    "print(f\"  å½¢çŠ¶: ({W.T.shape[0]}, {W.T.shape[1]}) @ ({dL_dz.shape[0]},) = ({dL_dx.shape[0]},)\")\n",
    "print(f\"  âˆ‚L/âˆ‚x = {dL_dx}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æ¢¯åº¦å½¢çŠ¶æ€»ç»“\")\n",
    "print(\"=\"*60)\n",
    "print(f\"å‚æ•° W: {W.shape} -> æ¢¯åº¦ âˆ‚L/âˆ‚W: {dL_dW.shape} âœ…\")\n",
    "print(f\"å‚æ•° b: {b.shape} -> æ¢¯åº¦ âˆ‚L/âˆ‚b: {dL_db.shape} âœ…\")\n",
    "print(f\"è¾“å…¥ x: {x.shape} -> æ¢¯åº¦ âˆ‚L/âˆ‚x: {dL_dx.shape} âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 å¸¸è§æ¿€æ´»å‡½æ•°çš„å¯¼æ•°\n",
    "\n",
    "ä¸åŒçš„æ¿€æ´»å‡½æ•°æœ‰ä¸åŒçš„å¯¼æ•°ï¼Œè¿™ç›´æ¥å½±å“æ¢¯åº¦çš„è®¡ç®—ã€‚\n",
    "\n",
    "| æ¿€æ´»å‡½æ•° | å…¬å¼ $\\sigma(z)$ | å¯¼æ•° $\\sigma'(z)$ | ç‰¹ç‚¹ |\n",
    "|---------|------------------|-------------------|------|\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-z}}$ | $\\sigma(z)(1-\\sigma(z))$ | é¥±å’Œï¼Œæ¢¯åº¦æ¶ˆå¤± |\n",
    "| **Tanh** | $\\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | $1 - \\tanh^2(z)$ | é›¶ä¸­å¿ƒï¼Œé¥±å’Œ |\n",
    "| **ReLU** | $\\max(0, z)$ | $\\begin{cases} 1 & z > 0 \\\\ 0 & z \\leq 0 \\end{cases}$ | ä¸é¥±å’Œï¼ŒDead ReLU |\n",
    "| **Leaky ReLU** | $\\max(0.01z, z)$ | $\\begin{cases} 1 & z > 0 \\\\ 0.01 & z \\leq 0 \\end{cases}$ | ç¼“è§£ Dead ReLU |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®ç°å¸¸è§æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"åŒæ›²æ­£åˆ‡å‡½æ•°\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"\n",
    "    Tanh å¯¼æ•°: tanh'(z) = 1 - tanhÂ²(z)\n",
    "    \n",
    "    æ¨å¯¼:\n",
    "    tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))\n",
    "    \n",
    "    ä»¤ t = tanh(z)\n",
    "    dt/dz = 1 - tÂ²\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU: Rectified Linear Unit\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"\n",
    "    ReLU å¯¼æ•°: \n",
    "    - z > 0: å¯¼æ•° = 1\n",
    "    - z <= 0: å¯¼æ•° = 0\n",
    "    \n",
    "    æ³¨æ„: z=0 å¤„ä¸å¯å¯¼ï¼Œé€šå¸¸å®šä¹‰ä¸º 0\n",
    "    \"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU: å…è®¸è´Ÿå€¼æœ‰å°æ¢¯åº¦\"\"\"\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "def leaky_relu_derivative(z, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU å¯¼æ•°:\n",
    "    - z > 0: å¯¼æ•° = 1\n",
    "    - z <= 0: å¯¼æ•° = alpha (é€šå¸¸ 0.01)\n",
    "    \"\"\"\n",
    "    return np.where(z > 0, 1.0, alpha)\n",
    "\n",
    "# å¯è§†åŒ–æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°\n",
    "z_range = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# å®šä¹‰æ¿€æ´»å‡½æ•°åˆ—è¡¨\n",
    "activations = [\n",
    "    (\"Sigmoid\", sigmoid, sigmoid_derivative),\n",
    "    (\"Tanh\", tanh, tanh_derivative),\n",
    "    (\"ReLU\", relu, relu_derivative),\n",
    "    (\"Leaky ReLU\", leaky_relu, leaky_relu_derivative)\n",
    "]\n",
    "\n",
    "for i, (name, func, deriv) in enumerate(activations):\n",
    "    # ä¸Šæ’ï¼šæ¿€æ´»å‡½æ•°\n",
    "    axes[0, i].plot(z_range, func(z_range), linewidth=2.5, color='blue')\n",
    "    axes[0, i].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0, i].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[0, i].set_title(f\"{name} æ¿€æ´»å‡½æ•°\", fontsize=14, fontweight='bold')\n",
    "    axes[0, i].set_xlabel('z', fontsize=12)\n",
    "    axes[0, i].set_ylabel('Ïƒ(z)', fontsize=12)\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ä¸‹æ’ï¼šå¯¼æ•°\n",
    "    axes[1, i].plot(z_range, deriv(z_range), linewidth=2.5, color='red')\n",
    "    axes[1, i].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1, i].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[1, i].set_title(f\"{name} å¯¼æ•°\", fontsize=14, fontweight='bold')\n",
    "    axes[1, i].set_xlabel('z', fontsize=12)\n",
    "    axes[1, i].set_ylabel(\"Ïƒ'(z)\", fontsize=12)\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nè§‚å¯Ÿè¦ç‚¹:\")\n",
    "print(\"1. Sigmoid/Tanh: åœ¨ |z| å¾ˆå¤§æ—¶å¯¼æ•°æ¥è¿‘ 0 â†’ æ¢¯åº¦æ¶ˆå¤±\")\n",
    "print(\"2. ReLU: z < 0 æ—¶å¯¼æ•°ä¸º 0 â†’ Dead ReLU é—®é¢˜\")\n",
    "print(\"3. Leaky ReLU: z < 0 æ—¶ä¿ç•™å°æ¢¯åº¦ â†’ ç¼“è§£ Dead ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2ï¸âƒ£ å¤šå±‚ç½‘ç»œçš„é“¾å¼æ³•åˆ™\n",
    "\n",
    "## 2.1 ä¸¤å±‚ç½‘ç»œç»“æ„\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}^{(1)} &= W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} \\quad &\\text{(ç¬¬1å±‚çº¿æ€§)} \\\\\n",
    "\\mathbf{a}^{(1)} &= \\sigma(\\mathbf{z}^{(1)}) \\quad &\\text{(ç¬¬1å±‚æ¿€æ´»)} \\\\\n",
    "\\mathbf{z}^{(2)} &= W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)} \\quad &\\text{(ç¬¬2å±‚çº¿æ€§)} \\\\\n",
    "\\mathbf{a}^{(2)} &= \\sigma(\\mathbf{z}^{(2)}) \\quad &\\text{(ç¬¬2å±‚æ¿€æ´»)} \\\\\n",
    "L &= \\frac{1}{2} \\|\\mathbf{a}^{(2)} - \\mathbf{y}\\|^2 \\quad &\\text{(MSEæŸå¤±)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**å½¢çŠ¶çº¦å®šï¼š**\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^{n_0}$ - è¾“å…¥\n",
    "- $\\mathbf{a}^{(1)} \\in \\mathbb{R}^{n_1}$ - éšè—å±‚\n",
    "- $\\mathbf{a}^{(2)} \\in \\mathbb{R}^{n_2}$ - è¾“å‡ºå±‚\n",
    "- $W^{(1)} \\in \\mathbb{R}^{n_1 \\times n_0}$, $W^{(2)} \\in \\mathbb{R}^{n_2 \\times n_1}$\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 åå‘ä¼ æ’­æ¨å¯¼\n",
    "\n",
    "### ğŸ”™ ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚åå‘ä¼ æ’­\n",
    "\n",
    "#### è¾“å‡ºå±‚ï¼ˆç¬¬2å±‚ï¼‰\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(2)}} = (\\mathbf{a}^{(2)} - \\mathbf{y}) \\odot \\sigma'(\\mathbf{z}^{(2)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} (\\mathbf{a}^{(1)})^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}} = \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "#### éšè—å±‚ï¼ˆç¬¬1å±‚ï¼‰- å…³é”®æ­¥éª¤ï¼\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{(1)}} \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n",
    "$$\n",
    "\n",
    "**Step 1:** è®¡ç®— $\\frac{\\partial L}{\\partial \\mathbf{a}^{(1)}}$\n",
    "\n",
    "ç”±äº $\\mathbf{z}^{(2)} = W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}$ï¼Œä½¿ç”¨é“¾å¼æ³•åˆ™ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{a}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(2)}} \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{a}^{(1)}} = (W^{(2)})^T \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "**Step 2:** ä¹˜ä»¥æ¿€æ´»å‡½æ•°å¯¼æ•°\n",
    "\n",
    "$$\n",
    "\\boxed{\\delta^{(1)} = \\left( (W^{(2)})^T \\delta^{(2)} \\right) \\odot \\sigma'(\\mathbf{z}^{(1)})}\n",
    "$$\n",
    "\n",
    "**Step 3:** è®¡ç®—æ¢¯åº¦\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} \\mathbf{x}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\delta^{(1)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” å…³é”®æ´å¯Ÿ\n",
    "\n",
    "è¯¯å·®åå‘ä¼ æ’­çš„é€šç”¨å…¬å¼ï¼š\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} = \\left( (W^{(l+1)})^T \\delta^{(l+1)} \\right) \\odot \\sigma'(\\mathbf{z}^{(l)})\n",
    "$$\n",
    "\n",
    "- $(W^{(l+1)})^T \\delta^{(l+1)}$: å°†è¯¯å·®ä»ç¬¬ $l+1$ å±‚ä¼ æ’­åˆ°ç¬¬ $l$ å±‚\n",
    "- $\\odot \\sigma'(\\mathbf{z}^{(l)})$: ä¹˜ä»¥æ¿€æ´»å‡½æ•°å¯¼æ•°ï¼ˆå±€éƒ¨æ¢¯åº¦ï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»é›¶å®ç°ï¼šä¸¤å±‚ç½‘ç»œçš„å®Œæ•´å‰å‘å’Œåå‘ä¼ æ’­\n",
    "\n",
    "class TwoLayerNetwork:\n",
    "    \"\"\"\n",
    "    ä¸¤å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œ\n",
    "    \n",
    "    ç»“æ„:\n",
    "    x -> [W1, b1] -> z1 -> Ïƒ -> a1 -> [W2, b2] -> z2 -> Ïƒ -> a2 -> L\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–ç½‘ç»œå‚æ•°\n",
    "        \n",
    "        å‚æ•°:\n",
    "        -----\n",
    "        input_size : int\n",
    "            è¾“å…¥ç»´åº¦ n0\n",
    "        hidden_size : int\n",
    "            éšè—å±‚ç»´åº¦ n1\n",
    "        output_size : int\n",
    "            è¾“å‡ºç»´åº¦ n2\n",
    "        activation : str\n",
    "            æ¿€æ´»å‡½æ•°ç±»å‹ ('sigmoid', 'tanh', 'relu')\n",
    "        \"\"\"\n",
    "        # ç¬¬1å±‚å‚æ•°ï¼ˆä½¿ç”¨ Xavier åˆå§‹åŒ–ï¼‰\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        \n",
    "        # ç¬¬2å±‚å‚æ•°\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "        # æ¿€æ´»å‡½æ•°é€‰æ‹©\n",
    "        self.activation = activation\n",
    "        if activation == 'sigmoid':\n",
    "            self.act_func = sigmoid\n",
    "            self.act_deriv = sigmoid_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.act_func = tanh\n",
    "            self.act_deriv = tanh_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.act_func = relu\n",
    "            self.act_deriv = relu_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"æœªçŸ¥æ¿€æ´»å‡½æ•°: {activation}\")\n",
    "        \n",
    "        # ç¼“å­˜å˜é‡ï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "        -----\n",
    "        x : ndarray, shape (n0,)\n",
    "            è¾“å…¥å‘é‡\n",
    "        \n",
    "        è¿”å›:\n",
    "        -----\n",
    "        a2 : ndarray, shape (n2,)\n",
    "            è¾“å‡ºå‘é‡\n",
    "        \"\"\"\n",
    "        # ä¿å­˜è¾“å…¥\n",
    "        self.cache['x'] = x\n",
    "        \n",
    "        # ç¬¬1å±‚å‰å‘ä¼ æ’­\n",
    "        # z1 = W1 @ x + b1\n",
    "        z1 = np.dot(self.W1, x) + self.b1\n",
    "        self.cache['z1'] = z1\n",
    "        \n",
    "        # a1 = Ïƒ(z1)\n",
    "        a1 = self.act_func(z1)\n",
    "        self.cache['a1'] = a1\n",
    "        \n",
    "        # ç¬¬2å±‚å‰å‘ä¼ æ’­\n",
    "        # z2 = W2 @ a1 + b2\n",
    "        z2 = np.dot(self.W2, a1) + self.b2\n",
    "        self.cache['z2'] = z2\n",
    "        \n",
    "        # a2 = Ïƒ(z2)\n",
    "        a2 = self.act_func(z2)\n",
    "        self.cache['a2'] = a2\n",
    "        \n",
    "        return a2\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        åå‘ä¼ æ’­ï¼šè®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦\n",
    "        \n",
    "        å‚æ•°:\n",
    "        -----\n",
    "        y : ndarray, shape (n2,)\n",
    "            ç›®æ ‡è¾“å‡º\n",
    "        \n",
    "        è¿”å›:\n",
    "        -----\n",
    "        grads : dict\n",
    "            åŒ…å«æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦\n",
    "        \"\"\"\n",
    "        # ä»ç¼“å­˜ä¸­è·å–å‰å‘ä¼ æ’­çš„ä¸­é—´å€¼\n",
    "        x = self.cache['x']\n",
    "        z1 = self.cache['z1']\n",
    "        a1 = self.cache['a1']\n",
    "        z2 = self.cache['z2']\n",
    "        a2 = self.cache['a2']\n",
    "        \n",
    "        # ===== ç¬¬2å±‚åå‘ä¼ æ’­ =====\n",
    "        \n",
    "        # è¾“å‡ºå±‚è¯¯å·®: Î´2 = (a2 - y) âŠ™ Ïƒ'(z2)\n",
    "        delta2 = (a2 - y) * self.act_deriv(z2)\n",
    "        \n",
    "        # ç¬¬2å±‚æ¢¯åº¦\n",
    "        grad_W2 = np.outer(delta2, a1)  # Î´2 @ a1^T\n",
    "        grad_b2 = delta2\n",
    "        \n",
    "        # ===== ç¬¬1å±‚åå‘ä¼ æ’­ =====\n",
    "        \n",
    "        # éšè—å±‚è¯¯å·®: Î´1 = (W2^T @ Î´2) âŠ™ Ïƒ'(z1)\n",
    "        # æ­¥éª¤æ‹†è§£:\n",
    "        # 1. W2^T @ Î´2: å°†è¯¯å·®åå‘ä¼ æ’­åˆ°éšè—å±‚\n",
    "        # 2. âŠ™ Ïƒ'(z1): ä¹˜ä»¥æ¿€æ´»å‡½æ•°å¯¼æ•°\n",
    "        delta1 = np.dot(self.W2.T, delta2) * self.act_deriv(z1)\n",
    "        \n",
    "        # ç¬¬1å±‚æ¢¯åº¦\n",
    "        grad_W1 = np.outer(delta1, x)  # Î´1 @ x^T\n",
    "        grad_b1 = delta1\n",
    "        \n",
    "        return {\n",
    "            'W2': grad_W2,\n",
    "            'b2': grad_b2,\n",
    "            'W1': grad_W1,\n",
    "            'b1': grad_b1\n",
    "        }\n",
    "    \n",
    "    def compute_loss(self, y):\n",
    "        \"\"\"\n",
    "        è®¡ç®— MSE æŸå¤±\n",
    "        \n",
    "        L = 0.5 * ||a2 - y||Â²\n",
    "        \"\"\"\n",
    "        a2 = self.cache['a2']\n",
    "        return 0.5 * np.sum((a2 - y) ** 2)\n",
    "\n",
    "# æµ‹è¯•ä¸¤å±‚ç½‘ç»œ\n",
    "print(\"=\" * 60)\n",
    "print(\"ä¸¤å±‚ç¥ç»ç½‘ç»œå‰å‘+åå‘ä¼ æ’­ç¤ºä¾‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# åˆ›å»ºç½‘ç»œ: 3 -> 4 -> 2\n",
    "net = TwoLayerNetwork(input_size=3, hidden_size=4, output_size=2, activation='sigmoid')\n",
    "\n",
    "# å‡†å¤‡æ•°æ®\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "y = np.array([1.0, 0.0])\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "a2 = net.forward(x)\n",
    "loss = net.compute_loss(y)\n",
    "\n",
    "print(f\"\\nç½‘ç»œç»“æ„: {x.shape[0]} -> {net.W1.shape[0]} -> {net.W2.shape[0]}\")\n",
    "print(f\"æ¿€æ´»å‡½æ•°: {net.activation}\")\n",
    "print(f\"\\nè¾“å…¥ x: {x}\")\n",
    "print(f\"éšè—å±‚æ¿€æ´» a1: {net.cache['a1']}\")\n",
    "print(f\"è¾“å‡º a2: {a2}\")\n",
    "print(f\"ç›®æ ‡ y: {y}\")\n",
    "print(f\"æŸå¤± L: {loss:.6f}\")\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "grads = net.backward(y)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æ¢¯åº¦è®¡ç®—ç»“æœ\")\n",
    "print(\"=\"*60)\n",
    "for name in ['W1', 'b1', 'W2', 'b2']:\n",
    "    print(f\"\\nâˆ‚L/âˆ‚{name} (shape: {grads[name].shape}):\")\n",
    "    print(grads[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸\n",
    "\n",
    "### ğŸ” é—®é¢˜æ ¹æº\n",
    "\n",
    "åœ¨å¤šå±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦éœ€è¦é€å±‚åå‘ä¼ æ’­ï¼š\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} = \\left( (W^{(l+1)})^T \\delta^{(l+1)} \\right) \\odot \\sigma'(\\mathbf{z}^{(l)})\n",
    "$$\n",
    "\n",
    "å¯¹äº $L$ å±‚ç½‘ç»œï¼Œç¬¬1å±‚çš„æ¢¯åº¦åŒ…å« $L-1$ æ¬¡çŸ©é˜µä¹˜æ³•å’Œæ¿€æ´»å‡½æ•°å¯¼æ•°ï¼š\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(1)}} \\propto \\prod_{l=2}^{L} (W^{(l)})^T \\cdot \\prod_{l=1}^{L-1} \\sigma'(\\mathbf{z}^{(l)})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ æ¢¯åº¦æ¶ˆå¤±ï¼ˆVanishing Gradientï¼‰\n",
    "\n",
    "**æ¡ä»¶ï¼š**\n",
    "- æ¿€æ´»å‡½æ•°å¯¼æ•° $\\sigma'(z) < 1$ï¼ˆå¦‚ Sigmoid: $0 < \\sigma'(z) \\leq 0.25$ï¼‰\n",
    "- æƒé‡è¾ƒå°\n",
    "\n",
    "**ç»“æœï¼š**\n",
    "\n",
    "$$\n",
    "\\prod_{l=1}^{L-1} \\sigma'(\\mathbf{z}^{(l)}) \\approx 0.25^{L-1} \\to 0 \\quad (L \\to \\infty)\n",
    "$$\n",
    "\n",
    "åº•å±‚ï¼ˆé è¿‘è¾“å…¥ï¼‰çš„æ¢¯åº¦è¶‹è¿‘äº0ï¼Œæ— æ³•å­¦ä¹ ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¥ æ¢¯åº¦çˆ†ç‚¸ï¼ˆExploding Gradientï¼‰\n",
    "\n",
    "**æ¡ä»¶ï¼š**\n",
    "- æ¿€æ´»å‡½æ•°å¯¼æ•°è¾ƒå¤§\n",
    "- æƒé‡åˆå§‹åŒ–è¿‡å¤§\n",
    "\n",
    "**ç»“æœï¼š**\n",
    "\n",
    "$$\n",
    "\\prod \\text{(æƒé‡ Ã— å¯¼æ•°)} \\to \\infty\n",
    "$$\n",
    "\n",
    "æ¢¯åº¦æŒ‡æ•°çº§å¢é•¿ï¼Œå¯¼è‡´æ•°å€¼æº¢å‡ºã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "1. **ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°**\n",
    "   - $z > 0$ æ—¶ $\\sigma'(z) = 1$ï¼Œä¸ä¼šè¡°å‡æ¢¯åº¦\n",
    "\n",
    "2. **æ‰¹æ ‡å‡†åŒ–ï¼ˆBatch Normalizationï¼‰**\n",
    "   - ç¨³å®šæ¿€æ´»å€¼çš„åˆ†å¸ƒ\n",
    "\n",
    "3. **æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰**\n",
    "   - æä¾›æ¢¯åº¦çš„\"é«˜é€Ÿå…¬è·¯\"ï¼š$\\mathbf{a}^{(l+1)} = \\mathbf{a}^{(l)} + F(\\mathbf{a}^{(l)})$\n",
    "\n",
    "4. **æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰**\n",
    "   - é™åˆ¶æ¢¯åº¦èŒƒæ•°ï¼š$\\mathbf{g} \\leftarrow \\min\\left(1, \\frac{\\text{threshold}}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºæ¢¯åº¦æ¶ˆå¤±é—®é¢˜\n",
    "\n",
    "def demonstrate_vanishing_gradient(n_layers=10, activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿæ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±\n",
    "    \n",
    "    å‚æ•°:\n",
    "    -----\n",
    "    n_layers : int\n",
    "        ç½‘ç»œå±‚æ•°\n",
    "    activation : str\n",
    "        æ¿€æ´»å‡½æ•°ç±»å‹\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æ¢¯åº¦æ¶ˆå¤±æ¼”ç¤ºï¼š{n_layers}å±‚ç½‘ç»œï¼Œæ¿€æ´»å‡½æ•°={activation}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # åˆå§‹åŒ–å‚æ•°\n",
    "    layer_size = 10\n",
    "    weights = []\n",
    "    z_values = []\n",
    "    \n",
    "    # éšæœºåˆå§‹åŒ–æƒé‡å’Œæ¿€æ´»å€¼\n",
    "    for i in range(n_layers):\n",
    "        W = np.random.randn(layer_size, layer_size) * 0.1\n",
    "        weights.append(W)\n",
    "        \n",
    "        # æ¨¡æ‹Ÿæ¿€æ´»å€¼ï¼ˆåœ¨æ¿€æ´»å‡½æ•°çš„æœ‰æ•ˆèŒƒå›´å†…ï¼‰\n",
    "        z = np.random.randn(layer_size) * 2\n",
    "        z_values.append(z)\n",
    "    \n",
    "    # é€‰æ‹©æ¿€æ´»å‡½æ•°\n",
    "    if activation == 'sigmoid':\n",
    "        act_deriv = sigmoid_derivative\n",
    "    elif activation == 'tanh':\n",
    "        act_deriv = tanh_derivative\n",
    "    elif activation == 'relu':\n",
    "        act_deriv = relu_derivative\n",
    "    \n",
    "    # æ¨¡æ‹Ÿåå‘ä¼ æ’­ï¼šä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚\n",
    "    delta = np.ones(layer_size)  # åˆå§‹æ¢¯åº¦\n",
    "    gradient_norms = [np.linalg.norm(delta)]\n",
    "    \n",
    "    for l in range(n_layers - 1, 0, -1):\n",
    "        # Î´^(l-1) = (W^(l))^T @ Î´^(l) âŠ™ Ïƒ'(z^(l-1))\n",
    "        delta = np.dot(weights[l].T, delta) * act_deriv(z_values[l-1])\n",
    "        gradient_norms.append(np.linalg.norm(delta))\n",
    "    \n",
    "    # åè½¬åˆ—è¡¨ï¼ˆä½¿å…¶ä»è¾“å…¥å±‚åˆ°è¾“å‡ºå±‚ï¼‰\n",
    "    gradient_norms = gradient_norms[::-1]\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"å„å±‚æ¢¯åº¦èŒƒæ•°:\")\n",
    "    for i, norm in enumerate(gradient_norms):\n",
    "        print(f\"  ç¬¬{i+1}å±‚: {norm:.6e}\")\n",
    "    \n",
    "    return gradient_norms\n",
    "\n",
    "# å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°\n",
    "np.random.seed(42)\n",
    "\n",
    "sigmoid_grads = demonstrate_vanishing_gradient(n_layers=10, activation='sigmoid')\n",
    "tanh_grads = demonstrate_vanishing_gradient(n_layers=10, activation='tanh')\n",
    "relu_grads = demonstrate_vanishing_gradient(n_layers=10, activation='relu')\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "layers = range(1, len(sigmoid_grads) + 1)\n",
    "plt.plot(layers, sigmoid_grads, 'o-', linewidth=2, markersize=8, label='Sigmoid')\n",
    "plt.plot(layers, tanh_grads, 's-', linewidth=2, markersize=8, label='Tanh')\n",
    "plt.plot(layers, relu_grads, '^-', linewidth=2, markersize=8, label='ReLU')\n",
    "\n",
    "plt.xlabel('å±‚æ•°ï¼ˆä»è¾“å…¥åˆ°è¾“å‡ºï¼‰', fontsize=14)\n",
    "plt.ylabel('æ¢¯åº¦èŒƒæ•°', fontsize=14)\n",
    "plt.title('ä¸åŒæ¿€æ´»å‡½æ•°çš„æ¢¯åº¦èŒƒæ•°å˜åŒ–', fontsize=16, fontweight='bold')\n",
    "plt.yscale('log')  # å¯¹æ•°åæ ‡\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nè§‚å¯Ÿ:\")\n",
    "print(\"1. Sigmoid: æ¢¯åº¦å¿«é€Ÿè¡°å‡ï¼Œå‡ºç°ä¸¥é‡æ¢¯åº¦æ¶ˆå¤±\")\n",
    "print(\"2. Tanh: æ¯” Sigmoid å¥½ï¼Œä½†ä»æœ‰æ¢¯åº¦æ¶ˆå¤±\")\n",
    "print(\"3. ReLU: æ¢¯åº¦ä¿æŒè¾ƒç¨³å®šï¼Œæœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3ï¸âƒ£ å‘é‡åŒ–æ‰¹é‡å®ç°\n",
    "\n",
    "## 3.1 æ‰¹é‡æ•°æ®çš„å‰å‘ä¼ æ’­\n",
    "\n",
    "å®é™…è®­ç»ƒä¸­ä½¿ç”¨ **mini-batch** æé«˜æ•ˆç‡ã€‚\n",
    "\n",
    "### ğŸ“Š å½¢çŠ¶çº¦å®šï¼ˆæ‰¹é‡ï¼‰\n",
    "\n",
    "è®¾ batch size = $B$ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X &\\in \\mathbb{R}^{B \\times n_0} \\quad &\\text{(Bä¸ªæ ·æœ¬)} \\\\\n",
    "Z^{(1)} &= XW^{(1)T} + b^{(1)} \\in \\mathbb{R}^{B \\times n_1} \\quad &\\text{(å¹¿æ’­åç½®)} \\\\\n",
    "A^{(1)} &= \\sigma(Z^{(1)}) \\in \\mathbb{R}^{B \\times n_1} \\quad &\\text{(é€å…ƒç´ )} \\\\\n",
    "Z^{(2)} &= A^{(1)}W^{(2)T} + b^{(2)} \\in \\mathbb{R}^{B \\times n_2} \\\\\n",
    "A^{(2)} &= \\sigma(Z^{(2)}) \\in \\mathbb{R}^{B \\times n_2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 æ‰¹é‡æ•°æ®çš„åå‘ä¼ æ’­\n",
    "\n",
    "æ¢¯åº¦éœ€è¦å¯¹ batch ç»´åº¦æ±‚å¹³å‡ï¼š\n",
    "\n",
    "$$\n",
    "\\Delta^{(2)} = (A^{(2)} - Y) \\odot \\sigma'(Z^{(2)}) \\quad \\in \\mathbb{R}^{B \\times n_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(2)}} = \\frac{1}{B} (\\Delta^{(2)})^T A^{(1)} \\quad \\in \\mathbb{R}^{n_2 \\times n_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{(2)}} = \\frac{1}{B} \\sum_{i=1}^{B} \\delta^{(2)}_i = \\frac{1}{B} \\mathbf{1}^T \\Delta^{(2)} \\quad \\in \\mathbb{R}^{n_2}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $\\mathbf{1} \\in \\mathbb{R}^B$ æ˜¯å…¨1å‘é‡ï¼Œå¯¹åº” `np.sum(..., axis=0)`ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»é›¶å®ç°ï¼šæ”¯æŒæ‰¹é‡æ•°æ®çš„ä¸¤å±‚ç½‘ç»œ\n",
    "\n",
    "class TwoLayerNetworkBatch:\n",
    "    \"\"\"\n",
    "    æ”¯æŒæ‰¹é‡æ•°æ®çš„ä¸¤å±‚ç¥ç»ç½‘ç»œ\n",
    "    \n",
    "    å…³é”®åŒºåˆ«:\n",
    "    - å‰å‘ä¼ æ’­: X @ W^T (çŸ©é˜µä¹˜æ³•)\n",
    "    - åå‘ä¼ æ’­: æ¢¯åº¦éœ€è¦å¯¹ batch ç»´åº¦æ±‚å¹³å‡\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid'):\n",
    "        # å‚æ•°åˆå§‹åŒ–ï¼ˆä¸å•æ ·æœ¬ç‰ˆæœ¬ç›¸åŒï¼‰\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        \n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "        # æ¿€æ´»å‡½æ•°\n",
    "        if activation == 'sigmoid':\n",
    "            self.act_func = sigmoid\n",
    "            self.act_deriv = sigmoid_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.act_func = relu\n",
    "            self.act_deriv = relu_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"æœªçŸ¥æ¿€æ´»å‡½æ•°: {activation}\")\n",
    "        \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        æ‰¹é‡å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "        -----\n",
    "        X : ndarray, shape (B, n0)\n",
    "            æ‰¹é‡è¾“å…¥\n",
    "        \n",
    "        è¿”å›:\n",
    "        -----\n",
    "        A2 : ndarray, shape (B, n2)\n",
    "            æ‰¹é‡è¾“å‡º\n",
    "        \"\"\"\n",
    "        self.cache['X'] = X\n",
    "        \n",
    "        # ç¬¬1å±‚\n",
    "        # Z1 = X @ W1^T + b1\n",
    "        # X: (B, n0), W1^T: (n0, n1) -> Z1: (B, n1)\n",
    "        # b1: (n1,) è‡ªåŠ¨å¹¿æ’­ä¸º (B, n1)\n",
    "        Z1 = np.dot(X, self.W1.T) + self.b1\n",
    "        self.cache['Z1'] = Z1\n",
    "        \n",
    "        A1 = self.act_func(Z1)\n",
    "        self.cache['A1'] = A1\n",
    "        \n",
    "        # ç¬¬2å±‚\n",
    "        Z2 = np.dot(A1, self.W2.T) + self.b2\n",
    "        self.cache['Z2'] = Z2\n",
    "        \n",
    "        A2 = self.act_func(Z2)\n",
    "        self.cache['A2'] = A2\n",
    "        \n",
    "        return A2\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        \"\"\"\n",
    "        æ‰¹é‡åå‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "        -----\n",
    "        Y : ndarray, shape (B, n2)\n",
    "            æ‰¹é‡ç›®æ ‡\n",
    "        \n",
    "        è¿”å›:\n",
    "        -----\n",
    "        grads : dict\n",
    "            å‚æ•°æ¢¯åº¦ï¼ˆå·²æ±‚å¹³å‡ï¼‰\n",
    "        \"\"\"\n",
    "        X = self.cache['X']\n",
    "        Z1 = self.cache['Z1']\n",
    "        A1 = self.cache['A1']\n",
    "        Z2 = self.cache['Z2']\n",
    "        A2 = self.cache['A2']\n",
    "        \n",
    "        B = X.shape[0]  # batch size\n",
    "        \n",
    "        # ç¬¬2å±‚åå‘ä¼ æ’­\n",
    "        # Î”2 = (A2 - Y) âŠ™ Ïƒ'(Z2)  shape: (B, n2)\n",
    "        Delta2 = (A2 - Y) * self.act_deriv(Z2)\n",
    "        \n",
    "        # âˆ‚L/âˆ‚W2 = (1/B) * Î”2^T @ A1\n",
    "        # Î”2^T: (n2, B), A1: (B, n1) -> (n2, n1)\n",
    "        grad_W2 = (1/B) * np.dot(Delta2.T, A1)\n",
    "        \n",
    "        # âˆ‚L/âˆ‚b2 = (1/B) * sum(Î”2, axis=0)\n",
    "        # å¯¹ batch ç»´åº¦æ±‚å’Œ  shape: (n2,)\n",
    "        grad_b2 = (1/B) * np.sum(Delta2, axis=0)\n",
    "        \n",
    "        # ç¬¬1å±‚åå‘ä¼ æ’­\n",
    "        # Î”1 = (Î”2 @ W2) âŠ™ Ïƒ'(Z1)\n",
    "        # Î”2: (B, n2), W2: (n2, n1) -> (B, n1)\n",
    "        Delta1 = np.dot(Delta2, self.W2) * self.act_deriv(Z1)\n",
    "        \n",
    "        # âˆ‚L/âˆ‚W1 = (1/B) * Î”1^T @ X\n",
    "        grad_W1 = (1/B) * np.dot(Delta1.T, X)\n",
    "        \n",
    "        # âˆ‚L/âˆ‚b1 = (1/B) * sum(Î”1, axis=0)\n",
    "        grad_b1 = (1/B) * np.sum(Delta1, axis=0)\n",
    "        \n",
    "        return {\n",
    "            'W2': grad_W2,\n",
    "            'b2': grad_b2,\n",
    "            'W1': grad_W1,\n",
    "            'b1': grad_b1\n",
    "        }\n",
    "    \n",
    "    def compute_loss(self, Y):\n",
    "        \"\"\"è®¡ç®—æ‰¹é‡ MSE æŸå¤±\"\"\"\n",
    "        A2 = self.cache['A2']\n",
    "        return 0.5 * np.mean(np.sum((A2 - Y) ** 2, axis=1))\n",
    "\n",
    "# æµ‹è¯•æ‰¹é‡ç½‘ç»œ\n",
    "print(\"=\" * 60)\n",
    "print(\"æ‰¹é‡ä¸¤å±‚ç½‘ç»œç¤ºä¾‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# åˆ›å»ºç½‘ç»œ\n",
    "net_batch = TwoLayerNetworkBatch(input_size=3, hidden_size=4, output_size=2, activation='sigmoid')\n",
    "\n",
    "# æ‰¹é‡æ•°æ® (batch_size=5)\n",
    "X_batch = np.random.randn(5, 3)\n",
    "Y_batch = np.random.randn(5, 2)\n",
    "\n",
    "print(f\"\\nè¾“å…¥å½¢çŠ¶: {X_batch.shape}\")\n",
    "print(f\"ç›®æ ‡å½¢çŠ¶: {Y_batch.shape}\")\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "A2_batch = net_batch.forward(X_batch)\n",
    "loss_batch = net_batch.compute_loss(Y_batch)\n",
    "\n",
    "print(f\"\\nè¾“å‡ºå½¢çŠ¶: {A2_batch.shape}\")\n",
    "print(f\"æŸå¤±: {loss_batch:.6f}\")\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "grads_batch = net_batch.backward(Y_batch)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æ¢¯åº¦å½¢çŠ¶éªŒè¯\")\n",
    "print(\"=\"*60)\n",
    "print(f\"W1: {net_batch.W1.shape} -> âˆ‚L/âˆ‚W1: {grads_batch['W1'].shape} âœ…\")\n",
    "print(f\"b1: {net_batch.b1.shape} -> âˆ‚L/âˆ‚b1: {grads_batch['b1'].shape} âœ…\")\n",
    "print(f\"W2: {net_batch.W2.shape} -> âˆ‚L/âˆ‚W2: {grads_batch['W2'].shape} âœ…\")\n",
    "print(f\"b2: {net_batch.b2.shape} -> âˆ‚L/âˆ‚b2: {grads_batch['b2'].shape} âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.3 å®Œæ•´è®­ç»ƒç¤ºä¾‹ï¼šæ‹Ÿåˆéçº¿æ€§å‡½æ•°\n",
    "\n",
    "ä½¿ç”¨ä¸¤å±‚ç½‘ç»œæ‹Ÿåˆ $y = \\sin(x)$ï¼Œå±•ç¤ºå®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´ç¤ºä¾‹ï¼šè®­ç»ƒä¸¤å±‚ç½‘ç»œæ‹Ÿåˆ sin(x)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"è®­ç»ƒä¸¤å±‚ç½‘ç»œæ‹Ÿåˆ sin(x)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X_train = np.linspace(-np.pi, np.pi, n_samples).reshape(-1, 1)  # (200, 1)\n",
    "y_train = np.sin(X_train) + 0.1 * np.random.randn(n_samples, 1)  # æ·»åŠ å™ªå£°\n",
    "\n",
    "# åˆ›å»ºç½‘ç»œ: 1 -> 10 -> 1\n",
    "net = TwoLayerNetworkBatch(input_size=1, hidden_size=10, output_size=1, activation='sigmoid')\n",
    "\n",
    "# è¶…å‚æ•°\n",
    "learning_rate = 0.5\n",
    "n_epochs = 2000\n",
    "batch_size = 32\n",
    "\n",
    "# è®­ç»ƒè®°å½•\n",
    "losses = []\n",
    "\n",
    "print(f\"\\næ•°æ®é›†å¤§å°: {n_samples}\")\n",
    "print(f\"ç½‘ç»œç»“æ„: {net.W1.shape[1]} -> {net.W1.shape[0]} -> {net.W2.shape[0]}\")\n",
    "print(f\"å­¦ä¹ ç‡: {learning_rate}\")\n",
    "print(f\"è®­ç»ƒè½®æ•°: {n_epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(\"\\nå¼€å§‹è®­ç»ƒ...\\n\")\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯\n",
    "for epoch in range(n_epochs):\n",
    "    # éšæœºæ‰“ä¹±æ•°æ®\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X_shuffled = X_train[indices]\n",
    "    y_shuffled = y_train[indices]\n",
    "    \n",
    "    # Mini-batch è®­ç»ƒ\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        X_batch = X_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        net.forward(X_batch)\n",
    "        \n",
    "        # åå‘ä¼ æ’­\n",
    "        grads = net.backward(y_batch)\n",
    "        \n",
    "        # æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°\n",
    "        net.W1 -= learning_rate * grads['W1']\n",
    "        net.b1 -= learning_rate * grads['b1']\n",
    "        net.W2 -= learning_rate * grads['W2']\n",
    "        net.b2 -= learning_rate * grads['b2']\n",
    "    \n",
    "    # è®¡ç®—æ•´ä¸ªè®­ç»ƒé›†çš„æŸå¤±\n",
    "    net.forward(X_train)\n",
    "    loss = net.compute_loss(y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss:.6f}\")\n",
    "\n",
    "print(\"\\nè®­ç»ƒå®Œæˆï¼\")\n",
    "print(f\"æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è®­ç»ƒç»“æœ\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# å·¦å›¾ï¼šè®­ç»ƒæ›²çº¿\n",
    "axes[0].plot(losses, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=14)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=14)\n",
    "axes[0].set_title('è®­ç»ƒæ›²çº¿', fontsize=16, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# å³å›¾ï¼šæ‹Ÿåˆç»“æœ\n",
    "y_pred = net.forward(X_train)\n",
    "\n",
    "axes[1].scatter(X_train, y_train, alpha=0.4, s=15, label='è®­ç»ƒæ•°æ®ï¼ˆå¸¦å™ªå£°ï¼‰', color='gray')\n",
    "axes[1].plot(X_train, y_pred, 'r-', linewidth=3, label='ä¸¤å±‚ç½‘ç»œæ‹Ÿåˆ')\n",
    "axes[1].plot(X_train, np.sin(X_train), 'g--', linewidth=2, label='çœŸå®å‡½æ•° sin(x)')\n",
    "axes[1].set_xlabel('x', fontsize=14)\n",
    "axes[1].set_ylabel('y', fontsize=14)\n",
    "axes[1].set_title('æ‹Ÿåˆç»“æœï¼ˆéçº¿æ€§æ¿€æ´»å‡½æ•°æˆåŠŸæ‹Ÿåˆï¼‰', fontsize=16, fontweight='bold')\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… æˆåŠŸï¼ä¸¤å±‚ç½‘ç»œé€šè¿‡åå‘ä¼ æ’­å­¦ä¹ åˆ°äº†éçº¿æ€§å‡½æ•°\")\n",
    "print(\"   è¿™æ­£æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒèƒ½åŠ›ï¼šé€šè¿‡å¤šå±‚éçº¿æ€§å˜æ¢é€¼è¿‘å¤æ‚å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ å°ç»“ä¸ä¸‹ä¸€æ­¥\n",
    "\n",
    "## âœ… æœ¬æ•™ç¨‹ä½ å­¦åˆ°äº†ä»€ä¹ˆ\n",
    "\n",
    "### 1. å•å±‚ç½‘ç»œåå‘ä¼ æ’­\n",
    "- å®Œæ•´æ¨å¯¼äº†æ¢¯åº¦å…¬å¼\n",
    "- ç†è§£äº†è¯¯å·®å‘é‡ $\\delta$ çš„å«ä¹‰\n",
    "- æŒæ¡äº† $\\frac{\\partial L}{\\partial W} = \\delta \\mathbf{x}^T$ çš„æ ¸å¿ƒå…¬å¼\n",
    "- å®ç°äº†å¸¸è§æ¿€æ´»å‡½æ•°çš„å¯¼æ•°\n",
    "\n",
    "### 2. å¤šå±‚ç½‘ç»œé“¾å¼æ³•åˆ™\n",
    "- è¯¯å·®çš„é€å±‚åå‘ä¼ æ’­æœºåˆ¶\n",
    "- ç†è§£äº†æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸çš„æ•°å­¦æ ¹æº\n",
    "- å¯¹æ¯”äº†ä¸åŒæ¿€æ´»å‡½æ•°å¯¹æ¢¯åº¦çš„å½±å“\n",
    "- ä»é›¶å®ç°äº†å®Œæ•´çš„ä¸¤å±‚ç½‘ç»œ\n",
    "\n",
    "### 3. å‘é‡åŒ–æ‰¹é‡å®ç°\n",
    "- æ‰¹é‡æ•°æ®çš„å½¢çŠ¶å¤„ç†æŠ€å·§\n",
    "- å¹¿æ’­æœºåˆ¶åœ¨æ¢¯åº¦è®¡ç®—ä¸­çš„åº”ç”¨\n",
    "- å®Œæ•´çš„è®­ç»ƒå¾ªç¯å®ç°\n",
    "- æˆåŠŸæ‹Ÿåˆéçº¿æ€§å‡½æ•°\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ \n",
    "\n",
    "ç°åœ¨ä½ å·²ç»æŒæ¡äº†åå‘ä¼ æ’­çš„æ•°å­¦åŸç†ï¼Œå¯ä»¥å¼€å§‹å­¦ä¹ ç¥ç»ç½‘ç»œçš„å®é™…åº”ç”¨ï¼š\n",
    "\n",
    "- **neural_networks/01_perceptron_and_history.ipynb** - ç¥ç»ç½‘ç»œçš„å†å²ä¸æ„ŸçŸ¥å™¨\n",
    "- **neural_networks/02_mlp_and_forward_propagation.ipynb** - å¤šå±‚æ„ŸçŸ¥å™¨è¯¦è§£\n",
    "- **neural_networks/03_backpropagation_algorithm.ipynb** - åå‘ä¼ æ’­ç®—æ³•çš„å·¥ç¨‹å®ç°\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ ç»ƒä¹ é¢˜\n",
    "\n",
    "### ç»ƒä¹  1ï¼šä¸‰å±‚ç½‘ç»œ\n",
    "æ‰©å±• `TwoLayerNetworkBatch` ç±»ï¼Œå®ç°ä¸‰å±‚ç½‘ç»œï¼ˆ2ä¸ªéšè—å±‚ï¼‰ã€‚\n",
    "\n",
    "### ç»ƒä¹  2ï¼šSoftmax + äº¤å‰ç†µ\n",
    "æ¨å¯¼å¹¶å®ç° Softmax æ¿€æ´»å‡½æ•°å’Œäº¤å‰ç†µæŸå¤±çš„åå‘ä¼ æ’­ã€‚\n",
    "\n",
    "### ç»ƒä¹  3ï¼šæ¢¯åº¦è£å‰ª\n",
    "åœ¨è®­ç»ƒå¾ªç¯ä¸­å®ç°æ¢¯åº¦è£å‰ªï¼Œè§‚å¯Ÿå¯¹æ¢¯åº¦çˆ†ç‚¸çš„ç¼“è§£æ•ˆæœã€‚\n",
    "\n",
    "### ç»ƒä¹  4ï¼šæ‰¹æ ‡å‡†åŒ–ï¼ˆæŒ‘æˆ˜ï¼‰\n",
    "æ¨å¯¼æ‰¹æ ‡å‡†åŒ–å±‚çš„åå‘ä¼ æ’­å…¬å¼ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å‚è€ƒèµ„æ–™\n",
    "\n",
    "- [Backpropagation Algorithm - CS231n](http://cs231n.github.io/optimization-2/)\n",
    "- [3Blue1Brown: åå‘ä¼ æ’­å¯è§†åŒ–](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "- ã€Šæ·±åº¦å­¦ä¹ ã€‹Goodfellow - ç¬¬6ç« \n",
    "- ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹é‚±é”¡é¹ - ç¬¬3ç« \n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œä½ å®Œæˆåå‘ä¼ æ’­çš„æ•°å­¦æ¨å¯¼ï¼ğŸ‰**\n",
    "\n",
    "ä½ ç°åœ¨ç†è§£äº†æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ ¸å¿ƒæœºåˆ¶ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
