# ğŸ“Š è¯„ä¼°æŒ‡æ ‡è®¡ç®—æŒ‡å—

> **å®šä½**ï¼šæŒ‡æ ‡çš„è®¡ç®—æ–¹æ³•å’Œä»£ç å®ç°é€ŸæŸ¥æ‰‹å†Œ
>
> **å‰ç½®æ¡ä»¶**ï¼šå·²åœ¨ [02_é—®é¢˜å®šä¹‰æŒ‡å—](../02_problem_definition_guide/metrics_selection_guide.md) ä¸­é€‰å®šè¯„ä¼°æŒ‡æ ‡

---

## ğŸ“Œ æœ¬æ–‡æ¡£å†…å®¹

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. æŒ‡æ ‡å…¬å¼ä¸sklearnä»£ç é€ŸæŸ¥
2. å¤šæŒ‡æ ‡æ‰¹é‡è®¡ç®—
3. è‡ªå®šä¹‰æŒ‡æ ‡å®ç°
4. è¯„ä¼°ç»“æœå¯è§†åŒ–

**ä¸åŒ…å«ï¼š** æŒ‡æ ‡é€‰æ‹©å†³ç­–ï¼ˆå‚è€ƒ 02_é—®é¢˜å®šä¹‰æŒ‡å—ï¼‰

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå‚è€ƒé€ŸæŸ¥è¡¨](#å¿«é€Ÿå‚è€ƒé€ŸæŸ¥è¡¨) â­
2. [å›å½’é—®é¢˜æŒ‡æ ‡](#å›å½’é—®é¢˜æŒ‡æ ‡)
3. [åˆ†ç±»é—®é¢˜æŒ‡æ ‡](#åˆ†ç±»é—®é¢˜æŒ‡æ ‡)
4. [å¤šæŒ‡æ ‡åŒæ—¶è®¡ç®—](#å¤šæŒ‡æ ‡åŒæ—¶è®¡ç®—)
5. [è‡ªå®šä¹‰æŒ‡æ ‡å®ç°](#è‡ªå®šä¹‰æŒ‡æ ‡å®ç°)
6. [è¯„ä¼°ç»“æœå¯è§†åŒ–](#è¯„ä¼°ç»“æœå¯è§†åŒ–)
7. [ä»£ç æ¨¡å—ä½¿ç”¨](#ä»£ç æ¨¡å—ä½¿ç”¨)

---

## å¿«é€Ÿå‚è€ƒé€ŸæŸ¥è¡¨

### å›å½’æŒ‡æ ‡é€ŸæŸ¥è¡¨

| æŒ‡æ ‡ | å…¬å¼ | sklearnä»£ç  | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------------|---------|------|------|
| **MAE** | mean(\|y-Å·\|) | `mean_absolute_error()` | é»˜è®¤é€‰æ‹©ã€æœ‰å¼‚å¸¸å€¼ | æ˜“è§£é‡Šã€å¯¹å¼‚å¸¸å€¼ç¨³å¥ | ä¸åŒºåˆ†å¤§å°è¯¯å·® |
| **RMSE** | âˆšmean((y-Å·)Â²) | `mean_squared_error(squared=False)` | å…³æ³¨å¤§è¯¯å·®ã€æ— å¼‚å¸¸å€¼ | æƒ©ç½šå¤§è¯¯å·® | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ |
| **MAPE** | mean(\|y-Å·\|/y)Ã—100% | `mean_absolute_percentage_error()` | éœ€è¦ç›¸å¯¹è¯¯å·® | ç™¾åˆ†æ¯”å½¢å¼æ˜“ç†è§£ | y=0æ—¶æ— å®šä¹‰ |
| **RÂ²** | 1 - SS_res/SS_tot | `r2_score()` | è¯„ä¼°æ‹Ÿåˆä¼˜åº¦ | å½’ä¸€åŒ–ï¼ˆ0-1ï¼‰ | ç‰¹å¾è¶Šå¤šè¶Šé«˜ |

**å…¸å‹é˜ˆå€¼**ï¼ˆä¾›å‚è€ƒï¼‰ï¼š
- MAE/RMSEï¼šå–å†³äºç›®æ ‡å˜é‡çš„å°ºåº¦ï¼Œä¸€èˆ¬<10%çš„å¹³å‡å€¼ä¸ºè‰¯å¥½
- MAPEï¼š<10%ä¼˜ç§€ï¼Œ10-20%è‰¯å¥½ï¼Œ>20%éœ€æ”¹è¿›
- RÂ²ï¼š>0.7è‰¯å¥½ï¼Œ>0.8ä¼˜ç§€

### åˆ†ç±»æŒ‡æ ‡é€ŸæŸ¥è¡¨

| æŒ‡æ ‡ | å…¬å¼ | sklearnä»£ç  | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------------|---------|------|------|
| **Accuracy** | (TP+TN)/Total | `accuracy_score()` | ç±»åˆ«å¹³è¡¡ | ç›´è§‚æ˜“æ‡‚ | ä¸å¹³è¡¡æ•°æ®å¤±æ•ˆ |
| **Precision** | TP/(TP+FP) | `precision_score()` | å…³æ³¨è¯¯æŠ¥ä»£ä»· | è¡¡é‡æŸ¥å‡†ç‡ | å¿½ç•¥æ¼æŠ¥ |
| **Recall** | TP/(TP+FN) | `recall_score()` | å…³æ³¨æ¼æŠ¥ä»£ä»· | è¡¡é‡æŸ¥å…¨ç‡ | å¿½ç•¥è¯¯æŠ¥ |
| **F1-Score** | 2PR/(P+R) | `f1_score()` | På’ŒRåŒç­‰é‡è¦ | å¹³è¡¡På’ŒR | ç­‰æƒé‡ä¸çµæ´» |
| **AUC** | ROCæ›²çº¿ä¸‹é¢ç§¯ | `roc_auc_score()` | æ’åºèƒ½åŠ›ã€é˜ˆå€¼ä¼˜åŒ– | ä¸å—é˜ˆå€¼å½±å“ | ä¸å¹³è¡¡æ•°æ®å¯èƒ½è¯¯å¯¼ |

**å…¸å‹é˜ˆå€¼**ï¼ˆä¾›å‚è€ƒï¼‰ï¼š
- Accuracyï¼š>0.85è‰¯å¥½ï¼ˆå¹³è¡¡æ•°æ®ï¼‰
- Precision/Recallï¼šå–å†³äºä¸šåŠ¡ï¼Œé€šå¸¸>0.7ä¸ºå¯æ¥å—
- F1-Scoreï¼š>0.75è‰¯å¥½
- AUCï¼š>0.8è‰¯å¥½ï¼Œ>0.9ä¼˜ç§€ï¼Œ0.5=éšæœº

### æŒ‡æ ‡é€‰æ‹©å¿«é€Ÿå†³ç­–ï¼ˆé€ŸæŸ¥ï¼‰

**å›å½’é—®é¢˜**ï¼š
```
æœ‰å¼‚å¸¸å€¼ï¼Ÿ
â”œâ”€ æ˜¯ â†’ MAEï¼ˆç¨³å¥ï¼‰
â””â”€ å¦ â†’ RMSEï¼ˆæƒ©ç½šå¤§è¯¯å·®ï¼‰

éœ€è¦ç›¸å¯¹è¯¯å·®ï¼Ÿ
â””â”€ æ˜¯ â†’ MAPEï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰

è¯„ä¼°æ¨¡å‹æ‹Ÿåˆåº¦ï¼Ÿ
â””â”€ æ˜¯ â†’ RÂ²ï¼ˆå½’ä¸€åŒ–æŒ‡æ ‡ï¼‰
```

**åˆ†ç±»é—®é¢˜**ï¼š
```
æ•°æ®å¹³è¡¡ï¼Ÿ
â”œâ”€ æ˜¯ â†’ Accuracyï¼ˆç›´è§‚ï¼‰
â””â”€ å¦ â†’ F1-Score æˆ– AUC

è¯¯æŠ¥ä»£ä»·é«˜ï¼Ÿï¼ˆå¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼‰
â””â”€ æ˜¯ â†’ Precisionï¼ˆå®å¯æ¼æŠ¥ï¼Œä¸èƒ½è¯¯æŠ¥ï¼‰

æ¼æŠ¥ä»£ä»·é«˜ï¼Ÿï¼ˆå¦‚ç–¾ç—…æ£€æµ‹ï¼‰
â””â”€ æ˜¯ â†’ Recallï¼ˆå®å¯è¯¯æŠ¥ï¼Œä¸èƒ½æ¼æŠ¥ï¼‰

éœ€è¦é˜ˆå€¼ä¼˜åŒ–ï¼Ÿ
â””â”€ æ˜¯ â†’ AUC + PRæ›²çº¿
```

âš ï¸ **æ³¨æ„**ï¼šå®Œæ•´çš„æŒ‡æ ‡é€‰æ‹©å†³ç­–è¯·å‚è€ƒ [02_é—®é¢˜å®šä¹‰æŒ‡å—/metrics_selection_guide.md](../02_problem_å®šä¹‰_guide/metrics_selection_guide.md)

---

## å›å½’é—®é¢˜æŒ‡æ ‡

### MAE (Mean Absolute Error) - å¹³å‡ç»å¯¹è¯¯å·®

**å…¬å¼**ï¼š
```
MAE = (1/n) Ã— Î£|y_true - y_pred|
```

**å«ä¹‰**ï¼šé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡ç»å¯¹å·®è·

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import mean_absolute_error

# è®¡ç®—MAE
mae = mean_absolute_error(y_true, y_pred)
print(f"MAE: {mae:.2f}")
```

**æ‰‹åŠ¨å®ç°**ï¼š
```python
import numpy as np

def calculate_mae(y_true, y_pred):
    """
    æ‰‹åŠ¨è®¡ç®—MAE

    å‚æ•°ï¼š
        y_true: çœŸå®å€¼ï¼Œshape (n_samples,)
        y_pred: é¢„æµ‹å€¼ï¼Œshape (n_samples,)

    è¿”å›ï¼š
        mae: å¹³å‡ç»å¯¹è¯¯å·®
    """
    return np.mean(np.abs(y_true - y_pred))

# ä½¿ç”¨
mae = calculate_mae(y_true, y_pred)
```

---

### RMSE (Root Mean Squared Error) - å‡æ–¹æ ¹è¯¯å·®

**å…¬å¼**ï¼š
```
MSE = (1/n) Ã— Î£(y_true - y_pred)Â²
RMSE = âˆšMSE
```

**å«ä¹‰**ï¼šè¯¯å·®çš„å¹³æ–¹å¹³å‡åå¼€æ–¹ï¼ˆå•ä½ä¸ç›®æ ‡ä¸€è‡´ï¼‰

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import mean_squared_error
import numpy as np

# è®¡ç®—RMSE
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print(f"RMSE: {rmse:.2f}")

# ä¹Ÿå¯ä»¥è¿™æ ·ï¼ˆsklearnæ–°ç‰ˆæœ¬ï¼‰
rmse = mean_squared_error(y_true, y_pred, squared=False)
```

**æ‰‹åŠ¨å®ç°**ï¼š
```python
def calculate_rmse(y_true, y_pred):
    """æ‰‹åŠ¨è®¡ç®—RMSE"""
    mse = np.mean((y_true - y_pred) ** 2)
    return np.sqrt(mse)
```

---

### MAPE (Mean Absolute Percentage Error) - å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®

**å…¬å¼**ï¼š
```
MAPE = (100%/n) Ã— Î£|y_true - y_pred| / |y_true|
```

**å«ä¹‰**ï¼šç›¸å¯¹è¯¯å·®çš„ç™¾åˆ†æ¯”è¡¨ç¤º

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import mean_absolute_percentage_error

# è®¡ç®—MAPEï¼ˆè¿”å›0-1èŒƒå›´ï¼Œéœ€ä¹˜100è½¬ä¸ºç™¾åˆ†æ¯”ï¼‰
mape = mean_absolute_percentage_error(y_true, y_pred) * 100
print(f"MAPE: {mape:.2f}%")
```

**æ‰‹åŠ¨å®ç°**ï¼š
```python
def calculate_mape(y_true, y_pred):
    """
    æ‰‹åŠ¨è®¡ç®—MAPE

    æ³¨æ„ï¼šy_true ä¸èƒ½åŒ…å«0å€¼
    """
    # è¿‡æ»¤æ‰y_trueä¸º0çš„æ ·æœ¬
    mask = y_true != 0
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
```

---

### RÂ² (R-Squared) - å†³å®šç³»æ•°

**å…¬å¼**ï¼š
```
RÂ² = 1 - (SS_res / SS_tot)
å…¶ä¸­ï¼š
  SS_res = Î£(y_true - y_pred)Â²  # æ®‹å·®å¹³æ–¹å’Œ
  SS_tot = Î£(y_true - y_mean)Â²  # æ€»å¹³æ–¹å’Œ
```

**å«ä¹‰**ï¼šæ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import r2_score

# è®¡ç®—RÂ²
r2 = r2_score(y_true, y_pred)
print(f"RÂ²: {r2:.3f}")
```

**æ‰‹åŠ¨å®ç°**ï¼š
```python
def calculate_r2(y_true, y_pred):
    """æ‰‹åŠ¨è®¡ç®—RÂ²"""
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    return 1 - (ss_res / ss_tot)
```

---

### å›å½’æŒ‡æ ‡å¿«é€Ÿå¯¹æ¯”

| æŒ‡æ ‡ | å…¬å¼ | å•ä½ | å¯¹å¼‚å¸¸å€¼ | sklearnå‡½æ•° |
|------|------|------|---------|-------------|
| MAE | mean(\|y - Å·\|) | ä¸yç›¸åŒ | ç¨³å¥ | `mean_absolute_error()` |
| RMSE | âˆšmean((y - Å·)Â²) | ä¸yç›¸åŒ | æ•æ„Ÿ | `mean_squared_error(squared=False)` |
| MAPE | mean(\|y - Å·\|/y)Ã—100% | ç™¾åˆ†æ¯” | æ•æ„Ÿ | `mean_absolute_percentage_error()` |
| RÂ² | 1 - SS_res/SS_tot | æ— é‡çº² | ä¸­ç­‰ | `r2_score()` |

---

## åˆ†ç±»é—®é¢˜æŒ‡æ ‡

### æ··æ·†çŸ©é˜µ (Confusion Matrix)

**åŸºç¡€æ¦‚å¿µ**ï¼š
```
              é¢„æµ‹ä¸ºæ­£    é¢„æµ‹ä¸ºè´Ÿ
å®é™…ä¸ºæ­£        TP         FN
              (çœŸæ­£ä¾‹)   (å‡è´Ÿä¾‹)
å®é™…ä¸ºè´Ÿ        FP         TN
              (å‡æ­£ä¾‹)   (çœŸè´Ÿä¾‹)
```

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import confusion_matrix

# è®¡ç®—æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, y_pred)
print(cm)
# [[TN, FP],
#  [FN, TP]]

# æå–å„å€¼
tn, fp, fn, tp = cm.ravel()
```

---

### Accuracy - å‡†ç¡®ç‡

**å…¬å¼**ï¼š
```
Accuracy = (TP + TN) / (TP + FP + FN + TN)
```

**å«ä¹‰**ï¼šæ‰€æœ‰é¢„æµ‹ä¸­æ­£ç¡®çš„æ¯”ä¾‹

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy:.3f}")
```

**ä»æ··æ·†çŸ©é˜µè®¡ç®—**ï¼š
```python
def calculate_accuracy(cm):
    """ä»æ··æ·†çŸ©é˜µè®¡ç®—å‡†ç¡®ç‡"""
    return (cm[0, 0] + cm[1, 1]) / cm.sum()
```

---

### Precision - ç²¾ç¡®ç‡

**å…¬å¼**ï¼š
```
Precision = TP / (TP + FP)
```

**å«ä¹‰**ï¼š"é¢„æµ‹ä¸ºæ­£"çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£ä¸ºæ­£çš„æ¯”ä¾‹

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import precision_score

precision = precision_score(y_true, y_pred)
print(f"Precision: {precision:.3f}")

# å¤šåˆ†ç±»ï¼ˆéœ€æŒ‡å®šaverageå‚æ•°ï¼‰
precision = precision_score(y_true, y_pred, average='macro')  # å®å¹³å‡
precision = precision_score(y_true, y_pred, average='weighted')  # åŠ æƒå¹³å‡
```

---

### Recall - å¬å›ç‡

**å…¬å¼**ï¼š
```
Recall = TP / (TP + FN)
```

**å«ä¹‰**ï¼š"å®é™…ä¸ºæ­£"çš„æ ·æœ¬ä¸­ï¼Œè¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import recall_score

recall = recall_score(y_true, y_pred)
print(f"Recall: {recall:.3f}")
```

---

### F1-Score - F1åˆ†æ•°

**å…¬å¼**ï¼š
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**å«ä¹‰**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import f1_score

f1 = f1_score(y_true, y_pred)
print(f"F1-Score: {f1:.3f}")

# å¤šåˆ†ç±»
f1_macro = f1_score(y_true, y_pred, average='macro')
f1_weighted = f1_score(y_true, y_pred, average='weighted')
```

---

### AUC (Area Under ROC Curve) - ROCæ›²çº¿ä¸‹é¢ç§¯

**å«ä¹‰**ï¼šROCæ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œåº¦é‡æ’åºèƒ½åŠ›

**sklearn å®ç°**ï¼š
```python
from sklearn.metrics import roc_auc_score

# éœ€è¦æ¦‚ç‡é¢„æµ‹å€¼ï¼ˆä¸æ˜¯0/1é¢„æµ‹ï¼‰
auc = roc_auc_score(y_true, y_pred_proba)
print(f"AUC: {auc:.3f}")

# å¤šåˆ†ç±»ï¼ˆéœ€è¦æŒ‡å®šmulti_classå‚æ•°ï¼‰
auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')  # One-vs-Rest
```

---

### åˆ†ç±»æŒ‡æ ‡å¿«é€Ÿå¯¹æ¯”

| æŒ‡æ ‡ | å…¬å¼ | å…³æ³¨ç‚¹ | sklearnå‡½æ•° |
|------|------|--------|-------------|
| Accuracy | (TP+TN)/Total | æ•´ä½“æ­£ç¡®ç‡ | `accuracy_score()` |
| Precision | TP/(TP+FP) | é¢„æµ‹ä¸ºæ­£ä¸­çš„å‡†ç¡®æ€§ | `precision_score()` |
| Recall | TP/(TP+FN) | çœŸæ­£ä¾‹çš„è¦†ç›–ç‡ | `recall_score()` |
| F1-Score | 2PR/(P+R) | På’ŒRçš„è°ƒå’Œå¹³å‡ | `f1_score()` |
| AUC | ROCæ›²çº¿ä¸‹é¢ç§¯ | æ’åºèƒ½åŠ› | `roc_auc_score()` |

---

## å¤šæŒ‡æ ‡åŒæ—¶è®¡ç®—

### ä½¿ç”¨ classification_report

**ä¸€æ¬¡æ€§è®¡ç®—å¤šä¸ªåˆ†ç±»æŒ‡æ ‡ï¼š**
```python
from sklearn.metrics import classification_report

# ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
report = classification_report(y_true, y_pred)
print(report)

# è¾“å‡ºç¤ºä¾‹ï¼š
#               precision    recall  f1-score   support
#            0       0.85      0.92      0.88      1000
#            1       0.78      0.65      0.71       500
#     accuracy                           0.83      1500
#    macro avg       0.82      0.79      0.80      1500
# weighted avg       0.83      0.83      0.83      1500

# è·å–å­—å…¸æ ¼å¼
report_dict = classification_report(y_true, y_pred, output_dict=True)
print(f"Class 1 F1-Score: {report_dict['1']['f1-score']:.3f}")
```

---

### è‡ªå®šä¹‰å¤šæŒ‡æ ‡å‡½æ•°ï¼ˆå›å½’ï¼‰

```python
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    mean_absolute_percentage_error
)
import numpy as np

def evaluate_regression(y_true, y_pred, metrics=None):
    """
    ä¸€æ¬¡æ€§è®¡ç®—å¤šä¸ªå›å½’æŒ‡æ ‡

    å‚æ•°ï¼š
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        metrics: è¦è®¡ç®—çš„æŒ‡æ ‡åˆ—è¡¨ï¼Œé»˜è®¤è®¡ç®—æ‰€æœ‰

    è¿”å›ï¼š
        results: å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰æŒ‡æ ‡
    """
    if metrics is None:
        metrics = ['mae', 'rmse', 'r2', 'mape']

    results = {}

    if 'mae' in metrics:
        results['mae'] = mean_absolute_error(y_true, y_pred)

    if 'rmse' in metrics:
        results['rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))

    if 'r2' in metrics:
        results['r2'] = r2_score(y_true, y_pred)

    if 'mape' in metrics:
        # è¿‡æ»¤0å€¼
        mask = y_true != 0
        if mask.any():
            results['mape'] = mean_absolute_percentage_error(
                y_true[mask], y_pred[mask]
            ) * 100
        else:
            results['mape'] = np.nan

    return results

# ä½¿ç”¨ç¤ºä¾‹
metrics = evaluate_regression(y_true, y_pred)
print(f"MAE:  {metrics['mae']:.2f}")
print(f"RMSE: {metrics['rmse']:.2f}")
print(f"RÂ²:   {metrics['r2']:.3f}")
print(f"MAPE: {metrics['mape']:.2f}%")
```

---

### è‡ªå®šä¹‰å¤šæŒ‡æ ‡å‡½æ•°ï¼ˆåˆ†ç±»ï¼‰

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)

def evaluate_classification(y_true, y_pred, y_pred_proba=None, metrics=None):
    """
    ä¸€æ¬¡æ€§è®¡ç®—å¤šä¸ªåˆ†ç±»æŒ‡æ ‡

    å‚æ•°ï¼š
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾ï¼ˆ0/1ï¼‰
        y_pred_proba: é¢„æµ‹æ¦‚ç‡ï¼ˆè®¡ç®—AUCéœ€è¦ï¼‰
        metrics: è¦è®¡ç®—çš„æŒ‡æ ‡åˆ—è¡¨ï¼Œé»˜è®¤è®¡ç®—æ‰€æœ‰

    è¿”å›ï¼š
        results: å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰æŒ‡æ ‡
    """
    if metrics is None:
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        if y_pred_proba is not None:
            metrics.append('auc')

    results = {}

    if 'accuracy' in metrics:
        results['accuracy'] = accuracy_score(y_true, y_pred)

    if 'precision' in metrics:
        results['precision'] = precision_score(y_true, y_pred)

    if 'recall' in metrics:
        results['recall'] = recall_score(y_true, y_pred)

    if 'f1' in metrics:
        results['f1'] = f1_score(y_true, y_pred)

    if 'auc' in metrics and y_pred_proba is not None:
        results['auc'] = roc_auc_score(y_true, y_pred_proba)

    return results

# ä½¿ç”¨ç¤ºä¾‹
metrics = evaluate_classification(y_true, y_pred, y_pred_proba)
print(f"Accuracy:  {metrics['accuracy']:.3f}")
print(f"Precision: {metrics['precision']:.3f}")
print(f"Recall:    {metrics['recall']:.3f}")
print(f"F1-Score:  {metrics['f1']:.3f}")
print(f"AUC:       {metrics['auc']:.3f}")
```

---

## è‡ªå®šä¹‰æŒ‡æ ‡å®ç°

### è‡ªå®šä¹‰å›å½’æŒ‡æ ‡

**ç¤ºä¾‹ï¼šWMAPEï¼ˆåŠ æƒMAPEï¼‰**
```python
def weighted_mape(y_true, y_pred, weights=None):
    """
    åŠ æƒMAPE - ä¸åŒæ ·æœ¬çš„è¯¯å·®æœ‰ä¸åŒæƒé‡

    å‚æ•°ï¼š
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        weights: æƒé‡ï¼ˆé»˜è®¤ä¸ºNoneï¼Œè¡¨ç¤ºç­‰æƒé‡ï¼‰

    è¿”å›ï¼š
        wmape: åŠ æƒMAPEï¼ˆç™¾åˆ†æ¯”ï¼‰
    """
    if weights is None:
        weights = np.ones_like(y_true)

    # è¿‡æ»¤0å€¼
    mask = y_true != 0
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    weights = weights[mask]

    # è®¡ç®—åŠ æƒMAPE
    weighted_errors = weights * np.abs((y_true - y_pred) / y_true)
    wmape = np.sum(weighted_errors) / np.sum(weights) * 100

    return wmape

# ä½¿ç”¨ç¤ºä¾‹
weights = np.array([1, 1, 2, 2, 3])  # åé¢çš„æ ·æœ¬æƒé‡æ›´å¤§
wmape = weighted_mape(y_true, y_pred, weights)
print(f"Weighted MAPE: {wmape:.2f}%")
```

---

### è‡ªå®šä¹‰åˆ†ç±»æŒ‡æ ‡

**ç¤ºä¾‹ï¼šPrecision@K**
```python
def precision_at_k(y_true, y_pred_proba, k=100):
    """
    Precision@K - Top Kä¸ªé¢„æµ‹ä¸­çš„ç²¾ç¡®ç‡

    é€‚ç”¨åœºæ™¯ï¼šèµ„æºæœ‰é™ï¼Œåªèƒ½å¤„ç†Top Kä¸ªæ ·æœ¬

    å‚æ•°ï¼š
        y_true: çœŸå®æ ‡ç­¾
        y_pred_proba: é¢„æµ‹æ¦‚ç‡
        k: å–Top Kä¸ªæ ·æœ¬

    è¿”å›ï¼š
        precision: Precision@K
    """
    # æŒ‰é¢„æµ‹æ¦‚ç‡æ’åºï¼Œå–Top K
    top_k_indices = np.argsort(y_pred_proba)[-k:]

    # è®¡ç®—Top Kä¸­çš„æ­£ä¾‹æ•°é‡
    true_positives = np.sum(y_true[top_k_indices])

    # Precision@K
    precision = true_positives / k

    return precision

# ä½¿ç”¨ç¤ºä¾‹
k = 100  # æ¯æœˆåªèƒ½è”ç³»100ä¸ªå®¢æˆ·
precision_k = precision_at_k(y_true, y_pred_proba, k=k)
print(f"Precision@{k}: {precision_k:.3f}")
print(f"Top {k}ä¸­æœ‰ {int(precision_k * k)} ä¸ªçœŸæµå¤±å®¢æˆ·")
```

---

## è¯„ä¼°ç»“æœå¯è§†åŒ–

### ROCæ›²çº¿

```python
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

def plot_roc_curve(y_true, y_pred_proba, title='ROC Curve'):
    """
    ç»˜åˆ¶ROCæ›²çº¿

    å‚æ•°ï¼š
        y_true: çœŸå®æ ‡ç­¾
        y_pred_proba: é¢„æµ‹æ¦‚ç‡ï¼ˆæ­£ç±»çš„æ¦‚ç‡ï¼‰
        title: å›¾è¡¨æ ‡é¢˜
    """
    # è®¡ç®—ROCæ›²çº¿
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)

    # ç»˜åˆ¶
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
             label='Random (AUC = 0.500)')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
plot_roc_curve(y_true, y_pred_proba)
```

---

### æ··æ·†çŸ©é˜µå¯è§†åŒ–

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def plot_confusion_matrix(y_true, y_pred, labels=None, title='Confusion Matrix'):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾

    å‚æ•°ï¼š
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        labels: ç±»åˆ«æ ‡ç­¾ï¼ˆå¦‚['Not Churn', 'Churn']ï¼‰
        title: å›¾è¡¨æ ‡é¢˜
    """
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)

    fig, ax = plt.subplots(figsize=(8, 6))
    disp.plot(ax=ax, cmap='Blues', values_format='d')
    plt.title(title)
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
plot_confusion_matrix(y_true, y_pred, labels=['Not Churn', 'Churn'])
```

---

### PRæ›²çº¿ï¼ˆPrecision-Recallï¼‰

```python
from sklearn.metrics import precision_recall_curve, average_precision_score

def plot_pr_curve(y_true, y_pred_proba, title='Precision-Recall Curve'):
    """
    ç»˜åˆ¶PRæ›²çº¿

    é€‚ç”¨äºä¸å¹³è¡¡æ•°æ®ï¼ˆæ¯”ROCæ›´æ•æ„Ÿï¼‰
    """
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)
    avg_precision = average_precision_score(y_true, y_pred_proba)

    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='darkorange', lw=2,
             label=f'PR curve (AP = {avg_precision:.3f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(title)
    plt.legend(loc="lower left")
    plt.grid(alpha=0.3)
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
plot_pr_curve(y_true, y_pred_proba)
```

---

## ä»£ç æ¨¡å—ä½¿ç”¨

### ä½¿ç”¨ src/model_evaluation.py

æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„è¯„ä¼°æ¨¡å—ï¼ˆ538è¡Œï¼‰ï¼ŒåŒ…å«ä¸Šè¿°æ‰€æœ‰åŠŸèƒ½ã€‚

**å›å½’è¯„ä¼°ç¤ºä¾‹ï¼š**
```python
from src import model_evaluation

# æ–¹æ³•1ï¼šä½¿ç”¨ä¾¿æ·å‡½æ•°
metrics = model_evaluation.evaluate_regression(
    y_true, y_pred,
    metrics=['mae', 'rmse', 'r2', 'mape']
)

print("å›å½’è¯„ä¼°ç»“æœï¼š")
for metric, value in metrics.items():
    print(f"  {metric.upper()}: {value:.3f}")
```

**åˆ†ç±»è¯„ä¼°ç¤ºä¾‹ï¼š**
```python
# æ–¹æ³•2ï¼šä½¿ç”¨ä¾¿æ·å‡½æ•°
metrics = model_evaluation.evaluate_classification(
    y_true, y_pred, y_pred_proba,
    metrics=['accuracy', 'precision', 'recall', 'f1', 'auc']
)

print("åˆ†ç±»è¯„ä¼°ç»“æœï¼š")
for metric, value in metrics.items():
    print(f"  {metric.capitalize()}: {value:.3f}")

# ç»˜åˆ¶å¯è§†åŒ–
model_evaluation.plot_roc_curve(y_true, y_pred_proba)
model_evaluation.plot_confusion_matrix(y_true, y_pred)
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **æŒ‡æ ‡é€‰æ‹©å†³ç­–**ï¼š[02_é—®é¢˜å®šä¹‰æŒ‡å—/metrics_selection_guide.md](../02_problem_definition_guide/metrics_selection_guide.md)
- **æ¨¡å‹æ¯”è¾ƒä¸é€‰æ‹©**ï¼š[model_comparison_and_selection.md](model_comparison_and_selection.md)
- **ä¸šåŠ¡ä»·å€¼è½¬åŒ–**ï¼š[business_value_translation.md](business_value_translation.md)
- **ä»£ç å®ç°**ï¼šsrc/model_evaluation.pyï¼ˆ538è¡Œï¼‰

**sklearnå®˜æ–¹æ–‡æ¡£**ï¼š
- [åˆ†ç±»æŒ‡æ ‡](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)
- [å›å½’æŒ‡æ ‡](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)

---

**æœ€åæ›´æ–°**ï¼š2024å¹´11æœˆ
