# ğŸ©º è¿‡æ‹Ÿåˆè¯Šæ–­ä¸æ”¹è¿›æŒ‡å—

> **æ ¸å¿ƒç›®æ ‡**ï¼šè¯†åˆ«è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆé—®é¢˜ï¼Œç»™å‡ºå…·ä½“æ”¹è¿›ç­–ç•¥
>
> â±ï¸ **é¢„è®¡ç”¨æ—¶**ï¼š1-1.5å°æ—¶æŒæ¡è¯Šæ–­å’Œæ”¹è¿›æ–¹æ³•

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯

**æœ¬æ–‡æ¡£é€‚ç”¨äºï¼š**
- âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œéœ€è¦è¯Šæ–­æ€§èƒ½é—®é¢˜
- âœ… è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ€§èƒ½å·®è·å¤§
- âœ… æ¨¡å‹æ€§èƒ½ä¸ç†æƒ³ï¼Œéœ€è¦æ”¹è¿›æ–¹å‘

**å¸¸è§ç—‡çŠ¶ï¼š**
- âŒ è®­ç»ƒé›†AUC=0.95ï¼Œæµ‹è¯•é›†AUC=0.70ï¼ˆè¿‡æ‹Ÿåˆï¼‰
- âŒ è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½å¾ˆå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰
- âŒ ä¸çŸ¥é“å¦‚ä½•æ”¹è¿›æ¨¡å‹

---

## ğŸ“‹ ç›®å½•

1. [è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆè¯†åˆ«](#è¿‡æ‹Ÿåˆæ¬ æ‹Ÿåˆè¯†åˆ«)
2. [è¯Šæ–­å·¥å…·](#è¯Šæ–­å·¥å…·)
3. [è¿‡æ‹Ÿåˆè§£å†³æ–¹æ¡ˆ](#è¿‡æ‹Ÿåˆè§£å†³æ–¹æ¡ˆ)
4. [æ¬ æ‹Ÿåˆè§£å†³æ–¹æ¡ˆ](#æ¬ æ‹Ÿåˆè§£å†³æ–¹æ¡ˆ)
5. [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)
6. [ä»£ç å®ç°](#ä»£ç å®ç°)

---

## è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆè¯†åˆ«

### å¿«é€Ÿè¯Šæ–­è¡¨

| ç—‡çŠ¶ | è®­ç»ƒé›†è¡¨ç° | éªŒè¯é›†è¡¨ç° | Gap | è¯Šæ–­ |
|------|------------|------------|-----|------|
| **æ¬ æ‹Ÿåˆ** | å·® | å·® | å° | æ¨¡å‹å¤ªç®€å•ï¼Œæ‹Ÿåˆèƒ½åŠ›ä¸è¶³ |
| **âœ… ç†æƒ³** | å¥½ | å¥½ | å° | æ¨¡å‹åˆšå¥½ï¼Œæ³›åŒ–èƒ½åŠ›å¼º |
| **è¿‡æ‹Ÿåˆ** | å¾ˆå¥½ | å·® | å¤§ | æ¨¡å‹å¤ªå¤æ‚ï¼Œè®°ä½äº†è®­ç»ƒæ•°æ® |

**Gapè®¡ç®—**ï¼š
```
Gap = è®­ç»ƒé›†æ€§èƒ½ - éªŒè¯é›†æ€§èƒ½

åˆ†ç±»ï¼šGap = Train AUC - Val AUC
å›å½’ï¼šGap = Val RMSE - Train RMSEï¼ˆè¶Šå¤§è¶Šè¿‡æ‹Ÿåˆï¼‰
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- Gap < 0.05ï¼šâœ… åˆé€‚
- Gap 0.05-0.10ï¼šâš ï¸ è½»åº¦è¿‡æ‹Ÿåˆ
- Gap > 0.10ï¼šâŒ ä¸¥é‡è¿‡æ‹Ÿåˆ

---

### å…·ä½“è¯Šæ–­ç¤ºä¾‹

#### ç¤ºä¾‹1ï¼šä¸¥é‡è¿‡æ‹Ÿåˆï¼ˆåˆ†ç±»ï¼‰
```
è®­ç»ƒé›† AUC: 0.95
éªŒè¯é›† AUC: 0.70
Gap: 0.25 âŒ ä¸¥é‡è¿‡æ‹Ÿåˆ

åŸå› ï¼šæ¨¡å‹è®°ä½äº†è®­ç»ƒæ•°æ®çš„å™ªå£°
è§£å†³ï¼šæ­£åˆ™åŒ–ã€ç®€åŒ–æ¨¡å‹ã€å¢åŠ æ•°æ®
```

#### ç¤ºä¾‹2ï¼šæ¬ æ‹Ÿåˆï¼ˆå›å½’ï¼‰
```
è®­ç»ƒé›† RMSE: 50
éªŒè¯é›† RMSE: 52
Gap: 2ï¼ˆå°ï¼Œä½†éƒ½å¾ˆé«˜ï¼‰âŒ æ¬ æ‹Ÿåˆ

åŸå› ï¼šæ¨¡å‹å¤ªç®€å•ï¼Œæ— æ³•æ•è·æ•°æ®è§„å¾‹
è§£å†³ï¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦ã€æ·»åŠ ç‰¹å¾
```

#### ç¤ºä¾‹3ï¼šç†æƒ³çŠ¶æ€
```
è®­ç»ƒé›† AUC: 0.82
éªŒè¯é›† AUC: 0.80
Gap: 0.02 âœ… ç†æƒ³

æ³›åŒ–èƒ½åŠ›è‰¯å¥½ï¼Œæ— éœ€è°ƒæ•´
```

---

## è¯Šæ–­å·¥å…·

### å·¥å…·1ï¼šå­¦ä¹ æ›²çº¿ï¼ˆLearning Curveï¼‰

**å«ä¹‰**ï¼šè§‚å¯Ÿè®­ç»ƒé›†å’ŒéªŒè¯é›†è¯¯å·®éš**æ ·æœ¬é‡**çš„å˜åŒ–

**åŸç†**ï¼š
```
Xè½´ï¼šè®­ç»ƒæ ·æœ¬æ•°ï¼ˆä»å°‘åˆ°å¤šï¼‰
Yè½´ï¼šæ¨¡å‹è¯¯å·®ï¼ˆRMSEæˆ–1-AUCï¼‰

ä¸¤æ¡æ›²çº¿ï¼š
- è®­ç»ƒé›†è¯¯å·®ï¼ˆTrain Errorï¼‰
- éªŒè¯é›†è¯¯å·®ï¼ˆValidation Errorï¼‰
```

**è¯Šæ–­æ¨¡å¼**ï¼š

```
1ï¸âƒ£ æ¬ æ‹Ÿåˆæ¨¡å¼
    è¯¯å·® â†‘
    â”‚   [Train]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚   [Val  ]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚    ä¸¤æ¡æ›²çº¿éƒ½å¾ˆé«˜ä¸”æ¥è¿‘
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ ·æœ¬é‡
    è§£å†³ï¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦

2ï¸âƒ£ ç†æƒ³æ¨¡å¼
    è¯¯å·® â†‘
    â”‚   [Train]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚   [Val  ]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸¤æ¡æ›²çº¿éƒ½ä½ä¸”æ¥è¿‘
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ ·æœ¬é‡
    ç»§ç»­ä½¿ç”¨ï¼

3ï¸âƒ£ è¿‡æ‹Ÿåˆæ¨¡å¼
    è¯¯å·® â†‘
    â”‚         [Val  ]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ éªŒè¯è¯¯å·®é«˜
    â”‚                    â†‘ å¤§Gap
    â”‚   [Train]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è®­ç»ƒè¯¯å·®ä½
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ ·æœ¬é‡
    è§£å†³ï¼šæ­£åˆ™åŒ–ã€å¢åŠ æ•°æ®

4ï¸âƒ£ é«˜æ–¹å·®ï¼ˆéœ€è¦æ›´å¤šæ•°æ®ï¼‰
    è¯¯å·® â†‘
    â”‚         [Val  ]â”€â”€â”€â”€â”€â”€â•²
    â”‚                    â†“ Gapç¼©å°ä¸­
    â”‚   [Train]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ ·æœ¬é‡
    è§£å†³ï¼šå¢åŠ è®­ç»ƒæ•°æ®ï¼ˆGapè¿˜åœ¨ç¼©å°ï¼‰
```

**ä»£ç å®ç°**ï¼š
```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np

def plot_learning_curve(model, X, y, cv=5, scoring='neg_mean_squared_error'):
    """
    ç»˜åˆ¶å­¦ä¹ æ›²çº¿

    å‚æ•°ï¼š
        model: æ¨¡å‹å¯¹è±¡
        X: ç‰¹å¾çŸ©é˜µ
        y: ç›®æ ‡å˜é‡
        cv: äº¤å‰éªŒè¯æŠ˜æ•°
        scoring: è¯„ä¼°æŒ‡æ ‡ï¼ˆéœ€è¦æ˜¯è´Ÿæ•°å½¢å¼ï¼Œå¦‚neg_mean_squared_errorï¼‰
    """
    # è®¡ç®—å­¦ä¹ æ›²çº¿
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y,
        cv=cv,
        scoring=scoring,
        train_sizes=np.linspace(0.1, 1.0, 10),  # 10ä¸ªæ ·æœ¬é‡ç‚¹
        n_jobs=-1
    )

    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    train_mean = -train_scores.mean(axis=1)  # è½¬ä¸ºæ­£æ•°
    train_std = train_scores.std(axis=1)
    val_mean = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)

    # ç»˜åˆ¶
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, label='Training Error', marker='o')
    plt.plot(train_sizes, val_mean, label='Validation Error', marker='s')

    # æ·»åŠ æ ‡å‡†å·®é˜´å½±
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)

    plt.xlabel('Number of Training Samples')
    plt.ylabel('Error (RMSE)')
    plt.title('Learning Curve')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()

    # è¯Šæ–­
    final_gap = val_mean[-1] - train_mean[-1]
    if final_gap > 0.1 * val_mean[-1]:  # Gap > 10%
        print("âš ï¸ è¯Šæ–­ï¼šè¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒå’ŒéªŒè¯è¯¯å·®å·®è·å¤§ï¼‰")
        print(f"   Train Error: {train_mean[-1]:.3f}")
        print(f"   Val Error:   {val_mean[-1]:.3f}")
        print(f"   Gap:         {final_gap:.3f}")
    elif train_mean[-1] > 0.5 * val_mean[-1]:  # è®­ç»ƒè¯¯å·®ä¹Ÿå¾ˆé«˜
        print("âš ï¸ è¯Šæ–­ï¼šæ¬ æ‹Ÿåˆï¼ˆè®­ç»ƒå’ŒéªŒè¯è¯¯å·®éƒ½å¾ˆé«˜ï¼‰")
    else:
        print("âœ… è¯Šæ–­ï¼šæ¨¡å‹çŠ¶æ€è‰¯å¥½")

# ä½¿ç”¨ç¤ºä¾‹
plot_learning_curve(model, X, y, cv=5, scoring='neg_mean_squared_error')
```

---

### å·¥å…·2ï¼šéªŒè¯æ›²çº¿ï¼ˆValidation Curveï¼‰

**å«ä¹‰**ï¼šè§‚å¯Ÿæ¨¡å‹å¤æ‚åº¦å¯¹æ€§èƒ½çš„å½±å“

**åŸç†**ï¼š
```
Xè½´ï¼šæ¨¡å‹å¤æ‚åº¦å‚æ•°ï¼ˆå¦‚æ ‘æ·±åº¦ã€æ­£åˆ™åŒ–å¼ºåº¦ï¼‰
Yè½´ï¼šæ¨¡å‹æ€§èƒ½ï¼ˆAUCæˆ–1-RMSEï¼‰

ä¸¤æ¡æ›²çº¿ï¼š
- è®­ç»ƒé›†æ€§èƒ½
- éªŒè¯é›†æ€§èƒ½
```

**è¯Šæ–­æ¨¡å¼**ï¼š
```
    æ€§èƒ½ â†‘
    â”‚        [Train]
    â”‚            â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚          â•±
    â”‚        â•±     [Val] â•±â•²
    â”‚      â•±           â•±    â•²  è¿‡æ‹ŸåˆåŒº
    â”‚    â•±           â•±        â•²
    â”‚  â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±            â•²
    â”‚  æ¬ æ‹ŸåˆåŒº    â†‘ æœ€ä¼˜ç‚¹
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ¨¡å‹å¤æ‚åº¦
                           ï¼ˆå¦‚ï¼šæ ‘æ·±åº¦ï¼‰

æ‰¾åˆ°éªŒè¯é›†æ€§èƒ½æœ€é«˜ç‚¹ = æœ€ä¼˜å¤æ‚åº¦
```

**ä»£ç å®ç°**ï¼š
```python
from sklearn.model_selection import validation_curve

def plot_validation_curve(model, X, y, param_name, param_range, cv=5, scoring='roc_auc'):
    """
    ç»˜åˆ¶éªŒè¯æ›²çº¿

    å‚æ•°ï¼š
        model: æ¨¡å‹å¯¹è±¡
        X: ç‰¹å¾çŸ©é˜µ
        y: ç›®æ ‡å˜é‡
        param_name: å‚æ•°åï¼ˆå¦‚'max_depth'ï¼‰
        param_range: å‚æ•°èŒƒå›´ï¼ˆå¦‚[1, 2, 3, 5, 10, 20]ï¼‰
        cv: äº¤å‰éªŒè¯æŠ˜æ•°
        scoring: è¯„ä¼°æŒ‡æ ‡
    """
    # è®¡ç®—éªŒè¯æ›²çº¿
    train_scores, val_scores = validation_curve(
        model, X, y,
        param_name=param_name,
        param_range=param_range,
        cv=cv,
        scoring=scoring,
        n_jobs=-1
    )

    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)

    # ç»˜åˆ¶
    plt.figure(figsize=(10, 6))
    plt.plot(param_range, train_mean, label='Training Score', marker='o')
    plt.plot(param_range, val_mean, label='Validation Score', marker='s')

    # æ ‡å‡†å·®é˜´å½±
    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1)
    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1)

    plt.xlabel(param_name)
    plt.ylabel(f'Score ({scoring})')
    plt.title(f'Validation Curve ({param_name})')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()

    # æ‰¾åˆ°æœ€ä¼˜å‚æ•°
    best_idx = val_mean.argmax()
    best_param = param_range[best_idx]
    print(f"âœ… æœ€ä¼˜ {param_name}: {best_param}")
    print(f"   Validation Score: {val_mean[best_idx]:.3f}")

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆæ ‘æ·±åº¦ï¼‰
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
param_range = [1, 2, 3, 5, 10, 15, 20, 30]

plot_validation_curve(
    model, X, y,
    param_name='max_depth',
    param_range=param_range,
    cv=5,
    scoring='roc_auc'
)
```

---

### å·¥å…·3ï¼šåå·®-æ–¹å·®åˆ†è§£

**ç†è®º**ï¼š
```
æ€»è¯¯å·® = åå·®Â² + æ–¹å·® + å™ªå£°

åå·®ï¼ˆBiasï¼‰ï¼šæ¬ æ‹Ÿåˆ
- æ¨¡å‹å¤ªç®€å•ï¼Œæ— æ³•æ•è·çœŸå®å…³ç³»

æ–¹å·®ï¼ˆVarianceï¼‰ï¼šè¿‡æ‹Ÿåˆ
- æ¨¡å‹å¤ªå¤æ‚ï¼Œå¯¹è®­ç»ƒæ•°æ®è¿‡æ•æ„Ÿ

å™ªå£°ï¼ˆIrreducible Errorï¼‰ï¼šä¸å¯é¿å…
- æ•°æ®æœ¬èº«çš„éšæœºæ€§
```

**æƒè¡¡å…³ç³»**ï¼š
```
    è¯¯å·® â†‘
    â”‚
    â”‚  [æ€»è¯¯å·®]    â•² â•±
    â”‚              â•³
    â”‚  [æ–¹å·®]    â•±   â•²
    â”‚  [åå·®]  â•²       â•±
    â”‚
    â”‚         æ¬ æ‹Ÿåˆ â†‘ è¿‡æ‹Ÿåˆ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ¨¡å‹å¤æ‚åº¦
                   æœ€ä¼˜ç‚¹
```

---

## è¿‡æ‹Ÿåˆè§£å†³æ–¹æ¡ˆ

### ç­–ç•¥1ï¼šæ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰

**åŸç†**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒ©ç½šé¡¹ï¼Œé™åˆ¶æ¨¡å‹å¤æ‚åº¦

**æ–¹æ³•**ï¼š

| æ­£åˆ™åŒ–ç±»å‹ | æƒ©ç½šé¡¹ | æ•ˆæœ | é€‚ç”¨æ¨¡å‹ |
|------------|--------|------|----------|
| **L1ï¼ˆLassoï¼‰** | Î»Â·\|w\| | æƒé‡ç¨€ç–åŒ–ï¼Œç‰¹å¾é€‰æ‹© | çº¿æ€§æ¨¡å‹ |
| **L2ï¼ˆRidgeï¼‰** | Î»Â·wÂ² | æƒé‡å¹³æ»‘åŒ– | çº¿æ€§æ¨¡å‹ã€NN |
| **Elastic Net** | Î»â‚Â·\|w\| + Î»â‚‚Â·wÂ² | L1+L2ç»„åˆ | çº¿æ€§æ¨¡å‹ |
| **Dropout** | éšæœºå¤±æ´»ç¥ç»å…ƒ | é˜²æ­¢ç¥ç»å…ƒå…±é€‚åº” | ç¥ç»ç½‘ç»œ |
| **æ ‘å‰ªæ** | é™åˆ¶æ ‘æ·±åº¦/å¶å­èŠ‚ç‚¹ | ç®€åŒ–å†³ç­–æ ‘ | æ ‘æ¨¡å‹ |

**ä»£ç ç¤ºä¾‹**ï¼š

**çº¿æ€§æ¨¡å‹æ­£åˆ™åŒ–**ï¼š
```python
from sklearn.linear_model import Ridge, Lasso

# L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰
ridge = Ridge(alpha=1.0)  # alphaè¶Šå¤§ï¼Œæ­£åˆ™åŒ–è¶Šå¼º
ridge.fit(X_train, y_train)

# L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# å¯¹æ¯”ä¸åŒæ­£åˆ™åŒ–å¼ºåº¦
alphas = [0.001, 0.01, 0.1, 1.0, 10.0]
for alpha in alphas:
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    train_score = model.score(X_train, y_train)
    val_score = model.score(X_val, y_val)
    print(f"Alpha={alpha:5.3f}: Train RÂ²={train_score:.3f}, Val RÂ²={val_score:.3f}")

# è¾“å‡ºç¤ºä¾‹ï¼š
# Alpha=0.001: Train RÂ²=0.850, Val RÂ²=0.720  è¿‡æ‹Ÿåˆ
# Alpha=0.010: Train RÂ²=0.830, Val RÂ²=0.780
# Alpha=0.100: Train RÂ²=0.810, Val RÂ²=0.805  â† æœ€ä¼˜
# Alpha=1.000: Train RÂ²=0.750, Val RÂ²=0.760
# Alpha=10.00: Train RÂ²=0.650, Val RÂ²=0.670  æ¬ æ‹Ÿåˆ
```

**æ ‘æ¨¡å‹æ­£åˆ™åŒ–**ï¼š
```python
from sklearn.ensemble import RandomForestClassifier

# é™åˆ¶æ ‘æ·±åº¦
rf = RandomForestClassifier(
    max_depth=10,          # é™åˆ¶æ ‘æ·±åº¦ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    min_samples_split=20,  # èŠ‚ç‚¹åˆ†è£‚æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=10,   # å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    max_features='sqrt',   # ç‰¹å¾å­é›†å¤§å°
    n_estimators=100
)
rf.fit(X_train, y_train)
```

---

### ç­–ç•¥2ï¼šå¢åŠ è®­ç»ƒæ•°æ®

**åŸç†**ï¼šæ›´å¤šæ•°æ®å¸®åŠ©æ¨¡å‹å­¦ä¹ çœŸå®è§„å¾‹è€Œéå™ªå£°

**æ–¹æ³•**ï¼š

| æ–¹æ³• | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **æ”¶é›†æ›´å¤šæ•°æ®** | æœ€ç›´æ¥æœ‰æ•ˆ | æ•°æ®å¯è·å– |
| **æ•°æ®å¢å¼º** | å›¾åƒæ—‹è½¬ã€æ–‡æœ¬åŒä¹‰æ›¿æ¢ | å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ |
| **åˆæˆæ•°æ®** | SMOTEï¼ˆè¿‡é‡‡æ ·ï¼‰ | ä¸å¹³è¡¡æ•°æ® |
| **è¿ç§»å­¦ä¹ ** | ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ | å°æ•°æ®é›† + å¤§æ¨¡å‹ |

**æ•°æ®å¢å¼ºç¤ºä¾‹ï¼ˆå›¾åƒï¼‰**ï¼š
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# å›¾åƒæ•°æ®å¢å¼º
datagen = ImageDataGenerator(
    rotation_range=20,      # éšæœºæ—‹è½¬Â±20åº¦
    width_shift_range=0.2,  # æ°´å¹³å¹³ç§»
    height_shift_range=0.2, # å‚ç›´å¹³ç§»
    horizontal_flip=True,   # æ°´å¹³ç¿»è½¬
    zoom_range=0.2          # éšæœºç¼©æ”¾
)

# ç”Ÿæˆå¢å¼ºæ•°æ®
datagen.fit(X_train)
model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=50)
```

---

### ç­–ç•¥3ï¼šç®€åŒ–æ¨¡å‹

**åŸç†**ï¼šé™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œå‡å°‘æ‹Ÿåˆå™ªå£°çš„èƒ½åŠ›

**æ–¹æ³•**ï¼š

| ç®€åŒ–æ–¹å¼ | ç¤ºä¾‹ | æ•ˆæœ |
|----------|------|------|
| **å‡å°‘ç‰¹å¾** | 100ä¸ªç‰¹å¾ â†’ 50ä¸ª | é™ä½ç»´åº¦ |
| **é™ä½æ¨¡å‹å¤æ‚åº¦** | å†³ç­–æ ‘æ·±åº¦30 â†’ 10 | ç®€åŒ–ç»“æ„ |
| **ä½¿ç”¨æ›´ç®€å•æ¨¡å‹** | XGBoost â†’ é€»è¾‘å›å½’ | é™ä½å®¹é‡ |
| **Early Stopping** | è®­ç»ƒæ—¶ç›‘æ§éªŒè¯è¯¯å·® | é˜²æ­¢è¿‡åº¦è®­ç»ƒ |

**ä»£ç ç¤ºä¾‹**ï¼š

**ç‰¹å¾é€‰æ‹©ï¼ˆå‡å°‘ç‰¹å¾ï¼‰**ï¼š
```python
from sklearn.feature_selection import SelectKBest, f_classif

# æ–¹æ³•1ï¼šåŸºäºç»Ÿè®¡æ£€éªŒé€‰æ‹©Top Kç‰¹å¾
selector = SelectKBest(f_classif, k=50)  # é€‰æ‹©50ä¸ªæœ€ä½³ç‰¹å¾
X_train_selected = selector.fit_transform(X_train, y_train)
X_val_selected = selector.transform(X_val)

# æ–¹æ³•2ï¼šåŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

# é€‰æ‹©é‡è¦æ€§é«˜äºé˜ˆå€¼çš„ç‰¹å¾
selector = SelectFromModel(rf, threshold='median')
X_train_selected = selector.fit_transform(X_train, y_train)
X_val_selected = selector.transform(X_val)

print(f"åŸå§‹ç‰¹å¾æ•°: {X_train.shape[1]}")
print(f"é€‰æ‹©åç‰¹å¾æ•°: {X_train_selected.shape[1]}")
```

**Early Stoppingï¼ˆæå‰åœæ­¢ï¼‰**ï¼š
```python
from sklearn.ensemble import GradientBoostingClassifier

# ä½¿ç”¨Early Stopping
model = GradientBoostingClassifier(
    n_estimators=1000,
    learning_rate=0.1,
    max_depth=3,
    validation_fraction=0.2,  # 20%æ•°æ®ç”¨äºéªŒè¯
    n_iter_no_change=10,       # 10è½®ä¸æ”¹è¿›åˆ™åœæ­¢
    tol=0.001
)
model.fit(X_train, y_train)

print(f"è®­ç»ƒè½®æ•°: {model.n_estimators_}")  # å®é™…è®­ç»ƒçš„è½®æ•°
```

---

### ç­–ç•¥4ï¼šé›†æˆæ–¹æ³•

**åŸç†**ï¼šç»„åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ï¼Œå‡å°‘å•ä¸ªæ¨¡å‹çš„æ–¹å·®

**æ–¹æ³•**ï¼š

| é›†æˆæ–¹æ³• | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|----------|------|----------|
| **Bagging** | å¤šä¸ªæ¨¡å‹ç‹¬ç«‹è®­ç»ƒï¼Œå–å¹³å‡ | é«˜æ–¹å·®æ¨¡å‹ï¼ˆå†³ç­–æ ‘ï¼‰ |
| **Boosting** | é¡ºåºè®­ç»ƒï¼Œçº æ­£å‰ä¸€ä¸ªæ¨¡å‹é”™è¯¯ | é«˜åå·®æ¨¡å‹ |
| **Stacking** | ç”¨å…ƒæ¨¡å‹ç»„åˆå¤šä¸ªåŸºæ¨¡å‹ | å¤šç§ç®—æ³•ç»„åˆ |

**ä»£ç ç¤ºä¾‹**ï¼š
```python
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

# Baggingï¼ˆå‡å°‘æ–¹å·®ï¼‰
base_model = DecisionTreeClassifier(max_depth=None)  # æ·±åº¦ä¸é™
bagging = BaggingClassifier(
    base_estimator=base_model,
    n_estimators=10,
    max_samples=0.8,
    random_state=42
)
bagging.fit(X_train, y_train)

# Random Forestå°±æ˜¯Baggingçš„ç‰¹ä¾‹
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)
```

---

### ç­–ç•¥5ï¼šäº¤å‰éªŒè¯

**åŸç†**ï¼šä½¿ç”¨å¤šæ¬¡æ•°æ®åˆ’åˆ†è¯„ä¼°æ¨¡å‹ï¼Œæ›´å¯é åœ°æ£€æµ‹è¿‡æ‹Ÿåˆ

**ä»£ç ç¤ºä¾‹**ï¼š
```python
from sklearn.model_selection import cross_val_score

# K-Foldäº¤å‰éªŒè¯
cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
print(f"CV AUC: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

# å¦‚æœCVæ ‡å‡†å·®å¾ˆå¤§ï¼ˆ>0.05ï¼‰ï¼Œè¯´æ˜æ¨¡å‹ä¸ç¨³å®š
if cv_scores.std() > 0.05:
    print("âš ï¸ æ¨¡å‹ä¸ç¨³å®šï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
```

---

## æ¬ æ‹Ÿåˆè§£å†³æ–¹æ¡ˆ

### ç­–ç•¥1ï¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦

**åŸç†**ï¼šä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œå¢åŠ æ‹Ÿåˆèƒ½åŠ›

**æ–¹æ³•**ï¼š

| å¢åŠ å¤æ‚åº¦æ–¹å¼ | ç¤ºä¾‹ | æ•ˆæœ |
|----------------|------|------|
| **å¢åŠ æ ‘æ·±åº¦** | max_depth=5 â†’ 15 | æ•è·æ›´å¤æ‚æ¨¡å¼ |
| **å¢åŠ ç¥ç»ç½‘ç»œå±‚æ•°** | 2å±‚ â†’ 5å±‚ | å¢åŠ è¡¨è¾¾èƒ½åŠ› |
| **ä½¿ç”¨æ›´å¤æ‚æ¨¡å‹** | çº¿æ€§å›å½’ â†’ XGBoost | æå‡æ‹Ÿåˆèƒ½åŠ› |
| **å¢åŠ å¤šé¡¹å¼ç‰¹å¾** | x â†’ [x, xÂ², xÂ³] | æ•è·éçº¿æ€§å…³ç³» |

**ä»£ç ç¤ºä¾‹**ï¼š

**å¢åŠ æ ‘æ·±åº¦**ï¼š
```python
from sklearn.tree import DecisionTreeClassifier

# æ¬ æ‹Ÿåˆï¼šæ ‘å¤ªæµ…
model_simple = DecisionTreeClassifier(max_depth=2)
model_simple.fit(X_train, y_train)
print(f"Train AUC: {model_simple.score(X_train, y_train):.3f}")
print(f"Val AUC: {model_simple.score(X_val, y_val):.3f}")
# è¾“å‡ºï¼šTrain AUC: 0.72, Val AUC: 0.70ï¼ˆéƒ½å¾ˆä½ï¼Œæ¬ æ‹Ÿåˆï¼‰

# å¢åŠ å¤æ‚åº¦
model_complex = DecisionTreeClassifier(max_depth=10)
model_complex.fit(X_train, y_train)
print(f"Train AUC: {model_complex.score(X_train, y_train):.3f}")
print(f"Val AUC: {model_complex.score(X_val, y_val):.3f}")
# è¾“å‡ºï¼šTrain AUC: 0.85, Val AUC: 0.82ï¼ˆæ”¹å–„ï¼‰
```

**å¤šé¡¹å¼ç‰¹å¾**ï¼š
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# åŸå§‹çº¿æ€§æ¨¡å‹ï¼ˆæ¬ æ‹Ÿåˆï¼‰
lr = LinearRegression()
lr.fit(X_train, y_train)
print(f"Linear - Train RÂ²: {lr.score(X_train, y_train):.3f}")
print(f"Linear - Val RÂ²: {lr.score(X_val, y_val):.3f}")

# æ·»åŠ å¤šé¡¹å¼ç‰¹å¾
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_val_poly = poly.transform(X_val)

lr_poly = LinearRegression()
lr_poly.fit(X_train_poly, y_train)
print(f"Poly - Train RÂ²: {lr_poly.score(X_train_poly, y_train):.3f}")
print(f"Poly - Val RÂ²: {lr_poly.score(X_val_poly, y_val):.3f}")
```

---

### ç­–ç•¥2ï¼šæ·»åŠ ç‰¹å¾

**åŸç†**ï¼šå¢åŠ ç‰¹å¾æ•°é‡ï¼Œå¸®åŠ©æ¨¡å‹æ•è·æ›´å¤šä¿¡æ¯

**æ–¹æ³•**ï¼š

| ç‰¹å¾æ·»åŠ æ–¹å¼ | è¯´æ˜ | ç¤ºä¾‹ |
|--------------|------|------|
| **ç‰¹å¾å·¥ç¨‹** | æ‰‹åŠ¨æ„é€ æœ‰æ„ä¹‰ç‰¹å¾ | å¹´é¾„ â†’ [å¹´é¾„æ®µ, æ˜¯å¦æˆå¹´] |
| **äº¤å‰ç‰¹å¾** | ç‰¹å¾ä¹‹é—´çš„ç»„åˆ | [èº«é«˜, ä½“é‡] â†’ BMI |
| **èšåˆç‰¹å¾** | åˆ†ç»„ç»Ÿè®¡ç‰¹å¾ | ç”¨æˆ·å†å²å¹³å‡æ¶ˆè´¹ |
| **åµŒå…¥ç‰¹å¾** | ç±»åˆ«ç‰¹å¾çš„ç¨ å¯†è¡¨ç¤º | Word2Vecã€å®ä½“åµŒå…¥ |

**ä»£ç ç¤ºä¾‹**ï¼š
```python
import pandas as pd

# å‡è®¾æ•°æ®
df = pd.DataFrame({
    'age': [25, 35, 45, 55],
    'income': [30000, 50000, 70000, 90000],
    'education': ['é«˜ä¸­', 'æœ¬ç§‘', 'ç¡•å£«', 'åšå£«']
})

# ç‰¹å¾å·¥ç¨‹
df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 100], labels=['é’å¹´', 'ä¸­å¹´', 'è€å¹´'])
df['high_income'] = (df['income'] > 60000).astype(int)

# äº¤å‰ç‰¹å¾
df['income_per_age'] = df['income'] / df['age']

# ç±»åˆ«ç¼–ç 
df['education_encoded'] = df['education'].map({
    'é«˜ä¸­': 1, 'æœ¬ç§‘': 2, 'ç¡•å£«': 3, 'åšå£«': 4
})

print(df)
```

---

### ç­–ç•¥3ï¼šå‡å°‘æ­£åˆ™åŒ–å¼ºåº¦

**åŸç†**ï¼šå¦‚æœæ¨¡å‹è¢«è¿‡åº¦æ­£åˆ™åŒ–ï¼Œå¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆ

**ä»£ç ç¤ºä¾‹**ï¼š
```python
from sklearn.linear_model import Ridge

# è¿‡åº¦æ­£åˆ™åŒ–ï¼ˆæ¬ æ‹Ÿåˆï¼‰
ridge_strong = Ridge(alpha=100.0)
ridge_strong.fit(X_train, y_train)
print(f"Strong Reg - Train RÂ²: {ridge_strong.score(X_train, y_train):.3f}")
print(f"Strong Reg - Val RÂ²: {ridge_strong.score(X_val, y_val):.3f}")
# è¾“å‡ºï¼šéƒ½å¾ˆä½

# å‡å°‘æ­£åˆ™åŒ–
ridge_weak = Ridge(alpha=0.1)
ridge_weak.fit(X_train, y_train)
print(f"Weak Reg - Train RÂ²: {ridge_weak.score(X_train, y_train):.3f}")
print(f"Weak Reg - Val RÂ²: {ridge_weak.score(X_val, y_val):.3f}")
# è¾“å‡ºï¼šæ”¹å–„
```

---

### ç­–ç•¥4ï¼šè®­ç»ƒæ›´é•¿æ—¶é—´

**åŸç†**ï¼šå¢åŠ è®­ç»ƒè½®æ•°ï¼Œè®©æ¨¡å‹å……åˆ†å­¦ä¹ 

**ä»£ç ç¤ºä¾‹**ï¼š
```python
from sklearn.neural_network import MLPClassifier

# è®­ç»ƒè½®æ•°ä¸è¶³ï¼ˆæ¬ æ‹Ÿåˆï¼‰
mlp_short = MLPClassifier(max_iter=10)
mlp_short.fit(X_train, y_train)

# å¢åŠ è®­ç»ƒè½®æ•°
mlp_long = MLPClassifier(max_iter=200)
mlp_long.fit(X_train, y_train)
```

---

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šå®¢æˆ·æµå¤±é¢„æµ‹çš„è¯Šæ–­å’Œæ”¹è¿›

#### èƒŒæ™¯
- æ•°æ®ï¼š5000ä¸ªå®¢æˆ·ï¼Œ15ä¸ªç‰¹å¾
- é—®é¢˜ï¼šäºŒåˆ†ç±»ï¼ˆæµå¤±/ä¸æµå¤±ï¼‰
- åˆå§‹æ¨¡å‹ï¼šé€»è¾‘å›å½’

#### Step 1ï¼šåˆå§‹è¯Šæ–­

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# è®­ç»ƒåˆå§‹æ¨¡å‹
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

lr = LogisticRegression()
lr.fit(X_train, y_train)

# è¯„ä¼°
train_auc = roc_auc_score(y_train, lr.predict_proba(X_train)[:, 1])
val_auc = roc_auc_score(y_val, lr.predict_proba(X_val)[:, 1])

print(f"Train AUC: {train_auc:.3f}")
print(f"Val AUC: {val_auc:.3f}")
print(f"Gap: {train_auc - val_auc:.3f}")

# è¾“å‡ºï¼š
# Train AUC: 0.720
# Val AUC: 0.715
# Gap: 0.005
```

**è¯Šæ–­**ï¼šGapå¾ˆå°ï¼Œä½†æ€§èƒ½éƒ½ä¸é«˜ â†’ **æ¬ æ‹Ÿåˆ**

#### Step 2ï¼šç»˜åˆ¶å­¦ä¹ æ›²çº¿ç¡®è®¤

```python
from src.model_evaluation import plot_learning_curve

plot_learning_curve(lr, X, y, cv=5, scoring='roc_auc')
```

**è§‚å¯Ÿ**ï¼šè®­ç»ƒå’ŒéªŒè¯æ›²çº¿éƒ½å¾ˆé«˜ä¸”æ¥è¿‘ â†’ ç¡®è®¤æ¬ æ‹Ÿåˆ

#### Step 3ï¼šæ”¹è¿›æ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šæ·»åŠ å¤šé¡¹å¼ç‰¹å¾**
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('lr', LogisticRegression())
])
pipeline.fit(X_train, y_train)

train_auc = roc_auc_score(y_train, pipeline.predict_proba(X_train)[:, 1])
val_auc = roc_auc_score(y_val, pipeline.predict_proba(X_val)[:, 1])

print(f"Train AUC: {train_auc:.3f}")  # 0.780
print(f"Val AUC: {val_auc:.3f}")      # 0.765
print(f"Gap: {train_auc - val_auc:.3f}")  # 0.015
```

**ç»“æœ**ï¼šæ€§èƒ½æå‡ï¼Œä½†å¯èƒ½å¯ä»¥åšå¾—æ›´å¥½

**æ–¹æ¡ˆ2ï¼šä½¿ç”¨æ›´å¤æ‚æ¨¡å‹ï¼ˆéšæœºæ£®æ—ï¼‰**
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, max_depth=10)
rf.fit(X_train, y_train)

train_auc = roc_auc_score(y_train, rf.predict_proba(X_train)[:, 1])
val_auc = roc_auc_score(y_val, rf.predict_proba(X_val)[:, 1])

print(f"Train AUC: {train_auc:.3f}")  # 0.920
print(f"Val AUC: {val_auc:.3f}")      # 0.785
print(f"Gap: {train_auc - val_auc:.3f}")  # 0.135
```

**æ–°é—®é¢˜**ï¼šGap=0.135 â†’ **è¿‡æ‹Ÿåˆï¼**

#### Step 4ï¼šè§£å†³è¿‡æ‹Ÿåˆ

```python
# å¢åŠ æ­£åˆ™åŒ–ï¼ˆé™åˆ¶æ ‘æ·±åº¦ï¼‰
rf_tuned = RandomForestClassifier(
    n_estimators=100,
    max_depth=6,           # é™åˆ¶æ·±åº¦
    min_samples_split=20,  # å¢åŠ åˆ†è£‚é™åˆ¶
    min_samples_leaf=10    # å¢åŠ å¶å­é™åˆ¶
)
rf_tuned.fit(X_train, y_train)

train_auc = roc_auc_score(y_train, rf_tuned.predict_proba(X_train)[:, 1])
val_auc = roc_auc_score(y_val, rf_tuned.predict_proba(X_val)[:, 1])

print(f"Train AUC: {train_auc:.3f}")  # 0.850
print(f"Val AUC: {val_auc:.3f}")      # 0.820
print(f"Gap: {train_auc - val_auc:.3f}")  # 0.030
```

**æœ€ç»ˆç»“æœ**ï¼šâœ… Gap<0.05ï¼Œæ€§èƒ½è‰¯å¥½

---

## ä»£ç å®ç°

### å®Œæ•´çš„è¯Šæ–­å’Œæ”¹è¿›æµç¨‹

```python
from sklearn.model_selection import learning_curve, validation_curve
import matplotlib.pyplot as plt
import numpy as np

class ModelDiagnostics:
    """æ¨¡å‹è¯Šæ–­å·¥å…·ç±»"""

    def __init__(self, model, X, y):
        self.model = model
        self.X = X
        self.y = y

    def diagnose(self):
        """
        å®Œæ•´è¯Šæ–­æµç¨‹

        è¿”å›è¯Šæ–­æŠ¥å‘Š
        """
        print("=" * 60)
        print("æ¨¡å‹è¯Šæ–­æŠ¥å‘Š")
        print("=" * 60)

        # 1. åŸºç¡€æ€§èƒ½
        self._check_basic_performance()

        # 2. å­¦ä¹ æ›²çº¿
        self._plot_learning_curve()

        # 3. è¯Šæ–­ç»“è®º
        self._diagnose_conclusion()

        # 4. æ”¹è¿›å»ºè®®
        self._suggest_improvements()

    def _check_basic_performance(self):
        """æ£€æŸ¥åŸºç¡€æ€§èƒ½"""
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import roc_auc_score

        X_train, X_val, y_train, y_val = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42
        )

        self.model.fit(X_train, y_train)

        # è®¡ç®—AUC
        train_auc = roc_auc_score(y_train, self.model.predict_proba(X_train)[:, 1])
        val_auc = roc_auc_score(y_val, self.model.predict_proba(X_val)[:, 1])
        gap = train_auc - val_auc

        print(f"\n1. åŸºç¡€æ€§èƒ½")
        print(f"   Train AUC: {train_auc:.3f}")
        print(f"   Val AUC:   {val_auc:.3f}")
        print(f"   Gap:       {gap:.3f}")

        self.train_auc = train_auc
        self.val_auc = val_auc
        self.gap = gap

    def _plot_learning_curve(self):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        print(f"\n2. å­¦ä¹ æ›²çº¿åˆ†æ")

        train_sizes, train_scores, val_scores = learning_curve(
            self.model, self.X, self.y,
            cv=5,
            scoring='roc_auc',
            train_sizes=np.linspace(0.1, 1.0, 10),
            n_jobs=-1
        )

        train_mean = train_scores.mean(axis=1)
        val_mean = val_scores.mean(axis=1)

        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, train_mean, label='Training Score', marker='o')
        plt.plot(train_sizes, val_mean, label='Validation Score', marker='s')
        plt.xlabel('Training Set Size')
        plt.ylabel('AUC Score')
        plt.title('Learning Curve')
        plt.legend()
        plt.grid(alpha=0.3)
        plt.show()

        print("   âœ… å­¦ä¹ æ›²çº¿å·²ç”Ÿæˆ")

    def _diagnose_conclusion(self):
        """è¯Šæ–­ç»“è®º"""
        print(f"\n3. è¯Šæ–­ç»“è®º")

        if self.gap > 0.10:
            print("   âŒ ä¸¥é‡è¿‡æ‹Ÿåˆ")
        elif self.gap > 0.05:
            print("   âš ï¸  è½»åº¦è¿‡æ‹Ÿåˆ")
        elif self.train_auc < 0.70 and self.val_auc < 0.70:
            print("   âŒ æ¬ æ‹Ÿåˆ")
        else:
            print("   âœ… æ¨¡å‹çŠ¶æ€è‰¯å¥½")

    def _suggest_improvements(self):
        """æ”¹è¿›å»ºè®®"""
        print(f"\n4. æ”¹è¿›å»ºè®®")

        if self.gap > 0.05:
            print("   è¿‡æ‹Ÿåˆæ”¹è¿›å»ºè®®ï¼š")
            print("   - å¢åŠ æ­£åˆ™åŒ–å¼ºåº¦")
            print("   - å‡å°‘æ¨¡å‹å¤æ‚åº¦ï¼ˆé™ä½æ ‘æ·±åº¦ï¼‰")
            print("   - å¢åŠ è®­ç»ƒæ•°æ®")
            print("   - ä½¿ç”¨Dropoutï¼ˆç¥ç»ç½‘ç»œï¼‰")
        elif self.train_auc < 0.70:
            print("   æ¬ æ‹Ÿåˆæ”¹è¿›å»ºè®®ï¼š")
            print("   - å¢åŠ æ¨¡å‹å¤æ‚åº¦")
            print("   - æ·»åŠ ç‰¹å¾ï¼ˆç‰¹å¾å·¥ç¨‹ï¼‰")
            print("   - å‡å°‘æ­£åˆ™åŒ–")
            print("   - ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹")
        else:
            print("   âœ… æ¨¡å‹å·²ä¼˜åŒ–ï¼Œå¯ä»¥ç»§ç»­")

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100)
diagnostics = ModelDiagnostics(model, X, y)
diagnostics.diagnose()
```

---

## âœ… å¿«é€Ÿå‚è€ƒå¡ç‰‡

### è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆå¿«é€Ÿåˆ¤æ–­

| ç—‡çŠ¶ | è®­ç»ƒé›† | éªŒè¯é›† | Gap | è¯Šæ–­ | è§£å†³æ–¹æ¡ˆ |
|------|--------|--------|-----|------|----------|
| ä¸¤è€…éƒ½å·® | ä½ | ä½ | å° | æ¬ æ‹Ÿåˆ | å¢åŠ å¤æ‚åº¦ã€æ·»åŠ ç‰¹å¾ |
| è®­ç»ƒå¥½éªŒè¯å·® | é«˜ | ä½ | å¤§ | è¿‡æ‹Ÿåˆ | æ­£åˆ™åŒ–ã€ç®€åŒ–æ¨¡å‹ |
| ä¸¤è€…éƒ½å¥½ | é«˜ | é«˜ | å° | âœ… ç†æƒ³ | ç»§ç»­ä½¿ç”¨ |

### æ”¹è¿›ç­–ç•¥é€ŸæŸ¥

**è¿‡æ‹Ÿåˆ â†’ å‡å°‘æ–¹å·®**ï¼š
1. æ­£åˆ™åŒ–ï¼ˆL1/L2/Dropoutï¼‰
2. ç®€åŒ–æ¨¡å‹ï¼ˆé™ä½æ·±åº¦ã€å‡å°‘ç‰¹å¾ï¼‰
3. å¢åŠ æ•°æ®
4. Early Stopping
5. é›†æˆæ–¹æ³•ï¼ˆBaggingï¼‰

**æ¬ æ‹Ÿåˆ â†’ å‡å°‘åå·®**ï¼š
1. å¢åŠ æ¨¡å‹å¤æ‚åº¦
2. æ·»åŠ ç‰¹å¾
3. å‡å°‘æ­£åˆ™åŒ–
4. è®­ç»ƒæ›´é•¿æ—¶é—´
5. ä½¿ç”¨æ›´å¼ºå¤§æ¨¡å‹

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

**ç›¸å…³æ–‡æ¡£**ï¼š
- **æ¨¡å‹æ¯”è¾ƒ**ï¼š[model_comparison_and_selection.md](model_comparison_and_selection.md)
- **æŒ‡æ ‡è®¡ç®—**ï¼š[metrics_calculation_guide.md](metrics_calculation_guide.md)
- **ä¸šåŠ¡è½¬åŒ–**ï¼š[business_value_translation.md](business_value_translation.md)

**æ¨èèµ„æº**ï¼š
- ã€ŠThe Elements of Statistical Learningã€‹ - Bias-Variance Trade-off
- sklearnæ–‡æ¡£ï¼š[Learning Curves](https://scikit-learn.org/stable/modules/learning_curve.html)

---

**æœ€åæ›´æ–°**ï¼š2024å¹´11æœˆ
**ä»£ç æ¨¡å—**ï¼šsrc/model_evaluation.py - `plot_learning_curve()`, `plot_validation_curve()`