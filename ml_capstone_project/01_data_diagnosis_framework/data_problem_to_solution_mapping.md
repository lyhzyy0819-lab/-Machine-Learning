# ğŸ”§ æ•°æ®é—®é¢˜â†’è§£å†³æ–¹æ¡ˆæ˜ å°„è¡¨

> **ç”¨é€”**ï¼šå·²è¯Šæ–­å‡ºé—®é¢˜ï¼Œå¿«é€ŸæŸ¥æ‰¾å¤„ç†æ–¹æ¡ˆ
> **ä½¿ç”¨**ï¼šæ ¹æ®é—®é¢˜ç±»å‹ï¼ŒæŸ¥è¡¨è·å¾—ä»£ç å’Œæ–¹æ¡ˆ

---

## ğŸ“‹ ä½¿ç”¨è¯´æ˜

æœ¬æ–‡æ¡£æŒ‰**é—®é¢˜ç±»å‹**ç»„ç»‡ï¼Œæ¯ä¸ªé—®é¢˜åŒ…å«ï¼š
1. âš¡ å¿«é€Ÿè¯†åˆ«æ–¹æ³•ï¼ˆæ£€æµ‹ä»£ç ï¼‰
2. ğŸ“Š ä¸¥é‡ç¨‹åº¦åˆ¤æ–­ï¼ˆæŸ¥è¡¨ï¼‰
3. ğŸ’¡ è§£å†³æ–¹æ¡ˆçŸ©é˜µï¼ˆå¤šç§æ–¹æ¡ˆå¯¹æ¯”ï¼‰
4. ğŸ’» ä»£ç ç¤ºä¾‹ï¼ˆç›´æ¥å¯ç”¨ï¼‰
5. âš ï¸ æ³¨æ„äº‹é¡¹ï¼ˆé¿å‘æŒ‡å—ï¼‰

**å¿«é€Ÿå¯¼èˆª**ï¼š
- [1. ç¼ºå¤±å€¼é—®é¢˜](#1-ç¼ºå¤±å€¼é—®é¢˜) - æœ€å¸¸è§
- [2. å¼‚å¸¸å€¼é—®é¢˜](#2-å¼‚å¸¸å€¼é—®é¢˜) - å½±å“å¤§
- [3. ç±»åˆ«ä¸å¹³è¡¡](#3-ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜) - åˆ†ç±»å¿…æŸ¥
- [4. é‡å¤å€¼é—®é¢˜](#4-é‡å¤å€¼é—®é¢˜) - ç®€å•
- [5. æ•°æ®æ³„æ¼](#5-æ•°æ®æ³„æ¼é£é™©) - ä¸¥é‡
- [6. æ•°æ®ç±»å‹](#6-æ•°æ®ç±»å‹é—®é¢˜) - åŸºç¡€

---

## 1. ç¼ºå¤±å€¼é—®é¢˜

### âš¡ å¿«é€Ÿè¯†åˆ«

```python
# æ£€æµ‹ç¼ºå¤±å€¼
missing_summary = df.isnull().sum()
missing_ratio = (df.isnull().sum() / len(df) * 100).round(2)

# åªæ˜¾ç¤ºæœ‰ç¼ºå¤±çš„åˆ—
print("ç¼ºå¤±å€¼ç»Ÿè®¡:")
print(missing_summary[missing_summary > 0])
print("\nç¼ºå¤±ç‡(%):")
print(missing_ratio[missing_ratio > 0])

# å¯è§†åŒ–ï¼ˆæ¨èï¼‰
import matplotlib.pyplot as plt
missing_cols = missing_ratio[missing_ratio > 0].sort_values(ascending=False)
missing_cols.plot(kind='barh', figsize=(10, 6))
plt.xlabel('ç¼ºå¤±ç‡ (%)')
plt.title('å„ç‰¹å¾ç¼ºå¤±å€¼æ¯”ä¾‹')
plt.show()
```

### ğŸ“Š ä¸¥é‡ç¨‹åº¦åˆ¤æ–­

| ç¼ºå¤±ç‡ | ä¸¥é‡ç¨‹åº¦ | æ¨èæ–¹æ¡ˆ | è¯´æ˜ |
|-------|---------|---------|------|
| <5% | âœ… è½»åº¦ | åˆ é™¤è¡Œæˆ–ç®€å•å¡«å…… | ä¿¡æ¯æŸå¤±å°ï¼Œå¿«é€Ÿå¤„ç† |
| 5-20% | âš ï¸ ä¸­åº¦ | æ™ºèƒ½å¡«å……ï¼ˆKNN/ä¸­ä½æ•°ï¼‰ | éœ€è¦ä¿ç•™ä¿¡æ¯ |
| 20-50% | âŒ ä¸¥é‡ | å»ºæ¨¡å¡«å……æˆ–åˆ é™¤åˆ— | æƒè¡¡ä¿¡æ¯vså‡†ç¡®æ€§ |
| >50% | âŒâŒ æä¸¥é‡ | åˆ é™¤åˆ— | ä¿¡æ¯å¤ªå°‘ï¼Œæ— æ„ä¹‰ |

### ğŸ’¡ è§£å†³æ–¹æ¡ˆçŸ©é˜µ

#### æ–¹æ¡ˆ1ï¼šåˆ é™¤æ³•ï¼ˆç¼ºå¤±<5%ï¼Œæ•°æ®é‡å……è¶³ï¼‰

**ä»£ç **ï¼š
```python
# åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ
df_clean = df.dropna()

print(f"åˆ é™¤å‰: {len(df)} è¡Œ")
print(f"åˆ é™¤å: {len(df_clean)} è¡Œ")
print(f"æŸå¤±: {len(df) - len(df_clean)} è¡Œ ({(len(df) - len(df_clean))/len(df)*100:.1f}%)")

# æˆ–åˆ é™¤ç¼ºå¤±ç‡>50%çš„åˆ—
threshold = 0.5
cols_to_drop = df.columns[df.isnull().mean() > threshold]
df_clean = df.drop(columns=cols_to_drop)
print(f"åˆ é™¤åˆ—: {list(cols_to_drop)}")
```

âœ… **é€‚ç”¨**ï¼šMCARï¼ˆå®Œå…¨éšæœºç¼ºå¤±ï¼‰ï¼Œæ•°æ®é‡å……è¶³ï¼ˆ>10Kï¼‰
âš ï¸ **æ³¨æ„**ï¼šåˆ é™¤ä¼šæŸå¤±ä¿¡æ¯ï¼Œç¡®ä¿æŸå¤±<10%

#### æ–¹æ¡ˆ2ï¼šç®€å•å¡«å……ï¼ˆç¼ºå¤±5-20%ï¼‰

**ä»£ç **ï¼š
```python
import pandas as pd
import numpy as np

# æ•°å€¼å‹ï¼šä¸­ä½æ•°å¡«å……ï¼ˆæ¯”å‡å€¼æ›´é²æ£’ï¼Œä¸å—å¼‚å¸¸å€¼å½±å“ï¼‰
for col in df.select_dtypes(include=[np.number]).columns:
    if df[col].isnull().sum() > 0:
        median_value = df[col].median()
        df[col].fillna(median_value, inplace=True)
        print(f"{col}: ç”¨ä¸­ä½æ•° {median_value:.2f} å¡«å……")

# ç±»åˆ«å‹ï¼šä¼—æ•°å¡«å……
for col in df.select_dtypes(include=['object', 'category']).columns:
    if df[col].isnull().sum() > 0:
        mode_value = df[col].mode()[0]
        df[col].fillna(mode_value, inplace=True)
        print(f"{col}: ç”¨ä¼—æ•° '{mode_value}' å¡«å……")

# æˆ–å¸¸æ•°å¡«å……ï¼ˆå¦‚æœç¼ºå¤±æœ¬èº«æœ‰æ„ä¹‰ï¼‰
df['income'].fillna(0, inplace=True)  # æ”¶å…¥ç¼ºå¤±å¯èƒ½è¡¨ç¤ºæ— æ”¶å…¥
df['city'].fillna('Unknown', inplace=True)  # åŸå¸‚ç¼ºå¤±æ ‡è®°ä¸ºæœªçŸ¥
```

âœ… **é€‚ç”¨**ï¼šMARï¼ˆéšæœºç¼ºå¤±ï¼‰ï¼Œåˆ†å¸ƒç®€å•ï¼Œå¿«é€Ÿå¤„ç†
âš ï¸ **æ³¨æ„**ï¼šä¼šä½ä¼°æ–¹å·®ï¼Œä¸é€‚åˆç¼ºå¤±ç‡>20%

#### æ–¹æ¡ˆ3ï¼šKNNå¡«å……ï¼ˆç¼ºå¤±>20%ï¼Œç‰¹å¾ç›¸å…³ï¼‰

**ä»£ç **ï¼š
```python
from sklearn.impute import KNNImputer
import pandas as pd

# åªå¯¹æ•°å€¼åˆ—ä½¿ç”¨KNNå¡«å……
numeric_cols = df.select_dtypes(include=[np.number]).columns
df_numeric = df[numeric_cols]

# KNNå¡«å……ï¼ˆä½¿ç”¨æœ€è¿‘çš„5ä¸ªé‚»å±…ï¼‰
imputer = KNNImputer(n_neighbors=5)
df_filled_numeric = pd.DataFrame(
    imputer.fit_transform(df_numeric),
    columns=numeric_cols,
    index=df.index
)

# åˆå¹¶å›åŸDataFrame
df[numeric_cols] = df_filled_numeric

print("KNNå¡«å……å®Œæˆ")
```

âœ… **é€‚ç”¨**ï¼šMARï¼Œç‰¹å¾é—´æœ‰ç›¸å…³æ€§ï¼ˆå¦‚èº«é«˜ä½“é‡ç›¸å…³ï¼‰
âš ï¸ **æ³¨æ„**ï¼šè®¡ç®—æˆæœ¬é«˜ï¼Œæ•°æ®é‡>50Kå¯èƒ½å¾ˆæ…¢

#### æ–¹æ¡ˆ4ï¼šå»ºæ¨¡å¡«å……ï¼ˆç¼ºå¤±>30%ï¼Œæœ€å‡†ç¡®ï¼‰

**ä»£ç **ï¼š
```python
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
import numpy as np

def model_imputation(df, col_to_fill):
    """
    ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹å¡«å……ç¼ºå¤±å€¼

    åŸç†ï¼šç”¨å…¶ä»–ç‰¹å¾é¢„æµ‹ç¼ºå¤±ç‰¹å¾
    """
    # åˆ†ç¦»æœ‰/æ— ç¼ºå¤±çš„æ•°æ®
    df_with_value = df[df[col_to_fill].notna()].copy()
    df_without_value = df[df[col_to_fill].isna()].copy()

    # é€‰æ‹©é¢„æµ‹ç‰¹å¾ï¼ˆé™¤äº†å¾…å¡«å……åˆ—ï¼Œå…¶ä»–æ•°å€¼åˆ—ï¼‰
    feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns
                    if c != col_to_fill]

    X_train = df_with_value[feature_cols].fillna(0)  # ç®€å•å¤„ç†å…¶ä»–ç¼ºå¤±
    y_train = df_with_value[col_to_fill]
    X_pred = df_without_value[feature_cols].fillna(0)

    # é€‰æ‹©æ¨¡å‹ï¼ˆæ•°å€¼ç”¨å›å½’ï¼Œç±»åˆ«ç”¨åˆ†ç±»ï¼‰
    if df[col_to_fill].dtype in [np.float64, np.int64]:
        model = RandomForestRegressor(n_estimators=100, random_state=42)
    else:
        model = RandomForestClassifier(n_estimators=100, random_state=42)

    # è®­ç»ƒå¹¶é¢„æµ‹
    model.fit(X_train, y_train)
    predictions = model.predict(X_pred)

    # å¡«å……
    df.loc[df[col_to_fill].isna(), col_to_fill] = predictions
    print(f"{col_to_fill}: æ¨¡å‹å¡«å…… {len(predictions)} ä¸ªç¼ºå¤±å€¼")

    return df

# ä½¿ç”¨ç¤ºä¾‹
df = model_imputation(df, 'age')
```

âœ… **é€‚ç”¨**ï¼šç¼ºå¤±ç‡é«˜ä½†ç‰¹å¾é‡è¦ï¼Œéœ€è¦æœ€å‡†ç¡®çš„å¡«å……
âš ï¸ **æ³¨æ„**ï¼šå¯èƒ½è¿‡æ‹Ÿåˆï¼Œè®¡ç®—æˆæœ¬æœ€é«˜

#### æ–¹æ¡ˆ5ï¼šä¿ç•™ç¼ºå¤±ä¿¡æ¯ï¼ˆMNARï¼‰

**ä»£ç **ï¼š
```python
# åˆ›å»ºç¼ºå¤±æŒ‡ç¤ºåˆ—ï¼ˆç¼ºå¤±æœ¬èº«å¯èƒ½æœ‰æ„ä¹‰ï¼‰
for col in df.columns:
    if df[col].isnull().sum() > 0:
        df[f'{col}_missing'] = df[col].isna().astype(int)
        print(f"åˆ›å»ºæŒ‡ç¤ºåˆ—: {col}_missing")

# ç„¶åç”¨ç®€å•æ–¹æ³•å¡«å……åŸåˆ—
df[col].fillna(df[col].median(), inplace=True)
```

âœ… **é€‚ç”¨**ï¼šMNARï¼ˆç¼ºå¤±æœ¬èº«æœ‰ä¿¡æ¯ï¼Œå¦‚å¯Œäººä¸æ„¿å¡«æ”¶å…¥ï¼‰
âš ï¸ **æ³¨æ„**ï¼šå¢åŠ ç‰¹å¾ç»´åº¦ï¼Œå¯èƒ½å¼•å…¥å™ªå£°

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **å…ˆåˆ’åˆ†train/testï¼Œå†å¡«å……**ï¼ˆé¿å…æ•°æ®æ³„æ¼ï¼‰
```python
# âŒ é”™è¯¯ï¼šåœ¨åˆ’åˆ†å‰å¡«å……
df_filled = df.fillna(df.median())
X_train, X_test = train_test_split(df_filled)

# âœ… æ­£ç¡®ï¼šå…ˆåˆ’åˆ†ï¼Œå†å¡«å……
X_train, X_test = train_test_split(df)
# åªç”¨è®­ç»ƒé›†çš„ä¸­ä½æ•°
fill_values = X_train.median()
X_train_filled = X_train.fillna(fill_values)
X_test_filled = X_test.fillna(fill_values)  # ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡
```

2. **æŸäº›ç®—æ³•åŸç”Ÿæ”¯æŒç¼ºå¤±å€¼**
- XGBoostã€LightGBMï¼šå¯ç›´æ¥å¤„ç†NaN
- å¦‚æœç”¨è¿™äº›ç®—æ³•ï¼Œå¯ä»¥è·³è¿‡å¡«å……

**å»¶ä¼¸é˜…è¯»** â†’ [common_data_issues.md](common_data_issues.md) ç¬¬1èŠ‚

---

## 2. å¼‚å¸¸å€¼é—®é¢˜

### âš¡ å¿«é€Ÿè¯†åˆ«

```python
import numpy as np
from scipy import stats

# æ–¹æ³•1ï¼šç®±çº¿å›¾å¯è§†åŒ–ï¼ˆæ¨èï¼‰
import matplotlib.pyplot as plt
df.boxplot(figsize=(15, 10), rot=45)
plt.title('å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆç®±çº¿å›¾ï¼‰')
plt.show()

# æ–¹æ³•2ï¼šIQRæ–¹æ³•æ£€æµ‹
def detect_outliers_iqr(df):
    outlier_summary = {}
    for col in df.select_dtypes(include=[np.number]).columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
        outlier_ratio = outliers / len(df) * 100

        if outliers > 0:
            outlier_summary[col] = {
                'count': outliers,
                'ratio': outlier_ratio,
                'bounds': (lower_bound, upper_bound)
            }

    return outlier_summary

outliers = detect_outliers_iqr(df)
for col, info in outliers.items():
    print(f"{col}: {info['count']} ä¸ªå¼‚å¸¸å€¼ ({info['ratio']:.1f}%)")
    print(f"  æ­£å¸¸èŒƒå›´: [{info['bounds'][0]:.2f}, {info['bounds'][1]:.2f}]")
```

### ğŸ“Š ä¸¥é‡ç¨‹åº¦åˆ¤æ–­ & æ€§è´¨åˆ¤æ–­

å…ˆåˆ¤æ–­**æ€§è´¨**ï¼Œå†å†³å®šæ–¹æ¡ˆï¼š

| å¼‚å¸¸å€¼æ€§è´¨ | åˆ¤æ–­ä¾æ® | ç¤ºä¾‹ | æ¨èæ–¹æ¡ˆ |
|-----------|---------|------|---------|
| **æ•°æ®é”™è¯¯** | ä¸ç¬¦åˆä¸šåŠ¡é€»è¾‘ | å¹´é¾„200å²ã€æ”¶å…¥ä¸ºè´Ÿ | åˆ é™¤æˆ–ä¿®æ­£ |
| **çœŸå®æå€¼** | ç¬¦åˆä¸šåŠ¡é€»è¾‘ä½†æç«¯ | å¯Œè±ªæ”¶å…¥æ˜¯æ™®é€šäºº100å€ | é²æ£’æ ‡å‡†åŒ–/logå˜æ¢ |
| **æ½œåœ¨å¼‚å¸¸** | å¯èƒ½æ˜¯æ¬ºè¯ˆ/å¼‚å¸¸äº‹ä»¶ | ä¿¡ç”¨å¡å¼‚å¸¸äº¤æ˜“ | å•ç‹¬å»ºæ¨¡ï¼ˆå¼‚å¸¸æ£€æµ‹ï¼‰ |

### ğŸ’¡ è§£å†³æ–¹æ¡ˆçŸ©é˜µ

#### æ–¹æ¡ˆAï¼šåˆ é™¤æˆ–ä¿®æ­£ï¼ˆæ•°æ®é”™è¯¯ï¼‰

**ä»£ç **ï¼š
```python
# ç¤ºä¾‹ï¼šå¤„ç†å¹´é¾„å¼‚å¸¸
print(f"ä¿®æ­£å‰: min={df['age'].min()}, max={df['age'].max()}")

# åˆ é™¤ä¸åˆç†çš„å€¼
df = df[(df['age'] >= 0) & (df['age'] <= 120)]

# æˆ–ä¿®æ­£æ˜æ˜¾çš„é”™è¯¯ï¼ˆå¦‚è¾“å…¥é”™è¯¯ï¼š200 â†’ 20ï¼‰
df.loc[df['age'] > 120, 'age'] = df[df['age'] > 120]['age'] / 10

print(f"ä¿®æ­£å: min={df['age'].min()}, max={df['age'].max()}")
```

âœ… **é€‚ç”¨**ï¼šæ˜ç¡®çš„æ•°æ®é”™è¯¯
âš ï¸ **æ³¨æ„**ï¼šéœ€è¦ä¸šåŠ¡çŸ¥è¯†åˆ¤æ–­

#### æ–¹æ¡ˆBï¼šé²æ£’æ ‡å‡†åŒ–ï¼ˆçœŸå®æå€¼ï¼‰

**ä»£ç **ï¼š
```python
from sklearn.preprocessing import RobustScaler

# RobustScalerï¼šä½¿ç”¨ä¸­ä½æ•°å’ŒIQRï¼Œä¸å—æå€¼å½±å“
scaler = RobustScaler()

numeric_cols = df.select_dtypes(include=[np.number]).columns
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

print("ä½¿ç”¨RobustScaleræ ‡å‡†åŒ–å®Œæˆ")
```

âœ… **é€‚ç”¨**ï¼šçœŸå®æå€¼ï¼Œéœ€è¦ä¿ç•™ä½†é™ä½å½±å“
âš ï¸ **æ³¨æ„**ï¼šæ¯”StandardScaleræ›´é²æ£’

#### æ–¹æ¡ˆCï¼šlogå˜æ¢ï¼ˆå³åæ•°æ®ï¼‰

**ä»£ç **ï¼š
```python
# logå˜æ¢ï¼ˆå¤„ç†å³åæ•°æ®ï¼Œå¦‚æ”¶å…¥ã€æˆ¿ä»·ï¼‰
df['income_log'] = np.log1p(df['income'])  # log1p = log(1+x)ï¼Œå¤„ç†0å€¼

# å¯¹æ¯”åŸå§‹å’Œå˜æ¢åçš„åˆ†å¸ƒ
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
df['income'].hist(bins=50, ax=ax1)
ax1.set_title('åŸå§‹åˆ†å¸ƒï¼ˆå³åï¼‰')
df['income_log'].hist(bins=50, ax=ax2)
ax2.set_title('logå˜æ¢åï¼ˆæ­£æ€ï¼‰')
plt.show()
```

âœ… **é€‚ç”¨**ï¼šå³ååˆ†å¸ƒï¼ˆæ”¶å…¥ã€æˆ¿ä»·ã€ç‚¹å‡»é‡ç­‰ï¼‰
âš ï¸ **æ³¨æ„**ï¼šå˜æ¢åè¦è®°å¾—åœ¨é¢„æµ‹æ—¶é€†å˜æ¢

#### æ–¹æ¡ˆDï¼šæˆªæ–­æ³•ï¼ˆWinsorizationï¼‰

**ä»£ç **ï¼š
```python
# è®¾ç½®ä¸Šä¸‹é™ï¼ˆ1%å’Œ99%åˆ†ä½æ•°ï¼‰
lower = df['income'].quantile(0.01)
upper = df['income'].quantile(0.99)

print(f"æˆªæ–­å‰: min={df['income'].min():.2f}, max={df['income'].max():.2f}")

# æˆªæ–­ï¼ˆå°†è¶…å‡ºèŒƒå›´çš„å€¼è®¾ä¸ºè¾¹ç•Œå€¼ï¼‰
df['income'] = df['income'].clip(lower=lower, upper=upper)

print(f"æˆªæ–­å: min={df['income'].min():.2f}, max={df['income'].max():.2f}")
```

âœ… **é€‚ç”¨**ï¼šä¿ç•™æ‰€æœ‰æ ·æœ¬ï¼Œä½†é™åˆ¶æå€¼å½±å“
âš ï¸ **æ³¨æ„**ï¼šæ”¹å˜äº†æ•°æ®åˆ†å¸ƒ

#### æ–¹æ¡ˆEï¼šä½¿ç”¨é²æ£’ç®—æ³•ï¼ˆæ— éœ€å¤„ç†å¼‚å¸¸å€¼ï¼‰

**æ¨èç®—æ³•**ï¼š
- **æ ‘æ¨¡å‹**ï¼šRandomForestã€XGBoostã€LightGBMï¼ˆå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼‰
- **é²æ£’å›å½’**ï¼šHuberRegressorã€RANSACRegressor

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import HuberRegressor

# æ–¹æ¡ˆ1ï¼šä½¿ç”¨æ ‘æ¨¡å‹ï¼ˆæ¨èï¼‰
model = RandomForestRegressor()  # æ— éœ€å¤„ç†å¼‚å¸¸å€¼

# æ–¹æ¡ˆ2ï¼šé²æ£’å›å½’
model = HuberRegressor()  # å¯¹å¼‚å¸¸å€¼é²æ£’çš„çº¿æ€§å›å½’
```

âœ… **é€‚ç”¨**ï¼šå¼‚å¸¸å€¼æ˜¯æ•°æ®çš„ä¸€éƒ¨åˆ†ï¼Œéœ€è¦ä¿ç•™
âš ï¸ **æ³¨æ„**ï¼šæ ‘æ¨¡å‹å¯¹å¼‚å¸¸å€¼æœ€é²æ£’

### âš ï¸ æ³¨æ„äº‹é¡¹

**å¼‚å¸¸å€¼ â‰  ä¸€å®šè¦åˆ é™¤**

åˆ¤æ–­æµç¨‹ï¼š
```
å‘ç°å¼‚å¸¸å€¼
   â†“
ä¸šåŠ¡é€»è¾‘åˆ¤æ–­
   â†“
â”Œâ”€ ä¸åˆç†ï¼ˆå¹´é¾„è´Ÿæ•°ã€æ”¶å…¥è´Ÿæ•°ï¼‰ â†’ æ•°æ®é”™è¯¯ â†’ åˆ é™¤
â”‚
â”œâ”€ åˆç†ä½†æç«¯ï¼ˆå¯Œè±ªã€è±ªå®…ï¼‰ â†’ çœŸå®æå€¼ â†’ é²æ£’å¤„ç†
â”‚
â””â”€ å¯èƒ½æ˜¯ç›®æ ‡ï¼ˆæ¬ºè¯ˆæ£€æµ‹ï¼‰ â†’ æ½œåœ¨å¼‚å¸¸ â†’ å•ç‹¬å»ºæ¨¡
```

**å»¶ä¼¸é˜…è¯»** â†’ [common_data_issues.md](common_data_issues.md) ç¬¬2èŠ‚

---

## 3. ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

### âš¡ å¿«é€Ÿè¯†åˆ«

```python
# æ£€æµ‹ç›®æ ‡å˜é‡åˆ†å¸ƒ
print("ç±»åˆ«åˆ†å¸ƒï¼ˆæ•°é‡ï¼‰:")
print(df['target'].value_counts())

print("\nç±»åˆ«åˆ†å¸ƒï¼ˆæ¯”ä¾‹ï¼‰:")
print(df['target'].value_counts(normalize=True))

# å¯è§†åŒ–
import matplotlib.pyplot as plt
df['target'].value_counts().plot(kind='bar')
plt.title('ç›®æ ‡å˜é‡åˆ†å¸ƒ')
plt.xlabel('ç±»åˆ«')
plt.ylabel('æ•°é‡')
plt.show()

# è®¡ç®—ä¸å¹³è¡¡æ¯”ä¾‹
value_counts = df['target'].value_counts()
max_ratio = value_counts.max() / value_counts.sum()
min_ratio = value_counts.min() / value_counts.sum()
imbalance_ratio = max_ratio / min_ratio
print(f"\nä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.1f}:1")
```

### ğŸ“Š ä¸¥é‡ç¨‹åº¦åˆ¤æ–­

| æ¯”ä¾‹ | ä¸å¹³è¡¡ç¨‹åº¦ | æ¨èæ–¹æ¡ˆ | è¯„ä¼°æŒ‡æ ‡ |
|------|----------|---------|---------|
| 4:6 ~ 5:5 | âœ… å¹³è¡¡ | æ— éœ€ç‰¹æ®Šå¤„ç† | Accuracy |
| 3:7 | âš ï¸ è½»åº¦ | è°ƒæ•´è¯„ä¼°æŒ‡æ ‡ | F1-Score |
| 2:8 ~ 1:9 | âŒ ä¸­åº¦ | SMOTEæˆ–ç±»æƒé‡ | F1/Precision/Recall |
| <1:10 | âŒâŒ ä¸¥é‡ | ç‰¹æ®Šç®—æ³•+é‡‡æ ·+é›†æˆ | AUC-ROCã€PR-AUC |

### ğŸ’¡ è§£å†³æ–¹æ¡ˆçŸ©é˜µ

#### æ–¹æ¡ˆ1ï¼šSMOTEè¿‡é‡‡æ ·ï¼ˆä¸­åº¦ä¸å¹³è¡¡ï¼‰

**ä»£ç **ï¼š
```python
from imblearn.over_sampling import SMOTE
from collections import Counter

print("é‡é‡‡æ ·å‰:", Counter(y))

# SMOTEï¼šåˆæˆå°‘æ•°ç±»æ ·æœ¬ï¼ˆæ¨èï¼‰
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("é‡é‡‡æ ·å:", Counter(y_resampled))
```

âœ… **é€‚ç”¨**ï¼šä¸­åº¦ä¸å¹³è¡¡ï¼ˆ1:5åˆ°1:10ï¼‰
âš ï¸ **æ³¨æ„**ï¼šå¯èƒ½è¿‡æ‹Ÿåˆï¼Œä»…ç”¨äºè®­ç»ƒé›†

#### æ–¹æ¡ˆ2ï¼šç±»æƒé‡è°ƒæ•´ï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰

**ä»£ç **ï¼š
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

# æ–¹æ³•Aï¼šè‡ªåŠ¨è®¡ç®—ç±»æƒé‡ï¼ˆæ¨èï¼‰
model = RandomForestClassifier(class_weight='balanced')
model.fit(X_train, y_train)

# æ–¹æ³•Bï¼šæ‰‹åŠ¨è®¾ç½®ç±»æƒé‡
# å¦‚æœ0ç±»:1ç±» = 9:1ï¼Œç»™1ç±»10å€æƒé‡
model = LogisticRegression(class_weight={0: 1, 1: 10})
model.fit(X_train, y_train)

# æ–¹æ³•Cï¼šè®¡ç®—ç±»æƒé‡
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight('balanced',
                                     classes=np.unique(y_train),
                                     y=y_train)
print("ç±»æƒé‡:", dict(enumerate(class_weights)))
```

âœ… **é€‚ç”¨**ï¼šæ‰€æœ‰ä¸å¹³è¡¡åœºæ™¯ï¼Œæ— éœ€æ”¹å˜æ•°æ®
âš ï¸ **æ³¨æ„**ï¼šä¸æ˜¯æ‰€æœ‰ç®—æ³•éƒ½æ”¯æŒclass_weightå‚æ•°

#### æ–¹æ¡ˆ3ï¼šæ”¹å˜è¯„ä¼°æŒ‡æ ‡ï¼ˆå¿…é¡»ï¼‰

**ä»£ç **ï¼š
```python
from sklearn.metrics import (classification_report, confusion_matrix,
                             f1_score, precision_score, recall_score,
                             roc_auc_score, average_precision_score)

# ä¸è¦åªçœ‹Accuracyï¼
print("Accuracy:", accuracy_score(y_true, y_pred))  # âŒ ä¸å¹³è¡¡æ•°æ®æ— æ„ä¹‰

# âœ… ä½¿ç”¨ä»¥ä¸‹æŒ‡æ ‡
print("F1-Score:", f1_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred))
print("Recall:", recall_score(y_true, y_pred))
print("AUC-ROC:", roc_auc_score(y_true, y_pred_proba))
print("PR-AUC:", average_precision_score(y_true, y_pred_proba))

# æ··æ·†çŸ©é˜µï¼ˆè¯¦ç»†åˆ†æï¼‰
print("\næ··æ·†çŸ©é˜µ:")
print(confusion_matrix(y_true, y_pred))

# åˆ†ç±»æŠ¥å‘Šï¼ˆç»¼åˆï¼‰
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_true, y_pred))
```

âœ… **é€‚ç”¨**ï¼šæ‰€æœ‰ä¸å¹³è¡¡åœºæ™¯ï¼ˆå¿…é¡»åšï¼‰
âš ï¸ **æ³¨æ„**ï¼šæ ¹æ®ä¸šåŠ¡ç›®æ ‡é€‰æ‹©æŒ‡æ ‡ï¼ˆPrecision vs Recallï¼‰

#### æ–¹æ¡ˆ4ï¼šä½¿ç”¨ä¸“é—¨ç®—æ³•ï¼ˆä¸¥é‡ä¸å¹³è¡¡ï¼‰

**ä»£ç **ï¼š
```python
import xgboost as xgb
from sklearn.ensemble import BalancedRandomForestClassifier

# æ–¹æ³•Aï¼šXGBoost with scale_pos_weight
scale_pos_weight = (y == 0).sum() / (y == 1).sum()  # è´Ÿç±»/æ­£ç±»
model = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight)

# æ–¹æ³•Bï¼šBalancedRandomForest
model = BalancedRandomForestClassifier(n_estimators=100)

model.fit(X_train, y_train)
```

âœ… **é€‚ç”¨**ï¼šä¸¥é‡ä¸å¹³è¡¡ï¼ˆ>1:10ï¼‰
âš ï¸ **æ³¨æ„**ï¼šé…åˆæ–¹æ¡ˆ2å’Œæ–¹æ¡ˆ3ä¸€èµ·ä½¿ç”¨

#### æ–¹æ¡ˆ5ï¼šé›†æˆå¤šç§æ–¹æ³•ï¼ˆæœ€ä½³å®è·µï¼‰

**ä»£ç **ï¼š
```python
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier

# ç»„åˆï¼šSMOTE + ç±»æƒé‡ + æ­£ç¡®çš„è¯„ä¼°æŒ‡æ ‡
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

model = RandomForestClassifier(class_weight='balanced', n_estimators=100)
model.fit(X_train_res, y_train_res)

# ä½¿ç”¨F1-Scoreè¯„ä¼°
y_pred = model.predict(X_test)
print("F1-Score:", f1_score(y_test, y_pred))
```

âœ… **é€‚ç”¨**ï¼šä¸¥é‡ä¸å¹³è¡¡ï¼Œè¿½æ±‚æœ€ä½³æ€§èƒ½
âš ï¸ **æ³¨æ„**ï¼šè®¡ç®—æˆæœ¬é«˜ï¼Œä½†æ•ˆæœæœ€å¥½

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **ä»…å¯¹è®­ç»ƒé›†é‡‡æ ·**ï¼ˆæµ‹è¯•é›†ä¿æŒåŸå§‹åˆ†å¸ƒï¼‰
```python
# âŒ é”™è¯¯
X_res, y_res = SMOTE().fit_resample(X, y)
X_train, X_test = train_test_split(X_res, y_res)

# âœ… æ­£ç¡®
X_train, X_test, y_train, y_test = train_test_split(X, y)
X_train_res, y_train_res = SMOTE().fit_resample(X_train, y_train)
# X_testä¿æŒä¸å˜
```

2. **æ ¹æ®ä¸šåŠ¡ç›®æ ‡é€‰æ‹©è¯„ä¼°æŒ‡æ ‡**
- æ¬ºè¯ˆæ£€æµ‹ï¼šRecallä¼˜å…ˆï¼ˆä¸èƒ½æ¼æ‰æ¬ºè¯ˆï¼‰
- åƒåœ¾é‚®ä»¶ï¼šPrecisionä¼˜å…ˆï¼ˆä¸èƒ½è¯¯åˆ¤æ­£å¸¸é‚®ä»¶ï¼‰
- å¹³è¡¡ï¼šF1-Score

**å»¶ä¼¸é˜…è¯»** â†’ [common_data_issues.md](common_data_issues.md) ç¬¬3èŠ‚

---

## 4. é‡å¤å€¼é—®é¢˜

### âš¡ å¿«é€Ÿè¯†åˆ«

```python
# æ£€æµ‹å®Œå…¨é‡å¤çš„è¡Œ
n_duplicates = df.duplicated().sum()
print(f"é‡å¤è¡Œæ•°: {n_duplicates} ({n_duplicates/len(df)*100:.1f}%)")

# æŸ¥çœ‹é‡å¤æ ·æœ¬
duplicates = df[df.duplicated(keep=False)]  # keep=Falseæ˜¾ç¤ºæ‰€æœ‰é‡å¤
print(f"\né‡å¤æ ·æœ¬ç¤ºä¾‹:")
print(duplicates.head(10))

# æ£€æµ‹ç‰¹å®šåˆ—é‡å¤ï¼ˆå¦‚IDåˆ—ï¼‰
if 'user_id' in df.columns:
    id_duplicates = df['user_id'].duplicated().sum()
    print(f"\nIDé‡å¤æ•°: {id_duplicates}")
```

### ğŸ’¡ è§£å†³æ–¹æ¡ˆ

**ç›´æ¥åˆ é™¤**ï¼ˆå¤§å¤šæ•°æƒ…å†µï¼‰

```python
# åˆ é™¤é‡å¤è¡Œï¼ˆä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°ï¼‰
print(f"åˆ é™¤å‰: {len(df)} è¡Œ")
df_clean = df.drop_duplicates()
print(f"åˆ é™¤å: {len(df_clean)} è¡Œ")
print(f"åˆ é™¤äº† {len(df) - len(df_clean)} è¡Œ")

# æˆ–æŒ‡å®šåˆ—åˆ¤æ–­é‡å¤ï¼ˆå¦‚åªçœ‹IDåˆ—ï¼‰
df_clean = df.drop_duplicates(subset=['user_id'])

# æˆ–ä¿ç•™æœ€åä¸€æ¬¡å‡ºç°
df_clean = df.drop_duplicates(keep='last')
```

### âš ï¸ æ³¨æ„äº‹é¡¹

1. ç¡®è®¤æ˜¯çœŸé‡å¤è¿˜æ˜¯æ•°æ®é”™è¯¯
2. æŸäº›ä¸šåŠ¡åœºæ™¯é‡å¤æ˜¯æ­£å¸¸çš„ï¼ˆå¦‚ç”¨æˆ·å¤šæ¬¡è´­ä¹°ï¼‰

---

## 5. æ•°æ®æ³„æ¼é£é™©

### âš¡ å¿«é€Ÿè¯†åˆ«

```python
# æ£€æŸ¥1ï¼šå”¯ä¸€å€¼æ¯”ä¾‹>95%çš„åˆ—ï¼ˆå¯èƒ½æ˜¯IDï¼‰
unique_ratios = df.nunique() / len(df)
potential_ids = unique_ratios[unique_ratios > 0.95].index.tolist()
print(f"ç–‘ä¼¼IDåˆ—: {potential_ids}")

# æ£€æŸ¥2ï¼šä¸ç›®æ ‡å®Œå…¨ç›¸å…³ï¼ˆrâ‰ˆ1.0ï¼‰
if 'target' in df.columns:
    corr_with_target = df.corr()['target'].abs().sort_values(ascending=False)
    potential_leakage = corr_with_target[corr_with_target > 0.99].index.tolist()
    print(f"\nç–‘ä¼¼æ³„æ¼ç‰¹å¾ï¼ˆä¸ç›®æ ‡ç›¸å…³r>0.99ï¼‰:")
    print(potential_leakage)

# æ£€æŸ¥3ï¼šå¸¸é‡æˆ–å‡†å¸¸é‡ï¼ˆæ–¹å·®â‰ˆ0ï¼‰
low_variance = df.var()
potential_constants = low_variance[low_variance < 0.01].index.tolist()
print(f"\nå‡†å¸¸é‡ç‰¹å¾ï¼ˆæ–¹å·®<0.01ï¼‰: {potential_constants}")
```

### ğŸ’¡ è§£å†³æ–¹æ¡ˆ

**åˆ é™¤é£é™©ç‰¹å¾**

```python
# åˆ é™¤IDåˆ—
id_cols = ['user_id', 'order_id', 'transaction_id']
df = df.drop(columns=[c for c in id_cols if c in df.columns])

# åˆ é™¤æ³„æ¼ç‰¹å¾
# ä¾‹å¦‚ï¼šé¢„æµ‹æ˜¯å¦è´­ä¹°ï¼Œä½†æœ‰"è´­ä¹°é‡‘é¢"åˆ—ï¼ˆåªæœ‰è´­ä¹°äº†æ‰æœ‰é‡‘é¢ï¼‰
df = df.drop(columns=['purchase_amount'])

# åˆ é™¤å¸¸é‡åˆ—
constant_cols = df.columns[df.nunique() == 1]
df = df.drop(columns=constant_cols)

print(f"åˆ é™¤åå‰©ä½™ç‰¹å¾: {df.shape[1]}")
```

### âš ï¸ æ³¨æ„äº‹é¡¹

**å¸¸è§æ•°æ®æ³„æ¼åœºæ™¯**ï¼š
1. IDåˆ—è¢«ç”¨ä½œç‰¹å¾
2. æœªæ¥ä¿¡æ¯ï¼ˆé¢„æµ‹tæ—¶åˆ»ï¼Œä½¿ç”¨äº†t+1æ—¶åˆ»çš„æ•°æ®ï¼‰
3. ç›®æ ‡å˜é‡çš„å˜ç§ï¼ˆé¢„æµ‹è´­ä¹°ï¼ŒåŒ…å«è´­ä¹°é‡‘é¢ï¼‰
4. æµ‹è¯•é›†ç»Ÿè®¡é‡æ³„æ¼åˆ°è®­ç»ƒé›†ï¼ˆé”™è¯¯çš„æ ‡å‡†åŒ–ï¼‰

**å»¶ä¼¸é˜…è¯»** â†’ [common_data_issues.md](common_data_issues.md) ç¬¬5èŠ‚

---

## 6. æ•°æ®ç±»å‹é—®é¢˜

### âš¡ å¿«é€Ÿè¯†åˆ«

```python
# æ£€æŸ¥æ•°æ®ç±»å‹
print(df.dtypes)

# æ‰¾å‡ºæ•°å€¼å‹è¢«è¯†åˆ«ä¸ºå­—ç¬¦ä¸²çš„åˆ—
for col in df.select_dtypes(include=['object']).columns:
    try:
        pd.to_numeric(df[col])
        print(f"{col}: æ•°å€¼å‹è¢«è¯†åˆ«ä¸ºå­—ç¬¦ä¸²")
    except:
        pass
```

### ğŸ’¡ è§£å†³æ–¹æ¡ˆçŸ©é˜µ

| é—®é¢˜ | æ£€æµ‹ | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| æ•°å€¼è¢«è¯†åˆ«ä¸ºå­—ç¬¦ä¸² | `df.dtypes` æ˜¾ç¤ºobject | `pd.to_numeric(df['col'], errors='coerce')` |
| ç±»åˆ«è¢«è¯†åˆ«ä¸ºæ•°å€¼ | ä¸šåŠ¡åˆ¤æ–­ | `df['zipcode'].astype('category')` |
| æ—¥æœŸè¢«è¯†åˆ«ä¸ºå­—ç¬¦ä¸² | `df.dtypes` æ˜¾ç¤ºobject | `pd.to_datetime(df['date'])` |

**ä»£ç **ï¼š
```python
# è½¬æ¢æ•°å€¼ç±»å‹
df['price'] = pd.to_numeric(df['price'], errors='coerce')  # æ— æ³•è½¬æ¢çš„å˜NaN

# è½¬æ¢ç±»åˆ«ç±»å‹
df['zipcode'] = df['zipcode'].astype('category')

# è½¬æ¢æ—¥æœŸç±»å‹
df['date'] = pd.to_datetime(df['date'])

# æ‰¹é‡è‡ªåŠ¨æ¨æ–­
df = df.convert_dtypes()  # pandasè‡ªåŠ¨æ¨æ–­ç±»å‹
```

---

## ğŸ¯ å¿«é€Ÿå†³ç­–æµç¨‹

```
é‡åˆ°æ•°æ®é—®é¢˜
   â†“
æŸ¥çœ‹æœ¬æ–‡æ¡£ç›®å½• â†’ æ‰¾åˆ°å¯¹åº”é—®é¢˜ç±»å‹
   â†“
æŸ¥çœ‹"å¿«é€Ÿè¯†åˆ«"ä»£ç  â†’ ç¡®è®¤é—®é¢˜
   â†“
æŸ¥çœ‹"ä¸¥é‡ç¨‹åº¦åˆ¤æ–­"è¡¨æ ¼ â†’ è¯„ä¼°ä¼˜å…ˆçº§
   â†“
æŸ¥çœ‹"è§£å†³æ–¹æ¡ˆçŸ©é˜µ" â†’ é€‰æ‹©åˆé€‚æ–¹æ¡ˆ
   â†“
å¤åˆ¶"ä»£ç ç¤ºä¾‹" â†’ ç›´æ¥ä½¿ç”¨
   â†“
å®Œæˆå¤„ç† â†’ è¿›å…¥ä¸‹ä¸€ä¸ªé—®é¢˜
```

---

## ğŸ“– ç›¸å…³æ–‡æ¡£

- **æ·±å…¥äº†è§£é—®é¢˜** â†’ [common_data_issues.md](common_data_issues.md)
- **ç³»ç»ŸåŒ–è¯Šæ–­** â†’ [data_diagnosis_decision_tree.md](data_diagnosis_decision_tree.md)
- **å®Œæ•´é¢„å¤„ç†** â†’ [../04_preprocessing_and_features/](../04_preprocessing_and_features/)

---

**æœ€åæ›´æ–°**ï¼š2024å¹´11æœˆ
**æ ¸å¿ƒä»·å€¼**ï¼šé—®é¢˜â†’æ–¹æ¡ˆä¸€å¯¹ä¸€æ˜ å°„ï¼Œä»£ç ç›´æ¥å¯ç”¨
**ä½¿ç”¨é¢‘ç‡**ï¼šæ¯æ¬¡è¯Šæ–­å‘ç°é—®é¢˜åæŸ¥é˜…

**ä¸‹ä¸€æ­¥** â†’ æŸ¥çœ‹ [04_preprocessing_and_features](../04_preprocessing_and_features/) è¿›è¡Œç³»ç»ŸåŒ–é¢„å¤„ç†ï¼
