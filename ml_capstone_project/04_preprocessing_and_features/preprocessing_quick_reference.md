# æ•°æ®é¢„å¤„ç†å¿«é€Ÿå‚è€ƒâ­

> **æ ¸å¿ƒç†å¿µ**ï¼š"15åˆ†é’Ÿå®ŒæˆåŸºç¡€é¢„å¤„ç†å†³ç­–ï¼Œå¿«é€Ÿè¿›å…¥å»ºæ¨¡é˜¶æ®µ"
>
> **é€‚ç”¨åœºæ™¯**ï¼šæ‹¿åˆ°æ–°æ•°æ®åçš„å¿«é€Ÿé¢„å¤„ç†ã€é¡¹ç›®åˆæœŸçš„Baselineå»ºç«‹ã€æ—¶é—´ç´§æ€¥çš„å¿«é€Ÿå®éªŒ

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿä½¿ç”¨æŒ‡å—ï¼ˆ3åˆ†é’Ÿå¿…è¯»ï¼‰](#1-å¿«é€Ÿä½¿ç”¨æŒ‡å—3åˆ†é’Ÿå¿…è¯»)
2. [5æ­¥å¿«é€Ÿé¢„å¤„ç†ï¼ˆ15åˆ†é’Ÿï¼‰](#2-5æ­¥å¿«é€Ÿé¢„å¤„ç†15åˆ†é’Ÿ)
3. [å¸¸ç”¨å†³ç­–çŸ©é˜µé€ŸæŸ¥](#3-å¸¸ç”¨å†³ç­–çŸ©é˜µé€ŸæŸ¥)
4. [å®æˆ˜ç¤ºä¾‹](#4-å®æˆ˜ç¤ºä¾‹)
5. [å¿«é€Ÿä»£ç æ¨¡æ¿](#5-å¿«é€Ÿä»£ç æ¨¡æ¿)

---

## 1. å¿«é€Ÿä½¿ç”¨æŒ‡å—ï¼ˆ3åˆ†é’Ÿå¿…è¯»ï¼‰

### 1.1 æœ¬æ–‡æ¡£å®šä½

**ä¸æ˜¯ä»€ä¹ˆ**ï¼š
- âŒ ä¸æ˜¯å®Œæ•´çš„é¢„å¤„ç†æ•™ç¨‹ï¼ˆè¯·çœ‹å…¶ä»–æ–‡æ¡£ï¼‰
- âŒ ä¸æ˜¯æœ€ä¼˜æ–¹æ¡ˆï¼ˆåªæ˜¯å¿«é€Ÿå¯ç”¨æ–¹æ¡ˆï¼‰
- âŒ ä¸æ›¿ä»£æ·±å…¥è¯Šæ–­ï¼ˆé‡è¦é¡¹ç›®è¯·ç”¨å®Œæ•´æµç¨‹ï¼‰

**æ˜¯ä»€ä¹ˆ**ï¼š
- âœ… 15åˆ†é’Ÿå¿«é€Ÿå†³ç­–å·¥å…·
- âœ… åŸºäºç»éªŒçš„å¸¸ç”¨æ–¹æ¡ˆé›†åˆ
- âœ… å¿«é€Ÿå»ºç«‹Baselineçš„èµ·ç‚¹

### 1.2 ä½•æ—¶ä½¿ç”¨æœ¬æ–‡æ¡£

| åœºæ™¯ | æ¨èä½¿ç”¨ | é¢„è®¡æ—¶é—´ |
|------|---------|---------|
| æ–°æ•°æ®å¿«é€Ÿé¢„å¤„ç† | âœ… æœ¬æ–‡æ¡£ | 15åˆ†é’Ÿ |
| å¿«é€ŸBaselineå»ºç«‹ | âœ… æœ¬æ–‡æ¡£ | 15-20åˆ†é’Ÿ |
| Kaggleå¿«é€Ÿè¯•éªŒ | âœ… æœ¬æ–‡æ¡£ | 10-15åˆ†é’Ÿ |
| é‡è¦é¡¹ç›® | âš ï¸ ç”¨å®Œæ•´æµç¨‹ | 1-2å°æ—¶ |
| ç”Ÿäº§ç¯å¢ƒ | âš ï¸ ç”¨å®Œæ•´æµç¨‹ | 2-3å°æ—¶ |

### 1.3 ä½¿ç”¨æµç¨‹

```
Step 1: æ‰«ææ•°æ®æ¦‚å†µï¼ˆ1åˆ†é’Ÿï¼‰
  â””â”€ è¿è¡Œå¿«é€Ÿè¯Šæ–­è„šæœ¬

Step 2: æŒ‰5æ­¥æµç¨‹å†³ç­–ï¼ˆ12åˆ†é’Ÿï¼‰
  â””â”€ é€æ­¥æŸ¥è¯¢å†³ç­–è¡¨

Step 3: æ‰§è¡Œé¢„å¤„ç†ä»£ç ï¼ˆ2åˆ†é’Ÿï¼‰
  â””â”€ ä½¿ç”¨å¿«é€Ÿä»£ç æ¨¡æ¿

æ€»è®¡ï¼š15åˆ†é’Ÿ
```

---

## 2. 5æ­¥å¿«é€Ÿé¢„å¤„ç†ï¼ˆ15åˆ†é’Ÿï¼‰

### Step 1: ç¼ºå¤±å€¼å¿«é€Ÿå¤„ç†ï¼ˆ3åˆ†é’Ÿï¼‰â­

#### å†³ç­–è¡¨ï¼šç¼ºå¤±å€¼å¤„ç†æ–¹æ¡ˆ

| ç¼ºå¤±ç‡ | ç¼ºå¤±æœºåˆ¶ | æ¨èæ–¹æ¡ˆ | ä»£ç ä½ç½® | ä¼˜å…ˆçº§ |
|--------|---------|---------|----------|--------|
| **<5%** | MCAR | åˆ é™¤è¡Œ | `data_preprocessing.py:519` | P0 |
| **5-20%** | MCAR/MAR | ä¸­ä½æ•°å¡«å……ï¼ˆæ•°å€¼ï¼‰<br>ä¼—æ•°å¡«å……ï¼ˆåˆ†ç±»ï¼‰ | `data_preprocessing.py:130`<br>`data_preprocessing.py:176` | P0 |
| **20-50%** | MAR | KNNå¡«å…… | SimpleImputer/KNNImputer | P1 |
| **>50%** | ä»»æ„ | **åˆ é™¤åˆ—** | `data_preprocessing.py:157` | P0 |
| ç‰¹æ®Šï¼šMNAR | - | å»ºæ¨¡ä¸ºç‰¹æ®Šç±»åˆ« | æ‰‹åŠ¨å¤„ç† | P2 |

#### å¿«é€Ÿåˆ¤æ–­ç¼ºå¤±æœºåˆ¶

**MCARï¼ˆå®Œå…¨éšæœºï¼‰**ï¼š
- ç¼ºå¤±ä¸ä»»ä½•å˜é‡æ— å…³
- ä¾‹å¦‚ï¼šä¼ æ„Ÿå™¨éšæœºæ•…éšœ

**MARï¼ˆéšæœºç¼ºå¤±ï¼‰**ï¼š
- ç¼ºå¤±ä¾èµ–äºå…¶ä»–å˜é‡
- ä¾‹å¦‚ï¼šå¹´è½»äººä¸æ„¿å¡«æ”¶å…¥

**MNARï¼ˆééšæœºï¼‰**ï¼š
- ç¼ºå¤±æœ¬èº«æœ‰æ„ä¹‰
- ä¾‹å¦‚ï¼šé«˜æ”¶å…¥è€…æ•…æ„ä¸å¡«

#### ä»£ç æ¨¡æ¿

```python
from src.data_preprocessing import handle_missing_values

# å¿«é€Ÿå¤„ç†ï¼šä¸­ä½æ•°ï¼ˆæ•°å€¼ï¼‰+ ä¼—æ•°ï¼ˆåˆ†ç±»ï¼‰
df_filled = handle_missing_values(
    df,
    numeric_strategy='median',
    categorical_strategy='mode',
    drop_threshold=0.5  # ç¼ºå¤±>50%çš„åˆ—ç›´æ¥åˆ é™¤
)

print(f"âœ“ ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")
```

---

### Step 2: å¼‚å¸¸å€¼å¿«é€Ÿå¤„ç†ï¼ˆ3åˆ†é’Ÿï¼‰â­

#### å†³ç­–è¡¨ï¼šå¼‚å¸¸å€¼å¤„ç†æ–¹æ¡ˆ

| å¼‚å¸¸å€¼æ€§è´¨ | æ¨èæ–¹æ³• | ä»£ç ä½ç½® | é€‚ç”¨åœºæ™¯ |
|-----------|---------|----------|---------|
| **æ•°æ®é”™è¯¯** | åˆ é™¤ | `data_preprocessing.py:234` | å¹´é¾„200å²ã€è´Ÿæ•°æ”¶å…¥ |
| **çœŸå®æå€¼+çº¿æ€§æ¨¡å‹** | é²æ£’æ ‡å‡†åŒ– | `data_preprocessing.py:413` | å¯Œè±ªæ”¶å…¥ã€è±ªå®…ä»·æ ¼ |
| **çœŸå®æå€¼+æ ‘æ¨¡å‹** | ä¿ç•™ | - | æ ‘æ¨¡å‹å¯¹å¼‚å¸¸å€¼é²æ£’ |
| **æ½œåœ¨æ¬ºè¯ˆ** | å•ç‹¬å»ºæ¨¡ | å¼‚å¸¸æ£€æµ‹æ¨¡å— | æ¬ºè¯ˆäº¤æ˜“ã€å¼‚å¸¸è¡Œä¸º |

#### å¿«é€Ÿæ£€æµ‹æ–¹æ³•

| æ–¹æ³• | ä½•æ—¶ä½¿ç”¨ | ä»£ç ä½ç½® | è®¡ç®—é€Ÿåº¦ |
|------|---------|----------|---------|
| **IQRæ–¹æ³•** | åæ€åˆ†å¸ƒã€å¿«é€Ÿç­›æŸ¥ | `data_preprocessing.py:193` | âš¡ å¿« |
| **3Ïƒæ³•** | æ­£æ€åˆ†å¸ƒ | æ‰‹åŠ¨è®¡ç®— | âš¡ å¿« |
| **Isolation Forest** | é«˜ç»´æ•°æ®ã€å¤šå˜é‡æ£€æµ‹ | sklearn.ensemble | âš ï¸ ä¸­ç­‰ |

#### ä»£ç æ¨¡æ¿

```python
from src.data_preprocessing import handle_outliers_iqr

# å¿«é€Ÿå¤„ç†ï¼šIQRæ–¹æ³• + æˆªæ–­ç­–ç•¥
df_clean = handle_outliers_iqr(
    df,
    columns=['income', 'age', 'transaction_amount'],  # æ•°å€¼åˆ—
    method='clip',  # æˆªæ–­åˆ°ä¸Šä¸‹ç•Œï¼ˆä¿ç•™æ ·æœ¬ï¼‰
    k=1.5  # IQRå€æ•°ï¼ˆ1.5ä¸ºæ ‡å‡†å€¼ï¼‰
)

print(f"âœ“ å¼‚å¸¸å€¼å¤„ç†å®Œæˆ")
```

---

### Step 3: ç‰¹å¾ç¼–ç ï¼ˆ3åˆ†é’Ÿï¼‰â­

#### å†³ç­–è¡¨ï¼šåˆ†ç±»ç‰¹å¾ç¼–ç æ–¹æ¡ˆ

| ç‰¹å¾ç±»å‹ | ç±»åˆ«æ•°ï¼ˆåŸºæ•°ï¼‰ | æ¨èç¼–ç  | ä»£ç ä½ç½® | é€‚ç”¨ç®—æ³• |
|---------|--------------|---------|----------|---------|
| **æ— åºåˆ†ç±»** | <10 | **One-Hot** | `data_preprocessing.py:374` | çº¿æ€§æ¨¡å‹ã€NN |
| **æ— åºåˆ†ç±»** | 10-50 | Targetç¼–ç  | `data_preprocessing.py:299` | ä»»æ„ |
| **æ— åºåˆ†ç±»** | >50 | Targetç¼–ç  + Frequency | ç»„åˆä½¿ç”¨ | æ ‘æ¨¡å‹ |
| **æœ‰åºåˆ†ç±»** | ä»»æ„ | Labelç¼–ç  | `data_preprocessing.py:383` | æ ‘æ¨¡å‹ |

#### å¿«é€Ÿåˆ¤æ–­ç¼–ç æ–¹å¼

**åŸºæ•°åˆ¤æ–­**ï¼š
```python
# æŸ¥çœ‹ç±»åˆ«æ•°
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
for col in categorical_cols:
    n_unique = df[col].nunique()
    print(f"{col}: {n_unique}ä¸ªç±»åˆ«")
```

**å»ºè®®**ï¼š
- åŸºæ•°<10ï¼šOne-Hotï¼ˆå®‰å…¨é€‰æ‹©ï¼‰
- åŸºæ•°10-50ï¼šTargetç¼–ç ï¼ˆé€‚åˆæ ‘æ¨¡å‹ï¼‰
- åŸºæ•°>50ï¼šè€ƒè™‘ç‰¹å¾å“ˆå¸Œæˆ–åˆ é™¤

#### ä»£ç æ¨¡æ¿

```python
from src.data_preprocessing import encode_categorical_features

# æ–¹å¼1ï¼šå¿«é€ŸOne-Hotç¼–ç ï¼ˆé€‚åˆä½åŸºæ•°ï¼‰
df_encoded = encode_categorical_features(
    df,
    columns=['gender', 'city', 'product_type'],
    method='onehot',
    drop_first=True  # é¿å…å¤šé‡å…±çº¿æ€§
)

# æ–¹å¼2ï¼šLabelç¼–ç ï¼ˆé€‚åˆæ ‘æ¨¡å‹ï¼‰
df_encoded = encode_categorical_features(
    df,
    columns=['education_level', 'rank'],  # æœ‰åºç±»åˆ«
    method='label'
)

print(f"âœ“ ç‰¹å¾ç¼–ç å®Œæˆ")
```

---

### Step 4: ç‰¹å¾ç¼©æ”¾ï¼ˆ3åˆ†é’Ÿï¼‰â­

#### å†³ç­–è¡¨ï¼šæ ¹æ®ç®—æ³•é€‰æ‹©ç¼©æ”¾æ–¹æ³•

| ç®—æ³•ç±»å‹ | æ˜¯å¦éœ€è¦ç¼©æ”¾ | æ¨èæ–¹æ³• | ä»£ç ä½ç½® | åŸå›  |
|---------|------------|---------|----------|------|
| **çº¿æ€§å›å½’/é€»è¾‘å›å½’** | âœ… å¿…é¡» | StandardScaler | `data_preprocessing.py:409` | åŸºäºæ¢¯åº¦ä¸‹é™ |
| **SVM** | âœ… å¿…é¡» | StandardScaler | åŒä¸Š | å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿ |
| **ç¥ç»ç½‘ç»œ** | âœ… å¿…é¡» | MinMaxScaler [0,1] | `data_preprocessing.py:411` | æ¿€æ´»å‡½æ•°éœ€è¦ |
| **KNN** | âœ… å¿…é¡» | StandardScaler | `data_preprocessing.py:409` | åŸºäºè·ç¦» |
| **PCA** | âœ… å¿…é¡» | StandardScaler | åŒä¸Š | æ–¹å·®æ•æ„Ÿ |
| **æ ‘æ¨¡å‹**ï¼ˆRF/XGBï¼‰ | âŒ ä¸éœ€è¦ | - | - | å¯¹å°ºåº¦ä¸æ•æ„Ÿ |
| **æœ´ç´ è´å¶æ–¯** | âŒ ä¸éœ€è¦ | - | - | åŸºäºæ¦‚ç‡ |

#### å¿«é€Ÿç¼©æ”¾æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | å…¬å¼ | ç»“æœèŒƒå›´ | ä½•æ—¶ä½¿ç”¨ |
|------|------|---------|---------|
| **StandardScaler** | (X-Î¼)/Ïƒ | å‡å€¼0ï¼Œæ–¹å·®1 | æ­£æ€åˆ†å¸ƒã€çº¿æ€§æ¨¡å‹ |
| **MinMaxScaler** | (X-min)/(max-min) | [0,1] | æœ‰ç•Œæ•°æ®ã€ç¥ç»ç½‘ç»œ |
| **RobustScaler** | (X-median)/IQR | - | æœ‰å¼‚å¸¸å€¼ |

#### ä»£ç æ¨¡æ¿

```python
from src.data_preprocessing import FeatureScaler

# æ–¹å¼1ï¼šæ ‡å‡†åŒ–ï¼ˆæœ€å¸¸ç”¨ï¼‰
scaler = FeatureScaler(method='standard')
df_scaled = scaler.fit_transform(df)

# æ–¹å¼2ï¼šå½’ä¸€åŒ–ï¼ˆç¥ç»ç½‘ç»œï¼‰
scaler = FeatureScaler(method='minmax')
df_scaled = scaler.fit_transform(df)

# æ–¹å¼3ï¼šæŒ‡å®šåˆ—ç¼©æ”¾
scaler = FeatureScaler(method='standard')
df_scaled = scaler.fit_transform(df, columns=['age', 'income', 'score'])

print(f"âœ“ ç‰¹å¾ç¼©æ”¾å®Œæˆ")
```

---

### Step 5: å¿«é€Ÿç‰¹å¾å·¥ç¨‹ï¼ˆ3åˆ†é’Ÿï¼‰â­

#### P0ä¼˜å…ˆçº§ç‰¹å¾ï¼ˆå¿…åšï¼‰

| ç‰¹å¾ç±»å‹ | æ“ä½œ | ä»£ç ä½ç½® | ä»·å€¼ |
|---------|------|----------|------|
| **æ—¶é—´ç‰¹å¾** | æå–å¹´/æœˆ/æ—¥/æ˜ŸæœŸ | `feature_engineering.py:525` | â­â­â­ |
| **ç¼ºå¤±å€¼æ ‡è®°** | is_missingåˆ— | æ‰‹åŠ¨ï¼š`df['col_missing'] = df['col'].isnull()` | â­â­â­ |
| **äº¤äº’ç‰¹å¾ï¼ˆä¹˜æ³•ï¼‰** | é‡è¦ç‰¹å¾ç›¸ä¹˜ | `feature_engineering.py:363` | â­â­â­ |

#### P1ä¼˜å…ˆçº§ç‰¹å¾ï¼ˆå»ºè®®ï¼‰

| ç‰¹å¾ç±»å‹ | æ“ä½œ | ä»£ç ä½ç½® | ä»·å€¼ |
|---------|------|----------|------|
| **æ¯”ä¾‹ç‰¹å¾** | éƒ¨åˆ†/æ€»è®¡ | æ‰‹åŠ¨è®¡ç®— | â­â­ |
| **èšåˆç‰¹å¾** | åˆ†ç»„ç»Ÿè®¡ | `feature_engineering.py:411` | â­â­ |
| **å¤šé¡¹å¼ç‰¹å¾** | XÂ² | `feature_engineering.py:211` | â­ |

#### ä»£ç æ¨¡æ¿

```python
from src.feature_engineering import extract_datetime_features, create_interaction_features

# 1. æ—¶é—´ç‰¹å¾æå–ï¼ˆå¦‚æœ‰datetimeåˆ—ï¼‰
if 'signup_date' in df.columns:
    df = extract_datetime_features(df, datetime_column='signup_date')
    print(f"âœ“ æ—¶é—´ç‰¹å¾æå–å®Œæˆ")

# 2. ç¼ºå¤±å€¼æ ‡è®°ï¼ˆé‡è¦ï¼ï¼‰
for col in ['income', 'age']:  # ç¼ºå¤±è¾ƒå¤šçš„åˆ—
    if df[col].isnull().sum() > 0:
        df[f'{col}_is_missing'] = df[col].isnull().astype(int)

# 3. äº¤äº’ç‰¹å¾ï¼ˆæ ¸å¿ƒç‰¹å¾çš„ä¹˜æ³•ï¼‰
df = create_interaction_features(
    df,
    columns=['age', 'income', 'tenure'],  # é€‰æ‹©2-3ä¸ªæ ¸å¿ƒç‰¹å¾
    operations=['*']  # åªåšä¹˜æ³•ï¼ˆæœ€æœ‰ç”¨ï¼‰
)

print(f"âœ“ å¿«é€Ÿç‰¹å¾å·¥ç¨‹å®Œæˆ")
```

---

## 3. å¸¸ç”¨å†³ç­–çŸ©é˜µé€ŸæŸ¥

### 3.1 æ•°æ®é‡ä¸é¢„å¤„ç†ç­–ç•¥

| æ ·æœ¬æ•° | ç‰¹å¾æ•° | æ¨èç­–ç•¥ | æ³¨æ„äº‹é¡¹ |
|--------|--------|---------|---------|
| <1K | <20 | ç®€å•å¡«å…… + æ ‡å‡†åŒ– | é¿å…å¤æ‚æ–¹æ³•ï¼ˆè¿‡æ‹Ÿåˆï¼‰ |
| 1K-10K | 20-100 | æ ‡å‡†æµç¨‹ | å¯ä»¥å°è¯•ç‰¹å¾å·¥ç¨‹ |
| 10K-100K | 100-1000 | å®Œæ•´æµç¨‹ + ç‰¹å¾é€‰æ‹© | æ³¨æ„è®¡ç®—æ•ˆç‡ |
| >100K | >1000 | è‡ªåŠ¨åŒ–Pipeline | ä½¿ç”¨å¹¶è¡Œå¤„ç† |

### 3.2 é—®é¢˜ç±»å‹ä¸é¢„å¤„ç†é‡ç‚¹

| é—®é¢˜ç±»å‹ | é¢„å¤„ç†é‡ç‚¹ | å…³é”®æ­¥éª¤ |
|---------|----------|---------|
| **åˆ†ç±»ï¼ˆå¹³è¡¡ï¼‰** | æ ‡å‡†åŒ– + ç¼–ç  | ç¼ºå¤±å€¼ã€ç¼–ç ã€ç¼©æ”¾ |
| **åˆ†ç±»ï¼ˆä¸å¹³è¡¡ï¼‰** | æ•°æ®å¹³è¡¡â­ | SMOTE/æƒé‡è°ƒæ•´ |
| **å›å½’** | å¼‚å¸¸å€¼å¤„ç†â­ | é²æ£’æ ‡å‡†åŒ–ã€logå˜æ¢ |
| **æ—¶é—´åºåˆ—** | æ—¶é—´ç‰¹å¾â­ | æ»‘åŠ¨çª—å£ã€æ»åç‰¹å¾ |
| **æ–‡æœ¬åˆ†ç±»** | ç¼–ç +é™ç»´ | TF-IDFã€è¯åµŒå…¥ |

### 3.3 ç®—æ³•ä¸é¢„å¤„ç†è¦æ±‚

| ç®—æ³• | ç¼ºå¤±å€¼ | å¼‚å¸¸å€¼ | ç¼–ç  | ç¼©æ”¾ |
|------|--------|--------|------|------|
| **çº¿æ€§å›å½’** | âŒ ä¸å…è®¸ | âš ï¸ æ•æ„Ÿ | One-Hot | âœ… å¿…é¡» |
| **é€»è¾‘å›å½’** | âŒ ä¸å…è®¸ | âš ï¸ æ•æ„Ÿ | One-Hot | âœ… å¿…é¡» |
| **å†³ç­–æ ‘** | âœ… å¯é€‰ | âœ… é²æ£’ | Label | âŒ ä¸éœ€è¦ |
| **éšæœºæ£®æ—** | âœ… å¯é€‰ | âœ… é²æ£’ | Label | âŒ ä¸éœ€è¦ |
| **XGBoost** | âœ… åŸç”Ÿæ”¯æŒ | âœ… é²æ£’ | Label | âŒ ä¸éœ€è¦ |
| **SVM** | âŒ ä¸å…è®¸ | âŒ æ•æ„Ÿ | One-Hot | âœ… å¿…é¡» |
| **ç¥ç»ç½‘ç»œ** | âŒ ä¸å…è®¸ | âš ï¸ æ•æ„Ÿ | One-Hot | âœ… å¿…é¡» |
| **KNN** | âŒ ä¸å…è®¸ | âŒ æ•æ„Ÿ | One-Hot | âœ… å¿…é¡» |

---

## 4. å®æˆ˜ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šå®¢æˆ·æµå¤±é¢„æµ‹ï¼ˆ15åˆ†é’Ÿå®Œæ•´æµç¨‹ï¼‰

**æ•°æ®æ¦‚å†µ**ï¼š
- 7043è¡Œ Ã— 21åˆ—
- ç›®æ ‡ï¼šé¢„æµ‹å®¢æˆ·æ˜¯å¦æµå¤±ï¼ˆäºŒåˆ†ç±»ï¼‰
- é—®é¢˜ï¼š11ä¸ªç¼ºå¤±å€¼ï¼Œéƒ¨åˆ†é«˜åŸºæ•°åˆ†ç±»ç‰¹å¾

**å¿«é€Ÿé¢„å¤„ç†æµç¨‹**ï¼š

```python
import pandas as pd
from src.data_preprocessing import *
from src.feature_engineering import *

# Step 1: åŠ è½½æ•°æ®
df = pd.read_csv('telco_customer_churn.csv')
print(f"æ•°æ®å½¢çŠ¶: {df.shape}")

# Step 2: ç¼ºå¤±å€¼å¤„ç†ï¼ˆ3åˆ†é’Ÿï¼‰
# - TotalChargesåˆ—æœ‰11ä¸ªç¼ºå¤±ï¼ˆ0.16%ï¼‰â†’ åˆ é™¤è¡Œ
df = df.dropna()
print(f"âœ“ Step 1: åˆ é™¤11è¡Œç¼ºå¤±æ•°æ®")

# Step 3: å¼‚å¸¸å€¼æ£€æŸ¥ï¼ˆå¿«é€Ÿè·³è¿‡ï¼‰
# - ä¸šåŠ¡å­—æ®µæ— æ˜æ˜¾å¼‚å¸¸å€¼
print(f"âœ“ Step 2: æ— å¼‚å¸¸å€¼éœ€å¤„ç†")

# Step 4: ç‰¹å¾ç¼–ç ï¼ˆ3åˆ†é’Ÿï¼‰
# - ä½åŸºæ•°åˆ†ç±»ï¼ˆgender, Partnerç­‰ï¼‰â†’ One-Hot
# - é«˜åŸºæ•°åˆ†ç±»ï¼ˆæ— ï¼‰
categorical_cols = ['gender', 'Partner', 'Dependents', 'PhoneService',
                    'MultipleLines', 'InternetService', 'OnlineSecurity',
                    'OnlineBackup', 'DeviceProtection', 'TechSupport',
                    'StreamingTV', 'StreamingMovies', 'Contract',
                    'PaperlessBilling', 'PaymentMethod']

df_encoded = encode_categorical_features(
    df,
    columns=categorical_cols,
    method='onehot',
    drop_first=True
)
print(f"âœ“ Step 3: One-Hotç¼–ç å®Œæˆï¼Œç‰¹å¾æ•°ï¼š{df_encoded.shape[1]}")

# Step 5: ç‰¹å¾ç¼©æ”¾ï¼ˆ3åˆ†é’Ÿï¼‰
# - ä½¿ç”¨é€»è¾‘å›å½’ â†’ æ ‡å‡†åŒ–
numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
scaler = FeatureScaler(method='standard')
df_scaled = scaler.fit_transform(df_encoded, columns=numeric_cols)
print(f"âœ“ Step 4: æ ‡å‡†åŒ–å®Œæˆ")

# Step 6: å¿«é€Ÿç‰¹å¾å·¥ç¨‹ï¼ˆ3åˆ†é’Ÿï¼‰
# - äº¤äº’ç‰¹å¾ï¼štenure Ã— MonthlyCharges = TotalValue
df_scaled['TotalValue'] = df_scaled['tenure'] * df_scaled['MonthlyCharges']
# - ç¼ºå¤±å€¼æ ‡è®°ï¼ˆè™½ç„¶å·²åˆ é™¤ï¼Œä½†å±•ç¤ºæ–¹æ³•ï¼‰
print(f"âœ“ Step 5: ç‰¹å¾å·¥ç¨‹å®Œæˆ")

# æœ€ç»ˆç»“æœ
print(f"\nâœ… é¢„å¤„ç†å®Œæˆï¼")
print(f"   æœ€ç»ˆå½¢çŠ¶: {df_scaled.shape}")
print(f"   æ€»è€—æ—¶: çº¦15åˆ†é’Ÿ")
```

**ç»“æœ**ï¼š
- è¾“å…¥ï¼š7043è¡Œ Ã— 21åˆ—
- è¾“å‡ºï¼š7032è¡Œ Ã— 47åˆ—ï¼ˆOne-Hotåï¼‰
- è€—æ—¶ï¼š15åˆ†é’Ÿ
- å¯ä»¥ç›´æ¥ç”¨äºå»ºæ¨¡

---

### ç¤ºä¾‹2ï¼šæˆ¿ä»·é¢„æµ‹ï¼ˆ10åˆ†é’Ÿå¿«é€Ÿå¤„ç†ï¼‰

**æ•°æ®æ¦‚å†µ**ï¼š
- 1460è¡Œ Ã— 80åˆ—
- ç›®æ ‡ï¼šé¢„æµ‹æˆ¿ä»·ï¼ˆå›å½’ï¼‰
- é—®é¢˜ï¼šå¤šåˆ—ç¼ºå¤±ã€åæ€åˆ†å¸ƒ

**å¿«é€Ÿé¢„å¤„ç†æµç¨‹**ï¼š

```python
# Step 1: ç¼ºå¤±å€¼å¤„ç†ï¼ˆ5åˆ†é’Ÿï¼‰
df_filled = handle_missing_values(
    df,
    numeric_strategy='median',
    categorical_strategy='mode',
    drop_threshold=0.5  # åˆ é™¤ç¼ºå¤±>50%çš„åˆ—
)

# Step 2: å¼‚å¸¸å€¼å¤„ç†ï¼ˆ2åˆ†é’Ÿï¼‰
# - SalePriceå³å â†’ logå˜æ¢
df_filled['SalePrice_log'] = np.log1p(df_filled['SalePrice'])

# - å…¶ä»–æ•°å€¼åˆ— â†’ IQRæˆªæ–­
numeric_cols = df_filled.select_dtypes(include=[np.number]).columns
df_clean = handle_outliers_iqr(
    df_filled,
    columns=numeric_cols,
    method='clip'
)

# Step 3: ç¼–ç +ç¼©æ”¾ï¼ˆ3åˆ†é’Ÿï¼‰
# - åˆ†ç±»ç‰¹å¾ â†’ Labelç¼–ç ï¼ˆå‡†å¤‡ç”¨æ ‘æ¨¡å‹ï¼‰
categorical_cols = df_clean.select_dtypes(include=['object']).columns
df_encoded = encode_categorical_features(
    df_clean,
    columns=categorical_cols,
    method='label'
)

# - æ•°å€¼ç‰¹å¾ â†’ æ ‡å‡†åŒ–ï¼ˆå¤‡ç”¨ï¼‰
scaler = FeatureScaler(method='standard')
df_final = scaler.fit_transform(df_encoded)

print(f"âœ… å¿«é€Ÿé¢„å¤„ç†å®Œæˆï¼å¯ç”¨äºæ ‘æ¨¡å‹")
```

---

## 5. å¿«é€Ÿä»£ç æ¨¡æ¿

### 5.1 ä¸€é”®é¢„å¤„ç†Pipelineï¼ˆæœ€å¿«ï¼‰

```python
from src.data_preprocessing import build_preprocessing_pipeline
from sklearn.model_selection import train_test_split

# åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
X = df.drop('target', axis=1)
y = df['target']

# è¯†åˆ«ç‰¹å¾ç±»å‹
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

# æ„å»ºPipeline
preprocessor = build_preprocessing_pipeline(
    numeric_features=numeric_features,
    categorical_features=categorical_features,
    numeric_strategy='median',
    categorical_strategy='most_frequent',
    scaling_method='standard'
)

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# æ‹Ÿåˆå¹¶è½¬æ¢
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

print(f"âœ… ä¸€é”®é¢„å¤„ç†å®Œæˆï¼")
```

### 5.2 æ‰‹åŠ¨Pipelineï¼ˆçµæ´»ï¼‰

```python
# 1. ç¼ºå¤±å€¼
df = handle_missing_values(df, numeric_strategy='median', categorical_strategy='mode')

# 2. å¼‚å¸¸å€¼
df = handle_outliers_iqr(df, columns=numeric_cols, method='clip')

# 3. ç¼–ç 
df = encode_categorical_features(df, columns=categorical_cols, method='onehot')

# 4. ç¼©æ”¾
scaler = FeatureScaler(method='standard')
df = scaler.fit_transform(df)

# 5. ç‰¹å¾å·¥ç¨‹ï¼ˆå¯é€‰ï¼‰
df = create_interaction_features(df, columns=['col1', 'col2'], operations=['*'])

print(f"âœ… æ‰‹åŠ¨é¢„å¤„ç†å®Œæˆï¼")
```

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

### éœ€è¦æ›´æ·±å…¥ï¼Ÿ

- **ç†è®ºå­¦ä¹ **ï¼š
  - [missing_values_strategies.md](missing_values_strategies.md) - ç¼ºå¤±å€¼å®Œæ•´æŒ‡å—
  - [outlier_detection_methods.md](outlier_detection_methods.md) - å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•
  - [feature_engineering_cookbook.md](feature_engineering_cookbook.md) - ç‰¹å¾å·¥ç¨‹è¯¦è§£

- **ç³»ç»ŸåŒ–æµç¨‹**ï¼š
  - [preprocessing_decision_tree.md](preprocessing_decision_tree.md) - å®Œæ•´å†³ç­–æ ‘
  - [preprocessing_checklist.md](preprocessing_checklist.md) - æ£€æŸ¥æ¸…å•

### å¸¸è§é—®é¢˜

**Q1: 15åˆ†é’Ÿå¤Ÿå—ï¼Ÿ**
A: å¯¹äºå¿«é€ŸBaselineå¤Ÿäº†ï¼Œé‡è¦é¡¹ç›®è¯·ç”¨å®Œæ•´æµç¨‹ï¼ˆ1-2å°æ—¶ï¼‰

**Q2: è¿™æ˜¯æœ€ä¼˜æ–¹æ¡ˆå—ï¼Ÿ**
A: ä¸æ˜¯ï¼Œä½†æ˜¯å¿«é€Ÿå¯ç”¨çš„æ–¹æ¡ˆï¼Œè¶³ä»¥å»ºç«‹Baseline

**Q3: å¯ä»¥è·³è¿‡æŸäº›æ­¥éª¤å—ï¼Ÿ**
A: å¯ä»¥ï¼Œä½†P0æ­¥éª¤ï¼ˆç¼ºå¤±å€¼ã€ç¼–ç ã€ç¼©æ”¾ï¼‰å»ºè®®éƒ½åš

**Q4: æ ‘æ¨¡å‹æ˜¯å¦éœ€è¦é¢„å¤„ç†ï¼Ÿ**
A: ç¼ºå¤±å€¼å’Œç¼–ç å¿…é¡»å¤„ç†ï¼Œç¼©æ”¾å¯ä»¥è·³è¿‡

---

**æœ€åæ›´æ–°**ï¼š2024å¹´11æœˆ
**é¢„è®¡ä½¿ç”¨æ—¶é—´**ï¼š15åˆ†é’Ÿ
**åç»­æ–‡æ¡£**ï¼š[preprocessing_decision_tree.md](preprocessing_decision_tree.md)
