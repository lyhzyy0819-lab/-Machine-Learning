# âš™ï¸ è¶…å‚æ•°è°ƒä¼˜å†³ç­–æ¨¡æ¿

> **ç”¨é€”**ï¼šç³»ç»ŸåŒ–çš„è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥ï¼Œé¿å…ç›²ç›®è¯•é”™
> **ä½¿ç”¨åœºæ™¯**ï¼šæ¨¡å‹åˆæ­¥è®­ç»ƒåï¼Œéœ€è¦æå‡æ€§èƒ½æ—¶
> **ä½¿ç”¨æ—¶é—´**ï¼šâš¡ å¿«é€Ÿæ¨¡å¼5åˆ†é’Ÿ | ğŸ“Š å®Œæ•´æ¨¡å¼20åˆ†é’Ÿ

---

## âš¡ è°ƒä¼˜ç­–ç•¥å¿«é€Ÿé€‰æ‹©ï¼ˆ2-5åˆ†é’Ÿï¼‰

### ä½¿ç”¨è¯´æ˜

1. **è¯„ä¼°è°ƒä¼˜éœ€æ±‚**ï¼ˆ1åˆ†é’Ÿï¼‰
2. **æŸ¥è¡¨é€‰æ‹©è°ƒä¼˜æ–¹æ³•**ï¼ˆ2åˆ†é’Ÿï¼‰
3. **è·å–å‚æ•°æœç´¢ç©ºé—´**ï¼ˆ2åˆ†é’Ÿï¼‰

---

### è°ƒä¼˜ç­–ç•¥é€‰æ‹©å¡ç‰‡ï¼ˆå¯å¤åˆ¶å¡«å†™ï¼‰

```
===========================================
è¶…å‚æ•°è°ƒä¼˜å†³ç­–å¡ç‰‡ - [é¡¹ç›®åç§°]
å†³ç­–æ—¥æœŸï¼š____å¹´____æœˆ____æ—¥
===========================================

ã€è°ƒä¼˜éœ€æ±‚è¯„ä¼°ã€‘
â–¡ å½“å‰æ¨¡å‹æ€§èƒ½ï¼š____ï¼ˆä¸»æŒ‡æ ‡å€¼ï¼‰
â–¡ ç›®æ ‡æ€§èƒ½ï¼š____
â–¡ æ€§èƒ½å·®è·ï¼š____ï¼ˆæ˜¯å¦å€¼å¾—è°ƒä¼˜ï¼‰

â–¡ æ—¶é—´é¢„ç®—ï¼š
  [ ] ä¸¥æ ¼ï¼ˆ<30åˆ†é’Ÿï¼‰â†’ æ‰‹åŠ¨è°ƒä¼˜1-2ä¸ªå…³é”®å‚æ•°
  [ ] å®½æ¾ï¼ˆ<2å°æ—¶ï¼‰â†’ ç½‘æ ¼/éšæœºæœç´¢
  [ ] å……è¶³ï¼ˆ>2å°æ—¶ï¼‰â†’ è´å¶æ–¯ä¼˜åŒ–

â–¡ è®¡ç®—èµ„æºï¼š
  [ ] æœ‰é™ï¼ˆå•æœºï¼‰
  [ ] å……è¶³ï¼ˆå¤šæ ¸/é›†ç¾¤ï¼‰

ã€å‚æ•°ç©ºé—´è§„æ¨¡ã€‘
â–¡ éœ€è°ƒä¼˜å‚æ•°æ•°é‡ï¼š____ä¸ª
  [ ] å°‘ï¼ˆ1-3ä¸ªï¼‰â†’ ç½‘æ ¼æœç´¢
  [ ] ä¸­ï¼ˆ4-10ä¸ªï¼‰â†’ éšæœºæœç´¢
  [ ] å¤šï¼ˆ>10ä¸ªï¼‰â†’ è´å¶æ–¯ä¼˜åŒ–

â–¡ æ¯ä¸ªå‚æ•°å€™é€‰å€¼ï¼š____ä¸ª
â–¡ æ€»æœç´¢ç©ºé—´ï¼š____ï¼ˆç»„åˆæ•°ï¼‰

ã€ç®—æ³•ç±»å‹ã€‘ï¼ˆå¿…å¡«ï¼‰
â–¡ [ ] æ ‘æ¨¡å‹ï¼ˆRandom Forest/XGBoost/LightGBMï¼‰
â–¡ [ ] çº¿æ€§æ¨¡å‹ï¼ˆLinear/Logistic/SVMï¼‰
â–¡ [ ] è¿‘é‚»æ¨¡å‹ï¼ˆKNNï¼‰
â–¡ [ ] ç¥ç»ç½‘ç»œï¼ˆæ·±åº¦å­¦ä¹ ï¼‰
â–¡ [ ] èšç±»ï¼ˆK-Meansç­‰ï¼‰

===========================================
â†’ æ¨èè°ƒä¼˜ç­–ç•¥
===========================================

ç­–ç•¥ï¼š________________
ç†ç”±ï¼š________________
é¢„è®¡è€—æ—¶ï¼š________________
æœç´¢ç©ºé—´ï¼š________________

===========================================
```

---

### è°ƒä¼˜æ–¹æ³•å¿«é€Ÿå†³ç­–çŸ©é˜µ

| å‚æ•°ç©ºé—´ | æ—¶é—´é¢„ç®— | æ¨èæ–¹æ³• | é¢„è®¡è€—æ—¶ | æœç´¢æ•ˆç‡ |
|---------|---------|---------|---------|---------|
| **å°ï¼ˆ<10ç»„åˆï¼‰** | å……è¶³ | ç½‘æ ¼æœç´¢ | çŸ­ | 100%ï¼ˆå…¨è¦†ç›–ï¼‰ |
| **ä¸­ï¼ˆ10-100ç»„åˆï¼‰** | å®½æ¾ | ç½‘æ ¼æœç´¢ | ä¸­ç­‰ | 100% |
| | ä¸¥æ ¼ | éšæœºæœç´¢ï¼ˆ30æ¬¡ï¼‰ | çŸ­ | ~30% |
| **å¤§ï¼ˆ100-1000ç»„åˆï¼‰** | å……è¶³ | éšæœºæœç´¢ï¼ˆ100æ¬¡ï¼‰ | ä¸­ç­‰ | ~10% |
| | å®½æ¾ | éšæœºæœç´¢ï¼ˆ50æ¬¡ï¼‰ | çŸ­ | ~5% |
| **è¶…å¤§ï¼ˆ>1000ç»„åˆï¼‰** | å……è¶³ | è´å¶æ–¯ä¼˜åŒ– | é•¿ | é«˜æ•ˆ |
| | å®½æ¾ | éšæœºæœç´¢ï¼ˆ30æ¬¡ï¼‰ | ä¸­ç­‰ | ~3% |
| **ä»»ä½•** | ä¸¥æ ¼ï¼ˆ<30åˆ†é’Ÿï¼‰ | æ‰‹åŠ¨è°ƒ1-2ä¸ªå…³é”®å‚æ•° | æçŸ­ | ç»éªŒä¾èµ– |

---

## ğŸ“‹ ç®—æ³•å…³é”®å‚æ•°é€ŸæŸ¥è¡¨

### æ ‘æ¨¡å‹

#### Random Forestï¼ˆéšæœºæ£®æ—ï¼‰

| å‚æ•° | ä½œç”¨ | æ¨èèŒƒå›´ | è°ƒä¼˜ä¼˜å…ˆçº§ | é»˜è®¤å€¼ |
|------|------|---------|-----------|--------|
| **n_estimators** | æ ‘çš„æ•°é‡ | [50, 100, 200, 500] | â­â­â­â­â­ | 100 |
| **max_depth** | æ ‘çš„æœ€å¤§æ·±åº¦ | [10, 20, 30, None] | â­â­â­â­ | None |
| **min_samples_split** | åˆ†è£‚æœ€å°æ ·æœ¬æ•° | [2, 5, 10] | â­â­â­ | 2 |
| **min_samples_leaf** | å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•° | [1, 2, 4] | â­â­ | 1 |
| **max_features** | åˆ†è£‚æ—¶è€ƒè™‘çš„ç‰¹å¾æ•° | ['sqrt', 'log2', None] | â­â­â­ | 'sqrt' |

**å¿«é€Ÿè°ƒä¼˜å»ºè®®**ï¼š
```python
# ç¬¬1ä¼˜å…ˆçº§ï¼šn_estimatorsï¼ˆæ ‘è¶Šå¤šè¶Šå¥½ï¼Œä½†æ”¶ç›Šé€’å‡ï¼‰
param_grid = {
    'n_estimators': [100, 200, 500],  # å…ˆè°ƒè¿™ä¸ª
}

# ç¬¬2ä¼˜å…ˆçº§ï¼šmax_depthï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
param_grid = {
    'n_estimators': [200],  # å›ºå®šä¸ºä¸Šä¸€æ­¥æœ€ä¼˜å€¼
    'max_depth': [10, 20, 30, None],
}

# ç¬¬3ä¼˜å…ˆçº§ï¼šmin_samples_splitï¼ˆå¾®è°ƒï¼‰
param_grid = {
    'n_estimators': [200],
    'max_depth': [20],  # å›ºå®šä¸ºä¸Šä¸€æ­¥æœ€ä¼˜å€¼
    'min_samples_split': [2, 5, 10],
}
```

---

#### XGBoost

| å‚æ•° | ä½œç”¨ | æ¨èèŒƒå›´ | è°ƒä¼˜ä¼˜å…ˆçº§ | é»˜è®¤å€¼ |
|------|------|---------|-----------|--------|
| **learning_rate (eta)** | å­¦ä¹ ç‡ | [0.01, 0.05, 0.1, 0.3] | â­â­â­â­â­ | 0.3 |
| **n_estimators** | æ ‘çš„æ•°é‡ | [100, 200, 500, 1000] | â­â­â­â­â­ | 100 |
| **max_depth** | æ ‘çš„æœ€å¤§æ·±åº¦ | [3, 5, 7, 9] | â­â­â­â­ | 6 |
| **min_child_weight** | å¶èŠ‚ç‚¹æœ€å°æƒé‡å’Œ | [1, 3, 5, 7] | â­â­â­ | 1 |
| **subsample** | æ ·æœ¬é‡‡æ ·æ¯”ä¾‹ | [0.6, 0.8, 1.0] | â­â­â­ | 1.0 |
| **colsample_bytree** | ç‰¹å¾é‡‡æ ·æ¯”ä¾‹ | [0.6, 0.8, 1.0] | â­â­â­ | 1.0 |
| **gamma** | åˆ†è£‚æœ€å°æŸå¤±å‡å°‘ | [0, 0.1, 0.2, 0.3] | â­â­ | 0 |
| **reg_alpha** | L1æ­£åˆ™åŒ– | [0, 0.01, 0.1, 1] | â­â­ | 0 |
| **reg_lambda** | L2æ­£åˆ™åŒ– | [1, 10, 100] | â­â­ | 1 |

**å¿«é€Ÿè°ƒä¼˜å»ºè®®**ï¼ˆåˆ†é˜¶æ®µï¼‰ï¼š
```python
# ç¬¬1é˜¶æ®µï¼šlearning_rate + n_estimatorsï¼ˆè”åˆè°ƒä¼˜ï¼‰
# åŸåˆ™ï¼šlearning_rateè¶Šå°ï¼Œn_estimatorsè¶Šå¤§
param_grid_1 = {
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [500, 1000] if lr < 0.05 else [100, 200],
}

# ç¬¬2é˜¶æ®µï¼šmax_depth + min_child_weight
param_grid_2 = {
    'learning_rate': [0.05],  # å›ºå®šä¸ºä¸Šä¸€æ­¥æœ€ä¼˜
    'n_estimators': [500],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
}

# ç¬¬3é˜¶æ®µï¼šsubsample + colsample_bytreeï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
param_grid_3 = {
    'learning_rate': [0.05],
    'n_estimators': [500],
    'max_depth': [5],
    'min_child_weight': [3],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
}

# ç¬¬4é˜¶æ®µï¼šæ­£åˆ™åŒ–å‚æ•°ï¼ˆå¯é€‰ï¼‰
param_grid_4 = {
    ...  # å‰é¢å‚æ•°å›ºå®š
    'reg_alpha': [0, 0.01, 0.1],
    'reg_lambda': [1, 10, 100],
}
```

---

#### LightGBM

| å‚æ•° | ä½œç”¨ | æ¨èèŒƒå›´ | è°ƒä¼˜ä¼˜å…ˆçº§ | é»˜è®¤å€¼ |
|------|------|---------|-----------|--------|
| **learning_rate** | å­¦ä¹ ç‡ | [0.01, 0.05, 0.1] | â­â­â­â­â­ | 0.1 |
| **n_estimators** | æ ‘çš„æ•°é‡ | [100, 500, 1000] | â­â­â­â­â­ | 100 |
| **num_leaves** | å¶èŠ‚ç‚¹æ•°é‡ | [31, 63, 127] | â­â­â­â­ | 31 |
| **max_depth** | æ ‘çš„æœ€å¤§æ·±åº¦ | [-1, 10, 20] | â­â­â­ | -1 |
| **min_child_samples** | å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•° | [20, 50, 100] | â­â­â­ | 20 |
| **subsample (bagging_fraction)** | æ ·æœ¬é‡‡æ ·æ¯”ä¾‹ | [0.6, 0.8, 1.0] | â­â­â­ | 1.0 |
| **colsample_bytree (feature_fraction)** | ç‰¹å¾é‡‡æ ·æ¯”ä¾‹ | [0.6, 0.8, 1.0] | â­â­â­ | 1.0 |

**LightGBM vs XGBooståŒºåˆ«**ï¼š
- LightGBMä½¿ç”¨`num_leaves`ï¼ˆå¶æ•°é‡ï¼‰è€Œé`max_depth`ï¼ˆæ·±åº¦ï¼‰
- åŸåˆ™ï¼š`num_leaves < 2^max_depth`
- æ¨èï¼šå…ˆè°ƒ`num_leaves`ï¼Œ`max_depth`è®¾ä¸º-1ï¼ˆæ— é™åˆ¶ï¼‰

---

### çº¿æ€§æ¨¡å‹

#### Logistic Regression / Linear Regression

| å‚æ•° | ä½œç”¨ | æ¨èèŒƒå›´ | è°ƒä¼˜ä¼˜å…ˆçº§ | é»˜è®¤å€¼ |
|------|------|---------|-----------|--------|
| **Cï¼ˆæ­£åˆ™åŒ–å¼ºåº¦å€’æ•°ï¼‰** | æ­£åˆ™åŒ– | [0.001, 0.01, 0.1, 1, 10, 100] | â­â­â­â­â­ | 1.0 |
| **penalty** | æ­£åˆ™åŒ–ç±»å‹ | ['l1', 'l2', 'elasticnet'] | â­â­â­â­ | 'l2' |
| **solver** | ä¼˜åŒ–ç®—æ³• | ['lbfgs', 'liblinear', 'saga'] | â­â­â­ | 'lbfgs' |
| **max_iter** | æœ€å¤§è¿­ä»£æ¬¡æ•° | [100, 500, 1000] | â­â­ | 100 |

**å¿«é€Ÿè°ƒä¼˜**ï¼š
```python
# ä¸»è¦è°ƒCï¼ˆæ­£åˆ™åŒ–å¼ºåº¦ï¼‰
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'penalty': ['l1', 'l2'],  # L1åšç‰¹å¾é€‰æ‹©ï¼ŒL2é˜²æ­¢è¿‡æ‹Ÿåˆ
}

# Cè¶Šå°ï¼Œæ­£åˆ™åŒ–è¶Šå¼ºï¼ˆæ¨¡å‹è¶Šç®€å•ï¼‰
# Cè¶Šå¤§ï¼Œæ­£åˆ™åŒ–è¶Šå¼±ï¼ˆæ¨¡å‹è¶Šå¤æ‚ï¼‰
```

---

#### SVMï¼ˆæ”¯æŒå‘é‡æœºï¼‰

| å‚æ•° | ä½œç”¨ | æ¨èèŒƒå›´ | è°ƒä¼˜ä¼˜å…ˆçº§ | é»˜è®¤å€¼ |
|------|------|---------|-----------|--------|
| **Cï¼ˆæƒ©ç½šå‚æ•°ï¼‰** | æ­£åˆ™åŒ– | [0.1, 1, 10, 100] | â­â­â­â­â­ | 1.0 |
| **kernel** | æ ¸å‡½æ•° | ['linear', 'rbf', 'poly'] | â­â­â­â­â­ | 'rbf' |
| **gammaï¼ˆRBFæ ¸ï¼‰** | æ ¸ç³»æ•° | ['scale', 'auto', 0.001, 0.01, 0.1, 1] | â­â­â­â­ | 'scale' |
| **degreeï¼ˆPolyæ ¸ï¼‰** | å¤šé¡¹å¼é˜¶æ•° | [2, 3, 4] | â­â­â­ | 3 |

**å¿«é€Ÿè°ƒä¼˜**ï¼ˆåˆ†æ ¸ç±»å‹ï¼‰ï¼š
```python
# ç¬¬1æ­¥ï¼šé€‰æ‹©æ ¸å‡½æ•°
param_grid_kernel = {
    'kernel': ['linear', 'rbf', 'poly'],
    'C': [1],  # å…ˆç”¨é»˜è®¤å€¼
}

# ç¬¬2æ­¥ï¼šæ ¹æ®é€‰å®šæ ¸è°ƒä¼˜
# å¦‚æœé€‰linearï¼š
param_grid_linear = {
    'kernel': ['linear'],
    'C': [0.1, 1, 10, 100],
}

# å¦‚æœé€‰rbfï¼š
param_grid_rbf = {
    'kernel': ['rbf'],
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 'scale'],
}
```

---

### è¿‘é‚»æ¨¡å‹

#### KNN

| å‚æ•° | ä½œç”¨ | æ¨èèŒƒå›´ | è°ƒä¼˜ä¼˜å…ˆçº§ | é»˜è®¤å€¼ |
|------|------|---------|-----------|--------|
| **n_neighbors** | é‚»å±…æ•°é‡k | [3, 5, 7, 9, 11, 15] | â­â­â­â­â­ | 5 |
| **weights** | æƒé‡æ–¹å¼ | ['uniform', 'distance'] | â­â­â­â­ | 'uniform' |
| **metric** | è·ç¦»åº¦é‡ | ['euclidean', 'manhattan', 'minkowski'] | â­â­â­ | 'minkowski' |
| **pï¼ˆMinkowskiè·ç¦»å‚æ•°ï¼‰** | è·ç¦»å¹‚æ¬¡ | [1, 2] | â­â­ | 2 |

**å¿«é€Ÿè°ƒä¼˜**ï¼š
```python
# ä¸»è¦è°ƒn_neighbors
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11, 15, 21],
    'weights': ['uniform', 'distance'],
}

# kè¿‡å°ï¼šè¿‡æ‹Ÿåˆï¼ˆå™ªå£°æ•æ„Ÿï¼‰
# kè¿‡å¤§ï¼šæ¬ æ‹Ÿåˆï¼ˆå†³ç­–è¾¹ç•Œè¿‡äºå¹³æ»‘ï¼‰
# ä¸€èˆ¬k=sqrt(n)é™„è¿‘
```

---

### èšç±»æ¨¡å‹

#### K-Means

| å‚æ•° | ä½œç”¨ | æ¨èèŒƒå›´ | è°ƒä¼˜ä¼˜å…ˆçº§ | é»˜è®¤å€¼ |
|------|------|---------|-----------|--------|
| **n_clusters** | èšç±»æ•°k | [2, 3, 4, 5, ...] | â­â­â­â­â­ | 8 |
| **init** | åˆå§‹åŒ–æ–¹æ³• | ['k-means++', 'random'] | â­â­â­ | 'k-means++' |
| **n_init** | åˆå§‹åŒ–æ¬¡æ•° | [10, 20, 50] | â­â­ | 10 |
| **max_iter** | æœ€å¤§è¿­ä»£æ¬¡æ•° | [300, 500, 1000] | â­ | 300 |

**ç¡®å®šæœ€ä¼˜kå€¼**ï¼š
```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Elbowæ³• + Silhouetteæ³•
inertias = []
silhouettes = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)

    inertias.append(kmeans.inertia_)  # SSE
    silhouettes.append(silhouette_score(X, kmeans.labels_))

# å¯è§†åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

ax1.plot(K_range, inertias, 'bo-')
ax1.set_xlabel('èšç±»æ•° k')
ax1.set_ylabel('SSEï¼ˆè‚˜éƒ¨æ³•ï¼‰')
ax1.set_title('Elbow Method')

ax2.plot(K_range, silhouettes, 'ro-')
ax2.set_xlabel('èšç±»æ•° k')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Silhouette Method')

plt.tight_layout()
plt.show()

# é€‰æ‹©ï¼šElbowæ‹ç‚¹ + Silhouetteæœ€å¤§å€¼
```

---

## ğŸ“Š å®Œæ•´è°ƒä¼˜å†³ç­–æ ‘ï¼ˆ20åˆ†é’Ÿç³»ç»ŸåŒ–è°ƒä¼˜ï¼‰

### è°ƒä¼˜æµç¨‹æ€»è§ˆ

```
æ¨¡å‹åˆæ­¥è®­ç»ƒå®Œæˆ
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒä¼˜ï¼ˆ2åˆ†é’Ÿï¼‰     â”‚
â”‚ - å½“å‰æ€§èƒ½vsç›®æ ‡æ€§èƒ½                  â”‚
â”‚ - æ€§èƒ½æå‡ç©ºé—´è¯„ä¼°                    â”‚
â”‚ - æ—¶é—´æˆæœ¬vsæ”¶ç›Š                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: ç¡®å®šè°ƒä¼˜ç­–ç•¥ï¼ˆ3åˆ†é’Ÿï¼‰         â”‚
â”‚ - è¯„ä¼°å‚æ•°ç©ºé—´å¤§å°                    â”‚
â”‚ - é€‰æ‹©æœç´¢æ–¹æ³•ï¼ˆç½‘æ ¼/éšæœº/è´å¶æ–¯ï¼‰   â”‚
â”‚ - è®¾å®šæ—¶é—´é¢„ç®—                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: å®šä¹‰æœç´¢ç©ºé—´ï¼ˆ5åˆ†é’Ÿï¼‰         â”‚
â”‚ - æŸ¥è¡¨è·å–å…³é”®å‚æ•°                    â”‚
â”‚ - è®¾å®šå€™é€‰å€¼èŒƒå›´                      â”‚
â”‚ - ä¼˜å…ˆçº§æ’åºï¼ˆåˆ†é˜¶æ®µè°ƒä¼˜ï¼‰            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: æ‰§è¡Œè°ƒä¼˜ï¼ˆ5-10åˆ†é’Ÿï¼‰          â”‚
â”‚ - è¿è¡Œæœç´¢ç®—æ³•                        â”‚
â”‚ - äº¤å‰éªŒè¯è¯„ä¼°                        â”‚
â”‚ - è®°å½•æœ€ä¼˜å‚æ•°                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: éªŒè¯å’Œéƒ¨ç½²ï¼ˆ2åˆ†é’Ÿï¼‰           â”‚
â”‚ - æµ‹è¯•é›†éªŒè¯                          â”‚
â”‚ - å¯¹æ¯”é»˜è®¤å‚æ•°                        â”‚
â”‚ - å†³å®šæ˜¯å¦ä½¿ç”¨                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
æœ€ä¼˜æ¨¡å‹ + è°ƒä¼˜æŠ¥å‘Š
```

---

### Step 1: åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒä¼˜ï¼ˆ2åˆ†é’Ÿï¼‰

#### å†³ç­–æ¸…å•

```
ã€æ˜¯å¦éœ€è¦è°ƒä¼˜ï¼Ÿã€‘

âœ… éœ€è¦è°ƒä¼˜çš„æƒ…å†µï¼š
- [ ] å½“å‰æ€§èƒ½ä¸ç›®æ ‡æœ‰æ˜æ˜¾å·®è·ï¼ˆå¦‚ç›¸å·®>5%ï¼‰
- [ ] Baselineæ¨¡å‹æ˜¾ç¤ºæ•°æ®æœ‰æ½œåŠ›ï¼ˆè®­ç»ƒé›†æ€§èƒ½å¥½ï¼‰
- [ ] æœ‰å……è¶³æ—¶é—´é¢„ç®—ï¼ˆ>30åˆ†é’Ÿï¼‰
- [ ] ä¸šåŠ¡ä»·å€¼æ˜¾è‘—ï¼ˆæ€§èƒ½æå‡1%å¸¦æ¥å·¨å¤§æ”¶ç›Šï¼‰

âŒ ä¸éœ€è¦è°ƒä¼˜çš„æƒ…å†µï¼š
- [ ] å½“å‰æ€§èƒ½å·²è¾¾æ ‡ï¼ˆæ»¡è¶³ä¸šåŠ¡éœ€æ±‚ï¼‰
- [ ] Baselineæ¨¡å‹å·²æ¥è¿‘ä¸Šé™ï¼ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ€§èƒ½æ¥è¿‘ï¼‰
- [ ] æ—¶é—´æåº¦ç´§è¿«ï¼ˆ<30åˆ†é’Ÿï¼‰
- [ ] æ•°æ®è´¨é‡é—®é¢˜ï¼ˆåº”å…ˆæ”¹è¿›æ•°æ®ï¼‰

âš ï¸ è°¨æ…è°ƒä¼˜çš„æƒ…å†µï¼š
- [ ] å°æ ·æœ¬æ•°æ®ï¼ˆ<1000ï¼‰â†’ å®¹æ˜“è¿‡æ‹Ÿåˆ
- [ ] éªŒè¯é›†æ€§èƒ½æ³¢åŠ¨å¤§ â†’ æ•°æ®ä¸ç¨³å®š
- [ ] ç‰¹å¾å·¥ç¨‹æœªå®Œæˆ â†’ å…ˆåšç‰¹å¾å·¥ç¨‹
```

**å†³ç­–ç¤ºä¾‹**ï¼š
```
åœºæ™¯ï¼šå®¢æˆ·æµå¤±é¢„æµ‹

å½“å‰æƒ…å†µï¼š
- Baselineï¼ˆLogisticï¼‰ï¼šF1=0.75
- ç›®æ ‡æ€§èƒ½ï¼šF1â‰¥0.82
- å·®è·ï¼š0.07ï¼ˆ9.3%ï¼‰
- æ—¶é—´é¢„ç®—ï¼š2å°æ—¶

åˆ¤æ–­ï¼š
âœ… éœ€è¦è°ƒä¼˜
  - å·®è·æ˜æ˜¾ï¼ˆ9.3% > 5%ï¼‰
  - æœ‰æ—¶é—´é¢„ç®—
  - ä¸šåŠ¡ä»·å€¼é«˜ï¼ˆæ¯æå‡1%èŠ‚çœ10ä¸‡å…ƒ/å¹´ï¼‰

é¢„æœŸï¼šè°ƒä¼˜å¯èƒ½æå‡3-5%ï¼ˆF1=0.78-0.80ï¼‰
```

---

### Step 2: ç¡®å®šè°ƒä¼˜ç­–ç•¥ï¼ˆ3åˆ†é’Ÿï¼‰

#### ç­–ç•¥å†³ç­–çŸ©é˜µ

| å‚æ•°æ•°é‡ | æ¯å‚æ•°å€™é€‰æ•° | æ€»ç»„åˆæ•° | æ—¶é—´é¢„ç®— | æ¨èç­–ç•¥ |
|---------|------------|---------|---------|---------|
| 2-3 | 3-5 | <50 | ä»»ä½• | ç½‘æ ¼æœç´¢ï¼ˆå…¨è¦†ç›–ï¼‰ |
| 3-5 | 3-5 | 50-200 | >1å°æ—¶ | ç½‘æ ¼æœç´¢ |
| 3-5 | 3-5 | 50-200 | <1å°æ—¶ | éšæœºæœç´¢ï¼ˆ30-50æ¬¡ï¼‰ |
| 5-10 | 3-5 | >200 | >2å°æ—¶ | éšæœºæœç´¢ï¼ˆ50-100æ¬¡ï¼‰ |
| 5-10 | 3-5 | >200 | <2å°æ—¶ | éšæœºæœç´¢ï¼ˆ30æ¬¡ï¼‰ |
| >10 | ä»»ä½• | >1000 | å……è¶³ | è´å¶æ–¯ä¼˜åŒ– |

#### ç¤ºä¾‹ï¼šXGBoostè°ƒä¼˜ç­–ç•¥

```python
# åœºæ™¯ï¼šä¸­å‹æ•°æ®é›†ï¼Œ2å°æ—¶æ—¶é—´é¢„ç®—

# æ–¹æ¡ˆ1ï¼šç½‘æ ¼æœç´¢ï¼ˆå…¨è¦†ç›–ï¼Œæ…¢ï¼‰
param_grid_full = {
    'learning_rate': [0.01, 0.05, 0.1],      # 3ä¸ª
    'n_estimators': [100, 200, 500],          # 3ä¸ª
    'max_depth': [3, 5, 7],                   # 3ä¸ª
    # æ€»ç»„åˆï¼š3Ã—3Ã—3 = 27ç§
}
# é¢„è®¡è€—æ—¶ï¼š27 Ã— 5åˆ†é’Ÿï¼ˆ5æŠ˜CVï¼‰ = 2.25å°æ—¶ âœ— è¶…æ—¶ï¼

# æ–¹æ¡ˆ2ï¼šéšæœºæœç´¢ï¼ˆéƒ¨åˆ†è¦†ç›–ï¼Œå¿«ï¼‰
param_distributions = {
    'learning_rate': [0.01, 0.05, 0.1, 0.3],
    'n_estimators': [100, 200, 300, 500, 1000],
    'max_depth': [3, 5, 7, 9],
    'min_child_weight': [1, 3, 5],
    # æ€»ç»„åˆï¼š4Ã—5Ã—4Ã—3 = 240ç§
}
n_iter = 30  # åªæœç´¢30æ¬¡
# é¢„è®¡è€—æ—¶ï¼š30 Ã— 5åˆ†é’Ÿ = 2.5å°æ—¶ âœ“ å¯æ¥å—ï¼ˆç•¥è¶…ï¼‰

# æ–¹æ¡ˆ3ï¼šåˆ†é˜¶æ®µç½‘æ ¼æœç´¢ï¼ˆæ¨èï¼‰
# ç¬¬1é˜¶æ®µï¼šç²—è°ƒï¼ˆå¿«ï¼‰
param_grid_coarse = {
    'learning_rate': [0.05, 0.1],
    'n_estimators': [100, 300],
    'max_depth': [5, 7],
}  # 8ç§ç»„åˆï¼Œ40åˆ†é’Ÿ

# ç¬¬2é˜¶æ®µï¼šç²¾è°ƒï¼ˆåŸºäºç¬¬1é˜¶æ®µç»“æœï¼‰
param_grid_fine = {
    'learning_rate': [0.03, 0.05, 0.07],  # å›´ç»•ç¬¬1é˜¶æ®µæœ€ä¼˜å€¼
    'n_estimators': [250, 300, 350],
    'max_depth': [5],  # å›ºå®š
}  # 9ç§ç»„åˆï¼Œ45åˆ†é’Ÿ

# æ€»è€—æ—¶ï¼š1.5å°æ—¶ âœ“ ç¬¦åˆé¢„ç®—
```

---

### Step 3: å®šä¹‰æœç´¢ç©ºé—´ï¼ˆ5åˆ†é’Ÿï¼‰

#### æœç´¢ç©ºé—´å®šä¹‰æ¨¡æ¿

```python
# æ¨¡æ¿ï¼šXGBooståˆ†é˜¶æ®µè°ƒä¼˜

# ===== ç¬¬1é˜¶æ®µï¼šlearning_rate + n_estimators =====
# ç›®æ ‡ï¼šç¡®å®šå­¦ä¹ ç‡å’Œæ ‘æ•°é‡çš„ç»„åˆ
# è€—æ—¶ï¼šçº¦30-60åˆ†é’Ÿ

param_grid_stage1 = {
    'learning_rate': [0.01, 0.05, 0.1, 0.3],
    'n_estimators': [100, 200, 500],
    # å…¶ä»–å‚æ•°ä½¿ç”¨é»˜è®¤å€¼
}

# ===== ç¬¬2é˜¶æ®µï¼šmax_depth + min_child_weight =====
# ç›®æ ‡ï¼šæ§åˆ¶æ ‘çš„å¤æ‚åº¦
# è€—æ—¶ï¼šçº¦30-60åˆ†é’Ÿ

param_grid_stage2 = {
    'learning_rate': [0.05],  # å›ºå®šä¸ºstage1æœ€ä¼˜
    'n_estimators': [200],     # å›ºå®šä¸ºstage1æœ€ä¼˜
    'max_depth': [3, 5, 7, 9],
    'min_child_weight': [1, 3, 5, 7],
}

# ===== ç¬¬3é˜¶æ®µï¼šsubsample + colsample_bytree =====
# ç›®æ ‡ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
# è€—æ—¶ï¼šçº¦20-40åˆ†é’Ÿ

param_grid_stage3 = {
    # å‰é¢å‚æ•°å›ºå®šä¸ºæœ€ä¼˜å€¼
    'learning_rate': [0.05],
    'n_estimators': [200],
    'max_depth': [5],
    'min_child_weight': [3],
    # æ–°è°ƒå‚æ•°
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
}

# ===== ç¬¬4é˜¶æ®µï¼šæ­£åˆ™åŒ–å‚æ•°ï¼ˆå¯é€‰ï¼‰ =====
# ç›®æ ‡ï¼šè¿›ä¸€æ­¥é˜²æ­¢è¿‡æ‹Ÿåˆ
# è€—æ—¶ï¼šçº¦20-40åˆ†é’Ÿ

param_grid_stage4 = {
    # æ‰€æœ‰å‰é¢å‚æ•°å›ºå®š
    ...
    # æ–°è°ƒå‚æ•°
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.01, 0.1],
    'reg_lambda': [1, 10, 100],
}
```

---

### Step 4: æ‰§è¡Œè°ƒä¼˜ï¼ˆ5-10åˆ†é’Ÿå®ç°ä»£ç ï¼‰

#### ç½‘æ ¼æœç´¢ä»£ç æ¨¡æ¿

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer, f1_score

# 1. å®šä¹‰æ¨¡å‹
model = RandomForestClassifier(random_state=42)

# 2. å®šä¹‰å‚æ•°ç½‘æ ¼
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
}

# 3. å®šä¹‰è¯„åˆ†å‡½æ•°ï¼ˆä½¿ç”¨ä¸šåŠ¡å…³æ³¨çš„æŒ‡æ ‡ï¼‰
scorer = make_scorer(f1_score, average='weighted')

# 4. ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring=scorer,
    cv=5,  # 5æŠ˜äº¤å‰éªŒè¯
    n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    verbose=2,  # æ˜¾ç¤ºè¿›åº¦
    return_train_score=True  # è¿”å›è®­ç»ƒé›†å¾—åˆ†ï¼ˆæ£€æŸ¥è¿‡æ‹Ÿåˆï¼‰
)

# 5. æ‰§è¡Œæœç´¢
print("å¼€å§‹ç½‘æ ¼æœç´¢...")
grid_search.fit(X_train, y_train)

# 6. æŸ¥çœ‹ç»“æœ
print("\næœ€ä¼˜å‚æ•°:")
print(grid_search.best_params_)

print(f"\næœ€ä¼˜å¾—åˆ†ï¼ˆCVï¼‰: {grid_search.best_score_:.4f}")

# 7. æµ‹è¯•é›†éªŒè¯
best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)
print(f"æµ‹è¯•é›†å¾—åˆ†: {test_score:.4f}")

# 8. è¯¦ç»†ç»“æœåˆ†æ
results_df = pd.DataFrame(grid_search.cv_results_)
print("\næ‰€æœ‰å‚æ•°ç»„åˆçš„æ€§èƒ½:")
print(results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]
      .sort_values('rank_test_score'))
```

#### éšæœºæœç´¢ä»£ç æ¨¡æ¿

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# 1. å®šä¹‰å‚æ•°åˆ†å¸ƒï¼ˆæ›´å¤§çš„æœç´¢ç©ºé—´ï¼‰
param_distributions = {
    'n_estimators': randint(50, 500),  # æ•´æ•°ï¼š50-500ä¹‹é—´éšæœº
    'max_depth': randint(5, 50),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.5, 0.5),  # æµ®ç‚¹æ•°ï¼š0.5-1.0ä¹‹é—´éšæœº
}

# 2. éšæœºæœç´¢
random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=30,  # æœç´¢30æ¬¡ï¼ˆè€Œéå…¨éƒ¨ç»„åˆï¼‰
    scoring='f1_weighted',
    cv=5,
    n_jobs=-1,
    verbose=2,
    random_state=42,
    return_train_score=True
)

# 3. æ‰§è¡Œæœç´¢
random_search.fit(X_train, y_train)

# 4. æŸ¥çœ‹ç»“æœï¼ˆåŒç½‘æ ¼æœç´¢ï¼‰
print("æœ€ä¼˜å‚æ•°:", random_search.best_params_)
print(f"æœ€ä¼˜å¾—åˆ†: {random_search.best_score_:.4f}")
```

#### è´å¶æ–¯ä¼˜åŒ–ä»£ç æ¨¡æ¿

```python
# éœ€è¦å®‰è£…ï¼špip install scikit-optimize
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# 1. å®šä¹‰æœç´¢ç©ºé—´ï¼ˆä½¿ç”¨skoptçš„spaceï¼‰
param_space = {
    'n_estimators': Integer(50, 500),
    'max_depth': Integer(5, 50),
    'min_samples_split': Integer(2, 20),
    'min_samples_leaf': Integer(1, 10),
    'max_features': Real(0.5, 1.0),
}

# 2. è´å¶æ–¯æœç´¢
bayes_search = BayesSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    search_spaces=param_space,
    n_iter=30,  # æœç´¢30æ¬¡ï¼ˆæ™ºèƒ½é‡‡æ ·ï¼‰
    scoring='f1_weighted',
    cv=5,
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# 3. æ‰§è¡Œæœç´¢ï¼ˆä¼šä¼˜å…ˆæœç´¢æ›´æœ‰å¸Œæœ›çš„åŒºåŸŸï¼‰
bayes_search.fit(X_train, y_train)

# 4. æŸ¥çœ‹ç»“æœ
print("æœ€ä¼˜å‚æ•°:", bayes_search.best_params_)
print(f"æœ€ä¼˜å¾—åˆ†: {bayes_search.best_score_:.4f}")
```

---

### Step 5: éªŒè¯å’Œéƒ¨ç½²ï¼ˆ2åˆ†é’Ÿï¼‰

#### éªŒè¯æ£€æŸ¥æ¸…å•

```
ã€è°ƒä¼˜ç»“æœéªŒè¯ã€‘

âœ… æ€§èƒ½æ£€æŸ¥ï¼š
- [ ] æµ‹è¯•é›†æ€§èƒ½æå‡ï¼ˆvs Baselineï¼‰ï¼š+____%
- [ ] CVæ€§èƒ½ç¨³å®šï¼ˆstd<0.05ï¼‰ï¼š____
- [ ] è®­ç»ƒé›†vsæµ‹è¯•é›†å·®å¼‚<10%ï¼ˆæ— è¿‡æ‹Ÿåˆï¼‰

âœ… æ•ˆç‡æ£€æŸ¥ï¼š
- [ ] è®­ç»ƒæ—¶é—´å¯æ¥å—ï¼š____ç§’
- [ ] é¢„æµ‹é€Ÿåº¦å¯æ¥å—ï¼š____ms
- [ ] æ¨¡å‹å¤§å°å¯æ¥å—ï¼š____MB

âœ… å¯¹æ¯”å†³ç­–ï¼š
- [ ] è°ƒä¼˜æ¨¡å‹ vs é»˜è®¤å‚æ•°ï¼šæå‡____%
- [ ] æ˜¯å¦å€¼å¾—éƒ¨ç½²ï¼š[ ] æ˜¯ [ ] å¦
- [ ] ç†ç”±ï¼š________________

ã€éƒ¨ç½²å†³ç­–ã€‘
â–¡ [ ] ä½¿ç”¨è°ƒä¼˜åçš„æ¨¡å‹
â–¡ [ ] ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆè°ƒä¼˜æ”¶ç›Šä¸å¤§ï¼‰
â–¡ [ ] éœ€è¦furtherè°ƒä¼˜ï¼ˆæ€§èƒ½ä»ä¸è¾¾æ ‡ï¼‰
```

---

## ğŸ“š å®æˆ˜è°ƒä¼˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šXGBooståˆ†ç±»è°ƒä¼˜ï¼ˆå®Œæ•´æµç¨‹ï¼‰

**èƒŒæ™¯**ï¼š
- ä»»åŠ¡ï¼šå®¢æˆ·æµå¤±é¢„æµ‹ï¼ˆäºŒåˆ†ç±»ï¼‰
- æ•°æ®ï¼š5000è¡Œ Ã— 25åˆ—
- Baselineï¼šF1=0.72ï¼ˆXGBoosté»˜è®¤å‚æ•°ï¼‰
- ç›®æ ‡ï¼šF1â‰¥0.80

**è°ƒä¼˜æµç¨‹**ï¼š

#### ç¬¬1é˜¶æ®µï¼šlearning_rate + n_estimators

```python
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

# ç¬¬1é˜¶æ®µå‚æ•°
param_grid_1 = {
    'learning_rate': [0.01, 0.05, 0.1, 0.3],
    'n_estimators': [100, 200, 300, 500],
}

xgb = XGBClassifier(random_state=42, use_label_encoder=False)

grid_1 = GridSearchCV(
    xgb, param_grid_1,
    scoring='f1',
    cv=5,
    verbose=1
)

grid_1.fit(X_train, y_train)

print("ç¬¬1é˜¶æ®µæœ€ä¼˜å‚æ•°:", grid_1.best_params_)
# è¾“å‡ºï¼š{'learning_rate': 0.05, 'n_estimators': 300}

print(f"ç¬¬1é˜¶æ®µæœ€ä¼˜F1: {grid_1.best_score_:.4f}")
# è¾“å‡ºï¼š0.7650ï¼ˆæå‡0.045ï¼‰
```

#### ç¬¬2é˜¶æ®µï¼šmax_depth + min_child_weight

```python
# å›ºå®šç¬¬1é˜¶æ®µæœ€ä¼˜å‚æ•°
param_grid_2 = {
    'learning_rate': [0.05],
    'n_estimators': [300],
    'max_depth': [3, 5, 7, 9],
    'min_child_weight': [1, 3, 5, 7],
}

grid_2 = GridSearchCV(
    xgb, param_grid_2,
    scoring='f1',
    cv=5
)

grid_2.fit(X_train, y_train)

print("ç¬¬2é˜¶æ®µæœ€ä¼˜å‚æ•°:", grid_2.best_params_)
# è¾“å‡ºï¼š{'max_depth': 5, 'min_child_weight': 3, ...}

print(f"ç¬¬2é˜¶æ®µæœ€ä¼˜F1: {grid_2.best_score_:.4f}")
# è¾“å‡ºï¼š0.7820ï¼ˆå†æå‡0.017ï¼‰
```

#### ç¬¬3é˜¶æ®µï¼šsubsample + colsample_bytree

```python
param_grid_3 = {
    'learning_rate': [0.05],
    'n_estimators': [300],
    'max_depth': [5],
    'min_child_weight': [3],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
}

grid_3 = GridSearchCV(
    xgb, param_grid_3,
    scoring='f1',
    cv=5
)

grid_3.fit(X_train, y_train)

print("ç¬¬3é˜¶æ®µæœ€ä¼˜å‚æ•°:", grid_3.best_params_)
# è¾“å‡ºï¼š{'subsample': 0.8, 'colsample_bytree': 0.8, ...}

print(f"ç¬¬3é˜¶æ®µæœ€ä¼˜F1: {grid_3.best_score_:.4f}")
# è¾“å‡ºï¼š0.7950ï¼ˆå†æå‡0.013ï¼‰
```

#### æœ€ç»ˆæµ‹è¯•

```python
# ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
best_params = {
    'learning_rate': 0.05,
    'n_estimators': 300,
    'max_depth': 5,
    'min_child_weight': 3,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42
}

final_model = XGBClassifier(**best_params)
final_model.fit(X_train, y_train)

# æµ‹è¯•é›†è¯„ä¼°
from sklearn.metrics import classification_report

y_pred = final_model.predict(X_test)
print(classification_report(y_test, y_pred))

# F1-Score: 0.81ï¼ˆè¾¾æ ‡ï¼ï¼‰
```

**è°ƒä¼˜æ€»ç»“**ï¼š
```
Baseline â†’ ç¬¬1é˜¶æ®µ â†’ ç¬¬2é˜¶æ®µ â†’ ç¬¬3é˜¶æ®µ â†’ æœ€ç»ˆ
0.72  â†’   0.765  â†’   0.782  â†’   0.795  â†’  0.81

æ€»æå‡ï¼š+0.09ï¼ˆ+12.5%ï¼‰
æ€»è€—æ—¶ï¼šçº¦1.5å°æ—¶
```

---

### æ¡ˆä¾‹2ï¼šRandom Forestå¿«é€Ÿè°ƒä¼˜

**èƒŒæ™¯**ï¼š
- ä»»åŠ¡ï¼šæˆ¿ä»·é¢„æµ‹ï¼ˆå›å½’ï¼‰
- æ•°æ®ï¼š1500è¡Œ Ã— 80åˆ—
- æ—¶é—´é¢„ç®—ï¼š30åˆ†é’Ÿï¼ˆç´§è¿«ï¼‰
- Baselineï¼šRÂ²=0.75

**å¿«é€Ÿè°ƒä¼˜ç­–ç•¥**ï¼š

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

# ç­–ç•¥ï¼šéšæœºæœç´¢ï¼Œå°‘é‡è¿­ä»£
param_distributions = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'max_features': ['sqrt', 'log2', 0.5],
}

rf = RandomForestRegressor(random_state=42)

random_search = RandomizedSearchCV(
    rf,
    param_distributions,
    n_iter=15,  # åªæœ15æ¬¡ï¼ˆå¿«é€Ÿï¼‰
    scoring='r2',
    cv=3,  # 3æŠ˜ï¼ˆè€Œé5æŠ˜ï¼ŒèŠ‚çœæ—¶é—´ï¼‰
    verbose=1,
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)

print("æœ€ä¼˜å‚æ•°:", random_search.best_params_)
# è¾“å‡ºï¼š{'n_estimators': 300, 'max_depth': 20, ...}

print(f"CV RÂ²: {random_search.best_score_:.4f}")
# è¾“å‡ºï¼š0.82ï¼ˆæå‡0.07ï¼‰

# è€—æ—¶ï¼šçº¦25åˆ†é’Ÿ âœ“
```

---

## âš ï¸ å¸¸è§é™·é˜±ä¸è§£å†³

### é™·é˜±1ï¼šè°ƒä¼˜å¯¼è‡´è¿‡æ‹Ÿåˆ

**é—®é¢˜**ï¼š
```python
# âŒ åœ¨æµ‹è¯•é›†ä¸Šè°ƒä¼˜
best_score = 0
for params in param_grid:
    model.set_params(**params)
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)  # ç”¨æµ‹è¯•é›†é€‰å‚æ•°ï¼
    if score > best_score:
        best_params = params

# é—®é¢˜ï¼šæµ‹è¯•é›†æ€§èƒ½è™šé«˜ï¼Œéƒ¨ç½²åæ€§èƒ½ä¸‹é™
```

**è§£å†³**ï¼š
```python
# âœ… ä½¿ç”¨äº¤å‰éªŒè¯
from sklearn.model_selection import cross_val_score

best_score = 0
for params in param_grid:
    model.set_params(**params)
    scores = cross_val_score(model, X_train, y_train, cv=5)
    score = scores.mean()  # ç”¨CVå¾—åˆ†é€‰å‚æ•°
    if score > best_score:
        best_params = params

# æœ€ååœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯ä¸€æ¬¡
model.set_params(**best_params)
model.fit(X_train, y_train)
final_score = model.score(X_test, y_test)
```

---

### é™·é˜±2ï¼šå°æ ·æœ¬å¤§é‡è°ƒä¼˜

**é—®é¢˜**ï¼š
```python
# æ ·æœ¬é‡ï¼š300è¡Œ
# âŒ è°ƒä¼˜10+ä¸ªå‚æ•°
param_grid = {
    'param1': [...],  # 5ä¸ªå€¼
    'param2': [...],  # 5ä¸ªå€¼
    ...
    'param10': [...], # 5ä¸ªå€¼
}  # æ€»ç»„åˆï¼š5^10 = è¿‘åƒä¸‡ç§

# é—®é¢˜ï¼šä¸¥é‡è¿‡æ‹Ÿåˆï¼Œè°ƒä¼˜ç»“æœä¸å¯é 
```

**è§£å†³**ï¼š
```python
# âœ… å°æ ·æœ¬ä»…è°ƒ1-2ä¸ªæœ€å…³é”®å‚æ•°
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # æ­£åˆ™åŒ–
}

# ä½¿ç”¨LOOäº¤å‰éªŒè¯å……åˆ†åˆ©ç”¨æ•°æ®
from sklearn.model_selection import LeaveOneOut
grid = GridSearchCV(model, param_grid, cv=LeaveOneOut())
```

---

### é™·é˜±3ï¼šå¿½ç•¥è®­ç»ƒ/æµ‹è¯•å·®å¼‚

**é—®é¢˜**ï¼š
```python
# âŒ åªçœ‹CVå¾—åˆ†
grid_search.fit(X_train, y_train)
print(f"CVå¾—åˆ†: {grid_search.best_score_}")  # 0.95

# éƒ¨ç½²åå‘ç°æµ‹è¯•é›†å¾—åˆ†ï¼š0.75ï¼ˆä¸¥é‡è¿‡æ‹Ÿåˆï¼‰
```

**è§£å†³**ï¼š
```python
# âœ… å¯¹æ¯”è®­ç»ƒé›†å’Œæµ‹è¯•é›†
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_

# è®­ç»ƒé›†æ€§èƒ½
train_score = best_model.score(X_train, y_train)

# æµ‹è¯•é›†æ€§èƒ½
test_score = best_model.score(X_test, y_test)

print(f"è®­ç»ƒé›†: {train_score:.4f}")
print(f"æµ‹è¯•é›†: {test_score:.4f}")
print(f"å·®å¼‚: {train_score - test_score:.4f}")

if train_score - test_score > 0.1:
    print("âš ï¸ è­¦å‘Šï¼šå­˜åœ¨è¿‡æ‹Ÿåˆï¼Œéœ€è¦å¢å¼ºæ­£åˆ™åŒ–")
```

---

## ğŸ”— ç›¸å…³èµ„æº

### å‰ç½®å­¦ä¹ 

ğŸ’¡ **ç¬¬ä¸€æ¬¡è°ƒä¼˜è¶…å‚æ•°**ï¼Œå»ºè®®å…ˆå­¦ä¹ ï¼š
- ğŸ“– 03_algorithm_selection_matrix/algorithm_comparison_table.mdï¼ˆæ¯ä¸ªç®—æ³•çš„å‚æ•°è¯´æ˜ï¼‰
- ğŸ“– ML_WORKFLOW_GUIDE.md - ç¬¬5éƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒé˜¶æ®µ

### åç»­æ­¥éª¤

ğŸ“– **å®Œæˆè°ƒä¼˜å**ï¼ŒæŸ¥çœ‹ï¼š
- ğŸ“‹ model_evaluation_template.mdï¼ˆè¯„ä¼°è°ƒä¼˜æ•ˆæœï¼‰
- ğŸ’» 08_code_templates/modeling_templates.pyï¼ˆè°ƒä¼˜ä»£ç æ¨¡æ¿ï¼‰

### æ·±å…¥å­¦ä¹ 

ğŸ“š **æƒ³æ·±å…¥ç†è§£è°ƒä¼˜æ–¹æ³•**ï¼Œå‚è€ƒï¼š
- ğŸ“– 06_comprehensive_project/phase3_supervised_solution.ipynbï¼ˆå®Œæ•´è°ƒä¼˜æµç¨‹ï¼‰
- ğŸ“– Scikit-learnå®˜æ–¹æ–‡æ¡£ï¼šGridSearchCV, RandomizedSearchCV

---

**æœ€åæ›´æ–°**ï¼š2024å¹´11æœˆ
**é€‚ç”¨åœºæ™¯**ï¼šæ¨¡å‹åˆæ­¥è®­ç»ƒåçš„æ€§èƒ½ä¼˜åŒ–
**å»ºè®®ä½¿ç”¨é¢‘ç‡**ï¼šæ¯ä¸ªç®—æ³•è°ƒä¼˜ä¸€æ¬¡
