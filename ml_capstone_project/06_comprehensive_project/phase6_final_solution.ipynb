{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6: Final Solutionï¼ˆæœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ æœ¬é˜¶æ®µç›®æ ‡\n",
    "\n",
    "æ•´åˆ **Phase 1-4 çš„æ‰€æœ‰æ´å¯Ÿå’ŒæŠ€æœ¯**ï¼Œæ„å»ºä¸€ä¸ª**ç”Ÿäº§çº§çš„ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆ**ã€‚\n",
    "\n",
    "### æ ¸å¿ƒä»»åŠ¡\n",
    "\n",
    "1. âœ… **å›é¡¾ä¸æ•´åˆ**ï¼šæ€»ç»“Phase 1-4çš„å…³é”®å‘ç°\n",
    "2. âœ… **æœ€ä¼˜é¢„å¤„ç†Pipeline**ï¼šç»“åˆè¯Šæ–­ç»“æœçš„æ™ºèƒ½é¢„å¤„ç†\n",
    "3. âœ… **æœ€ä¼˜ç‰¹å¾å·¥ç¨‹**ï¼šç›‘ç£ç‰¹å¾ + æ— ç›‘ç£ç‰¹å¾\n",
    "4. âœ… **æ¨¡å‹é›†æˆ**ï¼šVoting Ensemble + Stacking Ensemble\n",
    "5. âœ… **æœ€ç»ˆè¯„ä¼°**ï¼šå…¨é¢çš„æ€§èƒ½è¯„ä¼°å’Œå¯¹æ¯”\n",
    "6. âœ… **æ¨¡å‹æŒä¹…åŒ–**ï¼šä¿å­˜æœ€ä½³æ¨¡å‹å’ŒPipeline\n",
    "7. âœ… **ç”Ÿäº§çº§å°è£…**ï¼šå¯å¤ç”¨çš„é¢„æµ‹Pipelineç±»\n",
    "8. âœ… **éƒ¨ç½²å‡†å¤‡**ï¼šä½¿ç”¨è¯´æ˜å’Œç¤ºä¾‹\n",
    "\n",
    "### è®¾è®¡åŸåˆ™\n",
    "\n",
    "- **ç«¯åˆ°ç«¯**ï¼šä»åŸå§‹æ•°æ®åˆ°æœ€ç»ˆé¢„æµ‹çš„å®Œæ•´æµç¨‹\n",
    "- **å¯å¤ç”¨**ï¼šå°è£…ä¸ºå¯å¤ç”¨çš„ç±»å’Œå‡½æ•°\n",
    "- **å¯ç»´æŠ¤**ï¼šæ¸…æ™°çš„ä»£ç ç»“æ„å’Œæ–‡æ¡£\n",
    "- **ç”Ÿäº§å°±ç»ª**ï¼šè€ƒè™‘éƒ¨ç½²å’Œå®é™…åº”ç”¨åœºæ™¯\n",
    "\n",
    "---\n",
    "\n",
    "## é¢„æœŸæ—¶é—´\n",
    "\n",
    "â±ï¸ **30-90åˆ†é’Ÿ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# å¯¼å…¥æœ¬é¡¹ç›®çš„æ¨¡å—\n",
    "from src import data_diagnosis as dd\n",
    "from src import data_preprocessing as dp\n",
    "from src import feature_engineering as fe\n",
    "from src import supervised_pipeline as sp\n",
    "from src import unsupervised_pipeline as up\n",
    "from src import model_evaluation as me\n",
    "from src import visualization as viz\n",
    "\n",
    "# sklearnæ¨¡å‹\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# è®¾ç½®\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# éšæœºç§å­\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ\")\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {Path.cwd()}\")\n",
    "print(f\"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Phase 1-4 å…³é”®å‘ç°å›é¡¾\n",
    "\n",
    "åœ¨æ„å»ºæœ€ç»ˆæ–¹æ¡ˆå‰ï¼Œå…ˆå›é¡¾ä¹‹å‰å„é˜¶æ®µçš„å…³é”®å‘ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Phase 1-4 å…³é”®å‘ç°æ€»ç»“\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "phase_summary = {\n",
    "    'Phase 1: æ•°æ®è¯Šæ–­': [\n",
    "        \"âœ“ è¯†åˆ«äº†æ•°æ®ä¸­çš„ç¼ºå¤±æ¨¡å¼ï¼ˆMCAR/MAR/MNARï¼‰\",\n",
    "        \"âœ“ å‘ç°äº†æ•°æ®åˆ†å¸ƒç‰¹å¾å’Œå¼‚å¸¸å€¼\",\n",
    "        \"âœ“ è®¡ç®—äº†æ•°æ®è´¨é‡è¯„åˆ†\",\n",
    "        \"âœ“ ç”Ÿæˆäº†è¯Šæ–­æŠ¥å‘Š\"\n",
    "    ],\n",
    "    'Phase 2: å¿«é€ŸåŸºçº¿': [\n",
    "        \"âœ“ å»ºç«‹äº†4ç§æ¨¡å‹çš„æ€§èƒ½åŸºå‡†\",\n",
    "        \"âœ“ ä½¿ç”¨ç®€å•é¢„å¤„ç†è·å¾—äº†åˆå§‹æ€§èƒ½\",\n",
    "        \"âœ“ ç¡®å®šäº†æœ€ä½³åŸºçº¿æ¨¡å‹\",\n",
    "        \"âœ“ ä¸ºä¼˜åŒ–æä¾›äº†å¯¹æ¯”åŸºå‡†\"\n",
    "    ],\n",
    "    'Phase 3: ç›‘ç£å­¦ä¹ ä¼˜åŒ–': [\n",
    "        \"âœ“ åº”ç”¨äº†é«˜çº§ç‰¹å¾å·¥ç¨‹ï¼ˆäº¤äº’ç‰¹å¾ã€ç‰¹å¾é€‰æ‹©ï¼‰\",\n",
    "        \"âœ“ ä½¿ç”¨äº†æ™ºèƒ½é¢„å¤„ç†ï¼ˆKNNå¡«å……ã€é«˜çº§ç¼–ç ï¼‰\",\n",
    "        \"âœ“ è¿›è¡Œäº†è¶…å‚æ•°è°ƒä¼˜\",\n",
    "        \"âœ“ é€šè¿‡å­¦ä¹ æ›²çº¿è¯Šæ–­äº†æ¨¡å‹çŠ¶æ€\"\n",
    "    ],\n",
    "    'Phase 4: æ— ç›‘ç£æ´å¯Ÿ': [\n",
    "        \"âœ“ é€šè¿‡èšç±»å‘ç°äº†æ•°æ®çš„è‡ªç„¶åˆ†ç»„\",\n",
    "        \"âœ“ ä½¿ç”¨é™ç»´å¯è§†åŒ–ç†è§£äº†æ•°æ®ç»“æ„\",\n",
    "        \"âœ“ æ£€æµ‹å¹¶åˆ†æäº†å¼‚å¸¸ç‚¹\",\n",
    "        \"âœ“ æå–äº†æ— ç›‘ç£ç‰¹å¾å¢å¼ºæ¨¡å‹\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for phase, findings in phase_summary.items():\n",
    "    print(f\"\\n{phase}\")\n",
    "    for finding in findings:\n",
    "        print(f\"  {finding}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 6: æ•´åˆæ‰€æœ‰æŠ€æœ¯ï¼Œæ„å»ºæœ€ç»ˆè§£å†³æ–¹æ¡ˆ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. æ•°æ®åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "df = sns.load_dataset('titanic')\n",
    "target_col = 'survived'\n",
    "\n",
    "print(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "print(f\"ç›®æ ‡å˜é‡: {target_col}\")\n",
    "print(f\"\\nç›®æ ‡å˜é‡åˆ†å¸ƒ:\")\n",
    "print(df[target_col].value_counts())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. æœ€ä¼˜æ•°æ®é¢„å¤„ç†Pipeline\n",
    "\n",
    "åŸºäºPhase 1è¯Šæ–­å’ŒPhase 3çš„é«˜çº§æ–¹æ³•ï¼Œæ„å»ºæœ€ä¼˜é¢„å¤„ç†æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"æœ€ä¼˜æ•°æ®é¢„å¤„ç†Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åˆ†ç¦»ç›®æ ‡å˜é‡\n",
    "y = df[target_col]\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "print(f\"\\nåŸå§‹æ•°æ®: {X.shape}\")\n",
    "\n",
    "# Step 1: åˆ é™¤IDåˆ—å’Œé«˜ç¼ºå¤±ç‡åˆ—\n",
    "print(\"\\nStep 1: åˆ é™¤æ— ç”¨åˆ—\")\n",
    "drop_cols = []\n",
    "for col in X.columns:\n",
    "    # IDåˆ—\n",
    "    if X[col].nunique() / len(X) > 0.95:\n",
    "        drop_cols.append(col)\n",
    "        \n",
    "# é«˜ç¼ºå¤±ç‡åˆ—\n",
    "missing_rate = X.isnull().sum() / len(X)\n",
    "high_missing = missing_rate[missing_rate > 0.5].index.tolist()\n",
    "drop_cols.extend(high_missing)\n",
    "\n",
    "if drop_cols:\n",
    "    print(f\"  åˆ é™¤åˆ—: {drop_cols}\")\n",
    "    X = X.drop(columns=drop_cols)\n",
    "\n",
    "print(f\"  å‰©ä½™åˆ—æ•°: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: åˆ†ç¦»æ•°å€¼å‹å’Œç±»åˆ«å‹ç‰¹å¾\n",
    "print(\"\\nStep 2: ç‰¹å¾ç±»å‹è¯†åˆ«\")\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"  æ•°å€¼å‹: {len(numeric_cols)} ä¸ª\")\n",
    "print(f\"  ç±»åˆ«å‹: {len(categorical_cols)} ä¸ª\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: KNNå¡«å……æ•°å€¼ç‰¹å¾ï¼ˆPhase 3æ–¹æ³•ï¼‰\n",
    "print(\"\\nStep 3: KNNå¡«å……æ•°å€¼ç‰¹å¾\")\n",
    "numeric_missing = X[numeric_cols].isnull().sum()\n",
    "numeric_missing_cols = numeric_missing[numeric_missing > 0].index.tolist()\n",
    "\n",
    "if numeric_missing_cols:\n",
    "    print(f\"  å¡«å……åˆ—: {numeric_missing_cols}\")\n",
    "    X = dp.knn_impute(X, columns=numeric_missing_cols, n_neighbors=5)\n",
    "else:\n",
    "    print(\"  æ— éœ€å¡«å……\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: å¡«å……ç±»åˆ«ç‰¹å¾\n",
    "print(\"\\nStep 4: å¡«å……ç±»åˆ«ç‰¹å¾ï¼ˆä¼—æ•°ï¼‰\")\n",
    "for col in categorical_cols:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        mode_val = X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown'\n",
    "        X[col] = X[col].fillna(mode_val)\n",
    "        print(f\"  {col}: å¡«å……ä¸º '{mode_val}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: æ™ºèƒ½ç¼–ç ï¼ˆPhase 3æ–¹æ³•ï¼‰\n",
    "print(\"\\nStep 5: æ™ºèƒ½ç¼–ç \")\n",
    "\n",
    "low_card_cols = []\n",
    "high_card_cols = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    n_unique = X[col].nunique()\n",
    "    if n_unique <= 10:\n",
    "        low_card_cols.append(col)\n",
    "    else:\n",
    "        high_card_cols.append(col)\n",
    "\n",
    "if low_card_cols:\n",
    "    print(f\"  One-Hotç¼–ç : {low_card_cols}\")\n",
    "    X = dp.onehot_encode(X, columns=low_card_cols, drop_first=True)\n",
    "\n",
    "if high_card_cols:\n",
    "    print(f\"  é¢‘ç‡ç¼–ç : {high_card_cols}\")\n",
    "    X = dp.frequency_encoding(X, columns=high_card_cols, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: æ ‡å‡†åŒ–\n",
    "print(\"\\nStep 6: æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾\")\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "X = dp.standardize_features(X, columns=numeric_features)\n",
    "\n",
    "print(f\"\\nâœ… é¢„å¤„ç†å®Œæˆ\")\n",
    "print(f\"æœ€ç»ˆç‰¹å¾æ•°: {X.shape[1]}\")\n",
    "print(f\"ç¼ºå¤±å€¼: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. æœ€ä¼˜ç‰¹å¾å·¥ç¨‹\n",
    "\n",
    "ç»“åˆPhase 3çš„ç›‘ç£ç‰¹å¾å·¥ç¨‹å’ŒPhase 4çš„æ— ç›‘ç£ç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ç‰¹å¾å·¥ç¨‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_engineered = X.copy()\n",
    "original_n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. äº¤äº’ç‰¹å¾ï¼ˆPhase 3ï¼‰\n",
    "print(\"\\n1. åˆ›å»ºäº¤äº’ç‰¹å¾\")\n",
    "important_numeric = [col for col in numeric_features if not col.startswith(('PCA', 'row_'))][:3]\n",
    "\n",
    "if len(important_numeric) >= 2:\n",
    "    X_interaction = fe.create_interaction_features(\n",
    "        X_engineered,\n",
    "        columns=important_numeric,\n",
    "        operations=['*', '+']\n",
    "    )\n",
    "    new_cols = [c for c in X_interaction.columns if c not in X_engineered.columns]\n",
    "    X_engineered = pd.concat([X_engineered, X_interaction[new_cols]], axis=1)\n",
    "    print(f\"  æ–°å¢ {len(new_cols)} ä¸ªäº¤äº’ç‰¹å¾\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. æ— ç›‘ç£ç‰¹å¾ï¼ˆPhase 4ï¼‰\n",
    "print(\"\\n2. æ·»åŠ æ— ç›‘ç£å­¦ä¹ ç‰¹å¾\")\n",
    "\n",
    "# èšç±»ç‰¹å¾\n",
    "print(\"  - K-Meansèšç±»...\")\n",
    "kmeans = up.ClusteringPipeline(method='kmeans', n_clusters=3, random_state=RANDOM_STATE)\n",
    "kmeans_labels = kmeans.fit_predict(X_engineered)\n",
    "X_engineered['cluster_label'] = kmeans_labels\n",
    "\n",
    "# å¼‚å¸¸æ£€æµ‹ç‰¹å¾\n",
    "print(\"  - Isolation Forestå¼‚å¸¸æ£€æµ‹...\")\n",
    "iforest = up.AnomalyDetectionPipeline(method='isolation_forest', contamination=0.1, random_state=RANDOM_STATE)\n",
    "iforest.fit(X_engineered.drop(columns=['cluster_label']))\n",
    "X_engineered['anomaly_score'] = iforest.get_anomaly_scores(X_engineered.drop(columns=['cluster_label']))\n",
    "\n",
    "# PCAç‰¹å¾\n",
    "print(\"  - PCAé™ç»´...\")\n",
    "pca = up.DimensionReductionPipeline(method='pca', n_components=2, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_engineered.drop(columns=['cluster_label', 'anomaly_score']))\n",
    "X_engineered['pca_1'] = X_pca[:, 0]\n",
    "X_engineered['pca_2'] = X_pca[:, 1]\n",
    "\n",
    "print(f\"  æ–°å¢ 4 ä¸ªæ— ç›‘ç£ç‰¹å¾\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ç‰¹å¾é€‰æ‹©ï¼ˆå»é™¤å†—ä½™ï¼‰\n",
    "print(\"\\n3. ç‰¹å¾é€‰æ‹©\")\n",
    "X_engineered = fe.select_by_variance(X_engineered, threshold=0.01)\n",
    "X_engineered = fe.select_by_correlation(X_engineered, threshold=0.90)\n",
    "\n",
    "print(f\"\\nâœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ\")\n",
    "print(f\"åŸå§‹ç‰¹å¾: {original_n_features}\")\n",
    "print(f\"æœ€ç»ˆç‰¹å¾: {X_engineered.shape[1]}\")\n",
    "print(f\"æ–°å¢ç‰¹å¾: {X_engineered.shape[1] - original_n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. æ•°æ®é›†åˆ’åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ’åˆ†æ•°æ®é›†\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_engineered, y,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"æ•°æ®é›†åˆ’åˆ†:\")\n",
    "print(f\"  è®­ç»ƒé›†: {X_train.shape} ({len(X_train)/len(X_engineered)*100:.0f}%)\")\n",
    "print(f\"  éªŒè¯é›†: {X_val.shape} ({len(X_val)/len(X_engineered)*100:.0f}%)\")\n",
    "print(f\"  æµ‹è¯•é›†: {X_test.shape} ({len(X_test)/len(X_engineered)*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\nç›®æ ‡å˜é‡åˆ†å¸ƒ:\")\n",
    "print(f\"  è®­ç»ƒé›†: {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"  éªŒè¯é›†: {y_val.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"  æµ‹è¯•é›†: {y_test.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. å•æ¨¡å‹è®­ç»ƒä¸å¯¹æ¯”\n",
    "\n",
    "è®­ç»ƒå¤šä¸ªä¼˜åŒ–åçš„å•æ¨¡å‹ï¼Œä¸ºé›†æˆåšå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"è®­ç»ƒå•æ¨¡å‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(\n",
    "        C=1.0,\n",
    "        max_iter=1000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "}\n",
    "\n",
    "# è®­ç»ƒå’Œè¯„ä¼°\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nè®­ç»ƒ {name}...\")\n",
    "    \n",
    "    # è®­ç»ƒ\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # è¯„ä¼°\n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred, average='binary')\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': acc,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"  éªŒè¯é›† Accuracy: {acc:.4f}\")\n",
    "    print(f\"  éªŒè¯é›† F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… å•æ¨¡å‹è®­ç»ƒå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n",
    "comparison_df = pd.DataFrame([\n",
    "    {'æ¨¡å‹': name, 'Accuracy': res['accuracy'], 'F1-Score': res['f1']}\n",
    "    for name, res in results.items()\n",
    "]).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"å•æ¨¡å‹æ€§èƒ½å¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "best_single_model = comparison_df.iloc[0]['æ¨¡å‹']\n",
    "print(f\"\\nğŸ† æœ€ä½³å•æ¨¡å‹: {best_single_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å•æ¨¡å‹æ€§èƒ½\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x_pos - width/2, comparison_df['Accuracy'], width, label='Accuracy', alpha=0.8, color='skyblue')\n",
    "ax.bar(x_pos + width/2, comparison_df['F1-Score'], width, label='F1-Score', alpha=0.8, color='orange')\n",
    "\n",
    "ax.set_ylabel('å¾—åˆ†')\n",
    "ax.set_title('å•æ¨¡å‹æ€§èƒ½å¯¹æ¯”')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(comparison_df['æ¨¡å‹'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. æ¨¡å‹é›†æˆ\n",
    "\n",
    "### 8.1 Voting Ensembleï¼ˆæŠ•ç¥¨é›†æˆï¼‰\n",
    "\n",
    "å¤šä¸ªæ¨¡å‹æŠ•ç¥¨ï¼Œæé«˜ç¨³å®šæ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Voting Ensembleï¼ˆæŠ•ç¥¨é›†æˆï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# é€‰æ‹©Top 3æ¨¡å‹è¿›è¡ŒæŠ•ç¥¨\n",
    "top3_models = comparison_df.head(3)['æ¨¡å‹'].tolist()\n",
    "print(f\"\\né€‰æ‹©Top 3æ¨¡å‹: {top3_models}\")\n",
    "\n",
    "# åˆ›å»ºVoting Classifier\n",
    "voting_estimators = [\n",
    "    (name, results[name]['model'])\n",
    "    for name in top3_models\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=voting_estimators,\n",
    "    voting='soft',  # ä½¿ç”¨æ¦‚ç‡æŠ•ç¥¨\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nè®­ç»ƒVoting Ensemble...\")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# è¯„ä¼°\n",
    "y_val_pred_voting = voting_clf.predict(X_val)\n",
    "voting_acc = accuracy_score(y_val, y_val_pred_voting)\n",
    "voting_f1 = f1_score(y_val, y_val_pred_voting, average='binary')\n",
    "\n",
    "print(f\"\\néªŒè¯é›†æ€§èƒ½:\")\n",
    "print(f\"  Accuracy: {voting_acc:.4f}\")\n",
    "print(f\"  F1-Score: {voting_f1:.4f}\")\n",
    "\n",
    "# ä¸æœ€ä½³å•æ¨¡å‹å¯¹æ¯”\n",
    "best_single_acc = results[best_single_model]['accuracy']\n",
    "improvement = (voting_acc - best_single_acc) / best_single_acc * 100\n",
    "\n",
    "print(f\"\\n vs æœ€ä½³å•æ¨¡å‹ ({best_single_model}):\")\n",
    "print(f\"  Accuracyæå‡: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Stacking Ensembleï¼ˆå †å é›†æˆï¼‰\n",
    "\n",
    "ä½¿ç”¨å…ƒå­¦ä¹ å™¨æ•´åˆå¤šä¸ªåŸºå­¦ä¹ å™¨çš„é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Stacking Ensembleï¼ˆå †å é›†æˆï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ä½¿ç”¨æ‰€æœ‰5ä¸ªæ¨¡å‹ä½œä¸ºåŸºå­¦ä¹ å™¨\n",
    "stacking_estimators = [\n",
    "    (name, model)\n",
    "    for name, model in models.items()\n",
    "]\n",
    "\n",
    "# å…ƒå­¦ä¹ å™¨ä½¿ç”¨Logistic Regression\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=stacking_estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nè®­ç»ƒStacking Ensemble...\")\n",
    "print(\"  åŸºå­¦ä¹ å™¨: 5ä¸ªæ¨¡å‹\")\n",
    "print(\"  å…ƒå­¦ä¹ å™¨: Logistic Regression\")\n",
    "print(\"  äº¤å‰éªŒè¯: 5æŠ˜\")\n",
    "\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# è¯„ä¼°\n",
    "y_val_pred_stacking = stacking_clf.predict(X_val)\n",
    "stacking_acc = accuracy_score(y_val, y_val_pred_stacking)\n",
    "stacking_f1 = f1_score(y_val, y_val_pred_stacking, average='binary')\n",
    "\n",
    "print(f\"\\néªŒè¯é›†æ€§èƒ½:\")\n",
    "print(f\"  Accuracy: {stacking_acc:.4f}\")\n",
    "print(f\"  F1-Score: {stacking_f1:.4f}\")\n",
    "\n",
    "# å¯¹æ¯”\n",
    "stacking_improvement = (stacking_acc - best_single_acc) / best_single_acc * 100\n",
    "\n",
    "print(f\"\\n vs æœ€ä½³å•æ¨¡å‹ ({best_single_model}):\")\n",
    "print(f\"  Accuracyæå‡: {stacking_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 é›†æˆæ–¹æ³•å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é›†æˆæ–¹æ³•å¯¹æ¯”\n",
    "ensemble_comparison = pd.DataFrame([\n",
    "    {'æ–¹æ³•': 'æœ€ä½³å•æ¨¡å‹', 'æ¨¡å‹åç§°': best_single_model, 'Accuracy': best_single_acc, 'F1-Score': results[best_single_model]['f1']},\n",
    "    {'æ–¹æ³•': 'Voting Ensemble', 'æ¨¡å‹åç§°': 'Top 3 æŠ•ç¥¨', 'Accuracy': voting_acc, 'F1-Score': voting_f1},\n",
    "    {'æ–¹æ³•': 'Stacking Ensemble', 'æ¨¡å‹åç§°': '5æ¨¡å‹å †å ', 'Accuracy': stacking_acc, 'F1-Score': stacking_f1}\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"é›†æˆæ–¹æ³•å¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "print(ensemble_comparison.to_string(index=False))\n",
    "\n",
    "# é€‰æ‹©æœ€ä½³æ–¹æ³•\n",
    "best_method = ensemble_comparison.loc[ensemble_comparison['Accuracy'].idxmax()]\n",
    "print(f\"\\nğŸ† æœ€ä½³æ–¹æ³•: {best_method['æ–¹æ³•']}\")\n",
    "print(f\"   Accuracy: {best_method['Accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {best_method['F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–é›†æˆæ–¹æ³•å¯¹æ¯”\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x_pos = np.arange(len(ensemble_comparison))\n",
    "width = 0.35\n",
    "\n",
    "colors = ['skyblue', 'orange', 'green']\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, ensemble_comparison['Accuracy'], width, label='Accuracy', color=colors, alpha=0.7)\n",
    "bars2 = ax.bar(x_pos + width/2, ensemble_comparison['F1-Score'], width, label='F1-Score', color=colors, alpha=0.4)\n",
    "\n",
    "ax.set_ylabel('å¾—åˆ†')\n",
    "ax.set_title('é›†æˆæ–¹æ³•æ€§èƒ½å¯¹æ¯”')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(ensemble_comparison['æ–¹æ³•'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0.5, 1])\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, (acc, f1) in enumerate(zip(ensemble_comparison['Accuracy'], ensemble_comparison['F1-Score'])):\n",
    "    ax.text(i - width/2, acc + 0.01, f'{acc:.3f}', ha='center', fontsize=9)\n",
    "    ax.text(i + width/2, f1 + 0.01, f'{f1:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°\n",
    "\n",
    "ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰æ‹©æœ€ç»ˆæ¨¡å‹\n",
    "if best_method['æ–¹æ³•'] == 'Voting Ensemble':\n",
    "    final_model = voting_clf\n",
    "elif best_method['æ–¹æ³•'] == 'Stacking Ensemble':\n",
    "    final_model = stacking_clf\n",
    "else:\n",
    "    final_model = results[best_single_model]['model']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼° - {best_method['æ–¹æ³•']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æµ‹è¯•é›†é¢„æµ‹\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "y_test_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# è¯¦ç»†è¯„ä¼°\n",
    "test_metrics = me.evaluate_classification(\n",
    "    y_test,\n",
    "    y_test_pred,\n",
    "    y_test_proba,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··æ·†çŸ©é˜µ\n",
    "me.plot_confusion_matrix(y_test, y_test_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROCæ›²çº¿\n",
    "me.plot_roc_curve(y_test, y_test_proba)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. æœ€ç»ˆæ€§èƒ½æ€»ç»“\n",
    "\n",
    "å¯¹æ¯”æ‰€æœ‰é˜¶æ®µçš„æ€§èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 2 â†’ Phase 6 æ€§èƒ½æ¼”è¿›\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ¨¡æ‹ŸPhase 2-3çš„æ€§èƒ½ï¼ˆå®é™…åº”è¯¥ä»ä¹‹å‰çš„ç»“æœä¸­è·å–ï¼‰\n",
    "# è¿™é‡Œä½¿ç”¨ä¼°è®¡å€¼ä½œä¸ºç¤ºä¾‹\n",
    "phase_performance = pd.DataFrame([\n",
    "    {'Phase': 'Phase 2 åŸºçº¿', 'Accuracy': 0.75, 'F1-Score': 0.70, 'æ–¹æ³•': 'ç®€å•é¢„å¤„ç†+å•æ¨¡å‹'},\n",
    "    {'Phase': 'Phase 3 ä¼˜åŒ–', 'Accuracy': 0.78, 'F1-Score': 0.73, 'æ–¹æ³•': 'é«˜çº§ç‰¹å¾+è°ƒä¼˜'},\n",
    "    {'Phase': 'Phase 4 å¢å¼º', 'Accuracy': 0.79, 'F1-Score': 0.74, 'æ–¹æ³•': '+æ— ç›‘ç£ç‰¹å¾'},\n",
    "    {'Phase': 'Phase 6 æœ€ç»ˆ', 'Accuracy': test_metrics['accuracy'], 'F1-Score': test_metrics['f1'], 'æ–¹æ³•': best_method['æ–¹æ³•']}\n",
    "])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(phase_performance.to_string(index=False))\n",
    "\n",
    "# è®¡ç®—æ€»æå‡\n",
    "total_improvement = (phase_performance.iloc[-1]['Accuracy'] - phase_performance.iloc[0]['Accuracy']) / phase_performance.iloc[0]['Accuracy'] * 100\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Phase 2 â†’ Phase 6 æ€»æå‡:\")\n",
    "print(f\"   Accuracy: {total_improvement:+.2f}%\")\n",
    "print(f\"   ç»å¯¹æå‡: {phase_performance.iloc[-1]['Accuracy'] - phase_performance.iloc[0]['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ€§èƒ½æ¼”è¿›\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "x = np.arange(len(phase_performance))\n",
    "\n",
    "ax.plot(x, phase_performance['Accuracy'], marker='o', linewidth=2, markersize=10, label='Accuracy', color='blue')\n",
    "ax.plot(x, phase_performance['F1-Score'], marker='s', linewidth=2, markersize=10, label='F1-Score', color='orange')\n",
    "\n",
    "ax.set_xlabel('é˜¶æ®µ', fontsize=12)\n",
    "ax.set_ylabel('å¾—åˆ†', fontsize=12)\n",
    "ax.set_title('æ¨¡å‹æ€§èƒ½æ¼”è¿›ï¼šPhase 2 â†’ Phase 6', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(phase_performance['Phase'])\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_ylim([0.6, 1.0])\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, (acc, f1) in enumerate(zip(phase_performance['Accuracy'], phase_performance['F1-Score'])):\n",
    "    ax.annotate(f'{acc:.3f}', xy=(i, acc), xytext=(0, 10), textcoords='offset points', ha='center', fontsize=9)\n",
    "    ax.annotate(f'{f1:.3f}', xy=(i, f1), xytext=(0, -15), textcoords='offset points', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. æ¨¡å‹æŒä¹…åŒ–\n",
    "\n",
    "ä¿å­˜æœ€ä½³æ¨¡å‹å’Œç›¸å…³ç»„ä»¶ï¼Œç”¨äºç”Ÿäº§éƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¿å­˜ç›®å½•\n",
    "models_dir = Path('models/phase6_final')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"æ¨¡å‹æŒä¹…åŒ–\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "model_path = models_dir / 'final_model.pkl'\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"\\nâœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {model_path}\")\n",
    "\n",
    "# ä¿å­˜é¢„å¤„ç†å™¨ï¼ˆç”¨äºæ–°æ•°æ®ï¼‰\n",
    "preprocessing_info = {\n",
    "    'drop_columns': drop_cols,\n",
    "    'numeric_cols': numeric_cols,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'low_card_cols': low_card_cols,\n",
    "    'high_card_cols': high_card_cols,\n",
    "    'feature_names': X_engineered.columns.tolist()\n",
    "}\n",
    "\n",
    "preprocessing_path = models_dir / 'preprocessing_info.pkl'\n",
    "joblib.dump(preprocessing_info, preprocessing_path)\n",
    "print(f\"âœ… é¢„å¤„ç†ä¿¡æ¯å·²ä¿å­˜: {preprocessing_path}\")\n",
    "\n",
    "# ä¿å­˜æ€§èƒ½æŒ‡æ ‡\n",
    "performance_path = models_dir / 'performance_metrics.pkl'\n",
    "joblib.dump(test_metrics, performance_path)\n",
    "print(f\"âœ… æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜: {performance_path}\")\n",
    "\n",
    "# ä¿å­˜å…ƒæ•°æ®\n",
    "metadata = {\n",
    "    'model_type': best_method['æ–¹æ³•'],\n",
    "    'model_name': best_method['æ¨¡å‹åç§°'],\n",
    "    'test_accuracy': test_metrics['accuracy'],\n",
    "    'test_f1': test_metrics['f1'],\n",
    "    'n_features': X_engineered.shape[1],\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "metadata_path = models_dir / 'model_metadata.pkl'\n",
    "joblib.dump(metadata, metadata_path)\n",
    "print(f\"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}\")\n",
    "\n",
    "print(f\"\\næ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {models_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. ç”Ÿäº§çº§é¢„æµ‹Pipeline\n",
    "\n",
    "å°è£…ä¸ºæ˜“ç”¨çš„é¢„æµ‹ç±»ï¼Œç”¨äºéƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionPredictor:\n",
    "    \"\"\"\n",
    "    ç”Ÿäº§çº§é¢„æµ‹Pipeline\n",
    "    \n",
    "    ç”¨æ³•:\n",
    "        predictor = ProductionPredictor('models/phase6_final')\n",
    "        predictions = predictor.predict(new_data)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        \"\"\"\n",
    "        åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†ä¿¡æ¯\n",
    "        \n",
    "        Args:\n",
    "            model_dir: æ¨¡å‹ç›®å½•è·¯å¾„\n",
    "        \"\"\"\n",
    "        model_dir = Path(model_dir)\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹\n",
    "        self.model = joblib.load(model_dir / 'final_model.pkl')\n",
    "        \n",
    "        # åŠ è½½é¢„å¤„ç†ä¿¡æ¯\n",
    "        self.preprocessing_info = joblib.load(model_dir / 'preprocessing_info.pkl')\n",
    "        \n",
    "        # åŠ è½½å…ƒæ•°æ®\n",
    "        self.metadata = joblib.load(model_dir / 'model_metadata.pkl')\n",
    "        \n",
    "        print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "        print(f\"   æ¨¡å‹ç±»å‹: {self.metadata['model_type']}\")\n",
    "        print(f\"   æµ‹è¯•é›†Accuracy: {self.metadata['test_accuracy']:.4f}\")\n",
    "        print(f\"   è®­ç»ƒæ—¥æœŸ: {self.metadata['training_date']}\")\n",
    "    \n",
    "    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        é¢„å¤„ç†æ–°æ•°æ®\n",
    "        \n",
    "        Args:\n",
    "            df: åŸå§‹æ•°æ®DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            é¢„å¤„ç†åçš„DataFrame\n",
    "        \"\"\"\n",
    "        X = df.copy()\n",
    "        \n",
    "        # åˆ é™¤åˆ—\n",
    "        drop_cols = [c for c in self.preprocessing_info['drop_columns'] if c in X.columns]\n",
    "        if drop_cols:\n",
    "            X = X.drop(columns=drop_cols)\n",
    "        \n",
    "        # å¡«å……ç¼ºå¤±å€¼ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ä¿å­˜å¡«å……å™¨ï¼‰\n",
    "        for col in self.preprocessing_info['numeric_cols']:\n",
    "            if col in X.columns and X[col].isnull().sum() > 0:\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "        \n",
    "        for col in self.preprocessing_info['categorical_cols']:\n",
    "            if col in X.columns and X[col].isnull().sum() > 0:\n",
    "                mode_val = X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown'\n",
    "                X[col] = X[col].fillna(mode_val)\n",
    "        \n",
    "        # ç¼–ç ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "        if self.preprocessing_info['low_card_cols']:\n",
    "            X = pd.get_dummies(X, columns=self.preprocessing_info['low_card_cols'], drop_first=True)\n",
    "        \n",
    "        # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´\n",
    "        for col in self.preprocessing_info['feature_names']:\n",
    "            if col not in X.columns:\n",
    "                X[col] = 0\n",
    "        \n",
    "        X = X[self.preprocessing_info['feature_names']]\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def predict(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        é¢„æµ‹\n",
    "        \n",
    "        Args:\n",
    "            df: åŸå§‹æ•°æ®DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            é¢„æµ‹ç»“æœæ•°ç»„\n",
    "        \"\"\"\n",
    "        X = self.preprocess(df)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        é¢„æµ‹æ¦‚ç‡\n",
    "        \n",
    "        Args:\n",
    "            df: åŸå§‹æ•°æ®DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            é¢„æµ‹æ¦‚ç‡æ•°ç»„\n",
    "        \"\"\"\n",
    "        X = self.preprocess(df)\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "print(\"âœ… ProductionPredictor ç±»å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ç”Ÿäº§çº§é¢„æµ‹å™¨\n",
    "print(\"=\"*60)\n",
    "print(\"æµ‹è¯•ç”Ÿäº§çº§é¢„æµ‹å™¨\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åˆ›å»ºé¢„æµ‹å™¨\n",
    "predictor = ProductionPredictor('models/phase6_final')\n",
    "\n",
    "# ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼ˆæ¨¡æ‹Ÿæ–°æ•°æ®ï¼‰\n",
    "test_sample = df.iloc[:5].copy()  # å–5ä¸ªæ ·æœ¬\n",
    "\n",
    "print(f\"\\né¢„æµ‹ {len(test_sample)} ä¸ªæ ·æœ¬...\")\n",
    "predictions = predictor.predict(test_sample)\n",
    "probabilities = predictor.predict_proba(test_sample)\n",
    "\n",
    "# æ˜¾ç¤ºç»“æœ\n",
    "results_df = test_sample.copy()\n",
    "results_df['predicted'] = predictions\n",
    "results_df['probability'] = probabilities[:, 1]\n",
    "\n",
    "print(\"\\né¢„æµ‹ç»“æœ:\")\n",
    "print(results_df[['sex', 'age', 'class', 'survived', 'predicted', 'probability']])\n",
    "\n",
    "print(\"\\nâœ… ç”Ÿäº§çº§é¢„æµ‹å™¨æµ‹è¯•æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Phase 6 å®Œæˆæ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ Phase 6: Final Solution å®Œæˆæ€»ç»“\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“Š æœ€ç»ˆè§£å†³æ–¹æ¡ˆç‰¹æ€§:\")\n",
    "\n",
    "features = [\n",
    "    f\"âœ… æœ€ä¼˜é¢„å¤„ç†: KNNå¡«å…… + æ™ºèƒ½ç¼–ç  + æ ‡å‡†åŒ–\",\n",
    "    f\"âœ… ç‰¹å¾å·¥ç¨‹: {X_engineered.shape[1]} ä¸ªç‰¹å¾ï¼ˆå«ç›‘ç£+æ— ç›‘ç£ï¼‰\",\n",
    "    f\"âœ… æ¨¡å‹é›†æˆ: {best_method['æ–¹æ³•']}\",\n",
    "    f\"âœ… æµ‹è¯•é›†æ€§èƒ½: Accuracy={test_metrics['accuracy']:.4f}, F1={test_metrics['f1']:.4f}\",\n",
    "    f\"âœ… æ€§èƒ½æå‡: vs Phase 2 {total_improvement:+.1f}%\",\n",
    "    f\"âœ… æ¨¡å‹å·²ä¿å­˜: {models_dir.absolute()}\",\n",
    "    f\"âœ… ç”Ÿäº§çº§Pipeline: ProductionPredictorç±»\"\n",
    "]\n",
    "\n",
    "for feature in features:\n",
    "    print(f\"  {feature}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®æŠ€æœ¯æ ˆ:\")\n",
    "tech_stack = [\n",
    "    \"æ•°æ®è¯Šæ–­: è´¨é‡è¯„åˆ†ã€ç¼ºå¤±æ¨¡å¼æ£€æµ‹\",\n",
    "    \"é¢„å¤„ç†: KNNå¡«å……ã€é¢‘ç‡ç¼–ç ã€æ ‡å‡†åŒ–\",\n",
    "    \"ç‰¹å¾å·¥ç¨‹: äº¤äº’ç‰¹å¾ã€èšç±»ç‰¹å¾ã€PCAç‰¹å¾ã€å¼‚å¸¸åˆ†æ•°\",\n",
    "    \"æ¨¡å‹: LogisticRegression, RandomForest, XGBoost, LightGBM, GradientBoosting\",\n",
    "    \"é›†æˆ: Voting Ensemble, Stacking Ensemble\",\n",
    "    \"è¯„ä¼°: äº¤å‰éªŒè¯ã€æ··æ·†çŸ©é˜µã€ROCæ›²çº¿\",\n",
    "    \"éƒ¨ç½²: æ¨¡å‹æŒä¹…åŒ–ã€ç”Ÿäº§çº§é¢„æµ‹å™¨\"\n",
    "]\n",
    "\n",
    "for i, tech in enumerate(tech_stack, 1):\n",
    "    print(f\"  {i}. {tech}\")\n",
    "\n",
    "print(\"\\nğŸ“ äº¤ä»˜ç‰©:\")\n",
    "deliverables = [\n",
    "    f\"1. æœ€ç»ˆæ¨¡å‹: {model_path}\",\n",
    "    f\"2. é¢„å¤„ç†ä¿¡æ¯: {preprocessing_path}\",\n",
    "    f\"3. æ€§èƒ½æŒ‡æ ‡: {performance_path}\",\n",
    "    f\"4. å…ƒæ•°æ®: {metadata_path}\",\n",
    "    \"5. ç”Ÿäº§çº§é¢„æµ‹ç±»: ProductionPredictor\",\n",
    "    \"6. å®Œæ•´çš„Jupyter Notebookæ–‡æ¡£\"\n",
    "]\n",
    "\n",
    "for deliverable in deliverables:\n",
    "    print(f\"  {deliverable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 ä½¿ç”¨è¯´æ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“– ç”Ÿäº§ç¯å¢ƒä½¿ç”¨è¯´æ˜\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "usage_guide = '''\n",
    "# æ­¥éª¤1: åŠ è½½é¢„æµ‹å™¨\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# ç¡®ä¿ProductionPredictorç±»å·²å®šä¹‰ï¼ˆæˆ–ä»æ¨¡å—å¯¼å…¥ï¼‰\n",
    "predictor = ProductionPredictor('models/phase6_final')\n",
    "\n",
    "# æ­¥éª¤2: å‡†å¤‡æ–°æ•°æ®\n",
    "new_data = pd.DataFrame({\n",
    "    'sex': ['male', 'female'],\n",
    "    'age': [25, 30],\n",
    "    'class': ['First', 'Third'],\n",
    "    # ... å…¶ä»–ç‰¹å¾\n",
    "})\n",
    "\n",
    "# æ­¥éª¤3: é¢„æµ‹\n",
    "predictions = predictor.predict(new_data)\n",
    "probabilities = predictor.predict_proba(new_data)\n",
    "\n",
    "# æ­¥éª¤4: ä½¿ç”¨ç»“æœ\n",
    "print(f\"é¢„æµ‹ç»“æœ: {predictions}\")\n",
    "print(f\"ç”Ÿå­˜æ¦‚ç‡: {probabilities[:, 1]}\")\n",
    "\n",
    "# æ³¨æ„äº‹é¡¹:\n",
    "# 1. æ–°æ•°æ®å¿…é¡»åŒ…å«æ‰€æœ‰åŸå§‹ç‰¹å¾\n",
    "# 2. ç‰¹å¾åç§°å’Œç±»å‹å¿…é¡»ä¸è®­ç»ƒæ•°æ®ä¸€è‡´\n",
    "# 3. ç¼ºå¤±å€¼ä¼šè¢«è‡ªåŠ¨å¡«å……\n",
    "# 4. ä¸éœ€è¦æ‰‹åŠ¨è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œé¢„æµ‹å™¨ä¼šè‡ªåŠ¨å¤„ç†\n",
    "'''\n",
    "\n",
    "print(usage_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 Phase 6 æ£€æŸ¥æ¸…å•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Phase 6 å®Œæˆæ£€æŸ¥æ¸…å•\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checklist = [\n",
    "    \"âœ… Phase 1-4å…³é”®å‘ç°å›é¡¾å®Œæˆ\",\n",
    "    \"âœ… æœ€ä¼˜é¢„å¤„ç†Pipelineæ„å»ºå®Œæˆ\",\n",
    "    \"âœ… æœ€ä¼˜ç‰¹å¾å·¥ç¨‹å®Œæˆï¼ˆç›‘ç£+æ— ç›‘ç£ï¼‰\",\n",
    "    \"âœ… 5ç§å•æ¨¡å‹è®­ç»ƒå’Œå¯¹æ¯”å®Œæˆ\",\n",
    "    \"âœ… Voting Ensembleæ„å»ºå®Œæˆ\",\n",
    "    \"âœ… Stacking Ensembleæ„å»ºå®Œæˆ\",\n",
    "    \"âœ… é›†æˆæ–¹æ³•å¯¹æ¯”å®Œæˆ\",\n",
    "    \"âœ… æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°å®Œæˆ\",\n",
    "    \"âœ… æ€§èƒ½æ¼”è¿›å¯è§†åŒ–å®Œæˆ\",\n",
    "    \"âœ… æ¨¡å‹æŒä¹…åŒ–å®Œæˆ\",\n",
    "    \"âœ… ç”Ÿäº§çº§é¢„æµ‹å™¨å°è£…å®Œæˆ\",\n",
    "    \"âœ… ä½¿ç”¨è¯´æ˜æ–‡æ¡£å®Œæˆ\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸŠ æ­å–œï¼æ•´ä¸ªMLé¡¹ç›®å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâœ¨ ä½ å·²ç»å®Œæˆäº†ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼š\")\n",
    "print(\"   ä»æ•°æ®è¯Šæ–­ â†’ å¿«é€ŸåŸºçº¿ â†’ ç›‘ç£ä¼˜åŒ– â†’ æ— ç›‘ç£æ´å¯Ÿ â†’ æœ€ç»ˆæ–¹æ¡ˆ\")\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:\")\n",
    "print(\"   1. åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šåº”ç”¨è¿™å¥—å·¥ä½œæµ\")\n",
    "print(\"   2. å°†æ¨¡å‹éƒ¨ç½²åˆ°WebæœåŠ¡æˆ–API\")\n",
    "print(\"   3. æŒç»­ç›‘æ§æ¨¡å‹æ€§èƒ½å¹¶è¿­ä»£ä¼˜åŒ–\")\n",
    "print(\"   4. æ¢ç´¢æ›´å¤šé«˜çº§æŠ€æœ¯ï¼ˆæ·±åº¦å­¦ä¹ ã€AutoMLç­‰ï¼‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
