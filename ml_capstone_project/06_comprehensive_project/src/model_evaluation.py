"""
æ¨¡å‹è¯„ä¼°æ¨¡å—
============

æä¾›å…¨é¢çš„æ¨¡å‹è¯„ä¼°åŠŸèƒ½ï¼Œå¸®åŠ©é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚

ä¸»è¦åŠŸèƒ½:
- åˆ†ç±»æ¨¡å‹è¯„ä¼°ï¼ˆAccuracyã€Precisionã€Recallã€F1ã€ROC-AUCï¼‰
- å›å½’æ¨¡å‹è¯„ä¼°ï¼ˆMAEã€MSEã€RMSEã€RÂ²ã€MAPEï¼‰
- èšç±»æ¨¡å‹è¯„ä¼°ï¼ˆSilhouetteã€Calinski-Harabaszã€Davies-Bouldinï¼‰
- æ··æ·†çŸ©é˜µå¯è§†åŒ–
- ROCæ›²çº¿å’ŒPRæ›²çº¿
- å­¦ä¹ æ›²çº¿
- ç‰¹å¾é‡è¦æ€§åˆ†æ
- æ¨¡å‹å¯¹æ¯”

è¯„ä¼°æŒ‡æ ‡é€‰æ‹©åŸåˆ™:
- åˆ†ç±»: ä¸å¹³è¡¡æ•°æ®çœ‹F1å’ŒAUCï¼Œå¹³è¡¡æ•°æ®çœ‹Accuracy
- å›å½’: RMSEçœ‹ç»å¯¹è¯¯å·®ï¼ŒRÂ²çœ‹æ‹Ÿåˆç¨‹åº¦ï¼ŒMAPEçœ‹ç›¸å¯¹è¯¯å·®
- èšç±»: Silhouetteçœ‹ç°‡å†…ç´§å¯†åº¦ï¼ŒCH Indexçœ‹ç°‡é—´åˆ†ç¦»åº¦
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any, Optional, Union
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, roc_auc_score, roc_curve,
                            confusion_matrix, classification_report,
                            mean_absolute_error, mean_squared_error, r2_score,
                            silhouette_score, calinski_harabasz_score,
                            davies_bouldin_score, log_loss)
from sklearn.model_selection import learning_curve, cross_val_score
import warnings
warnings.filterwarnings('ignore')


# ==================== åˆ†ç±»æ¨¡å‹è¯„ä¼° ====================

def evaluate_classification(y_true: np.ndarray, y_pred: np.ndarray,
                           y_pred_proba: Optional[np.ndarray] = None,
                           average: str = 'binary',
                           verbose: bool = True) -> Dict[str, float]:
    """
    è¯„ä¼°åˆ†ç±»æ¨¡å‹

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        y_pred_proba: é¢„æµ‹æ¦‚ç‡ï¼ˆç”¨äºè®¡ç®—AUCï¼‰
        average: å¤šåˆ†ç±»å¹³å‡æ–¹å¼ ('binary', 'micro', 'macro', 'weighted')
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æŠ¥å‘Š

    Returns:
        åŒ…å«å„è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {}

    # åŸºç¡€æŒ‡æ ‡
    metrics['accuracy'] = accuracy_score(y_true, y_pred)
    metrics['precision'] = precision_score(y_true, y_pred, average=average, zero_division=0)
    metrics['recall'] = recall_score(y_true, y_pred, average=average, zero_division=0)
    metrics['f1'] = f1_score(y_true, y_pred, average=average, zero_division=0)

    # AUCï¼ˆå¦‚æœæä¾›äº†é¢„æµ‹æ¦‚ç‡ï¼‰
    if y_pred_proba is not None:
        try:
            if len(np.unique(y_true)) == 2:  # äºŒåˆ†ç±»
                metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)
            else:  # å¤šåˆ†ç±»
                metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba,
                                                  multi_class='ovr', average=average)
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è®¡ç®—ROC-AUC: {e}")

    # Log Loss
    if y_pred_proba is not None:
        try:
            metrics['log_loss'] = log_loss(y_true, y_pred_proba)
        except Exception:
            pass

    if verbose:
        print("\n" + "=" * 50)
        print("ğŸ“Š åˆ†ç±»æ¨¡å‹è¯„ä¼°ç»“æœ")
        print("=" * 50)
        for metric_name, metric_value in metrics.items():
            print(f"{metric_name.upper():<15}: {metric_value:.4f}")
        print("=" * 50 + "\n")

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred, zero_division=0))

    return metrics


def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray,
                         labels: Optional[List[str]] = None,
                         normalize: bool = False,
                         figsize: Tuple[int, int] = (8, 6)):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µ

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        labels: ç±»åˆ«æ ‡ç­¾åç§°
        normalize: æ˜¯å¦å½’ä¸€åŒ–ï¼ˆæ˜¾ç¤ºç™¾åˆ†æ¯”ï¼‰
        figsize: å›¾åƒå¤§å°
    """
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    # ç»˜å›¾
    plt.figure(figsize=figsize)
    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd',
                cmap='Blues', square=True,
                xticklabels=labels, yticklabels=labels)
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.title('æ··æ·†çŸ©é˜µ' + (' (å½’ä¸€åŒ–)' if normalize else ''))
    plt.tight_layout()
    plt.show()


def plot_roc_curve(y_true: np.ndarray, y_pred_proba: np.ndarray,
                  labels: Optional[List[str]] = None,
                  figsize: Tuple[int, int] = (8, 6)):
    """
    ç»˜åˆ¶ROCæ›²çº¿

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred_proba: é¢„æµ‹æ¦‚ç‡
        labels: ç±»åˆ«æ ‡ç­¾ï¼ˆå¤šåˆ†ç±»æ—¶ä½¿ç”¨ï¼‰
        figsize: å›¾åƒå¤§å°
    """
    plt.figure(figsize=figsize)

    n_classes = len(np.unique(y_true))

    if n_classes == 2:
        # äºŒåˆ†ç±»ROCæ›²çº¿
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
        auc = roc_auc_score(y_true, y_pred_proba)

        plt.plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {auc:.3f})')

    else:
        # å¤šåˆ†ç±»ROCæ›²çº¿ï¼ˆä¸€å¯¹å¤šï¼‰
        from sklearn.preprocessing import label_binarize

        y_true_bin = label_binarize(y_true, classes=np.unique(y_true))

        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])
            auc = roc_auc_score(y_true_bin[:, i], y_pred_proba[:, i])
            label_name = labels[i] if labels else f'Class {i}'
            plt.plot(fpr, tpr, linewidth=2, label=f'{label_name} (AUC = {auc:.3f})')

    # ç»˜åˆ¶å¯¹è§’çº¿
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡æ­£ç‡ (False Positive Rate)')
    plt.ylabel('çœŸæ­£ç‡ (True Positive Rate)')
    plt.title('ROCæ›²çº¿')
    plt.legend(loc='lower right')
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()


# ==================== å›å½’æ¨¡å‹è¯„ä¼° ====================

def evaluate_regression(y_true: np.ndarray, y_pred: np.ndarray,
                       verbose: bool = True) -> Dict[str, float]:
    """
    è¯„ä¼°å›å½’æ¨¡å‹

    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æŠ¥å‘Š

    Returns:
        åŒ…å«å„è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {}

    # åŸºç¡€æŒ‡æ ‡
    metrics['mae'] = mean_absolute_error(y_true, y_pred)
    metrics['mse'] = mean_squared_error(y_true, y_pred)
    metrics['rmse'] = np.sqrt(metrics['mse'])
    metrics['r2'] = r2_score(y_true, y_pred)

    # MAPEï¼ˆå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼‰
    # é¿å…é™¤ä»¥0
    mask = y_true != 0
    if mask.sum() > 0:
        metrics['mape'] = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        metrics['mape'] = np.inf

    # è°ƒæ•´RÂ²ï¼ˆæƒ©ç½šè¿‡å¤šç‰¹å¾ï¼‰
    n = len(y_true)
    p = 1  # å¦‚æœçŸ¥é“ç‰¹å¾æ•°ï¼Œå¯ä»¥ä¼ å…¥
    if n > p + 1:
        metrics['adj_r2'] = 1 - (1 - metrics['r2']) * (n - 1) / (n - p - 1)

    if verbose:
        print("\n" + "=" * 50)
        print("ğŸ“Š å›å½’æ¨¡å‹è¯„ä¼°ç»“æœ")
        print("=" * 50)
        print(f"{'MAE (å¹³å‡ç»å¯¹è¯¯å·®)':<25}: {metrics['mae']:.4f}")
        print(f"{'MSE (å‡æ–¹è¯¯å·®)':<25}: {metrics['mse']:.4f}")
        print(f"{'RMSE (å‡æ–¹æ ¹è¯¯å·®)':<25}: {metrics['rmse']:.4f}")
        print(f"{'RÂ² (å†³å®šç³»æ•°)':<25}: {metrics['r2']:.4f}")
        if 'mape' in metrics and metrics['mape'] != np.inf:
            print(f"{'MAPE (å¹³å‡ç™¾åˆ†æ¯”è¯¯å·®)':<25}: {metrics['mape']:.2f}%")
        print("=" * 50 + "\n")

        # RÂ²è§£é‡Š
        if metrics['r2'] > 0.9:
            print("âœ… RÂ² > 0.9: æ¨¡å‹æ‹Ÿåˆéå¸¸å¥½")
        elif metrics['r2'] > 0.7:
            print("âœ“  RÂ² > 0.7: æ¨¡å‹æ‹Ÿåˆè¾ƒå¥½")
        elif metrics['r2'] > 0.5:
            print("âš ï¸  RÂ² > 0.5: æ¨¡å‹æ‹Ÿåˆä¸€èˆ¬")
        else:
            print("âŒ RÂ² < 0.5: æ¨¡å‹æ‹Ÿåˆè¾ƒå·®")

    return metrics


def plot_regression_results(y_true: np.ndarray, y_pred: np.ndarray,
                           figsize: Tuple[int, int] = (12, 5)):
    """
    å¯è§†åŒ–å›å½’ç»“æœ

    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        figsize: å›¾åƒå¤§å°
    """
    fig, axes = plt.subplots(1, 2, figsize=figsize)

    # å·¦å›¾ï¼šé¢„æµ‹å€¼ vs çœŸå®å€¼
    axes[0].scatter(y_true, y_pred, alpha=0.5, edgecolors='k', s=50)
    axes[0].plot([y_true.min(), y_true.max()],
                [y_true.min(), y_true.max()],
                'r--', linewidth=2, label='Perfect Prediction')

    axes[0].set_xlabel('çœŸå®å€¼')
    axes[0].set_ylabel('é¢„æµ‹å€¼')
    axes[0].set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼')
    axes[0].legend()
    axes[0].grid(alpha=0.3)

    # å³å›¾ï¼šæ®‹å·®åˆ†å¸ƒ
    residuals = y_true - y_pred
    axes[1].scatter(y_pred, residuals, alpha=0.5, edgecolors='k', s=50)
    axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)

    axes[1].set_xlabel('é¢„æµ‹å€¼')
    axes[1].set_ylabel('æ®‹å·®')
    axes[1].set_title('æ®‹å·®åˆ†å¸ƒå›¾')
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.show()


# ==================== èšç±»æ¨¡å‹è¯„ä¼° ====================

def evaluate_clustering(X: np.ndarray, labels: np.ndarray,
                       verbose: bool = True) -> Dict[str, float]:
    """
    è¯„ä¼°èšç±»æ¨¡å‹

    Args:
        X: ç‰¹å¾æ•°æ®
        labels: èšç±»æ ‡ç­¾
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æŠ¥å‘Š

    Returns:
        åŒ…å«å„è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {}

    # èšç±»æŒ‡æ ‡ï¼ˆä¸éœ€è¦çœŸå®æ ‡ç­¾ï¼‰
    if len(np.unique(labels)) > 1:  # è‡³å°‘æœ‰2ä¸ªç°‡
        metrics['silhouette'] = silhouette_score(X, labels)
        metrics['calinski_harabasz'] = calinski_harabasz_score(X, labels)
        metrics['davies_bouldin'] = davies_bouldin_score(X, labels)

    if verbose:
        print("\n" + "=" * 50)
        print("ğŸ“Š èšç±»æ¨¡å‹è¯„ä¼°ç»“æœ")
        print("=" * 50)
        print(f"ç°‡æ•°é‡: {len(np.unique(labels))}")
        print(f"æ ·æœ¬æ•°: {len(labels)}")
        print()

        if 'silhouette' in metrics:
            print(f"{'Silhouette Score':<30}: {metrics['silhouette']:.4f}")
            print("  èŒƒå›´: [-1, 1]ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½")
            print("  > 0.7: å¼ºèšç±»ç»“æ„")
            print("  0.5-0.7: åˆç†èšç±»ç»“æ„")
            print("  < 0.5: èšç±»ç»“æ„è¾ƒå¼±")
            print()

        if 'calinski_harabasz' in metrics:
            print(f"{'Calinski-Harabasz Index':<30}: {metrics['calinski_harabasz']:.4f}")
            print("  å€¼è¶Šå¤§è¶Šå¥½ï¼ˆç°‡é—´åˆ†ç¦»åº¦é«˜ï¼Œç°‡å†…ç´§å¯†åº¦é«˜ï¼‰")
            print()

        if 'davies_bouldin' in metrics:
            print(f"{'Davies-Bouldin Index':<30}: {metrics['davies_bouldin']:.4f}")
            print("  å€¼è¶Šå°è¶Šå¥½ï¼ˆç°‡é—´åˆ†ç¦»åº¦é«˜ï¼‰")

        print("=" * 50 + "\n")

    return metrics


# ==================== å­¦ä¹ æ›²çº¿ ====================

def plot_learning_curve(estimator, X, y, cv=5, scoring='accuracy',
                       figsize: Tuple[int, int] = (10, 6)):
    """
    ç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼ˆè¯Šæ–­è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆï¼‰

    Args:
        estimator: æ¨¡å‹
        X: ç‰¹å¾æ•°æ®
        y: ç›®æ ‡å˜é‡
        cv: äº¤å‰éªŒè¯æŠ˜æ•°
        scoring: è¯„åˆ†æŒ‡æ ‡
        figsize: å›¾åƒå¤§å°
    """
    train_sizes, train_scores, val_scores = learning_curve(
        estimator, X, y, cv=cv, scoring=scoring,
        train_sizes=np.linspace(0.1, 1.0, 10),
        n_jobs=-1
    )

    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)

    # ç»˜å›¾
    plt.figure(figsize=figsize)

    # è®­ç»ƒé›†å¾—åˆ†
    plt.plot(train_sizes, train_mean, 'o-', color='r', label='è®­ç»ƒé›†å¾—åˆ†')
    plt.fill_between(train_sizes, train_mean - train_std,
                    train_mean + train_std, alpha=0.1, color='r')

    # éªŒè¯é›†å¾—åˆ†
    plt.plot(train_sizes, val_mean, 'o-', color='g', label='éªŒè¯é›†å¾—åˆ†')
    plt.fill_between(train_sizes, val_mean - val_std,
                    val_mean + val_std, alpha=0.1, color='g')

    plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
    plt.ylabel(f'{scoring.upper()}')
    plt.title('å­¦ä¹ æ›²çº¿')
    plt.legend(loc='best')
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

    # è¯Šæ–­å»ºè®®
    final_train_score = train_mean[-1]
    final_val_score = val_mean[-1]
    gap = final_train_score - final_val_score

    print("\nå­¦ä¹ æ›²çº¿è¯Šæ–­:")
    if gap > 0.1:
        print("âš ï¸  è¿‡æ‹Ÿåˆ: è®­ç»ƒé›†å¾—åˆ†æ˜¾è‘—é«˜äºéªŒè¯é›†")
        print("   å»ºè®®: å¢åŠ æ•°æ®é‡ã€æ­£åˆ™åŒ–ã€å‡å°‘æ¨¡å‹å¤æ‚åº¦")
    elif final_val_score < 0.7:
        print("âš ï¸  æ¬ æ‹Ÿåˆ: éªŒè¯é›†å¾—åˆ†è¾ƒä½")
        print("   å»ºè®®: å¢åŠ æ¨¡å‹å¤æ‚åº¦ã€å¢åŠ ç‰¹å¾ã€å‡å°‘æ­£åˆ™åŒ–")
    else:
        print("âœ… æ¨¡å‹æ‹Ÿåˆè‰¯å¥½")


# ==================== ç‰¹å¾é‡è¦æ€§ ====================

def plot_feature_importance(model, feature_names: List[str],
                          top_n: int = 20,
                          figsize: Tuple[int, int] = (10, 8)):
    """
    å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆéœ€æ”¯æŒfeature_importances_ï¼‰
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        top_n: æ˜¾ç¤ºå‰Nä¸ªé‡è¦ç‰¹å¾
        figsize: å›¾åƒå¤§å°
    """
    if not hasattr(model, 'feature_importances_'):
        print("âš ï¸  æ¨¡å‹ä¸æ”¯æŒfeature_importances_å±æ€§")
        return

    # è·å–ç‰¹å¾é‡è¦æ€§
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]

    # ç»˜å›¾
    plt.figure(figsize=figsize)
    plt.barh(range(top_n), importances[indices], color='steelblue')
    plt.yticks(range(top_n), [feature_names[i] for i in indices])
    plt.xlabel('é‡è¦æ€§')
    plt.title(f'Top {top_n} ç‰¹å¾é‡è¦æ€§')
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.show()


# ==================== æ¨¡å‹å¯¹æ¯” ====================

def compare_models(results: Dict[str, Dict[str, float]],
                  metric: str = 'accuracy',
                  figsize: Tuple[int, int] = (10, 6)):
    """
    å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½

    Args:
        results: æ¨¡å‹ç»“æœå­—å…¸ï¼Œæ ¼å¼ä¸º {model_name: {metric: value}}
        metric: è¦å¯¹æ¯”çš„æŒ‡æ ‡
        figsize: å›¾åƒå¤§å°
    """
    # æå–æ•°æ®
    model_names = list(results.keys())
    metric_values = [results[model][metric] for model in model_names]

    # åˆ›å»ºDataFrame
    df = pd.DataFrame({
        'æ¨¡å‹': model_names,
        metric: metric_values
    }).sort_values(metric, ascending=False)

    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼ˆæŒ‰ {metric.upper()} æ’åºï¼‰")
    print("=" * 60)
    print(df.to_string(index=False))
    print("=" * 60 + "\n")

    # ç»˜å›¾
    plt.figure(figsize=figsize)
    colors = ['gold' if i == 0 else 'steelblue' for i in range(len(df))]
    plt.barh(df['æ¨¡å‹'], df[metric], color=colors)

    # åœ¨æŸ±å­ä¸Šæ ‡æ³¨æ•°å€¼
    for i, v in enumerate(df[metric]):
        plt.text(v + 0.01, i, f'{v:.4f}', va='center')

    plt.xlabel(metric.upper())
    plt.title(f'æ¨¡å‹{metric.upper()}å¯¹æ¯”')
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.show()


# ==================== äº¤å‰éªŒè¯è¯„ä¼° ====================

def cross_validate_model(model, X, y, cv: int = 5,
                        scoring: str = 'accuracy',
                        verbose: bool = True) -> Dict[str, Any]:
    """
    äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹

    Args:
        model: æ¨¡å‹
        X: ç‰¹å¾æ•°æ®
        y: ç›®æ ‡å˜é‡
        cv: äº¤å‰éªŒè¯æŠ˜æ•°
        scoring: è¯„åˆ†æŒ‡æ ‡
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)

    results = {
        'scores': scores,
        'mean': scores.mean(),
        'std': scores.std(),
        'min': scores.min(),
        'max': scores.max()
    }

    if verbose:
        print(f"\n{cv}æŠ˜äº¤å‰éªŒè¯ç»“æœ ({scoring}):")
        print(f"  å„æŠ˜å¾—åˆ†: {[f'{s:.4f}' for s in scores]}")
        print(f"  å¹³å‡å¾—åˆ†: {results['mean']:.4f} (+/- {results['std']*2:.4f})")
        print(f"  å¾—åˆ†èŒƒå›´: [{results['min']:.4f}, {results['max']:.4f}]")

    return results


if __name__ == '__main__':
    # æµ‹è¯•ç¤ºä¾‹
    print("=== æ¨¡å‹è¯„ä¼°æ¨¡å—æµ‹è¯• ===\n")

    # æ¨¡æ‹Ÿåˆ†ç±»æ•°æ®
    np.random.seed(42)
    y_true = np.random.choice([0, 1], 1000, p=[0.7, 0.3])
    y_pred = y_true.copy()
    # æ·»åŠ ä¸€äº›é”™è¯¯
    error_indices = np.random.choice(1000, 100, replace=False)
    y_pred[error_indices] = 1 - y_pred[error_indices]
    y_pred_proba = np.random.rand(1000)

    # æµ‹è¯•åˆ†ç±»è¯„ä¼°
    print("1. åˆ†ç±»æ¨¡å‹è¯„ä¼°")
    metrics = evaluate_classification(y_true, y_pred, y_pred_proba)

    # æµ‹è¯•å›å½’è¯„ä¼°
    print("\n2. å›å½’æ¨¡å‹è¯„ä¼°")
    y_true_reg = np.random.randn(1000) * 10 + 50
    y_pred_reg = y_true_reg + np.random.randn(1000) * 2
    metrics_reg = evaluate_regression(y_true_reg, y_pred_reg)

    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
