"""
æ•°æ®è¯Šæ–­æ¨¡å—
============

æä¾›å…¨é¢çš„æ•°æ®è´¨é‡è¯Šæ–­åŠŸèƒ½ï¼Œå¸®åŠ©å¿«é€Ÿäº†è§£æ•°æ®ç‰¹å¾å’Œæ½œåœ¨é—®é¢˜ã€‚

ä¸»è¦åŠŸèƒ½:
- åŸºç¡€ç»Ÿè®¡ä¿¡æ¯åˆ†æ
- ç¼ºå¤±å€¼æ£€æµ‹ä¸å¯è§†åŒ–
- å¼‚å¸¸å€¼è¯†åˆ«
- æ•°æ®åˆ†å¸ƒåˆ†æ
- ç‰¹å¾ç›¸å…³æ€§åˆ†æ
- æ•°æ®ç±»å‹æ¨æ–­
- æ•°æ®è´¨é‡æŠ¥å‘Šç”Ÿæˆ

è¿™æ˜¯æœºå™¨å­¦ä¹ é¡¹ç›®çš„ç¬¬ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æœ€å…³é”®çš„ä¸€æ­¥ã€‚
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any, Optional
from scipy import stats
from collections import Counter
import warnings
warnings.filterwarnings('ignore')


# ==================== åŸºç¡€ç»Ÿè®¡åˆ†æ ====================

def basic_info(df: pd.DataFrame, show: bool = True) -> Dict[str, Any]:
    """
    è·å–æ•°æ®é›†çš„åŸºç¡€ä¿¡æ¯

    Args:
        df: æ•°æ®DataFrame
        show: æ˜¯å¦æ‰“å°ä¿¡æ¯

    Returns:
        åŒ…å«åŸºç¡€ä¿¡æ¯çš„å­—å…¸
    """
    info_dict = {
        'n_samples': len(df),
        'n_features': len(df.columns),
        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,
        'duplicated_rows': df.duplicated().sum(),
        'column_types': df.dtypes.value_counts().to_dict()
    }

    if show:
        print("=" * 60)
        print("ğŸ“Š æ•°æ®é›†åŸºç¡€ä¿¡æ¯")
        print("=" * 60)
        print(f"æ ·æœ¬æ•°é‡: {info_dict['n_samples']:,}")
        print(f"ç‰¹å¾æ•°é‡: {info_dict['n_features']}")
        print(f"å†…å­˜å ç”¨: {info_dict['memory_usage_mb']:.2f} MB")
        print(f"é‡å¤è¡Œæ•°: {info_dict['duplicated_rows']}")
        print(f"\næ•°æ®ç±»å‹åˆ†å¸ƒ:")
        for dtype, count in info_dict['column_types'].items():
            print(f"  {dtype}: {count}")
        print("=" * 60 + "\n")

    return info_dict


def column_summary(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç”Ÿæˆæ¯åˆ—çš„è¯¦ç»†æ‘˜è¦ä¿¡æ¯

    Args:
        df: æ•°æ®DataFrame

    Returns:
        åŒ…å«æ¯åˆ—ç»Ÿè®¡ä¿¡æ¯çš„DataFrame
    """
    summary = pd.DataFrame({
        'æ•°æ®ç±»å‹': df.dtypes,
        'ç¼ºå¤±å€¼æ•°é‡': df.isnull().sum(),
        'ç¼ºå¤±å€¼æ¯”ä¾‹(%)': (df.isnull().sum() / len(df) * 100).round(2),
        'å”¯ä¸€å€¼æ•°é‡': df.nunique(),
        'å”¯ä¸€å€¼æ¯”ä¾‹(%)': (df.nunique() / len(df) * 100).round(2),
    })

    # æ·»åŠ æ•°å€¼å‹ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col in df.columns:
            summary.loc[col, 'æœ€å°å€¼'] = df[col].min()
            summary.loc[col, 'æœ€å¤§å€¼'] = df[col].max()
            summary.loc[col, 'å‡å€¼'] = df[col].mean()
            summary.loc[col, 'æ ‡å‡†å·®'] = df[col].std()

    # æ·»åŠ ç±»åˆ«å‹ç‰¹å¾çš„ä¿¡æ¯
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    for col in categorical_cols:
        if col in df.columns:
            # è·å–æœ€å¸¸è§çš„å€¼
            if df[col].nunique() > 0:
                summary.loc[col, 'æœ€å¸¸è§å€¼'] = df[col].mode()[0] if len(df[col].mode()) > 0 else None
                summary.loc[col, 'æœ€å¸¸è§å€¼é¢‘æ•°'] = df[col].value_counts().iloc[0] if len(df[col]) > 0 else 0

    return summary


# ==================== ç¼ºå¤±å€¼åˆ†æ ====================

def missing_value_analysis(df: pd.DataFrame, threshold: float = 0.0) -> pd.DataFrame:
    """
    åˆ†ææ•°æ®é›†ä¸­çš„ç¼ºå¤±å€¼æƒ…å†µ

    Args:
        df: æ•°æ®DataFrame
        threshold: ç¼ºå¤±å€¼æ¯”ä¾‹é˜ˆå€¼ï¼Œåªæ˜¾ç¤ºç¼ºå¤±ç‡é«˜äºæ­¤å€¼çš„åˆ—

    Returns:
        ç¼ºå¤±å€¼ç»Ÿè®¡DataFrame
    """
    # è®¡ç®—ç¼ºå¤±å€¼ç»Ÿè®¡
    missing_stats = pd.DataFrame({
        'ç¼ºå¤±æ•°é‡': df.isnull().sum(),
        'ç¼ºå¤±æ¯”ä¾‹(%)': (df.isnull().sum() / len(df) * 100).round(2),
        'æ•°æ®ç±»å‹': df.dtypes
    })

    # ç­›é€‰ç¼ºå¤±å€¼å¤§äºé˜ˆå€¼çš„åˆ—
    missing_stats = missing_stats[missing_stats['ç¼ºå¤±æ¯”ä¾‹(%)'] > threshold * 100]

    # æŒ‰ç¼ºå¤±æ¯”ä¾‹é™åºæ’åº
    missing_stats = missing_stats.sort_values('ç¼ºå¤±æ¯”ä¾‹(%)', ascending=False)

    if len(missing_stats) > 0:
        print(f"\nğŸ” å‘ç° {len(missing_stats)} ä¸ªç‰¹å¾å­˜åœ¨ç¼ºå¤±å€¼ï¼ˆé˜ˆå€¼: {threshold*100}%ï¼‰\n")
        print(missing_stats)

        # ç¼ºå¤±å€¼ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        severe = missing_stats[missing_stats['ç¼ºå¤±æ¯”ä¾‹(%)'] > 50]
        moderate = missing_stats[(missing_stats['ç¼ºå¤±æ¯”ä¾‹(%)'] > 20) &
                                (missing_stats['ç¼ºå¤±æ¯”ä¾‹(%)'] <= 50)]
        mild = missing_stats[missing_stats['ç¼ºå¤±æ¯”ä¾‹(%)'] <= 20]

        print(f"\nğŸ“ˆ ç¼ºå¤±å€¼ä¸¥é‡ç¨‹åº¦åˆ†ç±»:")
        print(f"  ä¸¥é‡ç¼ºå¤± (>50%): {len(severe)} ä¸ªç‰¹å¾")
        print(f"  ä¸­åº¦ç¼ºå¤± (20%-50%): {len(moderate)} ä¸ªç‰¹å¾")
        print(f"  è½»åº¦ç¼ºå¤± (<=20%): {len(mild)} ä¸ªç‰¹å¾")
    else:
        print("\nâœ… æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼ï¼")

    return missing_stats


def visualize_missing_values(df: pd.DataFrame, figsize: Tuple[int, int] = (12, 6)):
    """
    å¯è§†åŒ–ç¼ºå¤±å€¼åˆ†å¸ƒ

    Args:
        df: æ•°æ®DataFrame
        figsize: å›¾åƒå¤§å°
    """
    missing_counts = df.isnull().sum()
    missing_cols = missing_counts[missing_counts > 0].sort_values(ascending=False)

    if len(missing_cols) == 0:
        print("âœ… æ•°æ®é›†æ— ç¼ºå¤±å€¼ï¼Œæ— éœ€å¯è§†åŒ–")
        return

    fig, axes = plt.subplots(1, 2, figsize=figsize)

    # å·¦å›¾ï¼šç¼ºå¤±å€¼æ•°é‡æŸ±çŠ¶å›¾
    missing_cols.plot(kind='barh', ax=axes[0], color='coral')
    axes[0].set_xlabel('ç¼ºå¤±å€¼æ•°é‡')
    axes[0].set_title('å„ç‰¹å¾ç¼ºå¤±å€¼æ•°é‡')
    axes[0].grid(axis='x', alpha=0.3)

    # å³å›¾ï¼šç¼ºå¤±å€¼æ¯”ä¾‹
    missing_ratio = (missing_cols / len(df) * 100).round(2)
    missing_ratio.plot(kind='barh', ax=axes[1], color='lightblue')
    axes[1].set_xlabel('ç¼ºå¤±æ¯”ä¾‹ (%)')
    axes[1].set_title('å„ç‰¹å¾ç¼ºå¤±å€¼æ¯”ä¾‹')
    axes[1].grid(axis='x', alpha=0.3)

    # åœ¨å³å›¾ä¸Šæ ‡æ³¨ç™¾åˆ†æ¯”
    for i, v in enumerate(missing_ratio):
        axes[1].text(v + 0.5, i, f'{v:.1f}%', va='center')

    plt.tight_layout()
    plt.show()


# ==================== å¼‚å¸¸å€¼æ£€æµ‹ ====================

def detect_outliers_iqr(series: pd.Series, k: float = 1.5) -> Tuple[np.ndarray, float, float]:
    """
    ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼

    åŸç†ï¼š
        IQR = Q3 - Q1ï¼ˆå››åˆ†ä½è·ï¼‰
        ä¸‹ç•Œ = Q1 - k * IQR
        ä¸Šç•Œ = Q3 + k * IQR
        è¶…å‡ºä¸Šä¸‹ç•Œçš„å€¼è¢«è§†ä¸ºå¼‚å¸¸å€¼

    Args:
        series: æ•°æ®åºåˆ—
        k: IQRå€æ•°ï¼Œé€šå¸¸å–1.5ï¼ˆæ¸©å’Œå¼‚å¸¸å€¼ï¼‰æˆ–3.0ï¼ˆæç«¯å¼‚å¸¸å€¼ï¼‰

    Returns:
        (å¼‚å¸¸å€¼ç´¢å¼•æ•°ç»„, ä¸‹ç•Œ, ä¸Šç•Œ)
    """
    # è®¡ç®—å››åˆ†ä½æ•°
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1

    # è®¡ç®—ä¸Šä¸‹ç•Œ
    lower_bound = Q1 - k * IQR
    upper_bound = Q3 + k * IQR

    # æ‰¾å‡ºå¼‚å¸¸å€¼çš„ç´¢å¼•
    outliers = ((series < lower_bound) | (series > upper_bound)).values

    return outliers, lower_bound, upper_bound


def detect_outliers_zscore(series: pd.Series, threshold: float = 3.0) -> np.ndarray:
    """
    ä½¿ç”¨Z-Scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼

    åŸç†ï¼š
        Z-Score = (x - mean) / std
        |Z-Score| > threshold çš„å€¼è¢«è§†ä¸ºå¼‚å¸¸å€¼

    Args:
        series: æ•°æ®åºåˆ—
        threshold: Z-Scoreé˜ˆå€¼ï¼Œé€šå¸¸å–3.0

    Returns:
        å¼‚å¸¸å€¼ç´¢å¼•æ•°ç»„
    """
    z_scores = np.abs(stats.zscore(series.dropna()))
    outliers = np.zeros(len(series), dtype=bool)
    outliers[series.notna()] = z_scores > threshold

    return outliers


def outlier_analysis(df: pd.DataFrame, method: str = 'iqr',
                    columns: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    å¯¹æ•°å€¼å‹ç‰¹å¾è¿›è¡Œå¼‚å¸¸å€¼åˆ†æ

    Args:
        df: æ•°æ®DataFrame
        method: æ£€æµ‹æ–¹æ³• ('iqr' æˆ– 'zscore')
        columns: è¦åˆ†æçš„åˆ—ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ•°å€¼åˆ—

    Returns:
        åŒ…å«å¼‚å¸¸å€¼ä¿¡æ¯çš„å­—å…¸
    """
    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns.tolist()

    outlier_report = {}

    print(f"\nğŸ” å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆæ–¹æ³•: {method.upper()}ï¼‰\n")
    print(f"{'ç‰¹å¾åç§°':<20} {'å¼‚å¸¸å€¼æ•°é‡':<12} {'å¼‚å¸¸å€¼æ¯”ä¾‹':<12} {'å¼‚å¸¸å€¼èŒƒå›´'}")
    print("-" * 70)

    for col in columns:
        if col not in df.columns:
            continue

        series = df[col].dropna()

        if len(series) == 0:
            continue

        # æ£€æµ‹å¼‚å¸¸å€¼
        if method == 'iqr':
            outliers, lower, upper = detect_outliers_iqr(series)
            outlier_range = f"<{lower:.2f} or >{upper:.2f}"
        else:  # zscore
            outliers = detect_outliers_zscore(series)
            outlier_range = "|Z-Score| > 3.0"

        n_outliers = outliers.sum()
        outlier_ratio = n_outliers / len(df) * 100

        outlier_report[col] = {
            'n_outliers': n_outliers,
            'outlier_ratio': outlier_ratio,
            'outlier_indices': np.where(outliers)[0].tolist()
        }

        print(f"{col:<20} {n_outliers:<12} {outlier_ratio:>6.2f}%      {outlier_range}")

    print("-" * 70)

    return outlier_report


# ==================== æ•°æ®åˆ†å¸ƒåˆ†æ ====================

def distribution_analysis(df: pd.DataFrame, columns: Optional[List[str]] = None,
                         figsize: Tuple[int, int] = (15, 10)):
    """
    åˆ†æå¹¶å¯è§†åŒ–æ•°å€¼å‹ç‰¹å¾çš„åˆ†å¸ƒ

    Args:
        df: æ•°æ®DataFrame
        columns: è¦åˆ†æçš„åˆ—ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ•°å€¼åˆ—
        figsize: å›¾åƒå¤§å°
    """
    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns.tolist()

    n_cols = len(columns)
    if n_cols == 0:
        print("âš ï¸  æ²¡æœ‰æ•°å€¼å‹ç‰¹å¾å¯ä¾›åˆ†æ")
        return

    # åŠ¨æ€è®¡ç®—å­å›¾å¸ƒå±€
    n_rows = (n_cols + 2) // 3
    n_plot_cols = min(n_cols, 3)

    fig, axes = plt.subplots(n_rows, n_plot_cols, figsize=figsize)
    if n_cols == 1:
        axes = np.array([axes])
    axes = axes.flatten()

    for idx, col in enumerate(columns):
        ax = axes[idx]
        data = df[col].dropna()

        # ç»˜åˆ¶ç›´æ–¹å›¾ + KDE
        ax.hist(data, bins=30, alpha=0.6, color='skyblue', edgecolor='black', density=True)
        data.plot(kind='kde', ax=ax, color='red', linewidth=2)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_val = data.mean()
        median_val = data.median()
        ax.axvline(mean_val, color='green', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')
        ax.axvline(median_val, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')

        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title(f'{col}', fontsize=12, fontweight='bold')
        ax.set_xlabel('')
        ax.set_ylabel('å¯†åº¦')
        ax.legend(fontsize=8)
        ax.grid(alpha=0.3)

    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(n_cols, len(axes)):
        axes[idx].axis('off')

    plt.tight_layout()
    plt.show()

    # æ‰“å°åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š åˆ†å¸ƒç»Ÿè®¡æ‘˜è¦\n")
    for col in columns:
        data = df[col].dropna()
        skewness = data.skew()
        kurtosis = data.kurtosis()

        print(f"{col}:")
        print(f"  ååº¦ (Skewness): {skewness:.3f}", end="")
        if abs(skewness) < 0.5:
            print(" - è¿‘ä¼¼å¯¹ç§°åˆ†å¸ƒ")
        elif skewness > 0:
            print(" - å³åï¼ˆæ­£åï¼‰")
        else:
            print(" - å·¦åï¼ˆè´Ÿåï¼‰")

        print(f"  å³°åº¦ (Kurtosis): {kurtosis:.3f}", end="")
        if abs(kurtosis) < 0.5:
            print(" - è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ")
        elif kurtosis > 0:
            print(" - å°–å³°åˆ†å¸ƒ")
        else:
            print(" - å¹³å³°åˆ†å¸ƒ")
        print()


# ==================== ç›¸å…³æ€§åˆ†æ ====================

def correlation_analysis(df: pd.DataFrame, method: str = 'pearson',
                        threshold: float = 0.5, figsize: Tuple[int, int] = (12, 10)):
    """
    åˆ†ææ•°å€¼å‹ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§

    Args:
        df: æ•°æ®DataFrame
        method: ç›¸å…³ç³»æ•°ç±»å‹ ('pearson', 'spearman', 'kendall')
        threshold: å¼ºç›¸å…³é˜ˆå€¼
        figsize: å›¾åƒå¤§å°

    Returns:
        ç›¸å…³ç³»æ•°çŸ©é˜µ
    """
    # åªé€‰æ‹©æ•°å€¼å‹ç‰¹å¾
    numeric_df = df.select_dtypes(include=[np.number])

    if numeric_df.shape[1] < 2:
        print("âš ï¸  æ•°å€¼å‹ç‰¹å¾å°‘äº2ä¸ªï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ")
        return None

    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = numeric_df.corr(method=method)

    # å¯è§†åŒ–çƒ­åŠ›å›¾
    plt.figure(figsize=figsize)
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # åªæ˜¾ç¤ºä¸‹ä¸‰è§’
    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f',
                cmap='coolwarm', center=0, square=True,
                linewidths=1, cbar_kws={"shrink": 0.8})
    plt.title(f'ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾ ({method.capitalize()})', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

    # æ‰¾å‡ºå¼ºç›¸å…³ç‰¹å¾å¯¹
    print(f"\nğŸ”— å¼ºç›¸å…³ç‰¹å¾å¯¹ï¼ˆ|ç›¸å…³ç³»æ•°| > {threshold}ï¼‰\n")
    strong_corr = []

    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_value = corr_matrix.iloc[i, j]
            if abs(corr_value) > threshold:
                strong_corr.append({
                    'ç‰¹å¾1': corr_matrix.columns[i],
                    'ç‰¹å¾2': corr_matrix.columns[j],
                    'ç›¸å…³ç³»æ•°': corr_value
                })

    if strong_corr:
        strong_corr_df = pd.DataFrame(strong_corr).sort_values('ç›¸å…³ç³»æ•°',
                                                                key=abs,
                                                                ascending=False)
        print(strong_corr_df.to_string(index=False))
        print(f"\nğŸ’¡ å»ºè®®: è€ƒè™‘ç§»é™¤{len(strong_corr)}å¯¹å¼ºç›¸å…³ç‰¹å¾ä¸­çš„ä¸€ä¸ªï¼Œä»¥é¿å…å¤šé‡å…±çº¿æ€§é—®é¢˜")
    else:
        print(f"âœ… æœªå‘ç°å¼ºç›¸å…³ç‰¹å¾å¯¹ï¼ˆé˜ˆå€¼: {threshold}ï¼‰")

    return corr_matrix


# ==================== æ•°æ®ç±»å‹æ¨æ–­ ====================

def infer_column_types(df: pd.DataFrame,
                      categorical_threshold: int = 20) -> Dict[str, List[str]]:
    """
    æ™ºèƒ½æ¨æ–­æ¯åˆ—çš„æ•°æ®ç±»å‹ï¼ˆæ•°å€¼å‹ã€ç±»åˆ«å‹ã€IDå‹ã€æ—¥æœŸå‹ç­‰ï¼‰

    Args:
        df: æ•°æ®DataFrame
        categorical_threshold: å”¯ä¸€å€¼æ•°é‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è§†ä¸ºç±»åˆ«å‹

    Returns:
        åˆ†ç±»åçš„åˆ—åå­—å…¸
    """
    column_types = {
        'numeric': [],        # æ•°å€¼å‹
        'categorical': [],    # ç±»åˆ«å‹
        'binary': [],         # äºŒå…ƒå‹
        'id': [],            # IDå‹ï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
        'datetime': [],      # æ—¥æœŸæ—¶é—´å‹
        'text': [],          # æ–‡æœ¬å‹
        'constant': []       # å¸¸é‡ï¼ˆåªæœ‰ä¸€ä¸ªå€¼ï¼‰
    }

    for col in df.columns:
        # å”¯ä¸€å€¼æ¯”ä¾‹
        unique_ratio = df[col].nunique() / len(df)
        n_unique = df[col].nunique()

        # å¸¸é‡æ£€æµ‹
        if n_unique == 1:
            column_types['constant'].append(col)
            continue

        # IDæ£€æµ‹ï¼ˆå”¯ä¸€å€¼æ¯”ä¾‹ > 95%ï¼‰
        if unique_ratio > 0.95:
            column_types['id'].append(col)
            continue

        # æ—¥æœŸæ—¶é—´æ£€æµ‹
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            column_types['datetime'].append(col)
            continue

        # æ•°å€¼å‹æ£€æµ‹
        if pd.api.types.is_numeric_dtype(df[col]):
            # äºŒå…ƒå‹æ£€æµ‹
            if n_unique == 2:
                column_types['binary'].append(col)
            else:
                column_types['numeric'].append(col)
            continue

        # ç±»åˆ«å‹ vs æ–‡æœ¬å‹
        if n_unique <= categorical_threshold:
            column_types['categorical'].append(col)
        else:
            column_types['text'].append(col)

    # æ‰“å°ç»“æœ
    print("\nğŸ·ï¸  æ•°æ®ç±»å‹æ™ºèƒ½æ¨æ–­ç»“æœ\n")
    for dtype, cols in column_types.items():
        if cols:
            print(f"{dtype.upper():.<20} {len(cols)} ä¸ªç‰¹å¾")
            print(f"  {', '.join(cols[:5])}" + (" ..." if len(cols) > 5 else ""))
            print()

    return column_types


# ==================== æ•°æ®è´¨é‡è¯„åˆ† ====================

def calculate_data_quality_score(df: pd.DataFrame, target: Optional[str] = None) -> Dict[str, Any]:
    """
    è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰

    è¯„åˆ†ç»´åº¦:
    1. å®Œæ•´æ€§ (30åˆ†): ç¼ºå¤±å€¼æƒ…å†µ
    2. ä¸€è‡´æ€§ (20åˆ†): æ•°æ®ç±»å‹ã€å¼‚å¸¸å€¼
    3. å‡†ç¡®æ€§ (20åˆ†): é‡å¤å€¼ã€å¸¸é‡åˆ—
    4. å¹³è¡¡æ€§ (15åˆ†): ç›®æ ‡å˜é‡åˆ†å¸ƒï¼ˆå¦‚æœ‰ï¼‰
    5. å¤šæ ·æ€§ (15åˆ†): ç‰¹å¾æ•°é‡ã€å”¯ä¸€å€¼

    Args:
        df: æ•°æ®DataFrame
        target: ç›®æ ‡å˜é‡åˆ—åï¼ˆå¯é€‰ï¼‰

    Returns:
        è¯„åˆ†è¯¦æƒ…å­—å…¸
    """
    scores = {}

    # 1. å®Œæ•´æ€§è¯„åˆ† (30åˆ†)
    missing_ratio = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])
    completeness_score = max(0, 30 * (1 - missing_ratio))
    scores['completeness'] = {
        'score': round(completeness_score, 2),
        'max': 30,
        'missing_ratio': round(missing_ratio * 100, 2)
    }

    # 2. ä¸€è‡´æ€§è¯„åˆ† (20åˆ†)
    # æ•°æ®ç±»å‹ä¸€è‡´æ€§
    type_consistency = 20
    # æ£€æŸ¥æ•°å€¼åˆ—æ˜¯å¦æœ‰å­—ç¬¦ä¸²æ··å…¥ç­‰é—®é¢˜ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å¤šçš„å”¯ä¸€å€¼ï¼ˆå¯èƒ½æ˜¯IDï¼‰
        if df[col].nunique() / len(df) > 0.95:
            type_consistency -= 2
    scores['consistency'] = {
        'score': max(0, round(type_consistency, 2)),
        'max': 20
    }

    # 3. å‡†ç¡®æ€§è¯„åˆ† (20åˆ†)
    accuracy_score = 20

    # é‡å¤è¡Œæ‰£åˆ†
    dup_ratio = df.duplicated().sum() / len(df)
    accuracy_score -= min(10, dup_ratio * 50)

    # å¸¸é‡åˆ—æ‰£åˆ†ï¼ˆæ— ä¿¡æ¯ç‰¹å¾ï¼‰
    constant_cols = [col for col in df.columns if df[col].nunique() == 1]
    accuracy_score -= min(10, len(constant_cols) * 2)

    scores['accuracy'] = {
        'score': max(0, round(accuracy_score, 2)),
        'max': 20,
        'duplicate_ratio': round(dup_ratio * 100, 2),
        'constant_columns': len(constant_cols)
    }

    # 4. å¹³è¡¡æ€§è¯„åˆ† (15åˆ†) - ä»…å½“æœ‰ç›®æ ‡å˜é‡æ—¶
    if target and target in df.columns:
        balance_score = 15
        if df[target].nunique() <= 20:  # åˆ†ç±»é—®é¢˜
            value_counts = df[target].value_counts()
            max_class_ratio = value_counts.max() / len(df)

            # æåº¦ä¸å¹³è¡¡æ‰£åˆ†
            if max_class_ratio > 0.9:
                balance_score = 5
            elif max_class_ratio > 0.8:
                balance_score = 8
            elif max_class_ratio > 0.7:
                balance_score = 11

            scores['balance'] = {
                'score': round(balance_score, 2),
                'max': 15,
                'max_class_ratio': round(max_class_ratio * 100, 2)
            }
        else:  # å›å½’é—®é¢˜
            scores['balance'] = {
                'score': 15,
                'max': 15,
                'note': 'å›å½’é—®é¢˜ï¼Œæ— éœ€å¹³è¡¡æ€§æ£€æŸ¥'
            }
    else:
        scores['balance'] = {
            'score': 15,
            'max': 15,
            'note': 'æ— ç›®æ ‡å˜é‡ï¼Œé»˜è®¤æ»¡åˆ†'
        }

    # 5. å¤šæ ·æ€§è¯„åˆ† (15åˆ†)
    diversity_score = 15

    # ç‰¹å¾æ•°é‡å¤ªå°‘æ‰£åˆ†
    n_features = df.shape[1] - (1 if target else 0)
    if n_features < 5:
        diversity_score -= 5
    elif n_features < 10:
        diversity_score -= 2

    # æ ·æœ¬æ•°é‡å¤ªå°‘æ‰£åˆ†
    if len(df) < 100:
        diversity_score -= 5
    elif len(df) < 500:
        diversity_score -= 2

    scores['diversity'] = {
        'score': max(0, round(diversity_score, 2)),
        'max': 15,
        'n_samples': len(df),
        'n_features': n_features
    }

    # è®¡ç®—æ€»åˆ†
    total_score = sum(s['score'] for s in scores.values())

    # è¯„çº§
    if total_score >= 90:
        grade = 'A (ä¼˜ç§€)'
    elif total_score >= 80:
        grade = 'B (è‰¯å¥½)'
    elif total_score >= 70:
        grade = 'C (ä¸­ç­‰)'
    elif total_score >= 60:
        grade = 'D (åŠæ ¼)'
    else:
        grade = 'F (éœ€è¦æ”¹è¿›)'

    result = {
        'total_score': round(total_score, 2),
        'grade': grade,
        'scores': scores
    }

    # æ‰“å°è¯„åˆ†
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®è´¨é‡è¯„åˆ†")
    print("=" * 60)
    print(f"\næ€»åˆ†: {total_score:.1f} / 100  -  ç­‰çº§: {grade}\n")
    print(f"{'ç»´åº¦':<15} {'å¾—åˆ†':<10} {'æ»¡åˆ†':<10} {'è¯¦æƒ…'}")
    print("-" * 60)
    print(f"{'å®Œæ•´æ€§':<15} {scores['completeness']['score']:<10.1f} {scores['completeness']['max']:<10} ç¼ºå¤±ç‡: {scores['completeness']['missing_ratio']:.1f}%")
    print(f"{'ä¸€è‡´æ€§':<15} {scores['consistency']['score']:<10.1f} {scores['consistency']['max']:<10} ç±»å‹ä¸€è‡´æ€§")
    print(f"{'å‡†ç¡®æ€§':<15} {scores['accuracy']['score']:<10.1f} {scores['accuracy']['max']:<10} é‡å¤ç‡: {scores['accuracy']['duplicate_ratio']:.1f}%")

    if target and target in df.columns and 'max_class_ratio' in scores['balance']:
        print(f"{'å¹³è¡¡æ€§':<15} {scores['balance']['score']:<10.1f} {scores['balance']['max']:<10} æœ€å¤§ç±»å æ¯”: {scores['balance']['max_class_ratio']:.1f}%")
    else:
        print(f"{'å¹³è¡¡æ€§':<15} {scores['balance']['score']:<10.1f} {scores['balance']['max']:<10} {scores['balance'].get('note', '')}")

    print(f"{'å¤šæ ·æ€§':<15} {scores['diversity']['score']:<10.1f} {scores['diversity']['max']:<10} {scores['diversity']['n_samples']}æ ·æœ¬, {scores['diversity']['n_features']}ç‰¹å¾")
    print("=" * 60 + "\n")

    return result


def detect_missing_pattern(df: pd.DataFrame, columns: Optional[List[str]] = None) -> Dict[str, str]:
    """
    æ£€æµ‹ç¼ºå¤±å€¼æ¨¡å¼ï¼ˆMCAR/MAR/MNARï¼‰

    MCAR (Missing Completely At Random): ç¼ºå¤±å®Œå…¨éšæœº
    MAR (Missing At Random): ç¼ºå¤±éšæœºï¼Œä½†ä¸å…¶ä»–å˜é‡ç›¸å…³
    MNAR (Missing Not At Random): ç¼ºå¤±ä¸éšæœºï¼Œä¸ç¼ºå¤±å€¼æœ¬èº«ç›¸å…³

    æ³¨: è¿™æ˜¯ç®€åŒ–çš„æ£€æµ‹æ–¹æ³•ï¼ŒçœŸå®çš„MCAR/MAR/MNARæ£€æµ‹éœ€è¦æ›´å¤æ‚çš„ç»Ÿè®¡æ£€éªŒ

    Args:
        df: æ•°æ®DataFrame
        columns: è¦æ£€æµ‹çš„åˆ—ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æœ‰ç¼ºå¤±å€¼çš„åˆ—

    Returns:
        æ¯åˆ—çš„ç¼ºå¤±æ¨¡å¼å­—å…¸
    """
    if columns is None:
        # åªåˆ†ææœ‰ç¼ºå¤±å€¼çš„åˆ—
        columns = df.columns[df.isnull().any()].tolist()

    if len(columns) == 0:
        print("âœ… æ•°æ®æ— ç¼ºå¤±å€¼ï¼Œæ— éœ€æ£€æµ‹ç¼ºå¤±æ¨¡å¼")
        return {}

    patterns = {}

    print("\nğŸ” ç¼ºå¤±å€¼æ¨¡å¼æ£€æµ‹\n")
    print(f"{'åˆ—å':<20} {'ç¼ºå¤±ç‡':<10} {'æ¨¡å¼':<10} {'å»ºè®®å¤„ç†æ–¹æ³•'}")
    print("-" * 70)

    for col in columns:
        if col not in df.columns:
            continue

        missing_rate = df[col].isnull().sum() / len(df)

        # ç®€åŒ–çš„æ¨¡å¼åˆ¤æ–­é€»è¾‘
        # 1. å¦‚æœç¼ºå¤±ç‡å¾ˆä½(<5%)ï¼Œå‡è®¾ä¸ºMCAR
        if missing_rate < 0.05:
            pattern = 'MCAR'
            suggestion = 'åˆ é™¤æˆ–ç®€å•å¡«å……'
        # 2. å¦‚æœç¼ºå¤±ç‡ä¸­ç­‰(5%-30%)ï¼Œå‡è®¾ä¸ºMAR
        elif missing_rate < 0.30:
            pattern = 'MAR'
            suggestion = 'KNN/è¿­ä»£å¡«å……'
        # 3. å¦‚æœç¼ºå¤±ç‡å¾ˆé«˜(>30%)ï¼Œå¯èƒ½ä¸ºMNAR
        else:
            pattern = 'MNAR'
            suggestion = 'å»ºæ¨¡å¤„ç†æˆ–åˆ é™¤'

        patterns[col] = pattern
        print(f"{col:<20} {missing_rate*100:>6.1f}%    {pattern:<10} {suggestion}")

    print("-" * 70 + "\n")

    return patterns


def save_diagnosis_report(report: Dict[str, Any], output_path: str, format: str = 'json'):
    """
    ä¿å­˜è¯Šæ–­æŠ¥å‘Šä¸ºæ–‡ä»¶

    Args:
        report: è¯Šæ–­æŠ¥å‘Šå­—å…¸
        output_path: è¾“å‡ºè·¯å¾„
        format: æ–‡ä»¶æ ¼å¼ ('json' æˆ– 'html')
    """
    import json
    from pathlib import Path

    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    if format == 'json':
        # ä¿å­˜ä¸ºJSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False, default=str)
        print(f"âœ… è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜ä¸ºJSON: {output_path}")

    elif format == 'html':
        # ä¿å­˜ä¸ºHTML
        html_content = _generate_html_report(report)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"âœ… è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜ä¸ºHTML: {output_path}")

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}ï¼Œè¯·ä½¿ç”¨'json'æˆ–'html'")


def _generate_html_report(report: Dict[str, Any]) -> str:
    """
    ç”ŸæˆHTMLæ ¼å¼çš„è¯Šæ–­æŠ¥å‘Š

    Args:
        report: è¯Šæ–­æŠ¥å‘Šå­—å…¸

    Returns:
        HTMLå­—ç¬¦ä¸²
    """
    html = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>æ•°æ®è¯Šæ–­æŠ¥å‘Š</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
            .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 30px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}
            h1 {{ color: #333; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }}
            h2 {{ color: #555; margin-top: 30px; }}
            table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
            th {{ background-color: #4CAF50; color: white; }}
            tr:nth-child(even) {{ background-color: #f2f2f2; }}
            .score {{ font-size: 48px; font-weight: bold; color: #4CAF50; text-align: center; margin: 20px 0; }}
            .warning {{ color: #ff9800; font-weight: bold; }}
            .error {{ color: #f44336; font-weight: bold; }}
            .success {{ color: #4CAF50; font-weight: bold; }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ“‹ æ•°æ®è¯Šæ–­æŠ¥å‘Š</h1>

            <h2>åŸºç¡€ä¿¡æ¯</h2>
            <table>
                <tr><th>æŒ‡æ ‡</th><th>å€¼</th></tr>
                <tr><td>æ ·æœ¬æ•°é‡</td><td>{n_samples}</td></tr>
                <tr><td>ç‰¹å¾æ•°é‡</td><td>{n_features}</td></tr>
                <tr><td>å†…å­˜å ç”¨</td><td>{memory_mb:.2f} MB</td></tr>
                <tr><td>é‡å¤è¡Œæ•°</td><td>{duplicated_rows}</td></tr>
            </table>

            <h2>æ•°æ®è´¨é‡è¯„åˆ†</h2>
            <div class="score">{quality_score:.1f} / 100</div>
            <p style="text-align: center; font-size: 24px;">{grade}</p>

            <h2>è¯Šæ–­å»ºè®®</h2>
            <ul>
                {suggestions}
            </ul>
        </div>
    </body>
    </html>
    """

    # æå–ä¿¡æ¯
    basic_info = report.get('basic_info', {})
    quality_score = report.get('quality_score', {}).get('total_score', 0)
    grade = report.get('quality_score', {}).get('grade', 'N/A')

    suggestions_html = ""
    if 'suggestions' in report and report['suggestions']:
        for suggestion in report['suggestions']:
            suggestions_html += f"<li>{suggestion}</li>"
    else:
        suggestions_html = "<li>æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— æ˜æ˜¾é—®é¢˜</li>"

    # å¡«å……HTMLæ¨¡æ¿
    html = html.format(
        n_samples=basic_info.get('n_samples', 0),
        n_features=basic_info.get('n_features', 0),
        memory_mb=basic_info.get('memory_usage_mb', 0),
        duplicated_rows=basic_info.get('duplicated_rows', 0),
        quality_score=quality_score,
        grade=grade,
        suggestions=suggestions_html
    )

    return html


# ==================== ç»¼åˆè¯Šæ–­æŠ¥å‘Š ====================

def generate_diagnosis_report(df: pd.DataFrame, target: Optional[str] = None,
                             save_path: Optional[str] = None) -> Dict[str, Any]:
    """
    ç”Ÿæˆå®Œæ•´çš„æ•°æ®è¯Šæ–­æŠ¥å‘Š

    Args:
        df: æ•°æ®DataFrame
        target: ç›®æ ‡å˜é‡åˆ—åï¼ˆå¯é€‰ï¼‰
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æä¾›åˆ™ä¿å­˜ä¸ºJSONå’ŒHTMLæ ¼å¼

    Returns:
        åŒ…å«æ‰€æœ‰è¯Šæ–­ä¿¡æ¯çš„å­—å…¸
    """
    print("\n" + "=" * 70)
    print(" " * 20 + "ğŸ“‹ æ•°æ®è¯Šæ–­æŠ¥å‘Š")
    print("=" * 70 + "\n")

    report = {}

    # 1. åŸºç¡€ä¿¡æ¯
    report['basic_info'] = basic_info(df, show=True)

    # 2. æ•°æ®ç±»å‹æ¨æ–­
    report['column_types'] = infer_column_types(df)

    # 3. ç¼ºå¤±å€¼åˆ†æ
    report['missing_values'] = missing_value_analysis(df, threshold=0.0)

    # 4. ç¼ºå¤±å€¼æ¨¡å¼æ£€æµ‹ï¼ˆæ–°å¢ï¼‰
    if len(report['missing_values']) > 0:
        report['missing_patterns'] = detect_missing_pattern(df)

    # 5. ç›®æ ‡å˜é‡åˆ†æï¼ˆå¦‚æœæä¾›ï¼‰
    if target and target in df.columns:
        print(f"\nğŸ¯ ç›®æ ‡å˜é‡åˆ†æ: {target}\n")
        if df[target].dtype in [np.int64, np.float64] and df[target].nunique() > 10:
            # å›å½’é—®é¢˜
            print(f"  ç±»å‹: å›å½’é—®é¢˜")
            print(f"  èŒƒå›´: [{df[target].min():.2f}, {df[target].max():.2f}]")
            print(f"  å‡å€¼: {df[target].mean():.2f}")
            print(f"  æ ‡å‡†å·®: {df[target].std():.2f}")
        else:
            # åˆ†ç±»é—®é¢˜
            print(f"  ç±»å‹: åˆ†ç±»é—®é¢˜")
            print(f"  ç±»åˆ«æ•°: {df[target].nunique()}")
            print(f"\n  ç±»åˆ«åˆ†å¸ƒ:")
            value_counts = df[target].value_counts()
            for val, count in value_counts.items():
                ratio = count / len(df) * 100
                print(f"    {val}: {count} ({ratio:.1f}%)")

            # æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡
            max_ratio = value_counts.max() / len(df)
            if max_ratio > 0.8:
                print(f"\n  âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡ï¼ˆæœ€å¤§ç±»å æ¯” {max_ratio*100:.1f}%ï¼‰")
                print(f"     å»ºè®®: è€ƒè™‘ä½¿ç”¨SMOTEã€è°ƒæ•´ç±»åˆ«æƒé‡æˆ–ä½¿ç”¨ç‰¹æ®Šçš„è¯„ä¼°æŒ‡æ ‡")

    # 6. æ•°å€¼ç‰¹å¾ç»Ÿè®¡
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if numeric_cols:
        print(f"\nğŸ“Š æ•°å€¼ç‰¹å¾æè¿°ç»Ÿè®¡\n")
        print(df[numeric_cols].describe().round(2))

    # 7. æ•°æ®è´¨é‡è¯„åˆ†ï¼ˆæ–°å¢ï¼‰
    report['quality_score'] = calculate_data_quality_score(df, target)

    # 8. è¯Šæ–­å»ºè®®
    print("\n" + "=" * 70)
    print("ğŸ’¡ è¯Šæ–­å»ºè®®")
    print("=" * 70)

    suggestions = []

    # ç¼ºå¤±å€¼å»ºè®®
    if len(report['missing_values']) > 0:
        suggestions.append("âœ“ å¤„ç†ç¼ºå¤±å€¼: å»ºè®®æŸ¥çœ‹ç¼ºå¤±æ¨¡å¼ï¼Œé€‰æ‹©åˆ é™¤ã€å¡«å……æˆ–å»ºæ¨¡æ–¹æ³•")

    # é‡å¤è¡Œå»ºè®®
    if report['basic_info']['duplicated_rows'] > 0:
        suggestions.append(f"âœ“ ç§»é™¤ {report['basic_info']['duplicated_rows']} è¡Œé‡å¤æ•°æ®")

    # IDåˆ—å»ºè®®
    if report['column_types']['id']:
        suggestions.append(f"âœ“ ç§»é™¤IDåˆ—: {', '.join(report['column_types']['id'][:3])}")

    # å¸¸é‡åˆ—å»ºè®®
    if report['column_types']['constant']:
        suggestions.append(f"âœ“ ç§»é™¤å¸¸é‡åˆ—: {', '.join(report['column_types']['constant'])}")

    # ç±»åˆ«å‹ç‰¹å¾å»ºè®®
    if report['column_types']['categorical']:
        suggestions.append(f"âœ“ ç¼–ç ç±»åˆ«ç‰¹å¾: è€ƒè™‘ä½¿ç”¨One-Hotæˆ–Label Encoding")

    report['suggestions'] = suggestions

    if suggestions:
        for i, suggestion in enumerate(suggestions, 1):
            print(f"{i}. {suggestion}")
    else:
        print("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— æ˜æ˜¾é—®é¢˜")

    print("\n" + "=" * 70 + "\n")

    # 9. ä¿å­˜æŠ¥å‘Šï¼ˆå¦‚æœæŒ‡å®šè·¯å¾„ï¼‰
    if save_path:
        from pathlib import Path
        save_path = Path(save_path)

        # ä¿å­˜JSONæ ¼å¼
        json_path = save_path.parent / f"{save_path.stem}.json"
        save_diagnosis_report(report, str(json_path), format='json')

        # ä¿å­˜HTMLæ ¼å¼
        html_path = save_path.parent / f"{save_path.stem}.html"
        save_diagnosis_report(report, str(html_path), format='html')

    return report


if __name__ == '__main__':
    # æµ‹è¯•ç¤ºä¾‹
    print("=== æ•°æ®è¯Šæ–­æ¨¡å—æµ‹è¯• ===\n")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    test_data = pd.DataFrame({
        'id': range(1000),
        'feature1': np.random.randn(1000),
        'feature2': np.random.randn(1000) * 10 + 50,
        'feature3': np.random.choice(['A', 'B', 'C'], 1000),
        'target': np.random.choice([0, 1], 1000, p=[0.7, 0.3])
    })

    # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
    test_data.loc[np.random.choice(1000, 50, replace=False), 'feature1'] = np.nan

    # æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼
    test_data.loc[np.random.choice(1000, 10, replace=False), 'feature2'] = 1000

    # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
    report = generate_diagnosis_report(test_data, target='target')

    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
