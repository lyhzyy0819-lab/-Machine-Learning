{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phase 1: æ•°æ®è¯Šæ–­\n\n## ç›®æ ‡\n\nç³»ç»ŸåŒ–è¯Šæ–­é™Œç”Ÿæ•°æ®çš„è´¨é‡å’Œç‰¹å¾ï¼Œä¸ºåç»­å»ºæ¨¡æä¾›å†³ç­–ä¾æ®ã€‚\n\n## è¯Šæ–­æµç¨‹\n\n```\næ•°æ®è§„æ¨¡ â†’ æ•°æ®ç±»å‹ â†’ ç¼ºå¤±å€¼ â†’ å¼‚å¸¸å€¼ â†’ åˆ†å¸ƒç‰¹å¾ â†’ ç›¸å…³æ€§ â†’ è´¨é‡è¯„åˆ†\n```\n\n## è¾“å‡º\n\n- æ•°æ®è¯Šæ–­æŠ¥å‘Šï¼ˆJSON + HTMLï¼‰\n- å¯è§†åŒ–å›¾è¡¨ï¼ˆç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€åˆ†å¸ƒã€ç›¸å…³æ€§ï¼‰\n- æ•°æ®è´¨é‡è¯„åˆ†\n- ä¸‹ä¸€æ­¥å¤„ç†å»ºè®®\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. ç¯å¢ƒå‡†å¤‡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åŸºç¡€åº“\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—\nfrom src import data_diagnosis as dd\nfrom src import visualization as viz\nfrom config import get_default_config\n\n# é…ç½®\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette('husl')\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: f'{x:.4f}')\n\nconfig = get_default_config()\nprint(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ!\")\nprint(f\"é…ç½®æ¨¡å¼: {config.MODE}\")\nprint(f\"è¾“å‡ºç›®å½•: {config.PHASE1_OUTPUT}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. åŠ è½½æ•°æ®\n\nä½¿ç”¨Titanicæ•°æ®é›†ï¼ˆåˆ†ç±»é—®é¢˜ï¼ŒåŒ…å«ç¼ºå¤±å€¼ã€ç±»åˆ«ä¸å¹³è¡¡ç­‰å…¸å‹é—®é¢˜ï¼‰"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åŠ è½½Titanicæ•°æ®é›†\ndf = sns.load_dataset('titanic')\ntarget_col = 'survived'\n\nprint(f\"âœ… æ•°æ®åŠ è½½æˆåŠŸ! å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—\")\nprint(f\"ç›®æ ‡å˜é‡: {target_col}\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. æ•°æ®è§„æ¨¡æ£€æŸ¥\n\næ£€æŸ¥æ ·æœ¬æ•°ã€ç‰¹å¾æ•°ï¼Œåˆ¤æ–­æ•°æ®é‡çº§å¯¹ç®—æ³•é€‰æ‹©çš„å½±å“ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åŸºç¡€ä¿¡æ¯\nbasic_info = dd.basic_info(df, show=True)\n\nn_samples = basic_info['n_samples']\nn_features = basic_info['n_features']\n\n# é‡çº§åˆ¤æ–­\nprint(f\"\\næ•°æ®é‡çº§: \", end=\"\")\nif n_samples < 1000:\n    print(f\"å°æ•°æ®é›†({n_samples}è¡Œ) â†’ å»ºè®®ç®€å•æ¨¡å‹\")\nelif n_samples < 100000:\n    print(f\"ä¸­ç­‰æ•°æ®é›†({n_samples}è¡Œ) â†’ å¯ç”¨é›†æˆå­¦ä¹ \")\nelse:\n    print(f\"å¤§æ•°æ®é›†({n_samples}è¡Œ) â†’ å¯ç”¨LightGBM/æ·±åº¦å­¦ä¹ \")\n\nprint(f\"ç‰¹å¾æ•°é‡: {n_features} â†’ \", end=\"\")\nif n_features / n_samples > 0.1:\n    print(\"ç‰¹å¾ç›¸å¯¹è¾ƒå¤šï¼Œè€ƒè™‘æ­£åˆ™åŒ–/ç‰¹å¾é€‰æ‹©\")\nelse:\n    print(\"æ¯”ä¾‹åˆç†\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. æ•°æ®ç±»å‹è¯†åˆ«\n\nè¯†åˆ«æ•°å€¼å‹ã€ç±»åˆ«å‹ã€IDå‹ã€å¸¸é‡ç­‰ï¼Œå†³å®šé¢„å¤„ç†ç­–ç•¥ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç±»å‹æ¨æ–­\ncolumn_types = dd.infer_column_types(df, categorical_threshold=20)\ncolumn_summary = dd.column_summary(df)\n\ndisplay(column_summary)\n\n# å¤„ç†å»ºè®®\nif column_types['id']:\n    print(f\"âš ï¸  IDåˆ—: {column_types['id']} â†’ å»ºæ¨¡å‰ç§»é™¤\")\nif column_types['constant']:\n    print(f\"âš ï¸  å¸¸é‡åˆ—: {column_types['constant']} â†’ æ— ä¿¡æ¯ï¼Œç§»é™¤\")\nif column_types['categorical']:\n    print(f\"âœ“ ç±»åˆ«å‹({len(column_types['categorical'])}ä¸ª) â†’ éœ€ç¼–ç \")\nif column_types['datetime']:\n    print(f\"âœ“ æ—¶é—´å‹({len(column_types['datetime'])}ä¸ª) â†’ æå–ç‰¹å¾\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. ç¼ºå¤±å€¼åˆ†æ\n\næ£€æµ‹ç¼ºå¤±å€¼æ•°é‡ã€ä½ç½®å’Œæ¨¡å¼ï¼ˆMCAR/MAR/MNARï¼‰ï¼Œå†³å®šå¡«å……æˆ–åˆ é™¤ç­–ç•¥ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç¼ºå¤±å€¼ç»Ÿè®¡\nmissing_stats = dd.missing_value_analysis(df, threshold=0.0)\n\nif len(missing_stats) > 0:\n    # ç¼ºå¤±æ¨¡å¼æ£€æµ‹\n    missing_patterns = dd.detect_missing_pattern(df)\n    \n    # å¯è§†åŒ–\n    viz.plot_missing_bar(df)\n    viz.plot_missing_heatmap(df)\nelse:\n    print(\"âœ… æ— ç¼ºå¤±å€¼\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. å¼‚å¸¸å€¼æ£€æµ‹\n\nä½¿ç”¨IQRå’ŒZ-Scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼ï¼Œè¯„ä¼°å¯¹æ¨¡å‹çš„å½±å“ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ•°å€¼å‹ç‰¹å¾å¼‚å¸¸å€¼æ£€æµ‹\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n\nif len(numeric_cols) > 0:\n    # IQRæ–¹æ³•\n    outlier_report_iqr = dd.outlier_analysis(df, method='iqr', columns=numeric_cols)\n    \n    # Z-Scoreæ–¹æ³•\n    outlier_report_zscore = dd.outlier_analysis(df, method='zscore', columns=numeric_cols)\n    \n    # å¯è§†åŒ–ï¼ˆå‰6ä¸ªç‰¹å¾ï¼‰\n    viz.plot_boxplots(df, columns=numeric_cols[:6])\n    \n    print(\"\\nå¤„ç†å»ºè®®: çº¿æ€§æ¨¡å‹éœ€ç§»é™¤/æˆªæ–­ï¼Œæ ‘æ¨¡å‹å¯ä¿ç•™\")\nelse:\n    print(\"âš ï¸  æ— æ•°å€¼å‹ç‰¹å¾\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. æ•°æ®åˆ†å¸ƒåˆ†æ\n\nåˆ†æååº¦ã€å³°åº¦ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦å˜æ¢ï¼ˆå¯¹æ•°ã€Box-Coxç­‰ï¼‰ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ•°å€¼ç‰¹å¾åˆ†å¸ƒ\nif len(numeric_cols) > 0:\n    dd.distribution_analysis(df, columns=numeric_cols[:6])\nelse:\n    print(\"âš ï¸  æ— æ•°å€¼å‹ç‰¹å¾\")\n\n# ç±»åˆ«ç‰¹å¾åˆ†å¸ƒ\ncategorical_cols = column_types['categorical'] + column_types['binary']\nif len(categorical_cols) > 0:\n    sample_cat_col = categorical_cols[0]\n    viz.plot_categorical_distribution(df, sample_cat_col, top_n=15)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. ç›¸å…³æ€§åˆ†æ\n\nå‘ç°å¼ºç›¸å…³ç‰¹å¾å¯¹ï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰å’Œä¸ç›®æ ‡å˜é‡ç›¸å…³çš„é‡è¦ç‰¹å¾ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç›¸å…³æ€§çƒ­åŠ›å›¾\nif len(numeric_cols) >= 2:\n    corr_matrix = dd.correlation_analysis(df, method='pearson', threshold=0.5)\n    \n    # ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§\n    if target_col in df.columns and target_col in numeric_cols:\n        print(f\"\\nä¸ç›®æ ‡å˜é‡ '{target_col}' çš„ç›¸å…³æ€§ï¼ˆTop 10ï¼‰:\")\n        target_corr = corr_matrix[target_col].drop(target_col).sort_values(key=abs, ascending=False)\n        \n        for feature, corr_val in target_corr.head(10).items():\n            strength = \"å¼º\" if abs(corr_val) > 0.7 else \"ä¸­\" if abs(corr_val) > 0.4 else \"å¼±\"\n            print(f\"  {feature:<20} {corr_val:>7.3f} ({strength})\")\nelse:\n    print(\"âš ï¸  ç‰¹å¾ä¸è¶³ï¼Œæ— æ³•åˆ†æç›¸å…³æ€§\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. æ•°æ®è´¨é‡è¯„åˆ†\n\nç»¼åˆè¯„åˆ†ï¼ˆ0-100ï¼‰ï¼Œåˆ¤æ–­æ•°æ®æ˜¯å¦ready for modelingã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è´¨é‡è¯„åˆ†\nquality_score = dd.calculate_data_quality_score(df, target=target_col)\ntotal_score = quality_score['total_score']\n\nprint(f\"æ•°æ®è´¨é‡è¯„åˆ†: {total_score:.1f}/100 ({quality_score['grade']})\")\n\n# å»ºè®®\nif total_score >= 80:\n    print(\"âœ… è´¨é‡è‰¯å¥½ï¼Œå¯è¿›å…¥Phase 2\")\nelif total_score >= 70:\n    print(\"âš ï¸  è´¨é‡ä¸­ç­‰ï¼Œå»ºè®®å¤„ç†ä¸»è¦é—®é¢˜åå»ºæ¨¡\")\nelse:\n    print(\"âŒ è´¨é‡è¾ƒå·®ï¼Œéœ€é‡ç‚¹æ”¹å–„æ•°æ®\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. ç»¼åˆæ‘˜è¦å›¾"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "viz.plot_data_quality_summary(df, target=target_col)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š\n\nä¿å­˜å®Œæ•´è¯Šæ–­ç»“æœä¸ºJSONå’ŒHTMLæ ¼å¼ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç”ŸæˆæŠ¥å‘Š\nreport_path = config.PHASE1_OUTPUT / 'diagnosis_report'\n\ndiagnosis_report = dd.generate_diagnosis_report(\n    df, \n    target=target_col,\n    save_path=str(report_path)\n)\n\nprint(f\"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ:\")\nprint(f\"   JSON: {report_path}.json\")\nprint(f\"   HTML: {report_path}.html\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. è¯Šæ–­æ€»ç»“ä¸è¡ŒåŠ¨æ¸…å•"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Phase 1 è¯Šæ–­æ€»ç»“\".center(60))\nprint(\"=\" * 60)\n\nprint(f\"\\n1. æ•°æ®è§„æ¨¡: {basic_info['n_samples']:,} è¡Œ Ã— {basic_info['n_features']} åˆ—\")\nprint(f\"2. å†…å­˜: {basic_info['memory_usage_mb']:.2f} MB\")\nprint(f\"3. è´¨é‡è¯„åˆ†: {quality_score['total_score']:.1f}/100 ({quality_score['grade']})\")\nprint(f\"4. ç¼ºå¤±å€¼: {df.isnull().sum().sum()} ä¸ª\")\nprint(f\"5. é‡å¤è¡Œ: {basic_info['duplicated_rows']} è¡Œ\")\n\nif target_col in df.columns:\n    print(f\"\\nç›®æ ‡å˜é‡ '{target_col}':\")\n    if df[target_col].nunique() <= 20:\n        print(f\"  ç±»å‹: åˆ†ç±» ({df[target_col].nunique()} ç±»)\")\n        value_counts = df[target_col].value_counts()\n        max_ratio = value_counts.max() / len(df)\n        print(f\"  ç±»åˆ«å¹³è¡¡: æœ€å¤§ç±»å æ¯” {max_ratio*100:.1f}%\")\n        if max_ratio > 0.8:\n            print(f\"  âš ï¸  ä¸¥é‡ä¸å¹³è¡¡!\")\n    else:\n        print(f\"  ç±»å‹: å›å½’\")\n        print(f\"  èŒƒå›´: [{df[target_col].min():.2f}, {df[target_col].max():.2f}]\")\n\nprint(\"\\n\" + \"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### è¿›å…¥Phase 2å‰çš„å‡†å¤‡å·¥ä½œ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ“‹ Phase 2 å‡†å¤‡æ¸…å•:\")\nprint(\"=\" * 60)\n\ntodo_list = []\n\nif df.isnull().sum().sum() > 0:\n    todo_list.append(\"å¤„ç†ç¼ºå¤±å€¼ï¼ˆå¡«å……/åˆ é™¤ï¼‰\")\nif column_types['id']:\n    todo_list.append(f\"ç§»é™¤IDåˆ—: {', '.join(column_types['id'])}\")\nif column_types['constant']:\n    todo_list.append(f\"ç§»é™¤å¸¸é‡åˆ—: {', '.join(column_types['constant'])}\")\nif basic_info['duplicated_rows'] > 0:\n    todo_list.append(f\"ç§»é™¤{basic_info['duplicated_rows']}è¡Œé‡å¤æ•°æ®\")\nif column_types['categorical']:\n    todo_list.append(f\"ç¼–ç {len(column_types['categorical'])}ä¸ªç±»åˆ«ç‰¹å¾\")\nif len(numeric_cols) > 0:\n    todo_list.append(\"å†³å®šå¼‚å¸¸å€¼å¤„ç†ç­–ç•¥\")\n\n# ç±»åˆ«ä¸å¹³è¡¡æ£€æŸ¥\nif target_col in df.columns and df[target_col].nunique() <= 20:\n    value_counts = df[target_col].value_counts()\n    if value_counts.max() / len(df) > 0.7:\n        todo_list.append(\"å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆSMOTE/æƒé‡è°ƒæ•´ï¼‰\")\n\nif todo_list:\n    for i, task in enumerate(todo_list, 1):\n        print(f\"  {i}. {task}\")\nelse:\n    print(\"  âœ… æ— éœ€ç‰¹æ®Šå¤„ç†ï¼Œå¯ç›´æ¥å»ºæ¨¡\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\\nâœ… Phase 1 å®Œæˆ! ä¸‹ä¸€æ­¥: phase2_quick_baseline.ipynb\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## å‚è€ƒèµ„æ–™\n\n- **å·¥ä½œæµæŒ‡å—**: `../ML_WORKFLOW_GUIDE.md` - æ•°æ®è¯Šæ–­å†³ç­–æ ‘\n- **è¯¦ç»†æ–¹æ³•**: `../01_data_diagnosis_framework/data_problem_to_solution_mapping.md`\n- **å¿«é€Ÿå‚è€ƒ**: `../01_data_diagnosis_framework/data_diagnosis_quick_reference.md`\n\n---\n\n**ä¸‹ä¸€æ­¥**: [Phase 2: å¿«é€ŸBaseline](phase2_quick_baseline.ipynb)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}