{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Integrated Approach (ç›‘ç£+æ— ç›‘ç£æ··åˆæ–¹æ¡ˆ)\n",
    "\n",
    "## ç›®æ ‡\n",
    "\n",
    "æ•´åˆPhase 3çš„ç›‘ç£å­¦ä¹ å’ŒPhase 4çš„æ— ç›‘ç£æ´å¯Ÿï¼Œæ¢ç´¢æ··åˆæ–¹æ¡ˆèƒ½å¦æå‡æ€§èƒ½ã€‚\n",
    "\n",
    "## 4ç§æ··åˆç­–ç•¥\n",
    "\n",
    "1. **èšç±»æ ‡ç­¾ä½œä¸ºç‰¹å¾** - å°†èšç±»IDåŠ å…¥ç‰¹å¾é›†\n",
    "2. **å¼‚å¸¸åˆ†æ•°ä½œä¸ºç‰¹å¾** - åˆ©ç”¨å¼‚å¸¸æ£€æµ‹ç»“æœ\n",
    "3. **PCAé™ç»´åå»ºæ¨¡** - åœ¨é™ç»´ç©ºé—´è®­ç»ƒ\n",
    "4. **åˆ†ç¾¤å»ºæ¨¡** - ä¸ºæ¯ä¸ªèšç±»è®­ç»ƒç‹¬ç«‹æ¨¡å‹\n",
    "\n",
    "## é¢„æœŸç»“æœ\n",
    "\n",
    "- è‡³å°‘1ç§æ–¹æ¡ˆæ€§èƒ½ â‰¥ Phase 3æœ€ä½³æ¨¡å‹\n",
    "- ç†è§£æ··åˆæ–¹æ¡ˆçš„é€‚ç”¨åœºæ™¯\n",
    "- é€‰æ‹©æœ€ä½³æ–¹æ¡ˆè¿›å…¥Phase 6\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MLåº“\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# è‡ªå®šä¹‰æ¨¡å—\n",
    "from src import data_preprocessing as dp\n",
    "from src import model_evaluation as me\n",
    "from src import visualization as viz\n",
    "from config import get_default_config\n",
    "\n",
    "# é…ç½®\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "config = get_default_config()\n",
    "RANDOM_STATE = config.RANDOM_STATE\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ!\")\n",
    "print(f\"éšæœºç§å­: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å›é¡¾Phase 3å’ŒPhase 4\n",
    "\n",
    "åŠ è½½å‰ç½®Phaseçš„ç»“æœï¼Œå»ºç«‹æ€§èƒ½åŸºçº¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®ï¼ˆé‡æ–°é¢„å¤„ç†ï¼Œä¿æŒä¸€è‡´æ€§ï¼‰\n",
    "df = sns.load_dataset('titanic')\n",
    "target_col = 'survived'\n",
    "\n",
    "# å¿«é€Ÿé¢„å¤„ç†ï¼ˆä¸Phase 3ä¸€è‡´ï¼‰\n",
    "X, y = dp.quick_preprocess(\n",
    "    df, \n",
    "    target=target_col,\n",
    "    handle_missing='simple',\n",
    "    encode_categorical='onehot',\n",
    "    scale_features=True\n",
    ")\n",
    "\n",
    "# æ•°æ®åˆ’åˆ†ï¼ˆä¸Phase 3ä¸€è‡´ï¼‰\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {X_train.shape}\")\n",
    "print(f\"éªŒè¯é›†: {X_val.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 Baseline: è®­ç»ƒä¸€ä¸ªæ ‡å‡†çš„éšæœºæ£®æ—ä½œä¸ºå¯¹æ¯”åŸºå‡†\n",
    "print(\"Phase 3 Baseline (æ ‡å‡†ç›‘ç£å­¦ä¹ ):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "baseline_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "baseline_val_score = baseline_model.score(X_val, y_val)\n",
    "baseline_test_score = baseline_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"éªŒè¯é›†å‡†ç¡®ç‡: {baseline_val_score:.4f}\")\n",
    "print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {baseline_test_score:.4f}\")\n",
    "print(\"\\nè¿™æ˜¯Phase 5è¦è¶…è¶Šçš„åŸºå‡†!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 4å›é¡¾: å‡†å¤‡æ— ç›‘ç£ç‰¹å¾\n",
    "\n",
    "ç”±äºPhase 4å¯èƒ½å°šæœªè¿è¡Œæˆ–ä¿å­˜ç‰¹å¾ï¼Œè¿™é‡Œæˆ‘ä»¬å¿«é€Ÿç”Ÿæˆæ— ç›‘ç£ç‰¹å¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "print(\"ç”ŸæˆPhase 4çš„æ— ç›‘ç£ç‰¹å¾...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. K-Meansèšç±»ï¼ˆ3ä¸ªç°‡ï¼‰\n",
    "kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE)\n",
    "cluster_labels_train = kmeans.fit_predict(X_train)\n",
    "cluster_labels_val = kmeans.predict(X_val)\n",
    "cluster_labels_test = kmeans.predict(X_test)\n",
    "\n",
    "print(f\"âœ“ K-Meansèšç±»å®Œæˆ: {len(np.unique(cluster_labels_train))} ä¸ªç°‡\")\n",
    "\n",
    "# 2. Isolation Forestå¼‚å¸¸æ£€æµ‹\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=RANDOM_STATE)\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "anomaly_scores_train = iso_forest.score_samples(X_train)\n",
    "anomaly_scores_val = iso_forest.score_samples(X_val)\n",
    "anomaly_scores_test = iso_forest.score_samples(X_test)\n",
    "\n",
    "print(f\"âœ“ å¼‚å¸¸æ£€æµ‹å®Œæˆ: å¼‚å¸¸åˆ†æ•°èŒƒå›´ [{anomaly_scores_train.min():.2f}, {anomaly_scores_train.max():.2f}]\")\n",
    "\n",
    "# 3. PCAé™ç»´ï¼ˆä¿ç•™95%æ–¹å·®ï¼‰\n",
    "pca = PCA(n_components=0.95, random_state=RANDOM_STATE)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(f\"âœ“ PCAé™ç»´å®Œæˆ: {X_train.shape[1]} â†’ {X_train_pca.shape[1]} ç»´\")\n",
    "print(f\"  ä¿ç•™æ–¹å·®æ¯”: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "print(\"\\nâœ… æ— ç›‘ç£ç‰¹å¾å‡†å¤‡å®Œæ¯•!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. æ–¹æ¡ˆ1: èšç±»æ ‡ç­¾ä½œä¸ºç‰¹å¾\n",
    "\n",
    "å°†K-Meansèšç±»çš„ç°‡IDä½œä¸ºæ–°ç‰¹å¾ï¼ˆOne-Hotç¼–ç ï¼‰åŠ å…¥è®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æ–¹æ¡ˆ1: èšç±»æ ‡ç­¾ä½œä¸ºç‰¹å¾\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# One-Hotç¼–ç èšç±»æ ‡ç­¾\n",
    "cluster_df_train = pd.get_dummies(cluster_labels_train, prefix='cluster')\n",
    "cluster_df_val = pd.get_dummies(cluster_labels_val, prefix='cluster')\n",
    "cluster_df_test = pd.get_dummies(cluster_labels_test, prefix='cluster')\n",
    "\n",
    "# åˆå¹¶åˆ°åŸç‰¹å¾é›†\n",
    "X_train_cluster = np.hstack([X_train, cluster_df_train.values])\n",
    "X_val_cluster = np.hstack([X_val, cluster_df_val.values])\n",
    "X_test_cluster = np.hstack([X_test, cluster_df_test.values])\n",
    "\n",
    "print(f\"ç‰¹å¾æ•°é‡: {X_train.shape[1]} â†’ {X_train_cluster.shape[1]} (+{cluster_df_train.shape[1]} ä¸ªèšç±»ç‰¹å¾)\")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model_cluster = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "model_cluster.fit(X_train_cluster, y_train)\n",
    "\n",
    "# è¯„ä¼°\n",
    "val_score_cluster = model_cluster.score(X_val_cluster, y_val)\n",
    "test_score_cluster = model_cluster.score(X_test_cluster, y_test)\n",
    "\n",
    "print(f\"\\néªŒè¯é›†å‡†ç¡®ç‡: {val_score_cluster:.4f} (Baseline: {baseline_val_score:.4f})\")\n",
    "print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score_cluster:.4f} (Baseline: {baseline_test_score:.4f})\")\n",
    "\n",
    "improvement_1 = (val_score_cluster - baseline_val_score) / baseline_val_score * 100\n",
    "print(f\"æ€§èƒ½å˜åŒ–: {improvement_1:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. æ–¹æ¡ˆ2: å¼‚å¸¸åˆ†æ•°ä½œä¸ºç‰¹å¾\n",
    "\n",
    "å°†Isolation Forestçš„å¼‚å¸¸åˆ†æ•°æ ‡å‡†åŒ–åä½œä¸ºæ–°ç‰¹å¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æ–¹æ¡ˆ2: å¼‚å¸¸åˆ†æ•°ä½œä¸ºç‰¹å¾\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ ‡å‡†åŒ–å¼‚å¸¸åˆ†æ•°\n",
    "scaler_anomaly = StandardScaler()\n",
    "anomaly_scaled_train = scaler_anomaly.fit_transform(anomaly_scores_train.reshape(-1, 1))\n",
    "anomaly_scaled_val = scaler_anomaly.transform(anomaly_scores_val.reshape(-1, 1))\n",
    "anomaly_scaled_test = scaler_anomaly.transform(anomaly_scores_test.reshape(-1, 1))\n",
    "\n",
    "# åˆå¹¶åˆ°åŸç‰¹å¾é›†\n",
    "X_train_anomaly = np.hstack([X_train, anomaly_scaled_train])\n",
    "X_val_anomaly = np.hstack([X_val, anomaly_scaled_val])\n",
    "X_test_anomaly = np.hstack([X_test, anomaly_scaled_test])\n",
    "\n",
    "print(f\"ç‰¹å¾æ•°é‡: {X_train.shape[1]} â†’ {X_train_anomaly.shape[1]} (+1 ä¸ªå¼‚å¸¸åˆ†æ•°ç‰¹å¾)\")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model_anomaly = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "model_anomaly.fit(X_train_anomaly, y_train)\n",
    "\n",
    "# è¯„ä¼°\n",
    "val_score_anomaly = model_anomaly.score(X_val_anomaly, y_val)\n",
    "test_score_anomaly = model_anomaly.score(X_test_anomaly, y_test)\n",
    "\n",
    "print(f\"\\néªŒè¯é›†å‡†ç¡®ç‡: {val_score_anomaly:.4f} (Baseline: {baseline_val_score:.4f})\")\n",
    "print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score_anomaly:.4f} (Baseline: {baseline_test_score:.4f})\")\n",
    "\n",
    "improvement_2 = (val_score_anomaly - baseline_val_score) / baseline_val_score * 100\n",
    "print(f\"æ€§èƒ½å˜åŒ–: {improvement_2:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. æ–¹æ¡ˆ3: PCAé™ç»´åå»ºæ¨¡\n",
    "\n",
    "åœ¨é™ç»´ç©ºé—´è®­ç»ƒæ¨¡å‹ï¼Œå‡å°‘ç‰¹å¾æ•°é‡ï¼Œå¯èƒ½é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æ–¹æ¡ˆ3: PCAé™ç»´åå»ºæ¨¡\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"ç‰¹å¾æ•°é‡: {X_train.shape[1]} â†’ {X_train_pca.shape[1]} (é™ç»´{X_train.shape[1] - X_train_pca.shape[1]}ç»´)\")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model_pca = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "train_time_pca = time.time() - start_time\n",
    "\n",
    "# è¯„ä¼°\n",
    "val_score_pca = model_pca.score(X_val_pca, y_val)\n",
    "test_score_pca = model_pca.score(X_test_pca, y_test)\n",
    "\n",
    "print(f\"\\néªŒè¯é›†å‡†ç¡®ç‡: {val_score_pca:.4f} (Baseline: {baseline_val_score:.4f})\")\n",
    "print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score_pca:.4f} (Baseline: {baseline_test_score:.4f})\")\n",
    "print(f\"è®­ç»ƒæ—¶é—´: {train_time_pca:.3f}ç§’\")\n",
    "\n",
    "improvement_3 = (val_score_pca - baseline_val_score) / baseline_val_score * 100\n",
    "print(f\"æ€§èƒ½å˜åŒ–: {improvement_3:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. æ–¹æ¡ˆ4: åˆ†ç¾¤å»ºæ¨¡\n",
    "\n",
    "ä¸ºæ¯ä¸ªèšç±»ç°‡è®­ç»ƒç‹¬ç«‹æ¨¡å‹ï¼Œé¢„æµ‹æ—¶æ ¹æ®æ ·æœ¬æ‰€å±ç°‡é€‰æ‹©å¯¹åº”æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æ–¹æ¡ˆ4: åˆ†ç¾¤å»ºæ¨¡\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_clusters = len(np.unique(cluster_labels_train))\n",
    "print(f\"è®­ç»ƒ{n_clusters}ä¸ªç°‡çš„ç‹¬ç«‹æ¨¡å‹...\\n\")\n",
    "\n",
    "# ä¸ºæ¯ä¸ªç°‡è®­ç»ƒæ¨¡å‹\n",
    "cluster_models = {}\n",
    "cluster_sizes = {}\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    # è·å–è¯¥ç°‡çš„æ•°æ®\n",
    "    mask_train = cluster_labels_train == cluster_id\n",
    "    X_cluster = X_train[mask_train]\n",
    "    y_cluster = y_train[mask_train]\n",
    "    \n",
    "    cluster_sizes[cluster_id] = len(X_cluster)\n",
    "    \n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=50,  # å‡å°‘æ ‘æ•°é‡ï¼ˆç°‡å†…æ ·æœ¬è¾ƒå°‘ï¼‰\n",
    "        max_depth=8,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    model.fit(X_cluster, y_cluster)\n",
    "    cluster_models[cluster_id] = model\n",
    "    \n",
    "    print(f\"  ç°‡{cluster_id}: {len(X_cluster)} ä¸ªæ ·æœ¬\")\n",
    "\n",
    "print(\"\\nâœ“ æ‰€æœ‰ç°‡çš„æ¨¡å‹è®­ç»ƒå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„æµ‹å‡½æ•°\n",
    "def predict_by_cluster(X, cluster_labels, cluster_models):\n",
    "    \"\"\"æ ¹æ®èšç±»æ ‡ç­¾é€‰æ‹©å¯¹åº”æ¨¡å‹è¿›è¡Œé¢„æµ‹\"\"\"\n",
    "    predictions = np.zeros(len(X))\n",
    "    \n",
    "    for cluster_id, model in cluster_models.items():\n",
    "        mask = cluster_labels == cluster_id\n",
    "        if mask.sum() > 0:\n",
    "            predictions[mask] = model.predict(X[mask])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# éªŒè¯é›†é¢„æµ‹\n",
    "y_val_pred = predict_by_cluster(X_val, cluster_labels_val, cluster_models)\n",
    "val_score_cluster_models = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# æµ‹è¯•é›†é¢„æµ‹\n",
    "y_test_pred = predict_by_cluster(X_test, cluster_labels_test, cluster_models)\n",
    "test_score_cluster_models = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"éªŒè¯é›†å‡†ç¡®ç‡: {val_score_cluster_models:.4f} (Baseline: {baseline_val_score:.4f})\")\n",
    "print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score_cluster_models:.4f} (Baseline: {baseline_test_score:.4f})\")\n",
    "\n",
    "improvement_4 = (val_score_cluster_models - baseline_val_score) / baseline_val_score * 100\n",
    "print(f\"æ€§èƒ½å˜åŒ–: {improvement_4:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. æ–¹æ¡ˆå¯¹æ¯”ä¸æœ€ä½³é€‰æ‹©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‡æ€»æ‰€æœ‰æ–¹æ¡ˆçš„ç»“æœ\n",
    "comparison_df = pd.DataFrame({\n",
    "    'æ–¹æ¡ˆ': [\n",
    "        'Baseline (Phase 3)',\n",
    "        'æ–¹æ¡ˆ1: èšç±»æ ‡ç­¾ç‰¹å¾',\n",
    "        'æ–¹æ¡ˆ2: å¼‚å¸¸åˆ†æ•°ç‰¹å¾',\n",
    "        'æ–¹æ¡ˆ3: PCAé™ç»´',\n",
    "        'æ–¹æ¡ˆ4: åˆ†ç¾¤å»ºæ¨¡'\n",
    "    ],\n",
    "    'éªŒè¯é›†å‡†ç¡®ç‡': [\n",
    "        baseline_val_score,\n",
    "        val_score_cluster,\n",
    "        val_score_anomaly,\n",
    "        val_score_pca,\n",
    "        val_score_cluster_models\n",
    "    ],\n",
    "    'æµ‹è¯•é›†å‡†ç¡®ç‡': [\n",
    "        baseline_test_score,\n",
    "        test_score_cluster,\n",
    "        test_score_anomaly,\n",
    "        test_score_pca,\n",
    "        test_score_cluster_models\n",
    "    ],\n",
    "    'æ€§èƒ½å˜åŒ–%': [\n",
    "        0.0,\n",
    "        improvement_1,\n",
    "        improvement_2,\n",
    "        improvement_3,\n",
    "        improvement_4\n",
    "    ],\n",
    "    'ç‰¹å¾æ•°': [\n",
    "        X_train.shape[1],\n",
    "        X_train_cluster.shape[1],\n",
    "        X_train_anomaly.shape[1],\n",
    "        X_train_pca.shape[1],\n",
    "        X_train.shape[1]\n",
    "    ]\n",
    "})\n",
    "\n",
    "# æŒ‰éªŒè¯é›†å‡†ç¡®ç‡æ’åº\n",
    "comparison_df = comparison_df.sort_values('éªŒè¯é›†å‡†ç¡®ç‡', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Phase 5 æ··åˆæ–¹æ¡ˆå¯¹æ¯”\".center(80))\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# å·¦å›¾: éªŒè¯é›†å‡†ç¡®ç‡å¯¹æ¯”\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.barh(comparison_df['æ–¹æ¡ˆ'], comparison_df['éªŒè¯é›†å‡†ç¡®ç‡'])\n",
    "bars1[0].set_color('gray')  # Baselineç”¨ç°è‰²\n",
    "ax1.set_xlabel('éªŒè¯é›†å‡†ç¡®ç‡', fontsize=12)\n",
    "ax1.set_title('éªŒè¯é›†æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "ax1.axvline(baseline_val_score, color='red', linestyle='--', label='Baseline')\n",
    "ax1.legend()\n",
    "\n",
    "# å³å›¾: æ€§èƒ½å˜åŒ–ç™¾åˆ†æ¯”\n",
    "ax2 = axes[1]\n",
    "colors = ['gray' if x == 0 else 'green' if x > 0 else 'red' \n",
    "          for x in comparison_df['æ€§èƒ½å˜åŒ–%']]\n",
    "bars2 = ax2.barh(comparison_df['æ–¹æ¡ˆ'], comparison_df['æ€§èƒ½å˜åŒ–%'], color=colors)\n",
    "ax2.set_xlabel('æ€§èƒ½å˜åŒ– (%)', fontsize=12)\n",
    "ax2.set_title('ç›¸å¯¹Baselineçš„æ€§èƒ½å˜åŒ–', fontsize=14, fontweight='bold')\n",
    "ax2.axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nå›¾ä¾‹: ç»¿è‰²=æ€§èƒ½æå‡, çº¢è‰²=æ€§èƒ½ä¸‹é™, ç°è‰²=Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ\n",
    "best_method = comparison_df.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"ğŸ† \" * 20)\n",
    "print(f\"\\næœ€ä½³æ–¹æ¡ˆ: {best_method['æ–¹æ¡ˆ']}\")\n",
    "print(f\"éªŒè¯é›†å‡†ç¡®ç‡: {best_method['éªŒè¯é›†å‡†ç¡®ç‡']:.4f}\")\n",
    "print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {best_method['æµ‹è¯•é›†å‡†ç¡®ç‡']:.4f}\")\n",
    "print(f\"æ€§èƒ½å˜åŒ–: {best_method['æ€§èƒ½å˜åŒ–%']:+.2f}%\")\n",
    "\n",
    "if best_method['æ€§èƒ½å˜åŒ–%'] > 0:\n",
    "    print(f\"\\nâœ… æ··åˆæ–¹æ¡ˆæˆåŠŸæå‡äº†æ€§èƒ½!\")\n",
    "elif best_method['æ€§èƒ½å˜åŒ–%'] == 0:\n",
    "    print(f\"\\nâœ“ æ··åˆæ–¹æ¡ˆä¸BaselineæŒå¹³ï¼ˆè¿™ä¹Ÿæ˜¯æ­£å¸¸ç»“æœï¼‰\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  æ··åˆæ–¹æ¡ˆæœªèƒ½æå‡æ€§èƒ½ï¼ˆå¯èƒ½æ•°æ®é›†è¾ƒç®€å•ï¼ŒBaselineå·²è¶³å¤Ÿï¼‰\")\n",
    "\n",
    "print(\"\\n\" + \"ğŸ† \" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Phase 5 æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Phase 5 æ··åˆæ–¹æ¡ˆæ€»ç»“\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. å®éªŒäº†4ç§æ··åˆæ–¹æ¡ˆ:\")\n",
    "print(\"   âœ“ æ–¹æ¡ˆ1: èšç±»æ ‡ç­¾ç‰¹å¾ - å¢åŠ ç°‡å½’å±ä¿¡æ¯\")\n",
    "print(\"   âœ“ æ–¹æ¡ˆ2: å¼‚å¸¸åˆ†æ•°ç‰¹å¾ - åˆ©ç”¨å¼‚å¸¸æ£€æµ‹ç»“æœ\")\n",
    "print(\"   âœ“ æ–¹æ¡ˆ3: PCAé™ç»´å»ºæ¨¡ - å‡å°‘ç‰¹å¾ç»´åº¦\")\n",
    "print(\"   âœ“ æ–¹æ¡ˆ4: åˆ†ç¾¤å»ºæ¨¡ - ä¸ºæ¯ä¸ªç°‡ç‹¬ç«‹å»ºæ¨¡\")\n",
    "\n",
    "print(\"\\n2. æ€§èƒ½å¯¹æ¯”:\")\n",
    "best_improvement = comparison_df[comparison_df['æ–¹æ¡ˆ'] != 'Baseline (Phase 3)']['æ€§èƒ½å˜åŒ–%'].max()\n",
    "worst_improvement = comparison_df[comparison_df['æ–¹æ¡ˆ'] != 'Baseline (Phase 3)']['æ€§èƒ½å˜åŒ–%'].min()\n",
    "print(f\"   æœ€ä½³æå‡: {best_improvement:+.2f}%\")\n",
    "print(f\"   æœ€å·®å˜åŒ–: {worst_improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\n3. å…³é”®å‘ç°:\")\n",
    "if best_improvement > 1:\n",
    "    print(\"   âœ… æ··åˆæ–¹æ¡ˆæ˜¾è‘—æå‡äº†æ€§èƒ½\")\n",
    "    print(\"   â†’ å»ºè®®: å°†æœ€ä½³æ–¹æ¡ˆç”¨äºPhase 6æœ€ç»ˆæ¨¡å‹\")\n",
    "elif best_improvement > 0:\n",
    "    print(\"   âœ“ æ··åˆæ–¹æ¡ˆæœ‰è½»å¾®æå‡\")\n",
    "    print(\"   â†’ å»ºè®®: å¯è€ƒè™‘é‡‡ç”¨ï¼Œä½†æå‡æœ‰é™\")\n",
    "else:\n",
    "    print(\"   âš ï¸  æ··åˆæ–¹æ¡ˆæœªèƒ½æ”¹å–„æ€§èƒ½\")\n",
    "    print(\"   â†’ åŸå› : å¯èƒ½æ•°æ®é›†è¾ƒç®€å•ï¼ŒBaselineå·²æ¥è¿‘æœ€ä¼˜\")\n",
    "    print(\"   â†’ å»ºè®®: Phase 6ç»§ç»­ä½¿ç”¨Phase 3çš„æœ€ä½³æ¨¡å‹\")\n",
    "\n",
    "print(\"\\n4. é€‚ç”¨åœºæ™¯æ€»ç»“:\")\n",
    "print(\"   - èšç±»æ ‡ç­¾ç‰¹å¾: æ•°æ®æœ‰æ˜æ˜¾å­ç¾¤ä½“æ—¶\")\n",
    "print(\"   - å¼‚å¸¸åˆ†æ•°ç‰¹å¾: å¼‚å¸¸æ ·æœ¬æœ‰ç‰¹æ®Šæ¨¡å¼æ—¶\")\n",
    "print(\"   - PCAé™ç»´: ç‰¹å¾é«˜ç»´ä¸”æœ‰å†—ä½™æ—¶\")\n",
    "print(\"   - åˆ†ç¾¤å»ºæ¨¡: ä¸åŒå­ç¾¤ä½“è§„å¾‹å·®å¼‚å¤§æ—¶\")\n",
    "\n",
    "print(\"\\n5. ä¸‹ä¸€æ­¥:\")\n",
    "print(f\"   â†’ è¿›å…¥Phase 6ï¼Œä½¿ç”¨ '{best_method['æ–¹æ¡ˆ']}' æ„å»ºæœ€ç»ˆPipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Phase 5 å®Œæˆ! ä¸‹ä¸€æ­¥: phase6_final_solution.ipynb\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒèµ„æ–™\n",
    "\n",
    "- **Phase 3ç»“æœ**: `phase3_supervised_solution.ipynb` - ç›‘ç£å­¦ä¹ æœ€ä½³æ¨¡å‹\n",
    "- **Phase 4ç»“æœ**: `phase4_unsupervised_insights.ipynb` - æ— ç›‘ç£å­¦ä¹ æ´å¯Ÿ\n",
    "- **å·¥ä½œæµæŒ‡å—**: `../ML_WORKFLOW_GUIDE.md`\n",
    "\n",
    "---\n",
    "\n",
    "**ä¸‹ä¸€æ­¥**: [Phase 6: æœ€ç»ˆæ–¹æ¡ˆ](phase6_final_solution.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
