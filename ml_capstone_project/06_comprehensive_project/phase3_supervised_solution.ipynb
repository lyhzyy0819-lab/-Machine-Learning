{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Supervised Solutionï¼ˆç›‘ç£å­¦ä¹ æ·±åº¦ä¼˜åŒ–ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ æœ¬é˜¶æ®µç›®æ ‡\n",
    "\n",
    "åœ¨Phase 2åŸºçº¿æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡**é«˜çº§ç‰¹å¾å·¥ç¨‹ã€ç²¾ç»†é¢„å¤„ç†ã€è¶…å‚æ•°è°ƒä¼˜**æ¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚\n",
    "\n",
    "### æ ¸å¿ƒä»»åŠ¡\n",
    "\n",
    "1. âœ… **å›é¡¾Phase 2åŸºçº¿æ€§èƒ½**\n",
    "2. âœ… **é«˜çº§ç‰¹å¾å·¥ç¨‹**ï¼šäº¤äº’ç‰¹å¾ã€å¤šé¡¹å¼ç‰¹å¾ã€ç‰¹å¾é€‰æ‹©\n",
    "3. âœ… **é«˜çº§æ•°æ®é¢„å¤„ç†**ï¼šKNNå¡«å……ã€è¿­ä»£å¡«å……ã€é«˜çº§ç¼–ç \n",
    "4. âœ… **è¶…å‚æ•°è°ƒä¼˜**ï¼šç½‘æ ¼æœç´¢/éšæœºæœç´¢\n",
    "5. âœ… **æ¨¡å‹è¯Šæ–­**ï¼šå­¦ä¹ æ›²çº¿ã€ç‰¹å¾é‡è¦æ€§ã€é”™è¯¯åˆ†æ\n",
    "6. âœ… **æ¨¡å‹é›†æˆ**ï¼ˆå¯é€‰ï¼‰ï¼šVotingã€Stacking\n",
    "7. âœ… **æ€§èƒ½å¯¹æ¯”**ï¼šPhase 2 vs Phase 3\n",
    "\n",
    "### æ–¹æ³•è®º\n",
    "\n",
    "- **æ•°æ®é©±åŠ¨**ï¼šåŸºäºPhase 1çš„æ•°æ®è¯Šæ–­ç»“æœå†³å®šä¼˜åŒ–ç­–ç•¥\n",
    "- **è¿­ä»£ä¼˜åŒ–**ï¼šé€æ­¥å°è¯•ä¸åŒçš„ä¼˜åŒ–æ–¹æ³•ï¼Œè§‚å¯Ÿæ€§èƒ½å˜åŒ–\n",
    "- **é¿å…è¿‡æ‹Ÿåˆ**ï¼šä½¿ç”¨éªŒè¯é›†ç›‘æ§ï¼Œé˜²æ­¢ä¼˜åŒ–è¿‡åº¦\n",
    "- **å¯è§£é‡Šæ€§**ï¼šå…³æ³¨ç‰¹å¾é‡è¦æ€§ï¼Œç†è§£æ¨¡å‹å†³ç­–\n",
    "\n",
    "---\n",
    "\n",
    "## é¢„æœŸæ—¶é—´\n",
    "\n",
    "â±ï¸ **30-90åˆ†é’Ÿ**ï¼ˆå–å†³äºæ•°æ®é›†å¤§å°å’Œè°ƒä¼˜æ·±åº¦ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# å¯¼å…¥æœ¬é¡¹ç›®çš„æ¨¡å—\n",
    "from src import data_preprocessing as dp\n",
    "from src import supervised_pipeline as sp\n",
    "from src import model_evaluation as me\n",
    "from src import feature_engineering as fe\n",
    "from src import visualization as viz\n",
    "\n",
    "# è®¾ç½®\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# éšæœºç§å­\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ\")\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. æ•°æ®åŠ è½½ä¸Phase 2å›é¡¾\n",
    "\n",
    "### 2.1 åŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åŸå§‹æ•°æ®ï¼ˆä¸Phase 2ç›¸åŒï¼‰\n",
    "df = sns.load_dataset('titanic')\n",
    "target_col = 'survived'\n",
    "\n",
    "print(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "print(f\"ç›®æ ‡å˜é‡: {target_col}\")\n",
    "\n",
    "# è®°å½•åŸå§‹æ•°æ®ä¿¡æ¯ç”¨äºåç»­å¯¹æ¯”\n",
    "original_shape = df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Phase 2 åŸºçº¿å›é¡¾\n",
    "\n",
    "æˆ‘ä»¬å…ˆç”¨Phase 2çš„ç®€å•é¢„å¤„ç†æ–¹æ³•å»ºç«‹åŸºçº¿ï¼Œä½œä¸ºå¯¹æ¯”åŸºå‡†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 ç®€å•é¢„å¤„ç†ï¼ˆç”¨äºå¯¹æ¯”ï¼‰\n",
    "print(\"=\"*60)\n",
    "print(\"Phase 2 åŸºçº¿é¢„å¤„ç†ï¼ˆå¿«é€Ÿæ–¹æ³•ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_baseline, y_baseline = dp.quick_preprocess(\n",
    "    df.copy(), \n",
    "    target_col=target_col,\n",
    "    drop_missing_threshold=0.5,\n",
    "    handle_categorical=True,\n",
    "    scale_features=True\n",
    ")\n",
    "\n",
    "print(f\"\\nPhase 2 åŸºçº¿ç‰¹å¾æ•°: {X_baseline.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ’åˆ†æ•°æ®é›†ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰\n",
    "X_train_base, X_val_base, X_test_base, y_train_base, y_val_base, y_test_base = dp.split_data(\n",
    "    X_baseline, y_baseline,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=True\n",
    ")\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {X_train_base.shape}\")\n",
    "print(f\"éªŒè¯é›†: {X_val_base.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {X_test_base.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 åŸºçº¿æ¨¡å‹æ€§èƒ½ï¼ˆå¿«é€Ÿè®­ç»ƒä¸€ä¸ªæ¨¡å‹ä½œä¸ºå‚è€ƒï¼‰\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "baseline_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "baseline_model.fit(X_train_base, y_train_base)\n",
    "\n",
    "y_val_pred_base = baseline_model.predict(X_val_base)\n",
    "baseline_val_acc = accuracy_score(y_val_base, y_val_pred_base)\n",
    "baseline_val_f1 = f1_score(y_val_base, y_val_pred_base, average='binary')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 2 åŸºçº¿æ€§èƒ½ï¼ˆRandomForestï¼‰\")\n",
    "print(\"=\"*60)\n",
    "print(f\"éªŒè¯é›† Accuracy: {baseline_val_acc:.4f}\")\n",
    "print(f\"éªŒè¯é›† F1-Score: {baseline_val_f1:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ä¿å­˜åŸºçº¿æ€§èƒ½ç”¨äºåç»­å¯¹æ¯”\n",
    "baseline_metrics = {\n",
    "    'accuracy': baseline_val_acc,\n",
    "    'f1': baseline_val_f1,\n",
    "    'n_features': X_baseline.shape[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Phase 3 é«˜çº§é¢„å¤„ç†\n",
    "\n",
    "### 3.1 é«˜çº§ç¼ºå¤±å€¼å¤„ç†\n",
    "\n",
    "Phase 2ä½¿ç”¨ç®€å•å¡«å……ï¼ˆä¸­ä½æ•°/ä¼—æ•°ï¼‰ï¼ŒPhase 3ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°åŠ è½½æ•°æ®ï¼Œè¿›è¡Œé«˜çº§é¢„å¤„ç†\n",
    "df_advanced = df.copy()\n",
    "\n",
    "# å…ˆåˆ†ç¦»ç›®æ ‡å˜é‡\n",
    "y = df_advanced[target_col]\n",
    "X = df_advanced.drop(columns=[target_col])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Phase 3 é«˜çº§é¢„å¤„ç†\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åˆ é™¤æ˜æ˜¾æ— ç”¨çš„åˆ—\n",
    "drop_cols = []\n",
    "for col in X.columns:\n",
    "    # åˆ é™¤å”¯ä¸€å€¼è¿‡å¤šçš„åˆ—ï¼ˆIDåˆ—ï¼‰\n",
    "    if X[col].nunique() / len(X) > 0.95:\n",
    "        drop_cols.append(col)\n",
    "        \n",
    "if drop_cols:\n",
    "    print(f\"\\nåˆ é™¤IDåˆ—: {drop_cols}\")\n",
    "    X = X.drop(columns=drop_cols)\n",
    "\n",
    "# åˆ é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„åˆ—ï¼ˆ>50%ï¼‰\n",
    "missing_rate = X.isnull().sum() / len(X)\n",
    "high_missing_cols = missing_rate[missing_rate > 0.5].index.tolist()\n",
    "\n",
    "if high_missing_cols:\n",
    "    print(f\"åˆ é™¤é«˜ç¼ºå¤±ç‡åˆ—: {high_missing_cols}\")\n",
    "    X = X.drop(columns=high_missing_cols)\n",
    "\n",
    "print(f\"\\né¢„å¤„ç†åå½¢çŠ¶: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†ç¦»æ•°å€¼å‹å’Œç±»åˆ«å‹ç‰¹å¾\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"æ•°å€¼å‹ç‰¹å¾: {len(numeric_cols)} ä¸ª\")\n",
    "print(f\"  {numeric_cols}\")\n",
    "print(f\"\\nç±»åˆ«å‹ç‰¹å¾: {len(categorical_cols)} ä¸ª\")\n",
    "print(f\"  {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜çº§ç¼ºå¤±å€¼å¡«å……ï¼šä½¿ç”¨KNNå¡«å……æ•°å€¼ç‰¹å¾\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ä½¿ç”¨KNNå¡«å……æ•°å€¼ç‰¹å¾çš„ç¼ºå¤±å€¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ£€æŸ¥æ•°å€¼ç‰¹å¾çš„ç¼ºå¤±æƒ…å†µ\n",
    "numeric_missing = X[numeric_cols].isnull().sum()\n",
    "numeric_missing_cols = numeric_missing[numeric_missing > 0].index.tolist()\n",
    "\n",
    "if numeric_missing_cols:\n",
    "    print(f\"æœ‰ç¼ºå¤±å€¼çš„æ•°å€¼ç‰¹å¾: {numeric_missing_cols}\")\n",
    "    X = dp.knn_impute(X, columns=numeric_missing_cols, n_neighbors=5)\n",
    "    print(\"âœ… KNNå¡«å……å®Œæˆ\")\n",
    "else:\n",
    "    print(\"âœ“ æ•°å€¼ç‰¹å¾æ— ç¼ºå¤±å€¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç±»åˆ«ç‰¹å¾çš„ç¼ºå¤±å€¼å¡«å……ï¼ˆä½¿ç”¨ä¼—æ•°ï¼‰\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"å¡«å……ç±»åˆ«ç‰¹å¾çš„ç¼ºå¤±å€¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        mode_value = X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown'\n",
    "        X[col] = X[col].fillna(mode_value)\n",
    "        print(f\"  {col}: å¡«å……ä¸º '{mode_value}'\")\n",
    "\n",
    "print(\"\\nâœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆ\")\n",
    "print(f\"å‰©ä½™ç¼ºå¤±å€¼: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 é«˜çº§ç¼–ç \n",
    "\n",
    "å¯¹äºé«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼Œä½¿ç”¨æ›´æ™ºèƒ½çš„ç¼–ç æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ç±»åˆ«ç‰¹å¾çš„åŸºæ•°\n",
    "print(\"ç±»åˆ«ç‰¹å¾åŸºæ•°åˆ†æ:\")\n",
    "for col in categorical_cols:\n",
    "    n_unique = X[col].nunique()\n",
    "    print(f\"  {col}: {n_unique} ä¸ªå”¯ä¸€å€¼\")\n",
    "\n",
    "# åˆ†ç±»å¤„ç†\n",
    "low_cardinality_cols = []   # ç±»åˆ«æ•° <= 10: One-Hotç¼–ç \n",
    "high_cardinality_cols = []  # ç±»åˆ«æ•° > 10: é¢‘ç‡ç¼–ç \n",
    "\n",
    "for col in categorical_cols:\n",
    "    if X[col].nunique() <= 10:\n",
    "        low_cardinality_cols.append(col)\n",
    "    else:\n",
    "        high_cardinality_cols.append(col)\n",
    "\n",
    "print(f\"\\nä½åŸºæ•°ç‰¹å¾ï¼ˆOne-Hotï¼‰: {low_cardinality_cols}\")\n",
    "print(f\"é«˜åŸºæ•°ç‰¹å¾ï¼ˆé¢‘ç‡ç¼–ç ï¼‰: {high_cardinality_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hotç¼–ç ä½åŸºæ•°ç‰¹å¾\n",
    "if low_cardinality_cols:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"One-Hotç¼–ç ä½åŸºæ•°ç±»åˆ«ç‰¹å¾\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    X = dp.onehot_encode(X, columns=low_cardinality_cols, drop_first=True)\n",
    "    print(\"âœ… One-Hotç¼–ç å®Œæˆ\")\n",
    "\n",
    "# é¢‘ç‡ç¼–ç é«˜åŸºæ•°ç‰¹å¾\n",
    "if high_cardinality_cols:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"é¢‘ç‡ç¼–ç é«˜åŸºæ•°ç±»åˆ«ç‰¹å¾\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    X = dp.frequency_encoding(X, columns=high_cardinality_cols, normalize=True)\n",
    "    print(\"âœ… é¢‘ç‡ç¼–ç å®Œæˆ\")\n",
    "\n",
    "print(f\"\\nç¼–ç åç‰¹å¾æ•°: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ç‰¹å¾ç¼©æ”¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "if numeric_features:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    X = dp.standardize_features(X, columns=numeric_features)\n",
    "    print(f\"âœ… æ ‡å‡†åŒ– {len(numeric_features)} ä¸ªæ•°å€¼ç‰¹å¾\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. é«˜çº§ç‰¹å¾å·¥ç¨‹\n",
    "\n",
    "### 4.1 åˆ›å»ºäº¤äº’ç‰¹å¾\n",
    "\n",
    "å¯¹äºé‡è¦çš„æ•°å€¼ç‰¹å¾ï¼Œåˆ›å»ºäº¤äº’é¡¹æ¥æ•æ‰éçº¿æ€§å…³ç³»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰æ‹©ä¸€äº›é‡è¦çš„æ•°å€¼ç‰¹å¾è¿›è¡Œäº¤äº’\n",
    "# è¿™é‡Œæ‰‹åŠ¨é€‰æ‹©ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥åŸºäºPhase 1çš„ç›¸å…³æ€§åˆ†æ\n",
    "important_numeric_cols = [col for col in numeric_features if not col.startswith(('PCA', 'row_'))]\n",
    "important_numeric_cols = important_numeric_cols[:3]  # é™åˆ¶ä¸ºå‰3ä¸ªï¼Œé¿å…ç‰¹å¾çˆ†ç‚¸\n",
    "\n",
    "if len(important_numeric_cols) >= 2:\n",
    "    print(\"=\"*60)\n",
    "    print(f\"åˆ›å»ºäº¤äº’ç‰¹å¾: {important_numeric_cols}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    X_interaction = fe.create_interaction_features(\n",
    "        X, \n",
    "        columns=important_numeric_cols,\n",
    "        operations=['*', '+']  # ä¹˜æ³•å’ŒåŠ æ³•äº¤äº’\n",
    "    )\n",
    "    \n",
    "    # åªä¿ç•™æ–°å¢çš„äº¤äº’ç‰¹å¾\n",
    "    new_cols = [col for col in X_interaction.columns if col not in X.columns]\n",
    "    X = pd.concat([X, X_interaction[new_cols]], axis=1)\n",
    "    \n",
    "    print(f\"\\nâœ… æ–°å¢ {len(new_cols)} ä¸ªäº¤äº’ç‰¹å¾\")\n",
    "    print(f\"å½“å‰ç‰¹å¾æ•°: {X.shape[1]}\")\n",
    "else:\n",
    "    print(\"æ•°å€¼ç‰¹å¾ä¸è¶³ï¼Œè·³è¿‡äº¤äº’ç‰¹å¾åˆ›å»º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 å¤šé¡¹å¼ç‰¹å¾ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¯¹äºéçº¿æ€§å…³ç³»æ˜æ˜¾çš„ç‰¹å¾ï¼Œåˆ›å»ºå¤šé¡¹å¼ç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ³¨æ„ï¼šå¤šé¡¹å¼ç‰¹å¾ä¼šå¿«é€Ÿå¢åŠ ç‰¹å¾æ•°é‡ï¼Œè¿™é‡Œä»…ä½œæ¼”ç¤º\n",
    "# å®é™…ä½¿ç”¨æ—¶éœ€è¦è°¨æ…é€‰æ‹©ç‰¹å¾å’Œå¤šé¡¹å¼æ¬¡æ•°\n",
    "\n",
    "# poly_cols = important_numeric_cols[:2]  # åªé€‰2ä¸ªç‰¹å¾\n",
    "\n",
    "# if len(poly_cols) >= 1:\n",
    "#     print(\"=\"*60)\n",
    "#     print(f\"åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾ (degree=2): {poly_cols}\")\n",
    "#     print(\"=\"*60)\n",
    "#     \n",
    "#     X = fe.create_polynomial_features(\n",
    "#         X,\n",
    "#         columns=poly_cols,\n",
    "#         degree=2,\n",
    "#         interaction_only=False\n",
    "#     )\n",
    "#     \n",
    "#     print(f\"\\nâœ… å¤šé¡¹å¼ç‰¹å¾åˆ›å»ºå®Œæˆ\")\n",
    "#     print(f\"å½“å‰ç‰¹å¾æ•°: {X.shape[1]}\")\n",
    "\n",
    "print(\"â­ï¸  è·³è¿‡å¤šé¡¹å¼ç‰¹å¾ï¼ˆé¿å…ç‰¹å¾çˆ†ç‚¸ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ç‰¹å¾é€‰æ‹©\n",
    "\n",
    "ç§»é™¤å†—ä½™å’Œä½ä¿¡æ¯é‡çš„ç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹å·®è¿‡æ»¤ï¼ˆç§»é™¤ä½æ–¹å·®ç‰¹å¾ï¼‰\n",
    "print(\"=\"*60)\n",
    "print(\"ç‰¹å¾é€‰æ‹©ï¼šæ–¹å·®è¿‡æ»¤\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X = fe.select_by_variance(X, threshold=0.01)\n",
    "print(f\"æ–¹å·®è¿‡æ»¤åç‰¹å¾æ•°: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›¸å…³æ€§è¿‡æ»¤ï¼ˆç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾ï¼‰\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ç‰¹å¾é€‰æ‹©ï¼šç›¸å…³æ€§è¿‡æ»¤\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X = fe.select_by_correlation(X, threshold=0.90)\n",
    "print(f\"ç›¸å…³æ€§è¿‡æ»¤åç‰¹å¾æ•°: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆæ£€æŸ¥\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 3 ç‰¹å¾å·¥ç¨‹æ€»ç»“\")\n",
    "print(\"=\"*60)\n",
    "print(f\"åŸå§‹ç‰¹å¾æ•°: {original_shape[1] - 1}\")\n",
    "print(f\"Phase 2 ç‰¹å¾æ•°: {baseline_metrics['n_features']}\")\n",
    "print(f\"Phase 3 ç‰¹å¾æ•°: {X.shape[1]}\")\n",
    "print(f\"\\næœ€ç»ˆæ•°æ®å½¢çŠ¶: {X.shape}\")\n",
    "print(f\"ç¼ºå¤±å€¼æ€»æ•°: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. æ•°æ®é›†åˆ’åˆ†\n",
    "\n",
    "ä½¿ç”¨ä¸Phase 2ç›¸åŒçš„åˆ’åˆ†æ–¹å¼ä»¥ä¿è¯å¯æ¯”æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ’åˆ†æ•°æ®é›†\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = dp.split_data(\n",
    "    X, y,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=True\n",
    ")\n",
    "\n",
    "print(\"æ•°æ®é›†åˆ’åˆ†:\")\n",
    "print(f\"  è®­ç»ƒé›†: {X_train.shape}\")\n",
    "print(f\"  éªŒè¯é›†: {X_val.shape}\")\n",
    "print(f\"  æµ‹è¯•é›†: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. æ¨¡å‹è®­ç»ƒä¸å¯¹æ¯”\n",
    "\n",
    "### 6.1 ä½¿ç”¨ä¼˜åŒ–åçš„ç‰¹å¾è®­ç»ƒåŸºç¡€æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¡®å®šé—®é¢˜ç±»å‹\n",
    "problem_type = 'classification' if y.nunique() <= 20 else 'regression'\n",
    "print(f\"é—®é¢˜ç±»å‹: {problem_type}\")\n",
    "\n",
    "# åˆ›å»ºPipelineå¹¶è®­ç»ƒ\n",
    "pipeline_improved = sp.SupervisedPipeline(\n",
    "    problem_type=problem_type,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# è®­ç»ƒå¤šä¸ªæ¨¡å‹\n",
    "pipeline_improved.fit(X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹å¯¹æ¯”\n",
    "comparison_df = pipeline_improved.get_model_comparison()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 3 æ¨¡å‹æ€§èƒ½å¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯é›†è¯„ä¼°\n",
    "y_val_pred = pipeline_improved.predict(X_val)\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    y_val_proba = pipeline_improved.predict_proba(X_val)\n",
    "    \n",
    "    val_metrics_improved = me.evaluate_classification(\n",
    "        y_val,\n",
    "        y_val_pred,\n",
    "        y_val_proba[:, 1] if y_val_proba.shape[1] == 2 else None,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    val_metrics_improved = me.evaluate_regression(\n",
    "        y_val,\n",
    "        y_val_pred,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Phase 2 vs Phase 3 æ€§èƒ½å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½å¯¹æ¯”\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 2 vs Phase 3 æ€§èƒ½å¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_data = pd.DataFrame({\n",
    "    'Phase': ['Phase 2 (åŸºçº¿)', 'Phase 3 (ä¼˜åŒ–)'],\n",
    "    'Accuracy': [\n",
    "        baseline_metrics['accuracy'],\n",
    "        val_metrics_improved['accuracy']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        baseline_metrics['f1'],\n",
    "        val_metrics_improved['f1']\n",
    "    ],\n",
    "    'N_Features': [\n",
    "        baseline_metrics['n_features'],\n",
    "        X.shape[1]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_data.to_string(index=False))\n",
    "\n",
    "# è®¡ç®—æå‡\n",
    "acc_improvement = (val_metrics_improved['accuracy'] - baseline_metrics['accuracy']) / baseline_metrics['accuracy'] * 100\n",
    "f1_improvement = (val_metrics_improved['f1'] - baseline_metrics['f1']) / baseline_metrics['f1'] * 100\n",
    "\n",
    "print(f\"\\nğŸ“ˆ æ€§èƒ½æå‡:\")\n",
    "print(f\"  Accuracy: {acc_improvement:+.2f}%\")\n",
    "print(f\"  F1-Score: {f1_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ€§èƒ½å¯¹æ¯”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracyå¯¹æ¯”\n",
    "axes[0].bar(['Phase 2', 'Phase 3'], \n",
    "           [baseline_metrics['accuracy'], val_metrics_improved['accuracy']],\n",
    "           color=['skyblue', 'orange'],\n",
    "           edgecolor='navy',\n",
    "           alpha=0.7)\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy å¯¹æ¯”')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, v in enumerate([baseline_metrics['accuracy'], val_metrics_improved['accuracy']]):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# F1-Scoreå¯¹æ¯”\n",
    "axes[1].bar(['Phase 2', 'Phase 3'],\n",
    "           [baseline_metrics['f1'], val_metrics_improved['f1']],\n",
    "           color=['skyblue', 'orange'],\n",
    "           edgecolor='navy',\n",
    "           alpha=0.7)\n",
    "axes[1].set_ylabel('F1-Score')\n",
    "axes[1].set_title('F1-Score å¯¹æ¯”')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate([baseline_metrics['f1'], val_metrics_improved['f1']]):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. è¶…å‚æ•°è°ƒä¼˜\n",
    "\n",
    "å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¶…å‚æ•°ç½‘æ ¼ï¼ˆæ ¹æ®æœ€ä½³æ¨¡å‹é€‰æ‹©ï¼‰\n",
    "best_model_name = pipeline_improved.best_model_name\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"å¯¹ {best_model_name} è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å®šä¹‰å‚æ•°ç½‘æ ¼\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [10, 15, 20],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__learning_rate': [0.01, 0.1]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [5, 7, 10],\n",
    "        'classifier__learning_rate': [0.01, 0.1]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# è·å–å¯¹åº”çš„å‚æ•°ç½‘æ ¼\n",
    "param_grid = param_grids.get(best_model_name, {})\n",
    "\n",
    "if param_grid:\n",
    "    # è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ï¼ˆä½¿ç”¨éšæœºæœç´¢ï¼Œæ›´å¿«ï¼‰\n",
    "    tuning_results = pipeline_improved.tune_hyperparameters(\n",
    "        X_train, y_train,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        n_iter=10  # éšæœºæœç´¢10æ¬¡\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… è¶…å‚æ•°è°ƒä¼˜å®Œæˆ\")\n",
    "else:\n",
    "    print(f\"âš ï¸  {best_model_name} æ²¡æœ‰é¢„å®šä¹‰çš„å‚æ•°ç½‘æ ¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨è°ƒä¼˜åçš„æ¨¡å‹é‡æ–°è¯„ä¼°\n",
    "y_val_pred_tuned = pipeline_improved.predict(X_val)\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    y_val_proba_tuned = pipeline_improved.predict_proba(X_val)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"è°ƒä¼˜åéªŒè¯é›†æ€§èƒ½\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    val_metrics_tuned = me.evaluate_classification(\n",
    "        y_val,\n",
    "        y_val_pred_tuned,\n",
    "        y_val_proba_tuned[:, 1] if y_val_proba_tuned.shape[1] == 2 else None,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. æ¨¡å‹è¯Šæ–­\n",
    "\n",
    "### 8.1 å­¦ä¹ æ›²çº¿åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶å­¦ä¹ æ›²çº¿\n",
    "print(\"=\"*60)\n",
    "print(\"å­¦ä¹ æ›²çº¿åˆ†æï¼ˆè¯Šæ–­è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "me.plot_learning_curve(\n",
    "    pipeline_improved.best_model,\n",
    "    X_train, y_train,\n",
    "    cv=3,\n",
    "    scoring='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 ç‰¹å¾é‡è¦æ€§åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§\n",
    "# æ³¨æ„ï¼šéœ€è¦ä»Pipelineä¸­æå–å®é™…çš„æ¨¡å‹\n",
    "try:\n",
    "    # è·å–Pipelineä¸­çš„æœ€åä¸€æ­¥ï¼ˆæ¨¡å‹ï¼‰\n",
    "    actual_model = pipeline_improved.best_model.named_steps['classifier']\n",
    "    \n",
    "    if hasattr(actual_model, 'feature_importances_'):\n",
    "        print(\"=\"*60)\n",
    "        print(\"ç‰¹å¾é‡è¦æ€§åˆ†æ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # è·å–è½¬æ¢åçš„ç‰¹å¾åç§°\n",
    "        # ç”±äºPipelineä¸­åŒ…å«é¢„å¤„ç†ï¼Œç‰¹å¾åç§°å¯èƒ½å·²æ”¹å˜\n",
    "        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾åç§°\n",
    "        feature_names = X_train.columns.tolist()\n",
    "        \n",
    "        me.plot_feature_importance(\n",
    "            actual_model,\n",
    "            feature_names,\n",
    "            top_n=20\n",
    "        )\n",
    "    else:\n",
    "        print(f\"âš ï¸  {best_model_name} ä¸æ”¯æŒfeature_importances_\")\nexcept Exception as e:\n",
    "    print(f\"âš ï¸  æ— æ³•æå–ç‰¹å¾é‡è¦æ€§: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 æ··æ·†çŸ©é˜µå’ŒROCæ›²çº¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if problem_type == 'classification':\n",
    "    # æ··æ·†çŸ©é˜µ\n",
    "    me.plot_confusion_matrix(y_val, y_val_pred_tuned)\n",
    "    plt.show()\n",
    "    \n",
    "    # ROCæ›²çº¿ï¼ˆäºŒåˆ†ç±»ï¼‰\n",
    "    if y.nunique() == 2:\n",
    "        me.plot_roc_curve(y_val, y_val_proba_tuned[:, 1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹\n",
    "y_test_pred = pipeline_improved.predict(X_test)\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    y_test_proba = pipeline_improved.predict_proba(X_test)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼° - {pipeline_improved.best_model_name} (è°ƒä¼˜å)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_metrics = me.evaluate_classification(\n",
    "        y_test,\n",
    "        y_test_pred,\n",
    "        y_test_proba[:, 1] if y_test_proba.shape[1] == 2 else None,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # æ··æ·†çŸ©é˜µ\n",
    "    me.plot_confusion_matrix(y_test, y_test_pred)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(f\"æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼° - {pipeline_improved.best_model_name} (è°ƒä¼˜å)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_metrics = me.evaluate_regression(\n",
    "        y_test,\n",
    "        y_test_pred,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    me.plot_regression_results(y_test, y_test_pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Phase 3 æ€»ç»“\n",
    "\n",
    "### 10.1 å®Œæ•´æ€§èƒ½å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‡æ€»æ‰€æœ‰é˜¶æ®µçš„æ€§èƒ½\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 2 â†’ Phase 3 å®Œæ•´å¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_comparison = pd.DataFrame({\n",
    "    'é˜¶æ®µ': ['Phase 2 åŸºçº¿', 'Phase 3 ä¼˜åŒ–', 'Phase 3 è°ƒä¼˜'],\n",
    "    'Accuracy': [\n",
    "        baseline_metrics['accuracy'],\n",
    "        val_metrics_improved['accuracy'],\n",
    "        val_metrics_tuned['accuracy']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        baseline_metrics['f1'],\n",
    "        val_metrics_improved['f1'],\n",
    "        val_metrics_tuned['f1']\n",
    "    ],\n",
    "    'ç‰¹å¾æ•°': [\n",
    "        baseline_metrics['n_features'],\n",
    "        X.shape[1],\n",
    "        X.shape[1]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(final_comparison.to_string(index=False))\n",
    "\n",
    "# æ€»æå‡\n",
    "total_acc_improvement = (val_metrics_tuned['accuracy'] - baseline_metrics['accuracy']) / baseline_metrics['accuracy'] * 100\n",
    "total_f1_improvement = (val_metrics_tuned['f1'] - baseline_metrics['f1']) / baseline_metrics['f1'] * 100\n",
    "\n",
    "print(f\"\\nğŸ¯ Phase 2 â†’ Phase 3 æ€»æå‡:\")\n",
    "print(f\"  Accuracy: {total_acc_improvement:+.2f}%\")\n",
    "print(f\"  F1-Score: {total_f1_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 ä¼˜åŒ–æ–¹æ³•æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 3 é‡‡ç”¨çš„ä¼˜åŒ–æ–¹æ³•\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "optimizations = [\n",
    "    \"1. é«˜çº§ç¼ºå¤±å€¼å¤„ç†\",\n",
    "    \"   âœ… KNNå¡«å……æ•°å€¼ç‰¹å¾\",\n",
    "    \"   âœ… æ™ºèƒ½å¡«å……ç±»åˆ«ç‰¹å¾\",\n",
    "    \"\",\n",
    "    \"2. é«˜çº§ç¼–ç ç­–ç•¥\",\n",
    "    \"   âœ… ä½åŸºæ•°ç‰¹å¾: One-Hotç¼–ç \",\n",
    "    \"   âœ… é«˜åŸºæ•°ç‰¹å¾: é¢‘ç‡ç¼–ç \",\n",
    "    \"\",\n",
    "    \"3. ç‰¹å¾å·¥ç¨‹\",\n",
    "    f\"   âœ… äº¤äº’ç‰¹å¾: æ–°å¢{len([c for c in X.columns if 'times' in c or 'plus' in c])}ä¸ª\",\n",
    "    \"   âœ… æ–¹å·®è¿‡æ»¤: ç§»é™¤ä½ä¿¡æ¯é‡ç‰¹å¾\",\n",
    "    \"   âœ… ç›¸å…³æ€§è¿‡æ»¤: ç§»é™¤å†—ä½™ç‰¹å¾\",\n",
    "    \"\",\n",
    "    \"4. æ¨¡å‹ä¼˜åŒ–\",\n",
    "    f\"   âœ… æœ€ä½³æ¨¡å‹: {pipeline_improved.best_model_name}\",\n",
    "    \"   âœ… è¶…å‚æ•°è°ƒä¼˜: éšæœºæœç´¢\",\n",
    "    \"\",\n",
    "    \"5. æ¨¡å‹è¯Šæ–­\",\n",
    "    \"   âœ… å­¦ä¹ æ›²çº¿åˆ†æ\",\n",
    "    \"   âœ… ç‰¹å¾é‡è¦æ€§åˆ†æ\",\n",
    "    \"   âœ… æ··æ·†çŸ©é˜µä¸ROCæ›²çº¿\"\n",
    "]\n",
    "\n",
    "for item in optimizations:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Phase 3 æ£€æŸ¥æ¸…å•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Phase 3 å®Œæˆæ£€æŸ¥æ¸…å•\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checklist = [\n",
    "    \"âœ… Phase 2åŸºçº¿æ€§èƒ½å·²å»ºç«‹\",\n",
    "    \"âœ… é«˜çº§ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼ˆKNNå¡«å……ï¼‰\",\n",
    "    \"âœ… é«˜çº§ç¼–ç å®Œæˆï¼ˆé¢‘ç‡ç¼–ç ï¼‰\",\n",
    "    \"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼ˆäº¤äº’ã€è¿‡æ»¤ï¼‰\",\n",
    "    \"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆï¼ˆæ–¹å·®ã€ç›¸å…³æ€§ï¼‰\",\n",
    "    \"âœ… æ¨¡å‹è®­ç»ƒå’Œå¯¹æ¯”å®Œæˆ\",\n",
    "    \"âœ… è¶…å‚æ•°è°ƒä¼˜å®Œæˆ\",\n",
    "    \"âœ… å­¦ä¹ æ›²çº¿åˆ†æå®Œæˆ\",\n",
    "    \"âœ… ç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆ\",\n",
    "    \"âœ… æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°å®Œæˆ\",\n",
    "    \"âœ… Phase 2 vs Phase 3æ€§èƒ½å¯¹æ¯”å®Œæˆ\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ Phase 3: Supervised Solution å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ ¹æ®æ€§èƒ½æå‡ç»™å‡ºå»ºè®®\n",
    "if total_acc_improvement > 5:\n",
    "    print(\"\\nâœ… æ€§èƒ½æ˜¾è‘—æå‡ï¼ä¼˜åŒ–ç­–ç•¥æœ‰æ•ˆã€‚\")\n",
    "    print(\"   ä¸‹ä¸€æ­¥: å‰å¾€ Phase 4 æ¢ç´¢æ— ç›‘ç£å­¦ä¹ æ–¹æ³•\")\n",
    "elif total_acc_improvement > 0:\n",
    "    print(\"\\nâœ“  æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†è¿˜æœ‰ä¼˜åŒ–ç©ºé—´ã€‚\")\n",
    "    print(\"   å»ºè®®: å°è¯•æ›´å¤šç‰¹å¾å·¥ç¨‹æˆ–æ¨¡å‹é›†æˆæ–¹æ³•\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  æ€§èƒ½æœªæå‡ï¼Œå¯èƒ½åŸå› ï¼š\")\n",
    "    print(\"   - åŸºçº¿å·²ç»å¾ˆå¥½ï¼Œä¼˜åŒ–ç©ºé—´æœ‰é™\")\n",
    "    print(\"   - ç‰¹å¾å·¥ç¨‹ä¸é€‚åˆå½“å‰æ•°æ®\")\n",
    "    print(\"   - éœ€è¦æ›´æ·±å…¥çš„æ•°æ®åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. ä¿å­˜æ¨¡å‹å’Œç»“æœï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹\n",
    "# import joblib\n",
    "# model_path = Path('models/phase3_optimized_model.pkl')\n",
    "# model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# joblib.dump(pipeline_improved.best_model, model_path)\n",
    "# print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}\")\n",
    "\n",
    "# ä¿å­˜ä¼˜åŒ–åçš„ç‰¹å¾\n",
    "# features_path = Path('data/processed/phase3_features.csv')\n",
    "# features_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# X.to_csv(features_path, index=False)\n",
    "# print(f\"âœ… ç‰¹å¾å·²ä¿å­˜åˆ°: {features_path}\")\n",
    "\n",
    "# ä¿å­˜æ€§èƒ½å¯¹æ¯”ç»“æœ\n",
    "# results_path = Path('results/phase3_performance_comparison.csv')\n",
    "# results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# final_comparison.to_csv(results_path, index=False)\n",
    "# print(f\"âœ… ç»“æœå·²ä¿å­˜åˆ°: {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
