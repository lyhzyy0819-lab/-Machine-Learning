# ğŸ§­ æœºå™¨å­¦ä¹ å®Œæ•´å·¥ä½œæµç¨‹å†³ç­–æ ‘

> **åƒåŒ»ç”Ÿè¯Šæ–­ç–¾ç—…ä¸€æ ·è¯Šæ–­MLé—®é¢˜**
> ä»é™Œç”Ÿæ•°æ®åˆ°æœ€ç»ˆæ–¹æ¡ˆçš„å®Œæ•´å†³ç­–æŒ‡å—

---

## ğŸ“– ä½¿ç”¨è¯´æ˜

### æœ¬æ–‡æ¡£çš„å®šä½

è¿™æ˜¯ä¸€ä»½**å®æˆ˜å¯¼å‘çš„å†³ç­–æŒ‡å—**ï¼Œä¸æ˜¯ç®—æ³•æ•™ç§‘ä¹¦ã€‚å½“ä½ é¢å¯¹ä¸€ä¸ªæ–°çš„æœºå™¨å­¦ä¹ é—®é¢˜æ—¶ï¼Œè¿™ä»½æ–‡æ¡£ä¼šåƒåŒ»ç”Ÿè¯Šæ–­ç–¾ç—…ä¸€æ ·ï¼Œå¼•å¯¼ä½ ä¸€æ­¥æ­¥åšå‡ºæ­£ç¡®çš„å†³ç­–ã€‚

### å¦‚ä½•ä½¿ç”¨æœ¬æ–‡æ¡£

1. **é¦–æ¬¡å­¦ä¹ **ï¼šä»å¤´åˆ°å°¾å®Œæ•´é˜…è¯»ä¸€éï¼Œç†è§£æ•´ä¸ªå†³ç­–æµç¨‹
2. **å®æˆ˜å‚è€ƒ**ï¼šé¢å¯¹æ–°é¡¹ç›®æ—¶ï¼ŒæŒ‰ç…§å†³ç­–æ ‘é€æ­¥æ¨è¿›
3. **æŸ¥æ¼è¡¥ç¼º**ï¼šé‡åˆ°å›°æƒ‘æ—¶ï¼ŒæŸ¥æ‰¾å¯¹åº”å†³ç­–ç‚¹çš„è¯¦ç»†è¯´æ˜

### æ–‡æ¡£ç»“æ„

```
ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®è¯Šæ–­é˜¶æ®µ â†’ äº†è§£æ•°æ®ï¼Œè¯†åˆ«é—®é¢˜
ç¬¬äºŒéƒ¨åˆ†ï¼šé—®é¢˜å®šä¹‰é˜¶æ®µ â†’ ç¡®å®šç›®æ ‡ï¼Œé€‰æ‹©æ–¹å‘
ç¬¬ä¸‰éƒ¨åˆ†ï¼šç®—æ³•é€‰æ‹©é˜¶æ®µ â†’ æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©ç®—æ³•
ç¬¬å››éƒ¨åˆ†ï¼šæ•°æ®å¤„ç†é˜¶æ®µ â†’ é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒé˜¶æ®µ â†’ è®­ç»ƒã€è°ƒä¼˜ã€éªŒè¯
ç¬¬å…­éƒ¨åˆ†ï¼šæ¨¡å‹è¯„ä¼°é˜¶æ®µ â†’ è¯„ä¼°æ•ˆæœï¼Œä¸šåŠ¡è½¬åŒ–
```

---

## ğŸ¯ ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®è¯Šæ–­é˜¶æ®µ

> **ç›®æ ‡**ï¼šå¿«é€Ÿäº†è§£æ•°æ®ç‰¹ç‚¹ï¼Œè¯†åˆ«æ½œåœ¨é—®é¢˜

### 1.1 æ•°æ®è¯Šæ–­æ£€æŸ¥æ¸…å•

#### âœ… ç¬¬ä¸€æ­¥ï¼šæ•°æ®æ¦‚è§ˆ

```python
# åŸºæœ¬ä¿¡æ¯æ£€æŸ¥
import pandas as pd
df = pd.read_csv('data.csv')

# 1. æ•°æ®è§„æ¨¡
print(f"æ•°æ®è§„æ¨¡: {df.shape[0]:,} è¡Œ Ã— {df.shape[1]} åˆ—")
print(f"å†…å­˜å ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# 2. ç‰¹å¾ç±»å‹ç»Ÿè®¡
print(f"æ•°å€¼ç‰¹å¾: {df.select_dtypes(include=['int', 'float']).shape[1]} ä¸ª")
print(f"åˆ†ç±»ç‰¹å¾: {df.select_dtypes(include=['object', 'category']).shape[1]} ä¸ª")
print(f"æ—¥æœŸç‰¹å¾: {df.select_dtypes(include=['datetime']).shape[1]} ä¸ª")
```

**è¯Šæ–­è¦ç‚¹**ï¼š
- æ•°æ®é‡çº§ï¼šå°(<1ä¸‡) / ä¸­(1-10ä¸‡) / å¤§(>10ä¸‡) / æµ·é‡(>100ä¸‡)
- ç‰¹å¾æ•°é‡ï¼šä½ç»´(<10) / ä¸­ç»´(10-50) / é«˜ç»´(50-100) / è¶…é«˜ç»´(>100)
- å†…å­˜å ç”¨ï¼šæ˜¯å¦éœ€è¦ä¼˜åŒ–æ•°æ®ç±»å‹æˆ–åˆ†å—å¤„ç†

---

#### âœ… ç¬¬äºŒæ­¥ï¼šæ•°æ®è´¨é‡æ£€æŸ¥

```python
# 1. ç¼ºå¤±å€¼åˆ†æ
missing_stats = df.isnull().sum()
missing_percent = (missing_stats / len(df)) * 100
print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
print(missing_percent[missing_percent > 0].sort_values(ascending=False))

# 2. é‡å¤å€¼æ£€æŸ¥
duplicate_count = df.duplicated().sum()
print(f"\né‡å¤è¡Œæ•°é‡: {duplicate_count} ({duplicate_count/len(df)*100:.2f}%)")

# 3. å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆæ•°å€¼ç‰¹å¾ï¼‰
numeric_cols = df.select_dtypes(include=['int', 'float']).columns
for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
    print(f"{col}: {outliers} ä¸ªå¼‚å¸¸å€¼ ({outliers/len(df)*100:.2f}%)")
```

**è¯Šæ–­æ ‡å‡†**ï¼š
- âœ… **ä¼˜ç§€**ï¼šç¼ºå¤±å€¼<5%ï¼Œæ— é‡å¤ï¼Œå¼‚å¸¸å€¼<1%
- âš ï¸ **ä¸€èˆ¬**ï¼šç¼ºå¤±å€¼5-20%ï¼Œå°‘é‡é‡å¤ï¼Œå¼‚å¸¸å€¼1-5%
- âŒ **è¾ƒå·®**ï¼šç¼ºå¤±å€¼>20%ï¼Œå¤§é‡é‡å¤ï¼Œå¼‚å¸¸å€¼>5%

---

#### âœ… ç¬¬ä¸‰æ­¥ï¼šç›®æ ‡å˜é‡åˆ†æï¼ˆå¦‚æœæœ‰ï¼‰

```python
# æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„ç›®æ ‡å˜é‡
target_col = 'your_target_column'  # æ ¹æ®ä¸šåŠ¡ç¡®å®š

if target_col in df.columns:
    print(f"\nç›®æ ‡å˜é‡: {target_col}")

    # åˆ¤æ–­æ˜¯å›å½’è¿˜æ˜¯åˆ†ç±»
    unique_values = df[target_col].nunique()
    total_values = len(df)

    if df[target_col].dtype in ['int64', 'float64']:
        if unique_values / total_values > 0.05:  # è¶…è¿‡5%æ˜¯å”¯ä¸€å€¼
            print("é—®é¢˜ç±»å‹: å›å½’é—®é¢˜ï¼ˆè¿ç»­å€¼ï¼‰")
        else:
            print("é—®é¢˜ç±»å‹: åˆ†ç±»é—®é¢˜ï¼ˆç¦»æ•£å€¼ï¼‰")
    else:
        print("é—®é¢˜ç±»å‹: åˆ†ç±»é—®é¢˜ï¼ˆç±»åˆ«å€¼ï¼‰")

    # ç±»åˆ«åˆ†å¸ƒï¼ˆåˆ†ç±»é—®é¢˜ï¼‰
    if unique_values < 20:
        print("\nç±»åˆ«åˆ†å¸ƒ:")
        print(df[target_col].value_counts(normalize=True))

        # æ£€æŸ¥ç±»åˆ«å¹³è¡¡
        min_ratio = df[target_col].value_counts(normalize=True).min()
        if min_ratio < 0.1:
            print("âš ï¸ è­¦å‘Š: ç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆSMOTE/æƒé‡è°ƒæ•´ï¼‰")
else:
    print("âš ï¸ æ— æ˜ç¡®ç›®æ ‡å˜é‡ â†’ æ— ç›‘ç£å­¦ä¹ æ–¹å‘")
```

**å…³é”®å†³ç­–ç‚¹**ï¼š
- **æœ‰ç›®æ ‡å˜é‡** â†’ ç›‘ç£å­¦ä¹ 
- **æ— ç›®æ ‡å˜é‡** â†’ æ— ç›‘ç£å­¦ä¹ 

---

### ğŸŒ³ 1.2 æ ¸å¿ƒå†³ç­–æ ‘ï¼šé—®é¢˜ç±»å‹è¯†åˆ«

```mermaid
graph TD
    A[å¼€å§‹: é™Œç”Ÿæ•°æ®] --> B{æ•°æ®æœ‰æ ‡ç­¾?}

    B -->|YES| C{ç›®æ ‡å˜é‡ç±»å‹?}
    B -->|NO| D[æ— ç›‘ç£å­¦ä¹ è·¯å¾„]

    C -->|è¿ç»­å€¼| E[å›å½’é—®é¢˜]
    C -->|ç¦»æ•£å€¼| F{ç±»åˆ«æ•°é‡?}

    F -->|2ç±»| G[äºŒåˆ†ç±»é—®é¢˜]
    F -->|å¤šç±»| H[å¤šåˆ†ç±»é—®é¢˜]

    E --> I{æ•°æ®é‡çº§?}
    G --> I
    H --> I

    I -->|<1ä¸‡| J[ç®€å•æ¨¡å‹: LR/DT]
    I -->|1-10ä¸‡| K[é›†æˆæ¨¡å‹: RF/XGBoost]
    I -->|>10ä¸‡| L[é«˜æ•ˆæ¨¡å‹: LightGBM]

    D --> M{åˆ†æç›®æ ‡?}
    M -->|åˆ†ç¾¤| N[èšç±»: K-Means/DBSCAN]
    M -->|é™ç»´| O[é™ç»´: PCA/t-SNE]
    M -->|å¼‚å¸¸| P[å¼‚å¸¸æ£€æµ‹: IF/LOF]
```

---

### 1.3 æ•°æ®è¯Šæ–­æŠ¥å‘Šæ¨¡æ¿

å®Œæˆä¸Šè¿°æ£€æŸ¥åï¼Œç”Ÿæˆä¸€ä»½**æ•°æ®è¯Šæ–­æŠ¥å‘Š**ï¼š

```markdown
# æ•°æ®è¯Šæ–­æŠ¥å‘Š

## 1. æ•°æ®æ¦‚è§ˆ
- æ•°æ®è§„æ¨¡: ___ è¡Œ Ã— ___ åˆ—
- å†…å­˜å ç”¨: ___ MB
- æ•°æ®æ¥æº: ___________
- æ”¶é›†æ—¶é—´: ___________

## 2. æ•°æ®è´¨é‡è¯„åˆ†: __/10
- ç¼ºå¤±å€¼æƒ…å†µ: ___% (è¯„åˆ†: ___)
- é‡å¤å€¼æƒ…å†µ: ___% (è¯„åˆ†: ___)
- å¼‚å¸¸å€¼æƒ…å†µ: ___% (è¯„åˆ†: ___)

## 3. é—®é¢˜ç±»å‹åˆ¤æ–­
- [ ] ç›‘ç£å­¦ä¹  - å›å½’é—®é¢˜
- [ ] ç›‘ç£å­¦ä¹  - äºŒåˆ†ç±»é—®é¢˜
- [ ] ç›‘ç£å­¦ä¹  - å¤šåˆ†ç±»é—®é¢˜
- [ ] æ— ç›‘ç£å­¦ä¹  - èšç±»
- [ ] æ— ç›‘ç£å­¦ä¹  - é™ç»´
- [ ] æ— ç›‘ç£å­¦ä¹  - å¼‚å¸¸æ£€æµ‹

## 4. ä¸»è¦æŒ‘æˆ˜
1. ___________
2. ___________
3. ___________

## 5. æ¨èæ–¹å‘
- é¦–é€‰æ–¹æ¡ˆ: ___________
- å¤‡é€‰æ–¹æ¡ˆ: ___________
- ä¸å»ºè®®: ___________

## 6. ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. ___________
2. ___________
```

---

## ğŸ¯ ç¬¬äºŒéƒ¨åˆ†ï¼šé—®é¢˜å®šä¹‰é˜¶æ®µ

> **ç›®æ ‡**ï¼šå°†ä¸šåŠ¡é—®é¢˜è½¬åŒ–ä¸ºæ˜ç¡®çš„æœºå™¨å­¦ä¹ é—®é¢˜

### 2.1 ä¸šåŠ¡é—®é¢˜ â†’ ML é—®é¢˜æ˜ å°„è¡¨

| ä¸šåŠ¡é—®é¢˜ | ML é—®é¢˜ç±»å‹ | å…¸å‹ç®—æ³• | è¯„ä¼°æŒ‡æ ‡ |
|---------|-----------|---------|---------|
| é¢„æµ‹æˆ¿ä»· | å›å½’ | çº¿æ€§å›å½’ã€XGBoost | RMSE, MAE, RÂ² |
| å®¢æˆ·æ˜¯å¦æµå¤± | äºŒåˆ†ç±» | é€»è¾‘å›å½’ã€éšæœºæ£®æ— | AUC, F1, Recall |
| é‚®ä»¶åˆ†ç±»ï¼ˆåƒåœ¾/æ­£å¸¸ï¼‰ | äºŒåˆ†ç±» | æœ´ç´ è´å¶æ–¯ã€SVM | Precision, Recall |
| æ‰‹å†™æ•°å­—è¯†åˆ« | å¤šåˆ†ç±» | SVMã€ç¥ç»ç½‘ç»œ | Accuracy, F1-macro |
| å®¢æˆ·åˆ†ç¾¤ | èšç±» | K-Meansã€GMM | è½®å»“ç³»æ•°ã€DBæŒ‡æ•° |
| æ•°æ®å¯è§†åŒ– | é™ç»´ | PCAã€t-SNE | æ–¹å·®è§£é‡Šç‡ |
| ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹ | å¼‚å¸¸æ£€æµ‹ | Isolation Forest | Precision@K, Recall@K |
| æ¨èç³»ç»Ÿ | ååŒè¿‡æ»¤ | ALSã€çŸ©é˜µåˆ†è§£ | NDCG, MAP |

---

### ğŸŒ³ 2.2 é—®é¢˜å®šä¹‰å†³ç­–æ ‘

```mermaid
graph TD
    A[ä¸šåŠ¡é—®é¢˜] --> B{æœ‰å†å²æ ‡æ³¨æ•°æ®?}

    B -->|æœ‰| C{è¦é¢„æµ‹ä»€ä¹ˆ?}
    B -->|æ— | D{æƒ³å‘ç°ä»€ä¹ˆ?}

    C -->|æ•°å€¼| E[å›å½’: é¢„æµ‹å…·ä½“å€¼]
    C -->|ç±»åˆ«| F[åˆ†ç±»: é¢„æµ‹æ‰€å±ç±»åˆ«]
    C -->|æ’åº| G[æ’åº: é¢„æµ‹ç›¸å¯¹é¡ºåº]

    D -->|ç¾¤ä½“å·®å¼‚| H[èšç±»: å‘ç°ä¸åŒç¾¤ä½“]
    D -->|ä¸»è¦æ¨¡å¼| I[é™ç»´: æå–ä¸»è¦ç‰¹å¾]
    D -->|å¼‚å¸¸ä¸ªä½“| J[å¼‚å¸¸æ£€æµ‹: è¯†åˆ«ç¦»ç¾¤ç‚¹]

    E --> K[é€‰æ‹©å›å½’ç®—æ³•]
    F --> L[é€‰æ‹©åˆ†ç±»ç®—æ³•]
    G --> M[é€‰æ‹©æ’åºç®—æ³•]
    H --> N[é€‰æ‹©èšç±»ç®—æ³•]
    I --> O[é€‰æ‹©é™ç»´ç®—æ³•]
    J --> P[é€‰æ‹©å¼‚å¸¸æ£€æµ‹ç®—æ³•]
```

---

### 2.3 æˆåŠŸæŒ‡æ ‡é€‰æ‹©æŒ‡å—

#### ğŸ“Š å›å½’é—®é¢˜æŒ‡æ ‡

| æŒ‡æ ‡ | å«ä¹‰ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|------|------|------|---------|
| **MAE** | å¹³å‡ç»å¯¹è¯¯å·® | ç›´è§‚æ˜“æ‡‚ï¼Œå•ä½ä¸ç›®æ ‡ä¸€è‡´ | å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ | ç›®æ ‡å€¼åˆ†å¸ƒå‡åŒ€ |
| **RMSE** | å‡æ–¹æ ¹è¯¯å·® | æƒ©ç½šå¤§è¯¯å·® | å—å¼‚å¸¸å€¼å½±å“å¤§ | éœ€è¦é‡è§†å¤§è¯¯å·® |
| **RÂ²** | å†³å®šç³»æ•° | 0-1èŒƒå›´ï¼Œæ˜“äºæ¯”è¾ƒ | å¯èƒ½ä¸ºè´Ÿ | æ¨¡å‹å¯¹æ¯” |
| **MAPE** | å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® | ç›¸å¯¹è¯¯å·®ï¼Œä¾¿äºè·¨æ•°æ®é›†æ¯”è¾ƒ | ç›®æ ‡å€¼æ¥è¿‘0æ—¶ä¸é€‚ç”¨ | é¢„æµ‹é”€é‡ã€ä»·æ ¼ |

**é€‰æ‹©å»ºè®®**ï¼š
- é»˜è®¤ä½¿ç”¨ **RMSE**ï¼ˆæœ€å¸¸ç”¨ï¼‰
- éœ€è¦ç›´è§‚è§£é‡Šç”¨ **MAE**
- è·¨é¡¹ç›®å¯¹æ¯”ç”¨ **RÂ²** æˆ– **MAPE**

---

#### ğŸ“Š åˆ†ç±»é—®é¢˜æŒ‡æ ‡

| æŒ‡æ ‡ | å«ä¹‰ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|------|------|------|---------|
| **Accuracy** | å‡†ç¡®ç‡ | ç›´è§‚ï¼Œæ˜“ç†è§£ | ç±»åˆ«ä¸å¹³è¡¡æ—¶å¤±æ•ˆ | ç±»åˆ«å¹³è¡¡çš„é—®é¢˜ |
| **Precision** | ç²¾ç¡®ç‡ | å…³æ³¨"é¢„æµ‹ä¸ºæ­£"çš„å‡†ç¡®æ€§ | å¿½ç•¥æ¼æŠ¥ | å‡å°‘è¯¯æŠ¥ï¼ˆåƒåœ¾é‚®ä»¶è¿‡æ»¤ï¼‰ |
| **Recall** | å¬å›ç‡ | å…³æ³¨"å®é™…ä¸ºæ­£"çš„è¦†ç›–ç‡ | å¿½ç•¥è¯¯æŠ¥ | å‡å°‘æ¼æŠ¥ï¼ˆç–¾ç—…è¯Šæ–­ï¼‰ |
| **F1-Score** | F1å€¼ | Precision å’Œ Recall çš„è°ƒå’Œå¹³å‡ | ä¸é€‚åˆæç«¯ä¸å¹³è¡¡ | ä¸¤è€…éƒ½é‡è¦ |
| **ROC-AUC** | ROCæ›²çº¿ä¸‹é¢ç§¯ | ä¸å—é˜ˆå€¼å½±å“ï¼Œé€‚åˆä¸å¹³è¡¡ | ä¸é€‚åˆæç«¯ä¸å¹³è¡¡(0.1%æ­£ä¾‹) | è¯„ä¼°æ¨¡å‹æ•´ä½“æ€§èƒ½ |
| **PR-AUC** | PRæ›²çº¿ä¸‹é¢ç§¯ | é€‚åˆæç«¯ä¸å¹³è¡¡ | è®¡ç®—å¤æ‚ | æ­£ä¾‹æå°‘çš„åœºæ™¯ |

**é€‰æ‹©å»ºè®®**ï¼š
- **ç±»åˆ«å¹³è¡¡** â†’ Accuracy
- **ç±»åˆ«è½»å¾®ä¸å¹³è¡¡** â†’ F1-Score
- **ç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡** â†’ PR-AUC
- **éœ€è¦è°ƒæ•´é˜ˆå€¼** â†’ ROC-AUC
- **è¯¯æŠ¥ä»£ä»·é«˜** â†’ Precision
- **æ¼æŠ¥ä»£ä»·é«˜** â†’ Recall

**å®é™…æ¡ˆä¾‹**ï¼š
```python
# ç¤ºä¾‹ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹ï¼ˆæ­£ä¾‹0.17%ï¼‰
# ç›®æ ‡ï¼šå°½å¯èƒ½æŠ“ä½æ¬ºè¯ˆï¼ˆé«˜Recallï¼‰ï¼ŒåŒæ—¶å‡å°‘è¯¯æŠ¥ï¼ˆåˆç†Precisionï¼‰
# æ¨èæŒ‡æ ‡ï¼šPR-AUCï¼ˆä¸»è¦ï¼‰ + Recall@Precision=0.9ï¼ˆä¸šåŠ¡çº¦æŸï¼‰

from sklearn.metrics import average_precision_score, precision_recall_curve

# è®¡ç®— PR-AUC
pr_auc = average_precision_score(y_true, y_scores)

# è®¡ç®—åœ¨ Precision=0.9 æ—¶çš„ Recall
precisions, recalls, thresholds = precision_recall_curve(y_true, y_scores)
idx = np.argmin(np.abs(precisions - 0.9))
recall_at_p90 = recalls[idx]

print(f"PR-AUC: {pr_auc:.4f}")
print(f"Recall@P=0.9: {recall_at_p90:.4f}")
```

---

#### ğŸ“Š èšç±»é—®é¢˜æŒ‡æ ‡

| æŒ‡æ ‡ | å«ä¹‰ | å–å€¼èŒƒå›´ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|-----|------|---------|------|------|
| **è½®å»“ç³»æ•°** | æ ·æœ¬ä¸å…¶ç°‡çš„ç›¸ä¼¼åº¦ | [-1, 1] | ä¸éœ€è¦çœŸå®æ ‡ç­¾ | è®¡ç®—å¤æ‚åº¦é«˜ |
| **DBæŒ‡æ•°** | ç°‡é—´è·ç¦»/ç°‡å†…è·ç¦» | [0, âˆ) | è®¡ç®—ç®€å• | å¯¹ç°‡å½¢çŠ¶æ•æ„Ÿ |
| **CHæŒ‡æ•°** | ç°‡é—´æ–¹å·®/ç°‡å†…æ–¹å·® | [0, âˆ) | è®¡ç®—å¿«é€Ÿ | å¯¹å‡¸å½¢ç°‡æ•ˆæœå¥½ |

**é€‰æ‹©å»ºè®®**ï¼š
- é»˜è®¤ä½¿ç”¨ **è½®å»“ç³»æ•°**ï¼ˆæœ€å¯é ï¼‰
- å¤§æ•°æ®é›†ç”¨ **DBæŒ‡æ•°**ï¼ˆé€Ÿåº¦å¿«ï¼‰
- é…åˆ**ä¸šåŠ¡è§£é‡Šæ€§**ï¼ˆæ¯ä¸ªç°‡çš„å®é™…æ„ä¹‰ï¼‰

---

### 2.4 é—®é¢˜å®šä¹‰æ£€æŸ¥æ¸…å•

åœ¨è¿›å…¥ç®—æ³•é€‰æ‹©å‰ï¼Œç¡®ä¿å®Œæˆä»¥ä¸‹æ£€æŸ¥ï¼š

- [ ] **ä¸šåŠ¡ç›®æ ‡æ˜ç¡®**ï¼šçŸ¥é“è¦è§£å†³ä»€ä¹ˆé—®é¢˜
- [ ] **MLé—®é¢˜ç±»å‹ç¡®å®š**ï¼šå›å½’/åˆ†ç±»/èšç±»/...
- [ ] **æˆåŠŸæŒ‡æ ‡é€‰æ‹©**ï¼šç¡®å®šä¸»è¦å’Œæ¬¡è¦æŒ‡æ ‡
- [ ] **åŸºå‡†æ€§èƒ½è®¾å®š**ï¼šç®€å•æ¨¡å‹çš„baselineæ˜¯å¤šå°‘
- [ ] **çº¦æŸæ¡ä»¶æ˜ç¡®**ï¼šæ—¶é—´/æˆæœ¬/å¯è§£é‡Šæ€§è¦æ±‚

**ç¤ºä¾‹**ï¼š
```markdown
## é—®é¢˜å®šä¹‰å¡ç‰‡

**ä¸šåŠ¡ç›®æ ‡**: é¢„æµ‹å®¢æˆ·åœ¨æœªæ¥3ä¸ªæœˆå†…æ˜¯å¦ä¼šæµå¤±

**MLé—®é¢˜ç±»å‹**: äºŒåˆ†ç±»é—®é¢˜

**æˆåŠŸæŒ‡æ ‡**:
- ä¸»è¦æŒ‡æ ‡: ROC-AUCï¼ˆè¯„ä¼°æ•´ä½“æ€§èƒ½ï¼‰
- æ¬¡è¦æŒ‡æ ‡: Recall@Precision=0.8ï¼ˆä¸šåŠ¡çº¦æŸï¼‰
- è§£é‡Š: å¸Œæœ›åœ¨ä¿è¯80%ç²¾ç¡®ç‡çš„å‰æä¸‹ï¼Œå°½å¯èƒ½é«˜çš„å¬å›ç‡

**åŸºå‡†æ€§èƒ½**:
- éšæœºçŒœæµ‹: AUC=0.5
- é€»è¾‘å›å½’: AUC=0.75ï¼ˆé¢„æœŸbaselineï¼‰
- ç›®æ ‡: AUC > 0.85

**çº¦æŸæ¡ä»¶**:
- è®­ç»ƒæ—¶é—´: < 30åˆ†é’Ÿ
- é¢„æµ‹å»¶è¿Ÿ: < 100ms
- å¯è§£é‡Šæ€§: éœ€è¦èƒ½å¤Ÿè§£é‡Šå‰10ä¸ªé‡è¦ç‰¹å¾
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šç®—æ³•é€‰æ‹©é˜¶æ®µ

> **ç›®æ ‡**ï¼šæ ¹æ®æ•°æ®ç‰¹ç‚¹å’Œé—®é¢˜ç±»å‹ï¼Œé€‰æ‹©åˆé€‚çš„ç®—æ³•

### ğŸŒ³ 3.1 ç®—æ³•é€‰æ‹©æ€»å†³ç­–æ ‘

```mermaid
graph TD
    A[é—®é¢˜ç±»å‹å·²ç¡®å®š] --> B{ç›‘ç£å­¦ä¹ ?}

    B -->|YES| C{å›å½’ or åˆ†ç±»?}
    B -->|NO| D{æ— ç›‘ç£å­¦ä¹ ç›®æ ‡?}

    C -->|å›å½’| E{æ•°æ®é‡?}
    C -->|åˆ†ç±»| F{æ•°æ®é‡?}

    E -->|<1ä¸‡| E1[çº¿æ€§å›å½’/å†³ç­–æ ‘]
    E -->|1-10ä¸‡| E2[éšæœºæ£®æ—/XGBoost]
    E -->|>10ä¸‡| E3[LightGBM]

    F -->|<1ä¸‡| F1{ç±»åˆ«å¹³è¡¡?}
    F -->|1-10ä¸‡| F2[éšæœºæ£®æ—/XGBoost]
    F -->|>10ä¸‡| F3[LightGBM]

    F1 -->|å¹³è¡¡| F1A[é€»è¾‘å›å½’/SVM]
    F1 -->|ä¸å¹³è¡¡| F1B[XGBoost+æƒé‡/SMOTE]

    D -->|èšç±»| G{æ•°æ®ç‰¹ç‚¹?}
    D -->|é™ç»´| H{ç›®æ ‡?}
    D -->|å¼‚å¸¸æ£€æµ‹| I{æ•°æ®åˆ†å¸ƒ?}

    G -->|çƒå½¢ç°‡| G1[K-Means]
    G -->|ä»»æ„å½¢çŠ¶| G2[DBSCAN]
    G -->|æ¦‚ç‡æ¨¡å‹| G3[GMM]

    H -->|å¯è§†åŒ–| H1[t-SNE/UMAP]
    H -->|ç‰¹å¾æå–| H2[PCA]
    H -->|åˆ†ç±»å‰é™ç»´| H3[LDA]

    I -->|é«˜æ–¯åˆ†å¸ƒ| I1[One-Class SVM]
    I -->|ä»»æ„åˆ†å¸ƒ| I2[Isolation Forest]
    I -->|å±€éƒ¨å¯†åº¦| I3[LOF]
```

---

### 3.2 ç›‘ç£å­¦ä¹ ç®—æ³•é€‰æ‹©è¯¦è§£

#### ğŸ“Œ å†³ç­–ç‚¹1ï¼šæ•°æ®é‡çº§

**< 1,000 æ ·æœ¬ï¼ˆå°æ•°æ®é›†ï¼‰**
- âœ… **æ¨è**ï¼šçº¿æ€§æ¨¡å‹ã€å†³ç­–æ ‘ã€æœ´ç´ è´å¶æ–¯ã€KNN
- âŒ **ä¸æ¨è**ï¼šæ·±åº¦ç¥ç»ç½‘ç»œã€å¤§å‹é›†æˆæ¨¡å‹
- **åŸå› **ï¼šå¤æ‚æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆ

**1,000 - 10,000 æ ·æœ¬ï¼ˆä¸­ç­‰æ•°æ®é›†ï¼‰**
- âœ… **æ¨è**ï¼šSVMã€éšæœºæ£®æ—ã€XGBoost
- âš ï¸ **è°¨æ…**ï¼šæ·±åº¦å­¦ä¹ ï¼ˆéœ€è¦æ•°æ®å¢å¼ºï¼‰
- **åŸå› **ï¼šé›†æˆæ¨¡å‹åœ¨ä¸­ç­‰æ•°æ®ä¸Šæ•ˆæœæœ€å¥½

**10,000 - 100,000 æ ·æœ¬ï¼ˆå¤§æ•°æ®é›†ï¼‰**
- âœ… **æ¨è**ï¼šXGBoostã€LightGBMã€ç¥ç»ç½‘ç»œ
- âœ… **ä¼˜åŒ–**ï¼šä½¿ç”¨ LightGBM æé€Ÿ
- **åŸå› **ï¼šå¯ä»¥å……åˆ†åˆ©ç”¨æ•°æ®ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

**> 100,000 æ ·æœ¬ï¼ˆæµ·é‡æ•°æ®é›†ï¼‰**
- âœ… **æ¨è**ï¼šLightGBMã€CatBoostã€æ·±åº¦å­¦ä¹ 
- âœ… **å¿…é¡»**ï¼šä½¿ç”¨é«˜æ•ˆç®—æ³•å’Œåˆ†å¸ƒå¼è®­ç»ƒ
- **åŸå› **ï¼šè®­ç»ƒé€Ÿåº¦æˆä¸ºä¸»è¦è€ƒè™‘å› ç´ 

---

#### ğŸ“Œ å†³ç­–ç‚¹2ï¼šç‰¹å¾ç»´åº¦

**ä½ç»´ï¼ˆ< 10 ç‰¹å¾ï¼‰**
- âœ… **æ¨è**ï¼šçº¿æ€§æ¨¡å‹ã€å†³ç­–æ ‘ã€SVM
- **åŸå› **ï¼šç®€å•æ¨¡å‹è¶³å¤Ÿï¼Œå¯è§£é‡Šæ€§å¼º

**ä¸­ç»´ï¼ˆ10-50 ç‰¹å¾ï¼‰**
- âœ… **æ¨è**ï¼šéšæœºæ£®æ—ã€XGBoostã€SVM
- **åŸå› **ï¼šéœ€è¦å¤„ç†ç‰¹å¾äº¤äº’

**é«˜ç»´ï¼ˆ50-1000 ç‰¹å¾ï¼‰**
- âœ… **æ¨è**ï¼šLassoå›å½’ã€éšæœºæ£®æ—ã€LightGBM
- âš ï¸ **æ³¨æ„**ï¼šè€ƒè™‘ç‰¹å¾é€‰æ‹©
- **åŸå› **ï¼šéœ€è¦æ­£åˆ™åŒ–å’Œç‰¹å¾é€‰æ‹©

**è¶…é«˜ç»´ï¼ˆ> 1000 ç‰¹å¾ï¼‰**
- âœ… **æ¨è**ï¼šLassoã€ElasticNetã€LightGBM
- âœ… **å¿…é¡»**ï¼šç‰¹å¾é€‰æ‹© + é™ç»´
- **åŸå› **ï¼šç»´åº¦ç¾éš¾ï¼Œå¿…é¡»é™ç»´

---

#### ğŸ“Œ å†³ç­–ç‚¹3ï¼šç±»åˆ«å¹³è¡¡ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰

**å¹³è¡¡æ•°æ®ï¼ˆæœ€å°ç±» > 30%ï¼‰**
- âœ… **æ ‡å‡†æ–¹æ³•**ï¼šä»»ä½•åˆ†ç±»ç®—æ³•
- **æŒ‡æ ‡**ï¼šAccuracy, F1-Score

**è½»å¾®ä¸å¹³è¡¡ï¼ˆæœ€å°ç±» 10-30%ï¼‰**
- âœ… **æ¨è**ï¼šXGBoost/LightGBM + è°ƒæ•´ scale_pos_weight
- **æŒ‡æ ‡**ï¼šF1-Score, ROC-AUC

**ä¸¥é‡ä¸å¹³è¡¡ï¼ˆæœ€å°ç±» 1-10%ï¼‰**
- âœ… **æ¨è**ï¼š
  - SMOTE è¿‡é‡‡æ · + XGBoost
  - è°ƒæ•´ç±»åˆ«æƒé‡
  - å¼‚å¸¸æ£€æµ‹ç®—æ³•
- **æŒ‡æ ‡**ï¼šPR-AUC, Recall@Precision

**æç«¯ä¸å¹³è¡¡ï¼ˆæœ€å°ç±» < 1%ï¼‰**
- âœ… **æ¨è**ï¼š
  - Isolation Forestï¼ˆå¼‚å¸¸æ£€æµ‹ï¼‰
  - One-Class SVM
  - å®šåˆ¶æŸå¤±å‡½æ•°
- **æŒ‡æ ‡**ï¼šPrecision@K, Recall@K

---

### 3.3 æ— ç›‘ç£å­¦ä¹ ç®—æ³•é€‰æ‹©è¯¦è§£

#### ğŸ“Œ èšç±»ç®—æ³•é€‰æ‹©

**åœºæ™¯ 1ï¼šçƒå½¢ç°‡ + å·²çŸ¥ç°‡æ•°**
- âœ… **é¦–é€‰**ï¼šK-Means
- **ä¼˜ç‚¹**ï¼šå¿«é€Ÿã€ç®€å•ã€ç»“æœç¨³å®š
- **ç¼ºç‚¹**ï¼šåªèƒ½æ‰¾çƒå½¢ç°‡

**åœºæ™¯ 2ï¼šä»»æ„å½¢çŠ¶ç°‡ + æœªçŸ¥ç°‡æ•°**
- âœ… **é¦–é€‰**ï¼šDBSCAN
- **ä¼˜ç‚¹**ï¼šè‡ªåŠ¨ç¡®å®šç°‡æ•°ã€å‘ç°ä»»æ„å½¢çŠ¶
- **ç¼ºç‚¹**ï¼šå‚æ•°æ•æ„Ÿ

**åœºæ™¯ 3ï¼šéœ€è¦æ¦‚ç‡åˆ†å¸ƒ**
- âœ… **é¦–é€‰**ï¼šGMMï¼ˆé«˜æ–¯æ··åˆæ¨¡å‹ï¼‰
- **ä¼˜ç‚¹**ï¼šè½¯èšç±»ã€æ¦‚ç‡è§£é‡Š
- **ç¼ºç‚¹**ï¼šè®¡ç®—å¤æ‚

**åœºæ™¯ 4ï¼šå±‚æ¬¡ç»“æ„**
- âœ… **é¦–é€‰**ï¼šå±‚æ¬¡èšç±»
- **ä¼˜ç‚¹**ï¼šæ ‘çŠ¶ç»“æ„ã€å¯è§£é‡Šæ€§å¼º
- **ç¼ºç‚¹**ï¼šè®¡ç®—å¤æ‚åº¦é«˜

---

#### ğŸ“Œ é™ç»´ç®—æ³•é€‰æ‹©

**åœºæ™¯ 1ï¼šæ•°æ®å¯è§†åŒ–ï¼ˆé™åˆ°2-3ç»´ï¼‰**
- âœ… **é¦–é€‰**ï¼št-SNE æˆ– UMAP
- **ä¼˜ç‚¹**ï¼šä¿ç•™å±€éƒ¨ç»“æ„ã€å¯è§†åŒ–æ•ˆæœå¥½
- **ç¼ºç‚¹**ï¼šä¸èƒ½ç”¨äºæ–°æ•°æ®é¢„æµ‹

**åœºæ™¯ 2ï¼šç‰¹å¾æå–ï¼ˆä¿ç•™ä¸»è¦ä¿¡æ¯ï¼‰**
- âœ… **é¦–é€‰**ï¼šPCA
- **ä¼˜ç‚¹**ï¼šçº¿æ€§å˜æ¢ã€å¯é€†ã€å¯è§£é‡Š
- **ç¼ºç‚¹**ï¼šåªèƒ½æ•è·çº¿æ€§å…³ç³»

**åœºæ™¯ 3ï¼šåˆ†ç±»å‰é™ç»´**
- âœ… **é¦–é€‰**ï¼šLDAï¼ˆçº¿æ€§åˆ¤åˆ«åˆ†æï¼‰
- **ä¼˜ç‚¹**ï¼šç›‘ç£é™ç»´ã€æœ€å¤§åŒ–ç±»é—´è·ç¦»
- **ç¼ºç‚¹**ï¼šéœ€è¦æ ‡ç­¾

**åœºæ™¯ 4ï¼šéçº¿æ€§é™ç»´**
- âœ… **é¦–é€‰**ï¼šKernel PCA æˆ– Autoencoder
- **ä¼˜ç‚¹**ï¼šæ•è·éçº¿æ€§å…³ç³»
- **ç¼ºç‚¹**ï¼šè®¡ç®—å¤æ‚

---

#### ğŸ“Œ å¼‚å¸¸æ£€æµ‹ç®—æ³•é€‰æ‹©

**åœºæ™¯ 1ï¼šé«˜ç»´æ•°æ® + å¤§æ•°æ®é‡**
- âœ… **é¦–é€‰**ï¼šIsolation Forest
- **ä¼˜ç‚¹**ï¼šå¿«é€Ÿã€é€‚åˆé«˜ç»´
- **ç¼ºç‚¹**ï¼šå¯¹å…¨å±€å¼‚å¸¸æ•æ„Ÿ

**åœºæ™¯ 2ï¼šæ•°æ®è¿‘ä¼¼é«˜æ–¯åˆ†å¸ƒ**
- âœ… **é¦–é€‰**ï¼šOne-Class SVM
- **ä¼˜ç‚¹**ï¼šç†è®ºåŸºç¡€æ‰å®
- **ç¼ºç‚¹**ï¼šé«˜ç»´æ•°æ®æ…¢

**åœºæ™¯ 3ï¼šå±€éƒ¨å¼‚å¸¸æ£€æµ‹**
- âœ… **é¦–é€‰**ï¼šLOFï¼ˆå±€éƒ¨ç¦»ç¾¤å› å­ï¼‰
- **ä¼˜ç‚¹**ï¼šæ£€æµ‹å±€éƒ¨å¼‚å¸¸
- **ç¼ºç‚¹**ï¼šå‚æ•°é€‰æ‹©å›°éš¾

---

### 3.4 æ··åˆæ–¹æ³•ï¼šç›‘ç£ + æ— ç›‘ç£ç»“åˆ

#### ğŸ’¡ åœºæ™¯ 1ï¼šå…ˆèšç±»ï¼Œå†åˆ†ç±»ï¼ˆCluster-Then-Predictï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- æ•°æ®å­˜åœ¨æ˜æ˜¾çš„ç¾¤ä½“å·®å¼‚
- ä¸åŒç¾¤ä½“çš„é¢„æµ‹æ¨¡å¼ä¸åŒ

**å®æ–½æ­¥éª¤**ï¼š
```python
# 1. å…ˆèšç±»åˆ†ç¾¤
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, random_state=42)
df['cluster'] = kmeans.fit_predict(X)

# 2. å¯¹æ¯ä¸ªç¾¤ä½“åˆ†åˆ«å»ºæ¨¡
models = {}
for cluster_id in range(4):
    mask = df['cluster'] == cluster_id
    X_cluster = X[mask]
    y_cluster = y[mask]

    model = XGBClassifier()
    model.fit(X_cluster, y_cluster)
    models[cluster_id] = model

# 3. é¢„æµ‹æ—¶å…ˆåˆ¤æ–­æ‰€å±ç°‡ï¼Œå†ç”¨å¯¹åº”æ¨¡å‹
def predict(X_new):
    cluster_id = kmeans.predict(X_new)[0]
    return models[cluster_id].predict(X_new)
```

**é¢„æœŸæå‡**ï¼šé€šå¸¸æå‡ 2-5% AUC

---

#### ğŸ’¡ åœºæ™¯ 2ï¼šå¼‚å¸¸åˆ†æ•°ä½œä¸ºç‰¹å¾

**é€‚ç”¨åœºæ™¯**ï¼š
- æ•°æ®ä¸­å­˜åœ¨å¼‚å¸¸æ¨¡å¼
- å¼‚å¸¸æ ·æœ¬æœ‰ç‰¹æ®Šé¢„æµ‹è§„å¾‹

**å®æ–½æ­¥éª¤**ï¼š
```python
# 1. è®¡ç®—å¼‚å¸¸åˆ†æ•°
from sklearn.ensemble import IsolationForest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
df['anomaly_score'] = iso_forest.fit_predict(X)

# 2. ä½œä¸ºæ–°ç‰¹å¾åŠ å…¥è®­ç»ƒ
X_new = np.column_stack([X, df['anomaly_score']])

# 3. è®­ç»ƒæ¨¡å‹
model = XGBClassifier()
model.fit(X_new, y)
```

**é¢„æœŸæå‡**ï¼šé€šå¸¸æå‡ 1-3% AUC

---

#### ğŸ’¡ åœºæ™¯ 3ï¼šPCAé™ç»´ + æ¨¡å‹è®­ç»ƒ

**é€‚ç”¨åœºæ™¯**ï¼š
- é«˜ç»´æ•°æ®ï¼ˆ>50ç‰¹å¾ï¼‰
- ç‰¹å¾é—´é«˜åº¦ç›¸å…³
- éœ€è¦åŠ é€Ÿè®­ç»ƒ

**å®æ–½æ­¥éª¤**ï¼š
```python
# 1. PCAé™ç»´
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95, random_state=42)  # ä¿ç•™95%æ–¹å·®
X_pca = pca.fit_transform(X)

print(f"åŸå§‹ç‰¹å¾: {X.shape[1]} â†’ PCAç‰¹å¾: {X_pca.shape[1]}")

# 2. ç”¨é™ç»´åçš„ç‰¹å¾è®­ç»ƒ
model = XGBClassifier()
model.fit(X_pca, y)
```

**æƒè¡¡**ï¼š
- âœ… **ä¼˜ç‚¹**ï¼šè®­ç»ƒé€Ÿåº¦æå‡ 2-5 å€
- âŒ **ç¼ºç‚¹**ï¼šAUC å¯èƒ½ç•¥é™ 1-2%
- ğŸ“Š **å»ºè®®**ï¼šåœ¨é€Ÿåº¦å’Œç²¾åº¦é—´æƒè¡¡

---

#### ğŸ’¡ åœºæ™¯ 4ï¼šèšç±»æ ‡ç­¾ä½œä¸ºç±»åˆ«ç‰¹å¾

**é€‚ç”¨åœºæ™¯**ï¼š
- å­˜åœ¨æ½œåœ¨åˆ†ç¾¤ç»“æ„
- æ¨¡å‹éœ€è¦æ•è·ç¾¤ä½“ç‰¹å¾

**å®æ–½æ­¥éª¤**ï¼š
```python
# 1. èšç±»å¾—åˆ°ç°‡æ ‡ç­¾
kmeans = KMeans(n_clusters=5, random_state=42)
cluster_labels = kmeans.fit_predict(X)

# 2. ä½œä¸ºç±»åˆ«ç‰¹å¾ç¼–ç 
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse=False)
cluster_encoded = encoder.fit_transform(cluster_labels.reshape(-1, 1))

# 3. æ‹¼æ¥åŸå§‹ç‰¹å¾
X_combined = np.column_stack([X, cluster_encoded])

# 4. è®­ç»ƒæ¨¡å‹
model = RandomForestClassifier()
model.fit(X_combined, y)
```

**é¢„æœŸæå‡**ï¼šé€šå¸¸æå‡ 1-3% AUC

---

### 3.5 ç®—æ³•é€‰æ‹©å¿«é€Ÿå‚è€ƒè¡¨

#### ç›‘ç£å­¦ä¹ å¿«é€Ÿå‚è€ƒ

| åœºæ™¯ | é¦–é€‰ç®—æ³• | å¤‡é€‰ç®—æ³• | ä¸æ¨è |
|------|---------|---------|--------|
| å°æ•°æ® + çº¿æ€§å…³ç³» | çº¿æ€§/é€»è¾‘å›å½’ | SVM | XGBoost, æ·±åº¦å­¦ä¹  |
| å°æ•°æ® + éçº¿æ€§ | å†³ç­–æ ‘, SVM | KNN | éšæœºæ£®æ—, æ·±åº¦å­¦ä¹  |
| ä¸­ç­‰æ•°æ® + å¹³è¡¡ | éšæœºæ£®æ—, XGBoost | SVM, LightGBM | çº¿æ€§æ¨¡å‹ |
| å¤§æ•°æ® + é«˜ç»´ | LightGBM | XGBoost, ç¥ç»ç½‘ç»œ | KNN, SVM |
| ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡ | SMOTE+XGBoost | Isolation Forest | æ ‡å‡†åˆ†ç±»å™¨ |
| éœ€è¦å¯è§£é‡Šæ€§ | é€»è¾‘å›å½’, å†³ç­–æ ‘ | çº¿æ€§å›å½’ | æ·±åº¦å­¦ä¹ , XGBoost |
| éœ€è¦æ¦‚ç‡è¾“å‡º | é€»è¾‘å›å½’, XGBoost | éšæœºæ£®æ— | SVM(éœ€è°ƒæ•´) |

#### æ— ç›‘ç£å­¦ä¹ å¿«é€Ÿå‚è€ƒ

| åœºæ™¯ | é¦–é€‰ç®—æ³• | å¤‡é€‰ç®—æ³• | ä¸æ¨è |
|------|---------|---------|--------|
| çƒå½¢ç°‡ + å·²çŸ¥K | K-Means | GMM | DBSCAN |
| ä»»æ„å½¢çŠ¶ç°‡ | DBSCAN | å±‚æ¬¡èšç±» | K-Means |
| æ•°æ®å¯è§†åŒ– | t-SNE, UMAP | PCA | K-Means |
| ç‰¹å¾é™ç»´ | PCA | Autoencoder | t-SNE |
| å¼‚å¸¸æ£€æµ‹ | Isolation Forest | LOF | K-Means |

---

## ğŸ¯ ç¬¬å››éƒ¨åˆ†ï¼šæ•°æ®å¤„ç†é˜¶æ®µ

> **ç›®æ ‡**ï¼šæ¸…æ´—æ•°æ®ã€å¤„ç†ç‰¹å¾ï¼Œä¸ºæ¨¡å‹è®­ç»ƒåšå‡†å¤‡

### 4.1 æ•°æ®é¢„å¤„ç†å†³ç­–æ ‘

#### ğŸŒ³ ç¼ºå¤±å€¼å¤„ç†å†³ç­–

```mermaid
graph TD
    A[å‘ç°ç¼ºå¤±å€¼] --> B{ç¼ºå¤±æ¯”ä¾‹?}

    B -->|>70%| C[åˆ é™¤è¯¥ç‰¹å¾]
    B -->|30-70%| D{ç¼ºå¤±éšæœº?}
    B -->|<30%| E{æ•°å€¼ or ç±»åˆ«?}

    D -->|éšæœº| E
    D -->|ééšæœº| F[åˆ›å»ºç¼ºå¤±æŒ‡ç¤ºå˜é‡ + å¡«å……]

    E -->|æ•°å€¼| G{åˆ†å¸ƒç‰¹ç‚¹?}
    E -->|ç±»åˆ«| H[ä¼—æ•°å¡«å…… or æ–°ç±»åˆ«'missing']

    G -->|æ­£æ€åˆ†å¸ƒ| G1[å‡å€¼å¡«å……]
    G -->|åæ€åˆ†å¸ƒ| G2[ä¸­ä½æ•°å¡«å……]
    G -->|æœ‰è¶‹åŠ¿| G3[KNN/æ’å€¼å¡«å……]
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# 1. æ£€æŸ¥ç¼ºå¤±æ¯”ä¾‹
missing_ratio = df.isnull().sum() / len(df)

# 2. åˆ é™¤é«˜ç¼ºå¤±ç‰¹å¾
high_missing_cols = missing_ratio[missing_ratio > 0.7].index
df = df.drop(columns=high_missing_cols)

# 3. æ•°å€¼ç‰¹å¾å¡«å……
from sklearn.impute import SimpleImputer
num_cols = df.select_dtypes(include=['float', 'int']).columns
imputer = SimpleImputer(strategy='median')  # ä¸­ä½æ•°ï¼ˆç¨³å¥ï¼‰
df[num_cols] = imputer.fit_transform(df[num_cols])

# 4. ç±»åˆ«ç‰¹å¾å¡«å……
cat_cols = df.select_dtypes(include=['object']).columns
df[cat_cols] = df[cat_cols].fillna('missing')  # æ–°ç±»åˆ«
```

---

#### ğŸŒ³ å¼‚å¸¸å€¼å¤„ç†å†³ç­–

```mermaid
graph TD
    A[å‘ç°å¼‚å¸¸å€¼] --> B{å¼‚å¸¸å€¼æ¥æº?}

    B -->|æ•°æ®é”™è¯¯| C[åˆ é™¤æˆ–ä¿®æ­£]
    B -->|çœŸå®æç«¯å€¼| D{å¯¹æ¨¡å‹å½±å“?}

    D -->|æ•æ„Ÿ<br/>çº¿æ€§æ¨¡å‹/KNN| E{å¼‚å¸¸æ¯”ä¾‹?}
    D -->|ä¸æ•æ„Ÿ<br/>æ ‘æ¨¡å‹/XGBoost| F[ä¿ç•™]

    E -->|<1%| E1[åˆ é™¤]
    E -->|1-5%| E2[æˆªæ–­/Winsorize]
    E -->|>5%| E3[è½¬æ¢/åˆ†ç®±]
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# æ–¹æ³•1ï¼šIQRæˆªæ–­
def cap_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    df[col] = df[col].clip(lower, upper)
    return df

# æ–¹æ³•2ï¼šWinsorizeï¼ˆä¿ç•™1%å’Œ99%åˆ†ä½æ•°ï¼‰
from scipy.stats import mstats
df['col_winsorized'] = mstats.winsorize(df['col'], limits=[0.01, 0.01])

# æ–¹æ³•3ï¼šå¯¹æ•°è½¬æ¢
df['col_log'] = np.log1p(df['col'])  # log(1 + x)ï¼Œå¤„ç†0å€¼
```

---

### 4.2 ç‰¹å¾å·¥ç¨‹å†³ç­–æ ‘

#### ğŸŒ³ ç‰¹å¾ç¼–ç å†³ç­–

```mermaid
graph TD
    A[ç±»åˆ«ç‰¹å¾] --> B{ç±»åˆ«æ•°é‡?}

    B -->|2ç±»<br/>äºŒå€¼ç‰¹å¾| C[Label Encoding<br/>0/1ç¼–ç ]
    B -->|3-10ç±»<br/>ä½åŸºæ•°| D[One-Hot Encoding<br/>ç‹¬çƒ­ç¼–ç ]
    B -->|10-50ç±»<br/>ä¸­åŸºæ•°| E{æ ‘æ¨¡å‹?}
    B -->|>50ç±»<br/>é«˜åŸºæ•°| F[Target Encoding<br/>ç›®æ ‡ç¼–ç ]

    E -->|YES<br/>XGBoost/LightGBM| G[Label Encoding<br/>æ ‘æ¨¡å‹å¯ç›´æ¥å¤„ç†]
    E -->|NO<br/>çº¿æ€§æ¨¡å‹| H[Frequency Encoding<br/>é¢‘ç‡ç¼–ç ]
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# 1. äºŒå€¼ç¼–ç 
df['gender'] = df['gender'].map({'Male': 0, 'Female': 1})

# 2. One-Hotç¼–ç ï¼ˆä½åŸºæ•°ï¼‰
df = pd.get_dummies(df, columns=['Contract'], drop_first=True)

# 3. Labelç¼–ç ï¼ˆæ ‘æ¨¡å‹ï¼‰
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['PaymentMethod_encoded'] = le.fit_transform(df['PaymentMethod'])

# 4. Targetç¼–ç ï¼ˆé«˜åŸºæ•°ï¼‰
def target_encode(df, col, target):
    """ç›®æ ‡ç¼–ç ï¼šç”¨è¯¥ç±»åˆ«çš„ç›®æ ‡å‡å€¼æ›¿ä»£"""
    target_mean = df.groupby(col)[target].mean()
    df[f'{col}_target_encoded'] = df[col].map(target_mean)
    return df

df = target_encode(df, 'HighCardinalityFeature', 'Churn')

# 5. é¢‘ç‡ç¼–ç 
df['Category_freq'] = df['Category'].map(df['Category'].value_counts())
```

---

#### ğŸŒ³ æ•°å€¼ç‰¹å¾å¤„ç†å†³ç­–

```mermaid
graph TD
    A[æ•°å€¼ç‰¹å¾] --> B{åˆ†å¸ƒç‰¹ç‚¹?}

    B -->|æ­£æ€åˆ†å¸ƒ| C[æ ‡å‡†åŒ–<br/>StandardScaler]
    B -->|åæ€åˆ†å¸ƒ| D[å¯¹æ•°è½¬æ¢ + æ ‡å‡†åŒ–]
    B -->|æœ‰ç•ŒèŒƒå›´| E[å½’ä¸€åŒ–<br/>MinMaxScaler]
    B -->|é•¿å°¾åˆ†å¸ƒ| F[RobustScaler<br/>å¯¹å¼‚å¸¸å€¼ç¨³å¥]

    A --> G{æ˜¯å¦åˆ†ç®±?}
    G -->|çº¿æ€§æ¨¡å‹| H[åˆ†ç®±å¢å¼ºéçº¿æ€§]
    G -->|æ ‘æ¨¡å‹| I[ä¸éœ€è¦åˆ†ç®±]
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# 1. æ ‡å‡†åŒ–ï¼ˆå‡å€¼0ï¼Œæ ‡å‡†å·®1ï¼‰
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[['tenure', 'MonthlyCharges']] = scaler.fit_transform(
    df[['tenure', 'MonthlyCharges']]
)

# 2. å½’ä¸€åŒ–ï¼ˆç¼©æ”¾åˆ°[0,1]ï¼‰
from sklearn.preprocessing import MinMaxScaler
minmax = MinMaxScaler()
df[['Age']] = minmax.fit_transform(df[['Age']])

# 3. å¯¹æ•°è½¬æ¢ï¼ˆå¤„ç†åæ€ï¼‰
df['TotalCharges_log'] = np.log1p(df['TotalCharges'])

# 4. ç¨³å¥ç¼©æ”¾ï¼ˆå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼‰
from sklearn.preprocessing import RobustScaler
robust = RobustScaler()
df[['OutlierFeature']] = robust.fit_transform(df[['OutlierFeature']])

# 5. åˆ†ç®±
df['tenure_bin'] = pd.cut(
    df['tenure'],
    bins=[0, 12, 24, 48, 100],
    labels=['0-1å¹´', '1-2å¹´', '2-4å¹´', '4å¹´+']
)
```

---

### 4.3 ç‰¹å¾å·¥ç¨‹æ¨¡å¼åº“

#### ğŸ’¡ æ¨¡å¼1ï¼šäº¤äº’ç‰¹å¾

**ä»€ä¹ˆæ—¶å€™åˆ›å»º**ï¼š
- ä¸¤ä¸ªç‰¹å¾ä¹‹é—´æœ‰ä¸šåŠ¡ä¸Šçš„è”ç³»
- çº¿æ€§æ¨¡å‹éœ€è¦æ•è·éçº¿æ€§å…³ç³»

**ç¤ºä¾‹**ï¼š
```python
# å®¢æˆ·æµå¤±é¢„æµ‹çš„äº¤äº’ç‰¹å¾
df['charges_per_month'] = df['TotalCharges'] / (df['tenure'] + 1)
df['charges_per_service'] = df['MonthlyCharges'] / (df['ServiceCount'] + 1)
df['is_new_customer'] = (df['tenure'] < 12).astype(int)

# æˆ¿ä»·é¢„æµ‹çš„äº¤äº’ç‰¹å¾
df['price_per_sqft'] = df['Price'] / df['LivingArea']
df['total_rooms'] = df['Bedrooms'] + df['Bathrooms']
df['age_renovated'] = df['YearBuilt'] - df['YearRemodeled']
```

---

#### ğŸ’¡ æ¨¡å¼2ï¼šèšåˆç‰¹å¾

**ä»€ä¹ˆæ—¶å€™åˆ›å»º**ï¼š
- å­˜åœ¨åˆ†ç»„ç»Ÿè®¡ä¿¡æ¯
- åŒä¸€å®ä½“æœ‰å¤šæ¡è®°å½•

**ç¤ºä¾‹**ï¼š
```python
# æŒ‰ç”¨æˆ·IDèšåˆ
user_stats = df.groupby('UserID').agg({
    'TransactionAmount': ['mean', 'sum', 'std', 'count'],
    'TransactionDate': lambda x: (x.max() - x.min()).days
}).reset_index()

user_stats.columns = ['UserID', 'avg_amount', 'total_amount',
                      'std_amount', 'transaction_count', 'active_days']

df = df.merge(user_stats, on='UserID', how='left')
```

---

#### ğŸ’¡ æ¨¡å¼3ï¼šæ—¶é—´ç‰¹å¾

**ä»€ä¹ˆæ—¶å€™åˆ›å»º**ï¼š
- æ•°æ®åŒ…å«æ—¥æœŸæ—¶é—´ä¿¡æ¯
- æ—¶é—´å‘¨æœŸæ€§å½±å“ç›®æ ‡å˜é‡

**ç¤ºä¾‹**ï¼š
```python
# ä»æ—¥æœŸæå–ç‰¹å¾
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['Quarter'] = df['Date'].dt.quarter
df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)
df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)

# æ—¶é—´é—´éš”ç‰¹å¾
df['DaysSinceLastPurchase'] = (pd.Timestamp.now() - df['LastPurchaseDate']).dt.days
```

---

#### ğŸ’¡ æ¨¡å¼4ï¼šæ–‡æœ¬ç‰¹å¾

**ä»€ä¹ˆæ—¶å€™åˆ›å»º**ï¼š
- æ•°æ®åŒ…å«æ–‡æœ¬ä¿¡æ¯
- æ–‡æœ¬å†…å®¹å½±å“ç›®æ ‡å˜é‡

**ç¤ºä¾‹**ï¼š
```python
# åŸºç¡€æ–‡æœ¬ç‰¹å¾
df['text_length'] = df['text'].str.len()
df['word_count'] = df['text'].str.split().str.len()
df['avg_word_length'] = df['text_length'] / (df['word_count'] + 1)

# TF-IDFç‰¹å¾
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features=100, stop_words='english')
tfidf_features = tfidf.fit_transform(df['text'])

# æ‹¼æ¥åˆ°åŸå§‹ç‰¹å¾
import scipy.sparse as sp
X_combined = sp.hstack([X, tfidf_features])
```

---

### 4.4 ç‰¹å¾é€‰æ‹©å†³ç­–æ ‘

```mermaid
graph TD
    A[ç‰¹å¾é€‰æ‹©] --> B{ç‰¹å¾æ•°é‡?}

    B -->|<20| C[å…¨éƒ¨ä¿ç•™<br/>æ¨¡å‹ä¼šè‡ªåŠ¨é€‰æ‹©]
    B -->|20-100| D{ä½¿ç”¨ä»€ä¹ˆæ¨¡å‹?}
    B -->|>100| E[å¿…é¡»ç‰¹å¾é€‰æ‹©]

    D -->|æ ‘æ¨¡å‹| F[ç‰¹å¾é‡è¦æ€§æ’åº<br/>ä¿ç•™Top-K]
    D -->|çº¿æ€§æ¨¡å‹| G[Lassoæ­£åˆ™åŒ–<br/>è‡ªåŠ¨é€‰æ‹©]

    E --> H[ä¸‰æ­¥æ³•]
    H --> H1[1. ç§»é™¤ä½æ–¹å·®ç‰¹å¾]
    H --> H2[2. ç§»é™¤é«˜ç›¸å…³ç‰¹å¾]
    H --> H3[3. åŸºäºé‡è¦æ€§é€‰æ‹©]
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# æ­¥éª¤1ï¼šç§»é™¤ä½æ–¹å·®ç‰¹å¾
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
X_selected = selector.fit_transform(X)

# æ­¥éª¤2ï¼šç§»é™¤é«˜ç›¸å…³ç‰¹å¾
corr_matrix = df.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]
df = df.drop(columns=to_drop)

# æ­¥éª¤3ï¼šåŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# é€‰æ‹©é‡è¦æ€§ > é˜ˆå€¼çš„ç‰¹å¾
selector = SelectFromModel(rf, threshold='median', prefit=True)
X_selected = selector.transform(X_train)

# æŸ¥çœ‹è¢«é€‰ä¸­çš„ç‰¹å¾
selected_features = X_train.columns[selector.get_support()]
print(f"é€‰ä¸­çš„ç‰¹å¾æ•°é‡: {len(selected_features)}/{len(X_train.columns)}")
```

---

### 4.5 æ•°æ®å¤„ç†å®Œæ•´æµç¨‹

```python
"""
å®Œæ•´çš„æ•°æ®å¤„ç†Pipelineç¤ºä¾‹
"""
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# 1. å®šä¹‰æ•°å€¼ç‰¹å¾å¤„ç†
numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# 2. å®šä¹‰ç±»åˆ«ç‰¹å¾å¤„ç†
categorical_features = ['Contract', 'PaymentMethod', 'InternetService']
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# 3. ç»„åˆå¤„ç†å™¨
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# 4. å®Œæ•´Pipelineï¼ˆé¢„å¤„ç† + æ¨¡å‹ï¼‰
from sklearn.ensemble import RandomForestClassifier

full_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])

# 5. ä¸€è¡Œä»£ç è®­ç»ƒ
full_pipeline.fit(X_train, y_train)
y_pred = full_pipeline.predict(X_test)
```

---

## ğŸ¯ ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒé˜¶æ®µ

> **ç›®æ ‡**ï¼šè®­ç»ƒæ¨¡å‹ã€è°ƒä¼˜å‚æ•°ã€éªŒè¯æ•ˆæœ

### 5.1 äº¤å‰éªŒè¯ç­–ç•¥

#### ğŸŒ³ äº¤å‰éªŒè¯æ–¹æ³•é€‰æ‹©

```mermaid
graph TD
    A[é€‰æ‹©äº¤å‰éªŒè¯] --> B{æ•°æ®ç‰¹ç‚¹?}

    B -->|ç±»åˆ«å¹³è¡¡| C[K-Fold CV<br/>K=5æˆ–10]
    B -->|ç±»åˆ«ä¸å¹³è¡¡| D[Stratified K-Fold<br/>ä¿æŒç±»åˆ«æ¯”ä¾‹]
    B -->|æ—¶é—´åºåˆ—| E[Time Series Split<br/>æ—¶é—´é¡ºåº]
    B -->|åˆ†ç»„æ•°æ®| F[Group K-Fold<br/>åŒç»„æ•°æ®ä¸è·¨fold]

    A --> G{æ•°æ®é‡?}
    G -->|<1000| H[Leave-One-Out<br/>æˆ–10-Fold]
    G -->|1000-10000| I[5-Fold æˆ– 10-Fold]
    G -->|>10000| J[3-Fold<br/>èŠ‚çœæ—¶é—´]
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
from sklearn.model_selection import (
    KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold
)

# 1. æ ‡å‡†KæŠ˜äº¤å‰éªŒè¯
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# 2. åˆ†å±‚KæŠ˜ï¼ˆä¿æŒç±»åˆ«æ¯”ä¾‹ï¼‰- æ¨èç”¨äºåˆ†ç±»é—®é¢˜
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 3. æ—¶é—´åºåˆ—åˆ†å‰²
tscv = TimeSeriesSplit(n_splits=5)

# 4. åˆ†ç»„KæŠ˜ï¼ˆåŒä¸€ç”¨æˆ·çš„æ•°æ®ä¸è·¨foldï¼‰
gkf = GroupKFold(n_splits=5)

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.model_selection import cross_val_score

scores = cross_val_score(
    model, X, y,
    cv=skf,  # ä½¿ç”¨åˆ†å±‚KæŠ˜
    scoring='roc_auc',
    n_jobs=-1
)

print(f"CV Mean: {scores.mean():.4f} (+/- {scores.std():.4f})")
```

---

### 5.2 è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥

#### ğŸŒ³ è°ƒä¼˜æ–¹æ³•é€‰æ‹©

```mermaid
graph TD
    A[è¶…å‚æ•°è°ƒä¼˜] --> B{å¯ç”¨æ—¶é—´?}

    B -->|å……è¶³<br/>>1å°æ—¶| C[Grid Search<br/>ç½‘æ ¼æœç´¢]
    B -->|æœ‰é™<br/>10-60åˆ†é’Ÿ| D[Random Search<br/>éšæœºæœç´¢]
    B -->|ç´§æ€¥<br/><10åˆ†é’Ÿ| E[ä½¿ç”¨é»˜è®¤å‚æ•°]

    C --> F{å‚æ•°ç©ºé—´?}
    F -->|å°<br/><100ç»„åˆ| G[å®Œå…¨ç½‘æ ¼æœç´¢]
    F -->|å¤§<br/>>100ç»„åˆ| H[éšæœºæœç´¢<br/>æˆ–è´å¶æ–¯ä¼˜åŒ–]
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# æ–¹æ³•1ï¼šç½‘æ ¼æœç´¢ï¼ˆç©·ä¸¾æ‰€æœ‰ç»„åˆï¼‰
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1]
}  # 3Ã—4Ã—3 = 36 ç§ç»„åˆ

grid_search = GridSearchCV(
    XGBClassifier(),
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)
print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")

# æ–¹æ³•2ï¼šéšæœºæœç´¢ï¼ˆéšæœºé‡‡æ ·Næ¬¡ï¼‰
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

param_dist = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 15),
    'learning_rate': uniform(0.01, 0.2),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4)
}

random_search = RandomizedSearchCV(
    XGBClassifier(),
    param_distributions=param_dist,
    n_iter=50,  # éšæœºå°è¯•50æ¬¡
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)
```

---

### 5.3 å¸¸è§ç®—æ³•çš„å…³é”®è¶…å‚æ•°

#### XGBoost / LightGBM å…³é”®å‚æ•°

**è°ƒä¼˜é¡ºåº**ï¼š
1. **æ ‘çš„æ•°é‡å’Œæ·±åº¦** â†’ æ§åˆ¶æ¨¡å‹å¤æ‚åº¦
2. **å­¦ä¹ ç‡** â†’ æ§åˆ¶æ”¶æ•›é€Ÿåº¦
3. **é‡‡æ ·æ¯”ä¾‹** â†’ æ§åˆ¶è¿‡æ‹Ÿåˆ
4. **æ­£åˆ™åŒ–å‚æ•°** â†’ è¿›ä¸€æ­¥é˜²æ­¢è¿‡æ‹Ÿåˆ

**æ¨èå‚æ•°èŒƒå›´**ï¼š
```python
# XGBoost
xgb_params = {
    # ç¬¬ä¸€è½®ï¼šæ ‘ç»“æ„
    'n_estimators': [100, 200, 300, 500],      # æ ‘çš„æ•°é‡
    'max_depth': [3, 5, 7, 10],                # æ ‘çš„æ·±åº¦

    # ç¬¬äºŒè½®ï¼šå­¦ä¹ ç‡
    'learning_rate': [0.01, 0.05, 0.1, 0.3],   # å­¦ä¹ ç‡

    # ç¬¬ä¸‰è½®ï¼šé‡‡æ ·
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],    # è¡Œé‡‡æ ·
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],  # åˆ—é‡‡æ ·

    # ç¬¬å››è½®ï¼šæ­£åˆ™åŒ–
    'reg_alpha': [0, 0.1, 1, 10],              # L1æ­£åˆ™
    'reg_lambda': [0, 0.1, 1, 10],             # L2æ­£åˆ™

    # ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    'scale_pos_weight': [1, 5, 10, 20]         # æ­£è´Ÿæ ·æœ¬æƒé‡æ¯”
}

# LightGBMï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰
lgb_params = {
    'n_estimators': [100, 200, 300],
    'num_leaves': [31, 50, 100],               # å¶å­æ•°ï¼ˆæ¯”max_depthæ›´ç›´æ¥ï¼‰
    'learning_rate': [0.01, 0.05, 0.1],
    'min_child_samples': [20, 50, 100],        # å¶å­æœ€å°æ ·æœ¬æ•°
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}
```

---

#### éšæœºæ£®æ—å…³é”®å‚æ•°

```python
rf_params = {
    'n_estimators': [100, 200, 300, 500],      # æ ‘çš„æ•°é‡ï¼ˆè¶Šå¤šè¶Šå¥½ï¼Œä½†æ›´æ…¢ï¼‰
    'max_depth': [10, 20, 30, None],           # æ ‘çš„æ·±åº¦ï¼ˆNone=ä¸é™åˆ¶ï¼‰
    'min_samples_split': [2, 5, 10],           # åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°
    'min_samples_leaf': [1, 2, 4],             # å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    'max_features': ['sqrt', 'log2', None],    # æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„ç‰¹å¾æ•°
    'bootstrap': [True, False]                  # æ˜¯å¦ä½¿ç”¨bootstrapé‡‡æ ·
}
```

---

### 5.4 æ¨¡å‹è®­ç»ƒå®Œæ•´æµç¨‹

```python
"""
å®Œæ•´çš„æ¨¡å‹è®­ç»ƒæµç¨‹ç¤ºä¾‹
"""

# 1. æ•°æ®åˆ†å‰²ï¼ˆè®­ç»ƒé›† + éªŒè¯é›†ï¼‰
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 2. å®šä¹‰å¤šä¸ªå€™é€‰æ¨¡å‹
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),
    'LightGBM': LGBMClassifier(random_state=42, verbose=-1)
}

# 3. è®­ç»ƒå¹¶è¯„ä¼°æ¯ä¸ªæ¨¡å‹
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score

results = []

for name, model in models.items():
    print(f"\nè®­ç»ƒ {name}...")

    # è®­ç»ƒ
    model.fit(X_train, y_train)

    # é¢„æµ‹
    y_pred = model.predict(X_val)
    y_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None

    # è¯„ä¼°
    acc = accuracy_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred, average='weighted')
    auc = roc_auc_score(y_val, y_proba) if y_proba is not None else None

    results.append({
        'Model': name,
        'Accuracy': acc,
        'F1-Score': f1,
        'ROC-AUC': auc
    })

    print(f"  Accuracy: {acc:.4f}")
    print(f"  F1-Score: {f1:.4f}")
    if auc:
        print(f"  ROC-AUC: {auc:.4f}")

# 4. ç»“æœå¯¹æ¯”
import pandas as pd
results_df = pd.DataFrame(results).sort_values('ROC-AUC', ascending=False)
print("\næ¨¡å‹å¯¹æ¯”ç»“æœ:")
print(results_df)

# 5. é€‰æ‹©æœ€ä½³æ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
best_model_name = results_df.iloc[0]['Model']
print(f"\næœ€ä½³æ¨¡å‹: {best_model_name}")
print("å¼€å§‹è¶…å‚æ•°è°ƒä¼˜...")

# 6. è¶…å‚æ•°è°ƒä¼˜ï¼ˆä»¥XGBoostä¸ºä¾‹ï¼‰
if best_model_name == 'XGBoost':
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.05, 0.1]
    }

    grid_search = GridSearchCV(
        XGBClassifier(random_state=42, eval_metric='logloss'),
        param_grid,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1
    )

    grid_search.fit(X_train, y_train)

    print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³CVå¾—åˆ†: {grid_search.best_score_:.4f}")

    # ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    final_model = grid_search.best_estimator_
    y_val_proba = final_model.predict_proba(X_val)[:, 1]
    final_auc = roc_auc_score(y_val, y_val_proba)
    print(f"éªŒè¯é›†AUC: {final_auc:.4f}")
```

---

## ğŸ¯ ç¬¬å…­éƒ¨åˆ†ï¼šæ¨¡å‹è¯„ä¼°é˜¶æ®µ

> **ç›®æ ‡**ï¼šå…¨é¢è¯„ä¼°æ¨¡å‹ï¼Œè½¬åŒ–ä¸ºä¸šåŠ¡ä»·å€¼

### 6.1 æ¨¡å‹è¯„ä¼°å®Œæ•´æ£€æŸ¥æ¸…å•

#### âœ… ç¬¬ä¸€æ­¥ï¼šæ€§èƒ½æŒ‡æ ‡è¯„ä¼°

**åˆ†ç±»é—®é¢˜**ï¼š
```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score,
    confusion_matrix, classification_report
)

# 1. åŸºç¡€æŒ‡æ ‡
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print("=== åˆ†ç±»æ€§èƒ½æŒ‡æ ‡ ===")
print(f"Accuracy:  {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision: {precision_score(y_test, y_pred):.4f}")
print(f"Recall:    {recall_score(y_test, y_pred):.4f}")
print(f"F1-Score:  {f1_score(y_test, y_pred):.4f}")
print(f"ROC-AUC:   {roc_auc_score(y_test, y_proba):.4f}")
print(f"PR-AUC:    {average_precision_score(y_test, y_proba):.4f}")

# 2. æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)
print("\næ··æ·†çŸ©é˜µ:")
print(cm)
print(f"TN={cm[0,0]}, FP={cm[0,1]}")
print(f"FN={cm[1,0]}, TP={cm[1,1]}")

# 3. è¯¦ç»†æŠ¥å‘Š
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))
```

**å›å½’é—®é¢˜**ï¼š
```python
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score
)
import numpy as np

y_pred = model.predict(X_test)

print("=== å›å½’æ€§èƒ½æŒ‡æ ‡ ===")
print(f"MAE:  {mean_absolute_error(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
print(f"RÂ²:   {r2_score(y_test, y_pred):.4f}")
```

---

#### âœ… ç¬¬äºŒæ­¥ï¼šå¯è§†åŒ–è¯„ä¼°

**ROCæ›²çº¿å’ŒPRæ›²çº¿**ï¼š
```python
from sklearn.metrics import roc_curve, precision_recall_curve
import matplotlib.pyplot as plt

# ROCæ›²çº¿
fpr, tpr, thresholds_roc = roc_curve(y_test, y_proba)
roc_auc = roc_auc_score(y_test, y_proba)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True, alpha=0.3)

# PRæ›²çº¿
precision, recall, thresholds_pr = precision_recall_curve(y_test, y_proba)
pr_auc = average_precision_score(y_test, y_proba)

plt.subplot(1, 2, 2)
plt.plot(recall, precision, label=f'PR Curve (AP = {pr_auc:.4f})', linewidth=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**ç‰¹å¾é‡è¦æ€§**ï¼š
```python
import pandas as pd

# è·å–ç‰¹å¾é‡è¦æ€§
if hasattr(model, 'feature_importances_'):
    importances = model.feature_importances_
    feature_names = X.columns

    # æ’åº
    indices = np.argsort(importances)[::-1][:20]  # Top 20

    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(indices)), importances[indices])
    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
    plt.xlabel('Feature Importance')
    plt.title('Top 20 Feature Importances')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

    # è¡¨æ ¼è¾“å‡º
    feature_imp_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values('Importance', ascending=False)

    print("\nTop 10 é‡è¦ç‰¹å¾:")
    print(feature_imp_df.head(10))
```

---

#### âœ… ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹è¯Šæ–­

**å­¦ä¹ æ›²çº¿ï¼ˆè¯Šæ–­è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆï¼‰**ï¼š
```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    model, X_train, y_train,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Training Score', linewidth=2)
plt.plot(train_sizes, val_mean, label='Validation Score', linewidth=2)
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)
plt.xlabel('Training Size')
plt.ylabel('ROC-AUC Score')
plt.title('Learning Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# è¯Šæ–­ç»“è®º
if train_mean[-1] > 0.95 and val_mean[-1] < 0.80:
    print("âš ï¸ è¯Šæ–­: è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒå¾—åˆ†>>éªŒè¯å¾—åˆ†ï¼‰")
    print("å»ºè®®: å¢åŠ æ­£åˆ™åŒ–ã€å‡å°‘æ¨¡å‹å¤æ‚åº¦ã€å¢åŠ æ•°æ®é‡")
elif train_mean[-1] < 0.70 and val_mean[-1] < 0.70:
    print("âš ï¸ è¯Šæ–­: æ¬ æ‹Ÿåˆï¼ˆè®­ç»ƒå’ŒéªŒè¯å¾—åˆ†éƒ½ä½ï¼‰")
    print("å»ºè®®: å¢åŠ æ¨¡å‹å¤æ‚åº¦ã€å¢åŠ ç‰¹å¾å·¥ç¨‹ã€å‡å°‘æ­£åˆ™åŒ–")
else:
    print("âœ… è¯Šæ–­: æ¨¡å‹æ‹Ÿåˆè‰¯å¥½")
```

---

### 6.2 ä¸šåŠ¡ä»·å€¼è½¬åŒ–

#### ğŸ’° ä¸šåŠ¡å½±å“åˆ†æ

å°†æŠ€æœ¯æŒ‡æ ‡è½¬åŒ–ä¸ºä¸šåŠ¡ä»·å€¼ï¼š

**ç¤ºä¾‹1ï¼šå®¢æˆ·æµå¤±é¢„æµ‹**
```python
# å‡è®¾ï¼š
# - æœˆæ´»å®¢æˆ· 100,000
# - å®é™…æµå¤±ç‡ 20%
# - æŒ½ç•™æˆæœ¬ 50å…ƒ/äºº
# - æŒ½ç•™æˆåŠŸç‡ 30%
# - å®¢æˆ·å¹´ä»·å€¼ 1,000å…ƒ

# æ¨¡å‹é¢„æµ‹ç»“æœ
y_true = 20000  # å®é™…æµå¤±å®¢æˆ·
y_pred_positive = 15000  # é¢„æµ‹ä¼šæµå¤±çš„å®¢æˆ·
TP = 12000  # é¢„æµ‹æ­£ç¡®çš„æµå¤±å®¢æˆ·
FP = 3000   # è¯¯æŠ¥ï¼ˆå®é™…ä¸æµå¤±ä½†é¢„æµ‹ä¼šæµå¤±ï¼‰

# ä¸šåŠ¡è®¡ç®—
retention_cost = y_pred_positive * 50  # æŒ½ç•™æˆæœ¬
retained_customers = TP * 0.3  # æˆåŠŸæŒ½ç•™çš„å®¢æˆ·
revenue_saved = retained_customers * 1000  # æŒ½ç•™ä»·å€¼

roi = (revenue_saved - retention_cost) / retention_cost * 100

print("=== ä¸šåŠ¡å½±å“åˆ†æ ===")
print(f"æŒ½ç•™æŠ•å…¥: {retention_cost:,.0f} å…ƒ")
print(f"æˆåŠŸæŒ½ç•™: {retained_customers:,.0f} äºº")
print(f"æŒ½ç•™ä»·å€¼: {revenue_saved:,.0f} å…ƒ")
print(f"ROI: {roi:.1f}%")
print(f"\nå‡€æ”¶ç›Š: {revenue_saved - retention_cost:,.0f} å…ƒ")
```

**ç¤ºä¾‹2ï¼šæ¬ºè¯ˆæ£€æµ‹**
```python
# å‡è®¾ï¼š
# - æ—¥äº¤æ˜“é‡ 1,000,000
# - æ¬ºè¯ˆç‡ 0.2%
# - å¹³å‡æ¬ºè¯ˆé‡‘é¢ 5,000å…ƒ
# - äººå·¥å®¡æ ¸æˆæœ¬ 10å…ƒ/ç¬”

# æ¨¡å‹é¢„æµ‹ï¼ˆPrecision=0.8, Recall=0.7ï¼‰
total_fraud = 2000  # å®é™…æ¬ºè¯ˆäº¤æ˜“
detected_fraud = 1400  # æˆåŠŸæ£€æµ‹ï¼ˆTPï¼‰
total_flagged = 1750  # æ€»é¢„è­¦æ•°ï¼ˆTP + FPï¼‰
FP = 350  # è¯¯æŠ¥

# ä¸šåŠ¡è®¡ç®—
fraud_prevented = detected_fraud * 5000  # é˜»æ­¢æŸå¤±
review_cost = total_flagged * 10  # å®¡æ ¸æˆæœ¬
net_benefit = fraud_prevented - review_cost

print("=== æ¬ºè¯ˆæ£€æµ‹ä¸šåŠ¡ä»·å€¼ ===")
print(f"é˜»æ­¢æ¬ºè¯ˆ: {detected_fraud:,} ç¬”")
print(f"é¿å…æŸå¤±: {fraud_prevented:,.0f} å…ƒ")
print(f"å®¡æ ¸æˆæœ¬: {review_cost:,.0f} å…ƒ")
print(f"å‡€æ”¶ç›Š: {net_benefit:,.0f} å…ƒ")
print(f"\næ£€æµ‹è¦†ç›–ç‡: {detected_fraud/total_fraud*100:.1f}%")
```

---

### 6.3 æ¨¡å‹å¯è§£é‡Šæ€§

```python
# SHAPå€¼åˆ†æï¼ˆé€‚ç”¨äºXGBoost/LightGBMï¼‰
import shap

# è®¡ç®—SHAPå€¼
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# å¯è§†åŒ–
shap.summary_plot(shap_values, X_test, plot_type="bar", max_display=10)
shap.summary_plot(shap_values, X_test)

# å•ä¸ªæ ·æœ¬è§£é‡Š
idx = 0
shap.force_plot(
    explainer.expected_value,
    shap_values[idx],
    X_test.iloc[idx]
)
```

---

## ğŸ“š é™„å½•ï¼šå®Œæ•´æ¡ˆä¾‹æ¼”ç¤º

### æ¡ˆä¾‹ï¼šç”µä¿¡å®¢æˆ·æµå¤±é¢„æµ‹å®Œæ•´æµç¨‹

ä»é™Œç”Ÿæ•°æ®åˆ°æœ€ç»ˆæ–¹æ¡ˆçš„å®Œæ•´å†³ç­–è¿‡ç¨‹æ¼”ç¤º...

ï¼ˆç”±äºç¯‡å¹…é™åˆ¶ï¼Œè¿™éƒ¨åˆ†å°†åœ¨å®é™…çš„ Notebook ä¸­è¯¦ç»†æ¼”ç¤ºï¼‰

---

## ğŸ“ æ€»ç»“ï¼šå†³ç­–æ€ç»´æ¡†æ¶

è®°ä½è¿™ä¸ªæ ¸å¿ƒæµç¨‹ï¼š

```
1. æ•°æ®è¯Šæ–­ â†’ äº†è§£æ•°æ®ï¼Œè¯†åˆ«é—®é¢˜ç±»å‹
2. é—®é¢˜å®šä¹‰ â†’ æ˜ç¡®ç›®æ ‡ï¼Œé€‰æ‹©æŒ‡æ ‡
3. ç®—æ³•é€‰æ‹© â†’ æ ¹æ®æ•°æ®å’Œé—®é¢˜ç‰¹ç‚¹é€‰æ‹©
4. æ•°æ®å¤„ç† â†’ æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹
5. æ¨¡å‹è®­ç»ƒ â†’ å¤šæ¨¡å‹å¯¹æ¯”ã€è¶…å‚æ•°è°ƒä¼˜
6. æ¨¡å‹è¯„ä¼° â†’ æ€§èƒ½è¯„ä¼°ã€ä¸šåŠ¡è½¬åŒ–
```

é¢å¯¹ä»»ä½•MLé—®é¢˜ï¼ŒæŒ‰ç…§è¿™ä¸ªæµç¨‹é€æ­¥æ¨è¿›ï¼Œåšå‡ºåˆç†çš„å†³ç­–ã€‚

---

**æœ€åæ›´æ–°**ï¼š2024-11-26
**ç‰ˆæœ¬**ï¼šv1.0
