# 📊 数据特征到算法的映射表

> 基于数据特征的系统化算法筛选 - 多维度决策矩阵

---

## 📋 使用说明

### 本文档的用途

1. **基于数据特征筛选算法**：根据数据量、分布、性能需求快速定位
2. **验证算法选择**：检查决策树给出的候选算法是否合理
3. **多维度对比**：从不同角度理解算法的适用场景

### 使用方式

```
方式1：按需查表（推荐）
  → 知道你的数据特征（如样本量5K）
  → 查找对应表格
  → 得到推荐算法列表

方式2：多维度验证
  → 已有候选算法（如从决策树得到）
  → 在多个维度的表格中验证
  → 确认算法在各维度都适用

方式3：综合决策
  → 依次查看各维度表格
  → 找到多个表格中都出现的算法
  → 这些算法最适合你的数据
```

---

## 🎯 维度1：数据量维度

### 数据量分类标准

| 级别 | 样本量范围 | 特征量范围 | 典型场景 |
|------|-----------|-----------|----------|
| **极小** | <100 | <10 | 小型实验、初步探索 |
| **小** | 100-1K | 10-20 | AB测试、调查问卷 |
| **中小** | 1K-10K | 20-100 | 业务数据、常规项目 |
| **中** | 10K-50K | 100-500 | 企业数据、用户行为 |
| **中大** | 50K-500K | 500-1K | 大规模用户、电商 |
| **大** | 500K-5M | 1K-10K | 互联网产品、物联网 |
| **超大** | >5M | >10K | 推荐系统、广告系统 |

---

### 监督学习：数据量与算法映射

#### 回归问题

| 数据量级 | 首选算法 | 备选算法 | 避免算法 | 原因 |
|---------|----------|----------|----------|------|
| **极小（<100）** | 线性回归、Ridge | KNN(k大) | XGBoost、神经网络 | 易过拟合，需简单模型 |
| **小（100-1K）** | Ridge、Lasso | 决策树(深度限制)、KNN | XGBoost、深度学习 | 正则化防过拟合 |
| **中小（1K-10K）** | 随机森林、Ridge | XGBoost、决策树 | 深度学习 | 集成方法开始有效 |
| **中（10K-50K）** | XGBoost、随机森林 | LightGBM、Ridge | KNN(太慢) | 集成方法性价比高 |
| **中大（50K-500K）** | XGBoost、LightGBM | 随机森林、神经网络 | KNN | 复杂模型收益明显 |
| **大（500K-5M）** | LightGBM、XGBoost | 神经网络、SGDRegressor | KNN、层次方法 | 需要可扩展算法 |
| **超大（>5M）** | LightGBM、SGD | Mini-Batch方法 | 批量训练算法 | 必须支持增量学习 |

**关键洞察**：
- 样本量<1K：简单模型 + 正则化
- 样本量1K-50K：集成方法的甜蜜区
- 样本量>50K：梯度提升优势明显
- 样本量>5M：必须考虑训练效率

---

#### 分类问题（平衡数据）

| 数据量级 | 首选算法 | 备选算法 | 避免算法 | 原因 |
|---------|----------|----------|----------|------|
| **极小（<100）** | 逻辑回归、KNN | 朴素贝叶斯 | XGBoost、SVM | 样本不足，简单为王 |
| **小（100-1K）** | 逻辑回归、决策树 | KNN、SVM(线性) | 深度学习 | 控制模型复杂度 |
| **中小（1K-10K）** | 随机森林、逻辑回归 | XGBoost、SVM | 深度学习 | 开始使用集成方法 |
| **中（10K-50K）** | XGBoost、随机森林 | LightGBM、SVM | 简单决策树 | 集成方法表现最佳 |
| **中大（50K-500K）** | XGBoost、LightGBM | 神经网络、随机森林 | KNN | 高复杂度模型 |
| **大（500K-5M）** | LightGBM、XGBoost | 神经网络、SGDClassifier | KNN、SVM(核方法) | 效率优先 |
| **超大（>5M）** | LightGBM、SGD | 在线学习算法 | 批量训练算法 | 必须增量学习 |

**关键洞察**：
- 样本量<1K：逻辑回归、决策树
- 样本量10K-500K：XGBoost/LightGBM的黄金区间
- 样本量>500K：考虑深度学习，但LightGBM通常够用

---

### 无监督学习：数据量与算法映射

#### 聚类问题

| 数据量级 | 首选算法 | 备选算法 | 避免算法 | 原因 |
|---------|----------|----------|----------|------|
| **极小（<100）** | K-Means、层次聚类 | GMM | DBSCAN | 样本太少，密度方法失效 |
| **小（100-1K）** | K-Means、层次聚类 | DBSCAN、GMM | - | 所有方法都适用 |
| **中小（1K-10K）** | K-Means、DBSCAN | GMM | 层次聚类(慢) | 密度方法开始有效 |
| **中（10K-50K）** | K-Means、DBSCAN | Mini-Batch K-Means | 层次聚类 | 效率开始重要 |
| **中大（50K-500K）** | Mini-Batch K-Means | DBSCAN(eps小心选择) | 层次聚类、GMM | 大数据优化算法 |
| **大（500K-5M）** | Mini-Batch K-Means | BIRCH | 标准K-Means、层次 | 必须可扩展 |
| **超大（>5M）** | Mini-Batch K-Means | BIRCH、流式聚类 | 批量算法 | 内存限制 |

**关键洞察**：
- 样本量<10K：K-Means、层次聚类、DBSCAN都可用
- 样本量>10K：避免层次聚类（O(n²)复杂度）
- 样本量>50K：Mini-Batch K-Means必备

---

#### 降维问题

| 数据量级 | 首选算法 | 备选算法 | 避免算法 | 原因 |
|---------|----------|----------|----------|------|
| **极小（<100）** | PCA | - | t-SNE(样本太少) | PCA够用 |
| **小（100-1K）** | PCA、t-SNE | - | - | 都适用 |
| **中小（1K-10K）** | PCA、t-SNE | UMAP | - | t-SNE效果最佳 |
| **中（10K-50K）** | PCA、UMAP | t-SNE(慢) | - | UMAP更快 |
| **中大（50K-500K）** | PCA、UMAP | Incremental PCA | t-SNE(太慢) | 避免t-SNE |
| **大（500K-5M）** | PCA、UMAP | TruncatedSVD | t-SNE | t-SNE不可行 |
| **超大（>5M）** | Incremental PCA、UMAP | Random Projection | t-SNE、批量PCA | 内存限制 |

**关键洞察**：
- 样本量<10K：t-SNE可视化效果最佳
- 样本量>10K：UMAP替代t-SNE
- 样本量>100K：避免t-SNE，使用PCA+UMAP

---

## 📈 维度2：数据分布特征

### 2.1 线性可分性

#### 如何判断线性可分性？

```python
# 方法1：可视化（2-3维）
plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis')
# 观察：能否用一条直线/平面分开？

# 方法2：线性模型基准测试
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr_score = cross_val_score(lr, X, y, cv=5).mean()

from sklearn.ensemble import RandomForest
rf = RandomForest()
rf_score = cross_val_score(rf, X, y, cv=5).mean()

if (rf_score - lr_score) < 0.05:
    print("线性可分")
else:
    print("非线性")
```

#### 线性 vs 非线性算法选择

| 数据特征 | 推荐算法 | 避免算法 | 原因 |
|---------|----------|----------|------|
| **线性可分** | 逻辑回归、线性SVM、Ridge | 核SVM、深度学习 | 简单模型足够，避免过拟合 |
| **弱非线性** | 随机森林、多项式特征+线性 | 深度学习 | 中等复杂度即可 |
| **强非线性** | XGBoost、RBF-SVM、深度学习 | 线性模型 | 需要强非线性能力 |

---

### 2.2 数据平衡性（分类问题）

#### 不平衡程度分类

| 类别 | 少数类比例 | 不平衡比 | 处理策略 |
|------|-----------|----------|----------|
| **平衡** | 40-50% | 1:1 ~ 1:1.5 | 无需特殊处理 |
| **轻度不平衡** | 20-40% | 1:1.5 ~ 1:4 | 类权重调整 |
| **中度不平衡** | 5-20% | 1:4 ~ 1:19 | SMOTE + 类权重 |
| **严重不平衡** | 1-5% | 1:19 ~ 1:99 | 专门采样技术 |
| **极端不平衡** | <1% | 1:99+ | 异常检测方法 |

#### 不平衡数据算法选择

| 不平衡程度 | 首选算法 | 处理方法 | 避免算法 |
|-----------|----------|----------|----------|
| **平衡** | 任意算法 | 无需特殊处理 | - |
| **轻度不平衡** | XGBoost、随机森林 | class_weight='balanced' | KNN（对不平衡敏感） |
| **中度不平衡** | XGBoost + SMOTE | scale_pos_weight | 朴素贝叶斯 |
| **严重不平衡** | Isolation Forest | 异常检测框架 | 标准分类算法 |
| **极端不平衡** | One-Class SVM | 单类学习 | 所有标准分类算法 |

**算法对不平衡的容忍度**：

| 算法 | 容忍度 | 推荐不平衡范围 | 调整参数 |
|------|-------|---------------|----------|
| **逻辑回归** | 低 | <1:4 | class_weight |
| **决策树** | 中 | <1:10 | class_weight, max_depth |
| **随机森林** | 高 | <1:20 | class_weight, balanced_subsample |
| **XGBoost** | 很高 | <1:50 | scale_pos_weight |
| **LightGBM** | 很高 | <1:50 | is_unbalance=True |

---

### 2.3 特征关系

#### 高维稀疏 vs 低维稠密

| 数据类型 | 特征数 | 非零比例 | 典型场景 | 推荐算法 |
|---------|-------|----------|----------|----------|
| **低维稠密** | <100 | >50% | 结构化数据 | 任意算法 |
| **中维稠密** | 100-1000 | >50% | 特征工程后 | XGBoost、神经网络 |
| **高维稠密** | >1000 | >50% | 图像、文本(embedding) | 降维+模型、深度学习 |
| **高维稀疏** | >1000 | <10% | 文本(TF-IDF)、类别编码 | 线性模型、TruncatedSVD |

#### 算法对高维稀疏数据的适应性

| 算法 | 稀疏数据支持 | 高维效果 | 推荐场景 |
|------|------------|----------|----------|
| **逻辑回归** | 优秀（原生支持） | 很好 | 文本分类首选 |
| **线性SVM** | 优秀 | 很好 | 高维文本、类别特征 |
| **朴素贝叶斯** | 优秀 | 很好 | 文本分类、高维稀疏 |
| **随机森林** | 一般（需转稠密） | 中等 | 特征数<1000 |
| **XGBoost** | 支持（但不如线性模型） | 中等 | 中等维度 |
| **深度学习** | 需Embedding | 好（Embedding后） | 需大量数据 |

---

### 2.4 多重共线性（回归问题）

#### 共线性程度判断

```python
# 方法1：相关系数矩阵
corr_matrix = X.corr()
high_corr = (corr_matrix.abs() > 0.9).sum().sum() > len(X.columns)

# 方法2：VIF（方差膨胀因子）
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
# VIF > 10 表示严重共线性
```

#### 算法对共线性的敏感度

| 算法 | 敏感度 | VIF容忍度 | 原因 |
|------|-------|----------|------|
| **线性回归（OLS）** | 极高 | <5 | 系数不稳定，标准误增大 |
| **Ridge** | 低 | <100 | L2正则化缓解共线性 |
| **Lasso** | 很低 | 任意 | 自动选择其中一个特征 |
| **ElasticNet** | 很低 | 任意 | 结合L1和L2优势 |
| **决策树** | 无影响 | 任意 | 分支选择不受共线性影响 |
| **随机森林** | 无影响 | 任意 | 随机特征选择 |
| **XGBoost** | 无影响 | 任意 | 基于增益分裂 |

**推荐策略**：

| 共线性程度 | VIF范围 | 推荐算法 | 处理方法 |
|-----------|---------|----------|----------|
| **无** | <5 | 任意算法 | 无需处理 |
| **轻度** | 5-10 | Ridge、决策树 | Ridge正则化 |
| **中度** | 10-50 | Lasso、随机森林 | Lasso特征选择 |
| **严重** | >50 | 随机森林、XGBoost | PCA降维或Lasso |

---

## ⚙️ 维度3：性能需求矩阵

### 3.1 准确性 vs 速度 vs 可解释性

#### 三维权衡图谱

```
           准确性
            ↑
            │  XGBoost
            │  LightGBM
            │  深度学习
            │
            │  随机森林
            │  SVM(RBF)
            │
            │  决策树
            │  逻辑回归
            │────────────────→ 速度
           ╱
          ╱   线性模型
         ╱    决策树
        ╱
       ╱      XGBoost
      ↓
   可解释性
```

#### 性能需求优先级矩阵

| 优先级 | 首选算法 | 备选算法 | 训练时间 | 预测时间 |
|-------|----------|----------|----------|----------|
| **准确性 > 速度 > 可解释性** | XGBoost、LightGBM | 深度学习、Stacking | 长 | 中 |
| **准确性 > 可解释性 > 速度** | 随机森林、决策树 | 线性模型+特征工程 | 中 | 快 |
| **速度 > 准确性 > 可解释性** | LightGBM、逻辑回归 | 朴素贝叶斯、KNN(少样本) | 快 | 极快 |
| **速度 > 可解释性 > 准确性** | 线性回归、逻辑回归 | 决策树(浅) | 极快 | 极快 |
| **可解释性 > 准确性 > 速度** | 线性模型、决策树 | GAM、规则学习 | 快 | 快 |
| **可解释性 > 速度 > 准确性** | 线性回归、逻辑回归 | 朴素贝叶斯 | 极快 | 极快 |

---

### 3.2 按问题类型的性能需求

#### 回归问题

| 需求优先级 | 典型场景 | 首选算法 | 备选算法 |
|-----------|----------|----------|----------|
| **准确性优先** | 房价预测、金融建模 | XGBoost、LightGBM | 随机森林、Stacking |
| **速度优先** | 实时定价、A/B测试 | 线性回归、Ridge | LightGBM(少迭代) |
| **可解释性优先** | 业务分析、政策制定 | 线性回归、Ridge | Lasso、决策树(浅) |
| **平衡型** | 通用预测任务 | 随机森林 | XGBoost(少迭代) |

#### 分类问题（二分类）

| 需求优先级 | 典型场景 | 首选算法 | 备选算法 |
|-----------|----------|----------|----------|
| **准确性优先** | 欺诈检测、疾病诊断 | XGBoost、LightGBM | Stacking、深度学习 |
| **速度优先** | 实时推荐、广告点击 | 逻辑回归、LightGBM | 朴素贝叶斯 |
| **可解释性优先** | 信贷审批、医疗诊断 | 逻辑回归、决策树 | 线性SVM、规则学习 |
| **平衡型** | 客户流失、营销响应 | 随机森林 | XGBoost(少迭代) |

#### 多分类问题

| 需求优先级 | 典型场景 | 首选算法 | 备选算法 |
|-----------|----------|----------|----------|
| **准确性优先** | 图像分类、文本分类 | XGBoost、LightGBM | 深度学习(大数据) |
| **速度优先** | 实时分类、在线服务 | 朴素贝叶斯、逻辑回归 | LightGBM |
| **可解释性优先** | 业务分类、风险评级 | 决策树、逻辑回归 | One-vs-Rest逻辑回归 |
| **平衡型** | 通用多分类 | 随机森林 | XGBoost |

#### 聚类问题

| 需求优先级 | 典型场景 | 首选算法 | 备选算法 |
|-----------|----------|----------|----------|
| **质量优先** | 客户细分、市场研究 | GMM、层次聚类 | DBSCAN、谱聚类 |
| **速度优先** | 大规模聚类、实时分群 | K-Means、Mini-Batch K-Means | BIRCH |
| **可解释性优先** | 业务分群、战略规划 | K-Means、层次聚类 | 决策树聚类 |
| **平衡型** | 通用聚类 | K-Means | DBSCAN |

---

## 🚀 维度4：实际约束条件

### 4.1 训练时间限制

| 时间限制 | 推荐算法 | 避免算法 | 备注 |
|---------|----------|----------|------|
| **<1分钟** | 线性模型、朴素贝叶斯 | XGBoost、SVM(核) | 极简模型 |
| **1-10分钟** | 决策树、随机森林(少树) | XGBoost(多迭代)、深度学习 | 中等复杂度 |
| **10-60分钟** | XGBoost、随机森林 | 网格搜索 | 常规调优 |
| **1-24小时** | XGBoost(深度调优)、深度学习 | - | 充分调优 |
| **>24小时** | 深度学习、AutoML | - | 大规模或超参搜索 |

### 4.2 预测时间限制

| 时间限制 | 推荐算法 | 避免算法 | 应用场景 |
|---------|----------|----------|----------|
| **<1ms** | 线性模型、查表 | 任何树模型 | 高频交易、实时竞价 |
| **1-10ms** | 逻辑回归、浅决策树 | XGBoost(深树)、KNN | 在线广告、推荐首页 |
| **10-100ms** | 随机森林(少树)、XGBoost | 深度学习 | API服务、搜索排序 |
| **100ms-1s** | XGBoost、随机森林 | - | 批量预测、离线服务 |
| **>1s** | 任意算法 | - | 离线分析、批处理 |

### 4.3 内存限制

| 内存限制 | 数据规模 | 推荐算法 | 处理策略 |
|---------|---------|----------|----------|
| **<1GB** | 小数据 | 任意算法 | 无限制 |
| **1-10GB** | 中等数据 | LightGBM、线性模型 | 控制树深度 |
| **10-100GB** | 大数据 | LightGBM、SGD、增量学习 | 分批训练、特征选择 |
| **>100GB** | 超大数据 | SGD、在线学习、Spark MLlib | 分布式、流式处理 |

### 4.4 可解释性要求

#### 行业可解释性需求

| 行业/领域 | 可解释性要求 | 推荐算法 | 避免算法 |
|----------|-------------|----------|----------|
| **金融（信贷）** | 极高（监管要求） | 逻辑回归、决策树 | XGBoost、神经网络 |
| **医疗诊断** | 极高（伦理要求） | 逻辑回归、专家规则 | 深度学习 |
| **法律/司法** | 极高（法律要求） | 逻辑回归、规则学习 | 黑盒模型 |
| **营销/广告** | 低（效果为王） | XGBoost、深度学习 | - |
| **推荐系统** | 低 | 任意算法 | - |
| **风险管理** | 高 | 逻辑回归、GAM | 复杂集成 |
| **科研分析** | 中高 | 线性模型、随机森林 | 深度学习 |

#### 算法可解释性评分

| 算法 | 可解释性 | 解释工具 | 适用场景 |
|------|---------|----------|----------|
| **线性回归/逻辑回归** | ⭐⭐⭐⭐⭐ | 系数解释 | 必须可解释场景 |
| **决策树** | ⭐⭐⭐⭐⭐ | 可视化树结构 | 业务规则提取 |
| **朴素贝叶斯** | ⭐⭐⭐⭐ | 条件概率 | 文本分类解释 |
| **KNN** | ⭐⭐⭐ | 邻居样本 | 相似案例推荐 |
| **随机森林** | ⭐⭐⭐ | 特征重要性、SHAP | 特征影响分析 |
| **XGBoost/LightGBM** | ⭐⭐ | SHAP、特征重要性 | 配合解释工具 |
| **SVM(核方法)** | ⭐ | - | 黑盒 |
| **深度学习** | ⭐ | Saliency Map、LIME | 需专门解释方法 |

---

## 📋 综合决策矩阵

### 典型场景快速查询表

#### 场景1：小数据集（<1K样本）

| 问题类型 | 数据特征 | 首选 | 备选 |
|---------|----------|------|------|
| 回归 | 线性 | Ridge、Lasso | 线性回归 |
| 回归 | 非线性 | 决策树、KNN | 随机森林(少树) |
| 二分类 | 平衡 | 逻辑回归、决策树 | SVM、KNN |
| 二分类 | 不平衡 | 逻辑回归+权重 | SMOTE+决策树 |
| 多分类 | 任意 | 决策树、朴素贝叶斯 | 逻辑回归(OvR) |

#### 场景2：中等数据集（1K-50K）

| 问题类型 | 准确性需求 | 首选 | 备选 |
|---------|-----------|------|------|
| 回归 | 高 | XGBoost、随机森林 | LightGBM |
| 回归 | 中 | 随机森林 | Ridge |
| 二分类 | 高 | XGBoost | LightGBM、随机森林 |
| 二分类 | 快速 | 逻辑回归 | 朴素贝叶斯 |
| 多分类 | 任意 | XGBoost | 随机森林 |

#### 场景3：大数据集（>50K）

| 问题类型 | 约束条件 | 首选 | 备选 |
|---------|---------|------|------|
| 回归 | 准确性优先 | LightGBM、XGBoost | 随机森林 |
| 回归 | 速度优先 | LightGBM、SGD | 线性回归 |
| 分类 | 准确性优先 | LightGBM、XGBoost | 神经网络 |
| 分类 | 速度优先 | LightGBM(少迭代) | 逻辑回归 |
| 聚类 | 任意 | Mini-Batch K-Means | BIRCH |

#### 场景4：特殊数据特征

| 数据特征 | 问题类型 | 首选 | 原因 |
|---------|---------|------|------|
| 高维稀疏 | 分类 | 逻辑回归、线性SVM | 原生支持稀疏 |
| 高度不平衡 | 二分类 | XGBoost+SMOTE | 对不平衡容忍度高 |
| 强非线性 | 回归/分类 | XGBoost、深度学习 | 强非线性拟合能力 |
| 多重共线性 | 回归 | Ridge、Lasso | 正则化处理 |
| 噪声很多 | 任意 | 随机森林、XGBoost | 鲁棒性强 |

---

## ✅ 使用检查清单

### 选择算法前的检查

- [ ] 明确问题类型（分类/回归/聚类/降维/异常检测）
- [ ] 知道数据量（样本数、特征数）
- [ ] 分析数据分布（平衡性、线性、稀疏性）
- [ ] 确定性能需求优先级（准确性/速度/可解释性）
- [ ] 了解实际约束（时间、内存、部署环境）

### 验证算法选择的合理性

**从多个维度验证**：
- [ ] 数据量维度：算法在该样本量下表现好？
- [ ] 数据分布维度：算法适合这种分布特征？
- [ ] 性能需求维度：算法满足性能优先级？
- [ ] 约束条件维度：算法符合实际限制？

**至少满足3个维度** → 算法选择合理

---

## 🎯 实战示例

### 示例1：房价预测

```
数据画像：
• 问题类型：回归
• 样本量：5,000
• 特征数：20
• 数据分布：可能非线性，无明显稀疏性
• 性能需求：准确性 > 可解释性 > 速度
• 约束：训练时间<1小时，预测<100ms

多维度查表：
✓ 数据量维度(5K) → 推荐：XGBoost、随机森林、Ridge
✓ 分布维度(非线性) → 推荐：XGBoost、随机森林
✓ 性能需求(准确性优先) → 推荐：XGBoost、LightGBM
✓ 约束条件(时间充足) → 都满足

最终选择：
• Baseline：Ridge（线性baseline）
• 优化：XGBoost（首选，准确性高）
• 备选：随机森林（鲁棒性好）
```

### 示例2：文本分类（高维稀疏）

```
数据画像：
• 问题类型：多分类（10个类别）
• 样本量：20,000
• 特征数：5,000（TF-IDF）
• 数据分布：高维稀疏（<5%非零）
• 性能需求：速度 > 准确性
• 约束：预测时间<10ms

多维度查表：
✓ 数据量维度(20K) → XGBoost、随机森林
✗ 分布维度(高维稀疏) → 线性模型、朴素贝叶斯（更适合）
✓ 性能需求(速度优先) → 逻辑回归、朴素贝叶斯
✓ 约束条件(预测快) → 线性模型

最终选择：
• Baseline：朴素贝叶斯（最快）
• 优化：逻辑回归（multinomial，准确性更好）
• 备选：线性SVM（如果准确性要求高）

说明：虽然数据量支持XGBoost，但高维稀疏特征
      和速度需求使线性模型更合适
```

### 示例3：欺诈检测（极端不平衡）

```
数据画像：
• 问题类型：二分类（欺诈检测）
• 样本量：100,000
• 特征数：50
• 数据分布：极端不平衡（欺诈率0.5%）
• 性能需求：准确性（召回率）> 速度
• 约束：需要可解释性

多维度查表：
✓ 数据量维度(100K) → LightGBM、XGBoost
✓ 分布维度(极端不平衡) → 异常检测方法、XGBoost
✗ 性能需求(可解释性) → 逻辑回归、决策树（更可解释）
✓ 约束条件 → 需平衡

最终选择：
方案1（推荐）：
• 异常检测框架：Isolation Forest

方案2（如果需要概率输出）：
• XGBoost + scale_pos_weight + SHAP解释

方案3（最可解释）：
• 逻辑回归 + 类权重 + 欠采样

说明：极端不平衡情况，异常检测方法通常更合适
```

---

## 🔗 相关资源

### 本章其他文档
- **README.md** - 章节导航和学习指南
- **algorithm_selection_decision_tree.md** - 可视化决策流程
- **algorithm_comparison_table.md** - 14个算法详细对比

### 其他章节
- **01_data_diagnosis_framework** - 分析数据特征
- **02_problem_definition_guide** - 确定问题类型
- **04_preprocessing_and_features** - 数据预处理
- **05_model_evaluation** - 模型评估

---

**关键提示**：不同维度可能给出不同建议，需要根据实际情况权衡。通常数据分布特征和性能需求的权重最高！

**最后更新**：2024年11月
