# 🌳 算法选择决策树

> 系统化的算法选择流程 - 从数据特征到候选算法的完整路径

---

## 📋 使用说明

### 本文档的用途

1. **快速筛选候选算法**：5-10分钟内定位3-5个候选算法
2. **系统化决策**：不再凭感觉，而是基于数据特征逐步判断
3. **避免盲目试错**：减少不必要的实验，提高效率

### 如何使用

```
Step 1: 准备数据画像
  → 问题类型、样本数、特征数、数据分布、性能需求

Step 2: 按决策树走流程
  → 从顶层开始，逐层判断
  → 每个节点都有明确的判断标准

Step 3: 得到候选算法列表
  → 3-5个候选算法
  → 包含：Baseline算法 + 优化算法

Step 4: 验证合理性
  → 使用 data_to_algorithm_mapping.md 验证
  → 查看 algorithm_comparison_table.md 了解详情
```

---

## 🎯 顶层决策：问题类型分流

```
┌─────────────────────────────────────────────┐
│       开始：面对一个机器学习问题            │
└─────────────────────────────────────────────┘
                    ↓
         ┌─────────────────────┐
         │   是否有标签数据？   │
         └─────────────────────┘
                ↓           ↓
               有          没有
                ↓           ↓
        ┌───────────┐   ┌───────────┐
        │ 监督学习  │   │无监督学习 │
        └───────────┘   └───────────┘
             ↓               ↓
    ┌────────┴────────┐     ├─ 聚类
    │                 │     ├─ 降维
  预测类别？      预测数值？ └─ 异常检测
    ↓                 ↓
【分类问题】      【回归问题】
```

### 快速判断清单

**有标签数据（监督学习）？**
- ✅ 是：每个样本都有对应的正确答案 → 监督学习
- ❌ 否：只有输入特征，没有标签 → 无监督学习
- ⚠️ 部分有标签：半监督学习（见09_future_extensions）

**预测什么类型的输出？**
- 类别（是/否、A/B/C）→ **分类问题**
- 数值（价格、温度、数量）→ **回归问题**

---

## 📊 决策树 I：回归问题

### 完整流程图

```
【回归问题】
    ↓
┌────────────────────┐
│   样本量多少？      │
└────────────────────┘
    ↓
    ├─ <500 样本（小数据）
    │    ↓
    │  ┌────────────────────┐
    │  │ 特征与目标线性关系？│
    │  └────────────────────┘
    │    ↓           ↓
    │   线性        非线性
    │    ↓           ↓
    │  线性回归    决策树
    │  Ridge       KNN（k=3-5）
    │
    ├─ 500-50K（中等数据）
    │    ↓
    │  ┌────────────────────┐
    │  │  特征数量多少？    │
    │  └────────────────────┘
    │    ↓           ↓
    │   <100       >100
    │    ↓           ↓
    │  随机森林     Ridge/Lasso
    │  XGBoost      XGBoost
    │
    └─ >50K（大数据）
         ↓
       LightGBM（首选）
       XGBoost
       随机森林
```

### 决策点详解

#### 决策点1：样本量

| 样本量范围 | 推荐方向 | 原因 |
|-----------|----------|------|
| **<500** | 线性模型、决策树、KNN | 避免过拟合，模型要简单 |
| **500-5K** | 随机森林、XGBoost | 性价比最优，不易过拟合 |
| **5K-50K** | XGBoost、LightGBM、随机森林 | 可以使用复杂模型 |
| **>50K** | LightGBM、XGBoost | 大数据优化算法 |

#### 决策点2：线性关系

**如何判断线性关系？**
```python
# 方法1：可视化（小数据）
import matplotlib.pyplot as plt
plt.scatter(X[:,0], y)  # 看散点图趋势

# 方法2：相关系数
correlation = np.corrcoef(X[:,0], y)[0,1]
# |r| > 0.7 → 强线性关系
# 0.3 < |r| < 0.7 → 中等线性关系
# |r| < 0.3 → 弱线性关系

# 方法3：线性模型快速测试
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
lr = LinearRegression().fit(X_train, y_train)
score = r2_score(y_test, lr.predict(X_test))
# R² > 0.7 → 线性关系较强
# R² < 0.5 → 考虑非线性模型
```

#### 决策点3：特征数量

| 特征数 | 样本数 | 推荐策略 |
|-------|--------|----------|
| p < n/10 | 充足 | 任意算法 |
| n/10 < p < n/2 | 适中 | 集成方法、正则化 |
| p > n/2 | 不足 | Ridge/Lasso降维，或特征选择 |
| p > n | 高维小样本 | Lasso、Ridge（必须正则化） |

### 推荐算法组合

#### 场景1：小数据 + 线性关系（<500样本）
```
Baseline：
• 线性回归（最简单）
• Ridge（防过拟合）

优化方案：
• Lasso（自动特征选择）
• ElasticNet（L1+L2正则化）
```

#### 场景2：小数据 + 非线性（<500样本）
```
Baseline：
• 决策树（深度≤5）
• KNN（k=5-10）

优化方案：
• 随机森林（n_estimators=50-100）
• 多项式特征 + Ridge
```

#### 场景3：中等数据（500-50K）
```
Baseline：
• 随机森林（快速验证）
• Ridge

优化方案：
• XGBoost（高准确性）
• LightGBM（速度快）
• 随机森林（鲁棒性好）
```

#### 场景4：大数据（>50K）
```
首选：
• LightGBM（速度快，内存友好）
• XGBoost（稳定，可扩展）

备选：
• 随机森林（并行训练）
• SGDRegressor（在线学习）
```

---

## 🏷️ 决策树 II：分类问题

### 完整流程图

```
【分类问题】
    ↓
┌────────────────────┐
│  二分类 or 多分类？ │
└────────────────────┘
    ↓           ↓
  二分类       多分类
    ↓           ↓
【二分类决策树】【多分类决策树】
```

---

### 二分类决策树

```
【二分类问题】
    ↓
┌─────────────────────┐
│   数据是否平衡？    │
│ (少数类比例>20%？)  │
└─────────────────────┘
    ↓           ↓
  平衡         不平衡
    ↓           ↓
┌─────────┐   ┌─────────────────┐
│样本量？  │   │  不平衡比例？    │
└─────────┘   └─────────────────┘
    ↓               ↓
    ├─ <1K         ├─ 轻度不平衡(10-20%)
    │  ↓           │  → XGBoost + 类权重
    │ 逻辑回归     │  → SMOTE + 随机森林
    │ 决策树       │
    │              ├─ 中度不平衡(5-10%)
    ├─ 1K-50K      │  → XGBoost + scale_pos_weight
    │  ↓           │  → SMOTE + LightGBM
    │ 随机森林     │
    │ XGBoost      └─ 严重不平衡(<5%)
    │                 → 异常检测方法
    └─ >50K           → class_weight='balanced'
       ↓              → 欠采样 + XGBoost
     LightGBM
     XGBoost
```

### 决策点详解

#### 决策点1：数据平衡性

**如何判断？**
```python
# 计算类别分布
from collections import Counter
class_dist = Counter(y)
minority_ratio = min(class_dist.values()) / sum(class_dist.values())

if minority_ratio > 0.4:
    print("平衡数据")
elif minority_ratio > 0.2:
    print("轻度不平衡")
elif minority_ratio > 0.05:
    print("中度不平衡")
else:
    print("严重不平衡")
```

**处理策略**：

| 不平衡程度 | 少数类比例 | 策略 |
|-----------|-----------|------|
| 平衡 | >40% | 无需特殊处理 |
| 轻度不平衡 | 20-40% | 类权重调整 |
| 中度不平衡 | 5-20% | SMOTE + 类权重 |
| 严重不平衡 | <5% | 异常检测方法 / 采样 |

#### 决策点2：样本量（平衡数据）

| 样本量 | Baseline | 优化方案 |
|-------|----------|----------|
| <1K | 逻辑回归、决策树 | SVM（RBF核）、随机森林 |
| 1K-50K | 随机森林、逻辑回归 | XGBoost、LightGBM |
| >50K | LightGBM、逻辑回归 | XGBoost、神经网络 |

#### 决策点3：可解释性需求

| 需求级别 | 推荐算法 | 避免算法 |
|---------|----------|----------|
| **必须可解释** | 逻辑回归、决策树 | XGBoost、神经网络 |
| **适度可解释** | 随机森林、浅层决策树 | 深度神经网络 |
| **不需要** | XGBoost、LightGBM | - |

### 推荐算法组合

#### 场景1：平衡数据 + 小样本（<1K）
```
Baseline：
• 逻辑回归（线性可分）
• 决策树（非线性）

优化方案：
• SVM（RBF核，高维效果好）
• 随机森林（鲁棒性）
• KNN（k=3-10，简单有效）
```

#### 场景2：平衡数据 + 中等样本（1K-50K）
```
Baseline：
• 随机森林（稳定）
• 逻辑回归（快速）

优化方案：
• XGBoost（高准确性）★ 首选
• LightGBM（速度快）
• SVM（特征<100时）
```

#### 场景3：平衡数据 + 大样本（>50K）
```
首选：
• LightGBM（速度 + 准确性）
• XGBoost（稳定性好）

备选：
• 逻辑回归（基准线）
• 随机森林（鲁棒）
```

#### 场景4：不平衡数据（任意样本量）
```
轻度不平衡（20-40%）：
• XGBoost + class_weight
• 随机森林 + class_weight

中度不平衡（5-20%）：
• SMOTE + XGBoost ★ 首选
• SMOTE + LightGBM
• XGBoost + scale_pos_weight

严重不平衡（<5%）：
• Isolation Forest（异常检测）
• One-Class SVM
• 欠采样 + XGBoost
• 集成方法（BalancedRandomForest）
```

---

### 多分类决策树

```
【多分类问题】
    ↓
┌─────────────────────┐
│   类别数量多少？    │
└─────────────────────┘
    ↓
    ├─ 3-10类（常规多分类）
    │    ↓
    │  ┌──────────────┐
    │  │  样本量？    │
    │  └──────────────┘
    │    ↓
    │    ├─ <1K   → 决策树、朴素贝叶斯
    │    ├─ 1K-50K → 随机森林、XGBoost
    │    └─ >50K   → LightGBM、XGBoost
    │
    ├─ 10-100类（中等多分类）
    │    ↓
    │  XGBoost（首选）
    │  LightGBM
    │  One-vs-Rest策略
    │
    └─ >100类（高基数分类）
        ↓
      LightGBM（处理高基数）
      神经网络
      Label Embedding
```

### 推荐算法组合

#### 场景1：3-10类 + 小样本（<1K）
```
Baseline：
• 决策树（简单直观）
• 朴素贝叶斯（快速）

优化方案：
• 随机森林
• SVM（OvR策略）
```

#### 场景2：3-10类 + 中大样本（>1K）
```
Baseline：
• 随机森林
• 逻辑回归（multinomial）

优化方案：
• XGBoost（首选）
• LightGBM（速度快）
• 随机森林（鲁棒）
```

#### 场景3：10-100类
```
首选：
• XGBoost
• LightGBM

策略：
• One-vs-Rest（如果某些类别很少）
• 分层分类（如果类别有层次结构）
```

#### 场景4：>100类（高基数）
```
首选：
• LightGBM（原生支持高基数）
• XGBoost

高级方法：
• Label Embedding（降维）
• 神经网络（Softmax层）
• 分层分类（减少类别数）
```

---

## 🧩 决策树 III：聚类问题

### 完整流程图

```
【聚类问题】
    ↓
┌────────────────────┐
│  是否知道簇数K？    │
└────────────────────┘
    ↓           ↓
   知道        不知道
    ↓           ↓
┌─────────┐   ┌─────────────────┐
│簇形状？  │   │  数据分布特点？  │
└─────────┘   └─────────────────┘
    ↓               ↓
    ├─ 球形/凸形   ├─ 密度变化大
    │  ↓           │  → DBSCAN
    │ K-Means      │  → Mean Shift
    │              │
    ├─ 椭圆形      ├─ 层次结构
    │  ↓           │  → 层次聚类
    │ GMM          │
    │              └─ 不确定
    └─ 复杂形状        → 尝试多种K
       ↓              → Elbow Method
     DBSCAN           → Silhouette Score
     谱聚类
```

### 决策点详解

#### 决策点1：是否知道簇数K

**知道簇数（业务明确）**：
- 客户分群：高价值/中价值/低价值 → K=3
- 用户类型：活跃/沉默/流失 → K=3
- 产品线：A/B/C/D → K=4

**不知道簇数（探索性分析）**：
需要尝试多个K值，使用评估指标选择：
- Elbow Method（肘部法则）
- Silhouette Score（轮廓系数）
- Davies-Bouldin Index

#### 决策点2：簇形状特征

**如何判断？**
```python
# 可视化（降维到2D）
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X)
plt.scatter(X_2d[:,0], X_2d[:,1], alpha=0.5)
plt.show()

# 观察形状：
# • 圆形/凸形 → K-Means
# • 椭圆形 → GMM
# • 不规则形状 → DBSCAN
```

#### 决策点3：数据量

| 样本量 | 推荐算法 | 避免算法 |
|-------|----------|----------|
| <1K | K-Means、层次聚类 | - |
| 1K-50K | K-Means、DBSCAN、GMM | 层次聚类（太慢） |
| >50K | K-Means、Mini-Batch K-Means | 层次聚类、GMM |

### 推荐算法组合

#### 场景1：知道K + 球形簇
```
首选：
• K-Means（最快、最简单）

优化：
• K-Means++（更好的初始化）
• Mini-Batch K-Means（大数据）
```

#### 场景2：知道K + 椭圆形簇
```
首选：
• GMM（高斯混合模型）

备选：
• K-Means（作为baseline）
```

#### 场景3：不知道K + 密度变化大
```
首选：
• DBSCAN（自动发现簇数）

参数调优：
• eps：通过k-distance图确定
• min_samples：一般设为维度+1
```

#### 场景4：需要层次结构
```
首选：
• 层次聚类（Hierarchical Clustering）

方法选择：
• Agglomerative（自下而上）
• Ward linkage（常用）
```

#### 场景5：复杂形状 + 不知道K
```
方案1（推荐）：
• 尝试多个算法：K-Means、DBSCAN、GMM
• 使用Silhouette Score对比

方案2：
• 谱聚类（Spectral Clustering）
• 适合复杂形状，但较慢
```

---

## 📉 决策树 IV：降维问题

### 完整流程图

```
【降维问题】
    ↓
┌────────────────────┐
│   降维目的是什么？  │
└────────────────────┘
    ↓
    ├─ 数据可视化（降至2-3维）
    │    ↓
    │  ┌──────────────────┐
    │  │ 是否需要保留     │
    │  │ 全局结构？       │
    │  └──────────────────┘
    │    ↓           ↓
    │   是          否
    │    ↓           ↓
    │  PCA         t-SNE
    │              UMAP
    │
    ├─ 特征提取（保留信息）
    │    ↓
    │  ┌──────────────────┐
    │  │  线性 or 非线性？ │
    │  └──────────────────┘
    │    ↓           ↓
    │   线性        非线性
    │    ↓           ↓
    │  PCA         Kernel PCA
    │  SVD         Autoencoder
    │
    └─ 加速训练（减少特征）
         ↓
       PCA（保留90-95%方差）
       特征选择（SelectKBest）
       Lasso特征选择
```

### 决策点详解

#### 决策点1：降维目的

| 目的 | 推荐算法 | 目标维度 |
|------|----------|----------|
| **数据可视化** | t-SNE、UMAP、PCA | 2-3维 |
| **特征提取** | PCA、Kernel PCA | 保留90-95%方差 |
| **去噪** | PCA、Autoencoder | 保留主要成分 |
| **加速训练** | PCA、特征选择 | 50-80%原始维度 |

#### 决策点2：线性 vs 非线性

**如何判断？**
```python
# 方法1：PCA解释方差比例
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(X)
cumsum = np.cumsum(pca.explained_variance_ratio_)

# 如果前10个成分能解释>90%方差 → 线性关系强
# 如果需要很多成分才能解释>90% → 非线性关系

# 方法2：对比PCA和t-SNE可视化
# 如果PCA可视化效果好 → 线性足够
# 如果t-SNE效果明显更好 → 需要非线性
```

#### 决策点3：数据量

| 样本量 | 推荐算法 | 避免算法 |
|-------|----------|----------|
| <10K | PCA、t-SNE | - |
| 10K-100K | PCA、UMAP | t-SNE（太慢） |
| >100K | PCA、UMAP、Mini-Batch PCA | t-SNE |

### 推荐算法组合

#### 场景1：数据可视化（降至2D/3D）
```
快速可视化：
• PCA（最快，看全局结构）

高质量可视化：
• t-SNE（保留局部结构，效果好）
• UMAP（速度快 + 效果好）★ 推荐

对比方法：
• 先用PCA看全局
• 再用t-SNE/UMAP看细节
```

#### 场景2：特征提取（保留信息）
```
线性关系：
• PCA（首选，快速高效）
• TruncatedSVD（稀疏矩阵）

非线性关系：
• Kernel PCA（RBF核）
• Autoencoder（深度学习方法）
```

#### 场景3：去噪
```
首选：
• PCA（保留主要成分，过滤噪声）

高级方法：
• Denoising Autoencoder
• ICA（独立成分分析）
```

#### 场景4：加速训练
```
首选：
• PCA（保留90-95%方差）
• 特征选择（SelectKBest、RFE）

大数据：
• TruncatedSVD
• Incremental PCA
```

---

## 🚨 决策树 V：异常检测

### 完整流程图

```
【异常检测】
    ↓
┌────────────────────┐
│   样本量多少？      │
└────────────────────┘
    ↓
    ├─ <10K（小数据）
    │    ↓
    │  ┌────────────────┐
    │  │ 特征维度？     │
    │  └────────────────┘
    │    ↓           ↓
    │   低维        高维
    │  (<10)       (>10)
    │    ↓           ↓
    │  Z-Score     Isolation
    │  LOF         Forest
    │
    └─ >10K（大数据）
         ↓
       Isolation Forest（首选）
       One-Class SVM
```

### 决策点详解

#### 决策点1：样本量

| 样本量 | 推荐算法 | 原因 |
|-------|----------|------|
| <1K | LOF、Z-Score | 简单方法即可 |
| 1K-100K | Isolation Forest | 平衡性能和效果 |
| >100K | Isolation Forest、SGD One-Class SVM | 可扩展算法 |

#### 决策点2：特征维度

| 维度 | 推荐方法 | 避免方法 |
|------|----------|----------|
| 低维（<5） | Z-Score、IQR、LOF | - |
| 中维（5-20） | Isolation Forest、LOF | Z-Score（多维效果差） |
| 高维（>20） | Isolation Forest、Autoencoder | LOF（维度灾难） |

#### 决策点3：异常比例

**如何设置？**
```python
# 如果已知异常比例
contamination = 0.05  # 5%的异常

# 如果未知，可以尝试
contamination = 'auto'  # 自动估计

# 或尝试多个值
for cont in [0.01, 0.05, 0.1]:
    model = IsolationForest(contamination=cont)
    # 评估结果
```

### 推荐算法组合

#### 场景1：小数据 + 低维（<1K样本, <10维）
```
简单方法：
• Z-Score（单变量）
• IQR方法（箱线图）

进阶方法：
• LOF（局部异常因子）
• Isolation Forest
```

#### 场景2：小数据 + 高维（<1K样本, >10维）
```
首选：
• Isolation Forest

备选：
• One-Class SVM（RBF核）
• Autoencoder（如果样本量>5K）
```

#### 场景3：大数据（>10K样本）
```
首选：
• Isolation Forest（快速 + 准确）

备选：
• One-Class SVM（准确但较慢）
• LOF（中等数据量，<100K）
```

#### 场景4：时间序列异常检测
```
专用方法：
• ARIMA残差分析
• LSTM Autoencoder
• Prophet（Facebook）

通用方法：
• Isolation Forest（用滞后特征）
```

---

## 📋 实战决策模板

### 模板：快速决策（5分钟）

```
□ Step 1: 问题画像（2分钟）
  问题类型：_______（分类/回归/聚类/降维/异常检测）
  样本数量：_______
  特征数量：_______
  数据特征：_______（平衡/不平衡，线性/非线性）
  性能需求：_______（准确性/速度/可解释性优先级）

□ Step 2: 走决策树（2分钟）
  决策路径：___ → ___ → ___ → ___

□ Step 3: 候选算法（1分钟）
  Baseline：
  • ___________（最简单，快速验证）
  • ___________

  优化方案：
  • ___________（追求性能）
  • ___________
  • ___________
```

### 案例1：房价预测（回归）

```
Step 1: 问题画像
  问题类型：回归
  样本数量：5,000
  特征数量：20
  数据特征：中等规模，可能非线性
  性能需求：准确性优先

Step 2: 决策路径
  回归 → 中等数据(5K) → 特征数中等(<100)

Step 3: 候选算法
  Baseline：
  • Ridge回归（线性baseline）
  • 随机森林（非线性baseline）

  优化方案：
  • XGBoost（首选，高准确性）
  • LightGBM（速度更快）
  • 随机森林（鲁棒性好）
```

### 案例2：客户流失预测（二分类）

```
Step 1: 问题画像
  问题类型：二分类
  样本数量：50,000
  特征数量：30
  数据特征：不平衡（流失率10%）
  性能需求：准确性 > 速度

Step 2: 决策路径
  分类 → 二分类 → 不平衡 → 中度不平衡(10%)

Step 3: 候选算法
  Baseline：
  • 逻辑回归 + class_weight

  优化方案：
  • SMOTE + XGBoost ★ 首选
  • XGBoost + scale_pos_weight
  • LightGBM + is_unbalance=True
```

### 案例3：客户分群（聚类）

```
Step 1: 问题画像
  问题类型：聚类
  样本数量：20,000
  特征数量：15
  是否知道K：否（探索性分析）

Step 2: 决策路径
  聚类 → 不知道K → 尝试多种方法

Step 3: 候选算法
  方案1（推荐）：
  • K-Means（K=2-10，用Elbow Method选K）
  • DBSCAN（密度聚类，自动发现K）
  • 对比Silhouette Score选最佳

  方案2（如果需要层次）：
  • 层次聚类（查看树状图）
```

---

## ✅ 决策检查清单

使用决策树后，检查以下内容：

### 数据理解检查
- [ ] 问题类型明确（分类/回归/聚类等）
- [ ] 数据量清楚（样本数、特征数）
- [ ] 数据特征已分析（平衡性、线性、分布）
- [ ] 性能需求已明确（优先级排序）

### 算法选择检查
- [ ] 列出了Baseline算法（2个）
- [ ] 列出了优化算法（2-3个）
- [ ] 每个算法都有选择理由
- [ ] 考虑了数据特征的匹配

### 实施计划检查
- [ ] Baseline实验计划（<30分钟）
- [ ] 优化实验计划（1-3小时）
- [ ] 评估指标已确定
- [ ] 后续调优方向明确

---

## 🔗 相关资源

### 本章其他文档
- **data_to_algorithm_mapping.md** - 验证算法选择的合理性
- **algorithm_comparison_table.md** - 查看算法详细信息

### 其他章节
- **01_data_diagnosis_framework** - 如果数据特征不清楚
- **02_problem_definition_guide** - 如果问题类型不确定
- **04_preprocessing_and_features** - 确定算法后的数据处理
- **05_model_evaluation** - 确定评估指标

---

**关键提示**：决策树是指导而非铁律，实际应用中需要根据具体情况灵活调整。建议先按决策树选择，再通过实验验证！

**最后更新**：2024年11月
