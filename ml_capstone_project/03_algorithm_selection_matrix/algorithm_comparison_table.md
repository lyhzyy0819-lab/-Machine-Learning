# ğŸ“Š æœºå™¨å­¦ä¹ ç®—æ³•å…¨é¢å¯¹æ¯”è¡¨

> **å¿«é€ŸæŸ¥æ‰¾é€‚åˆä½ é—®é¢˜çš„ç®—æ³•**
> åŒ…å«ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ çš„æ‰€æœ‰ä¸»æµç®—æ³•è¯¦ç»†å¯¹æ¯”

---

## ğŸ“– ä½¿ç”¨è¯´æ˜

### å¦‚ä½•ä½¿ç”¨æœ¬å¯¹æ¯”è¡¨

1. **å¿«é€ŸæŸ¥æ‰¾**ï¼šæ ¹æ®ä½ çš„é—®é¢˜ç±»å‹ï¼ˆå›å½’/åˆ†ç±»/èšç±»ç­‰ï¼‰ç›´æ¥è·³è½¬åˆ°å¯¹åº”ç« èŠ‚
2. **è¯¦ç»†å¯¹æ¯”**ï¼šæŸ¥çœ‹æ¯ä¸ªç®—æ³•çš„é€‚ç”¨åœºæ™¯ã€ä¼˜ç¼ºç‚¹ã€å‚æ•°è°ƒä¼˜å»ºè®®
3. **å†³ç­–çŸ©é˜µ**ï¼šä½¿ç”¨æœ¬æ–‡æœ«å°¾çš„å†³ç­–çŸ©é˜µå¿«é€Ÿç­›é€‰å€™é€‰ç®—æ³•

### ç¬¦å·è¯´æ˜

- âœ… **æ¨è** - è¯¥åœºæ™¯ä¸‹çš„ä¼˜é€‰ç®—æ³•
- âš ï¸ **è°¨æ…** - å¯ä»¥ä½¿ç”¨ä½†æœ‰ä¸€å®šé™åˆ¶
- âŒ **ä¸æ¨è** - ä¸é€‚åˆè¯¥åœºæ™¯
- â­ **éš¾åº¦** - å­¦ä¹ å’Œä½¿ç”¨éš¾åº¦ï¼ˆ1-5æ˜Ÿï¼‰
- ğŸš€ **é€Ÿåº¦** - è®­ç»ƒå’Œé¢„æµ‹é€Ÿåº¦ï¼ˆ1-5æ˜Ÿï¼‰

---

## ğŸ“‘ ç›®å½•

### ç¬¬ä¸€éƒ¨åˆ†ï¼šç›‘ç£å­¦ä¹ ç®—æ³•
1. [çº¿æ€§å›å½’ (Linear Regression)](#1-çº¿æ€§å›å½’-linear-regression)
2. [é€»è¾‘å›å½’ (Logistic Regression)](#2-é€»è¾‘å›å½’-logistic-regression)
3. [å†³ç­–æ ‘ (Decision Tree)](#3-å†³ç­–æ ‘-decision-tree)
4. [éšæœºæ£®æ— (Random Forest)](#4-éšæœºæ£®æ—-random-forest)
5. [æ”¯æŒå‘é‡æœº (SVM)](#5-æ”¯æŒå‘é‡æœº-svm)
6. [Kè¿‘é‚» (KNN)](#6-kè¿‘é‚»-knn)
7. [XGBoost](#7-xgboost)
8. [LightGBM](#8-lightgbm)

### ç¬¬äºŒéƒ¨åˆ†ï¼šæ— ç›‘ç£å­¦ä¹ ç®—æ³•
9. [K-Means èšç±»](#9-k-means-èšç±»)
10. [DBSCAN](#10-dbscan)
11. [å±‚æ¬¡èšç±» (Hierarchical Clustering)](#11-å±‚æ¬¡èšç±»-hierarchical-clustering)
12. [é«˜æ–¯æ··åˆæ¨¡å‹ (GMM)](#12-é«˜æ–¯æ··åˆæ¨¡å‹-gmm)
13. [PCA (ä¸»æˆåˆ†åˆ†æ)](#13-pca-ä¸»æˆåˆ†åˆ†æ)
14. [t-SNE](#14-t-sne)

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šç®—æ³•é€‰æ‹©å†³ç­–çŸ©é˜µ
- [æŒ‰æ•°æ®é‡é€‰æ‹©](#æŒ‰æ•°æ®é‡é€‰æ‹©)
- [æŒ‰é—®é¢˜ç±»å‹é€‰æ‹©](#æŒ‰é—®é¢˜ç±»å‹é€‰æ‹©)
- [æŒ‰æ€§èƒ½è¦æ±‚é€‰æ‹©](#æŒ‰æ€§èƒ½è¦æ±‚é€‰æ‹©)
- [å¿«é€Ÿå‚è€ƒè¡¨](#å¿«é€Ÿå‚è€ƒè¡¨)

---

# ç¬¬ä¸€éƒ¨åˆ†ï¼šç›‘ç£å­¦ä¹ ç®—æ³•

## 1. çº¿æ€§å›å½’ (Linear Regression)

### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šå¯»æ‰¾ç‰¹å¾å’Œç›®æ ‡å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»

**æ•°å­¦è¡¨è¾¾**ï¼šy = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™

**å­¦ä¹ æ–¹å¼**ï¼šæœ€å°äºŒä¹˜æ³•ï¼ˆOLSï¼‰æˆ–æ¢¯åº¦ä¸‹é™

---

### âœ… é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ˜¯å¦é€‚ç”¨ | è¯´æ˜ |
|------|---------|------|
| **æ•°æ®é‡** | 100+ æ ·æœ¬ | âœ… å°æ•°æ®é›†ä¹Ÿèƒ½å·¥ä½œè‰¯å¥½ |
| **ç‰¹å¾å…³ç³»** | çº¿æ€§å…³ç³» | âœ… ç‰¹å¾ä¸ç›®æ ‡å‘ˆçº¿æ€§å…³ç³»æ—¶æ•ˆæœæœ€å¥½ |
| **ç›®æ ‡å˜é‡** | è¿ç»­å€¼ | âœ… ä¸“é—¨ç”¨äºå›å½’é—®é¢˜ |
| **é«˜ç»´æ•°æ®** | ä¸­ç­‰ç»´åº¦ | âš ï¸ é«˜ç»´éœ€è¦æ­£åˆ™åŒ–ï¼ˆRidge/Lassoï¼‰ |
| **å¤šé‡å…±çº¿æ€§** | æ— å¼ºç›¸å…³ | âš ï¸ ç‰¹å¾é—´å¼ºç›¸å…³ä¼šå¯¼è‡´ä¸ç¨³å®š |

**å…¸å‹åº”ç”¨**ï¼š
- æˆ¿ä»·é¢„æµ‹ï¼ˆé¢ç§¯ã€ä½ç½® â†’ ä»·æ ¼ï¼‰
- é”€å”®é¢„æµ‹ï¼ˆå¹¿å‘ŠæŠ•å…¥ â†’ é”€é‡ï¼‰
- æ¸©åº¦é¢„æµ‹ï¼ˆå†å²æ•°æ® â†’ æœªæ¥æ¸©åº¦ï¼‰

---

### ğŸ‘ ä¼˜ç‚¹

1. **ç®€å•å¿«é€Ÿ** - è®­ç»ƒå’Œé¢„æµ‹é€Ÿåº¦æå¿«ï¼ŒO(nÂ·pÂ²) å¤æ‚åº¦
2. **æ˜“äºè§£é‡Š** - å¯ä»¥ç›´æ¥çœ‹å‡ºæ¯ä¸ªç‰¹å¾çš„å½±å“æƒé‡
3. **ç¨³å®šæ€§å¥½** - ä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆç›¸å¯¹äºå¤æ‚æ¨¡å‹ï¼‰
4. **æ— éœ€è°ƒå‚** - å‡ ä¹æ²¡æœ‰è¶…å‚æ•°éœ€è¦è°ƒæ•´
5. **å¯æ‰©å±•** - å®¹æ˜“æ·»åŠ æ­£åˆ™åŒ–ï¼ˆRidge/Lasso/ElasticNetï¼‰

---

### ğŸ‘ ç¼ºç‚¹

1. **åªèƒ½å»ºæ¨¡çº¿æ€§å…³ç³»** - æ— æ³•æ•æ‰å¤æ‚çš„éçº¿æ€§æ¨¡å¼
2. **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ** - æç«¯å€¼ä¼šä¸¥é‡å½±å“å›å½’çº¿
3. **å®¹æ˜“æ¬ æ‹Ÿåˆ** - å¯¹äºå¤æ‚é—®é¢˜è¡¨ç°ä¸ä½³
4. **å‡è®¾è¦æ±‚ä¸¥æ ¼** - éœ€è¦æ»¡è¶³çº¿æ€§ã€ç‹¬ç«‹æ€§ã€åŒæ–¹å·®ç­‰å‡è®¾
5. **å¤šé‡å…±çº¿æ€§é—®é¢˜** - ç‰¹å¾é—´ç›¸å…³ä¼šå¯¼è‡´ç³»æ•°ä¸ç¨³å®š

---

### âš™ï¸ å…³é”®å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `fit_intercept` | æ˜¯å¦æ‹Ÿåˆæˆªè·é¡¹ | `True`ï¼ˆé»˜è®¤ï¼‰ |
| `normalize` | æ˜¯å¦æ ‡å‡†åŒ–ç‰¹å¾ | `False`ï¼ˆå»ºè®®æ‰‹åŠ¨æ ‡å‡†åŒ–ï¼‰ |

**æ­£åˆ™åŒ–ç‰ˆæœ¬å‚æ•°**ï¼š

**Ridge (L2æ­£åˆ™åŒ–)**ï¼š
```python
from sklearn.linear_model import Ridge
model = Ridge(alpha=1.0)  # alphaè¶Šå¤§ï¼Œæ­£åˆ™åŒ–è¶Šå¼º
```

**Lasso (L1æ­£åˆ™åŒ–)**ï¼š
```python
from sklearn.linear_model import Lasso
model = Lasso(alpha=1.0)  # å¯ä»¥å®ç°ç‰¹å¾é€‰æ‹©
```

**ElasticNet (L1+L2)**ï¼š
```python
from sklearn.linear_model import ElasticNet
model = ElasticNet(alpha=1.0, l1_ratio=0.5)  # l1_ratioæ§åˆ¶L1å’ŒL2æ¯”ä¾‹
```

---

### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† |
|---------|------|
| è®­ç»ƒé€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (æå¿«) |
| é¢„æµ‹é€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (æå¿«) |
| å†…å­˜å ç”¨ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (å¾ˆå°) |
| å¯è§£é‡Šæ€§ | â­â­â­â­â­ (å®Œç¾) |
| å‡†ç¡®æ€§ | â­â­â­ (ä¸­ç­‰ï¼Œå–å†³äºæ•°æ®) |
| å­¦ä¹ éš¾åº¦ | â­ (éå¸¸ç®€å•) |

---

### ğŸ’¡ ä½¿ç”¨å»ºè®®

**ä½•æ—¶ä½œä¸ºé¦–é€‰**ï¼š
- âœ… ç‰¹å¾ä¸ç›®æ ‡å‘ˆçº¿æ€§å…³ç³»
- âœ… éœ€è¦å¿«é€Ÿå»ºç«‹ baseline
- âœ… éœ€è¦é«˜åº¦å¯è§£é‡Šæ€§
- âœ… æ•°æ®é‡è¾ƒå°ï¼ˆ<1ä¸‡ï¼‰

**ä½•æ—¶ä¸æ¨è**ï¼š
- âŒ ç‰¹å¾ä¸ç›®æ ‡å‘ˆæ˜æ˜¾éçº¿æ€§å…³ç³»
- âŒ æ•°æ®å­˜åœ¨å¤§é‡å¼‚å¸¸å€¼
- âŒ è¿½æ±‚æœ€é«˜é¢„æµ‹ç²¾åº¦ï¼ˆåº”é€‰æ‹©é›†æˆæ¨¡å‹ï¼‰

**æœ€ä½³å®è·µ**ï¼š
```python
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# 1. å»ºè®®ä½¿ç”¨Pipelineï¼ˆè‡ªåŠ¨æ ‡å‡†åŒ–ï¼‰
model = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', Ridge(alpha=10.0))
])

# 2. ä½¿ç”¨Ridgeè€Œéæ™®é€šçº¿æ€§å›å½’ï¼ˆæ›´ç¨³å¥ï¼‰
# 3. äº¤å‰éªŒè¯é€‰æ‹©alpha
from sklearn.model_selection import GridSearchCV
param_grid = {'regressor__alpha': [0.1, 1, 10, 100]}
grid = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid.fit(X_train, y_train)
```

---

## 2. é€»è¾‘å›å½’ (Logistic Regression)

### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡ Sigmoid å‡½æ•°å°†çº¿æ€§ç»„åˆæ˜ å°„åˆ° [0,1] æ¦‚ç‡

**æ•°å­¦è¡¨è¾¾**ï¼šP(y=1|x) = 1 / (1 + e^(-(Î²â‚€ + Î²â‚xâ‚ + ... + Î²â‚™xâ‚™)))

**å­¦ä¹ æ–¹å¼**ï¼šæœ€å¤§ä¼¼ç„¶ä¼°è®¡ + æ¢¯åº¦ä¸‹é™

---

### âœ… é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ˜¯å¦é€‚ç”¨ | è¯´æ˜ |
|------|---------|------|
| **é—®é¢˜ç±»å‹** | äºŒåˆ†ç±» | âœ… ä¸“é—¨è®¾è®¡ç”¨äºäºŒåˆ†ç±» |
| **å¤šåˆ†ç±»** | å¯æ‰©å±• | âœ… ä½¿ç”¨ One-vs-Rest æˆ– Softmax |
| **æ•°æ®é‡** | 100+ æ ·æœ¬ | âœ… å°æ•°æ®é›†ä¹Ÿèƒ½å·¥ä½œ |
| **ç‰¹å¾å…³ç³»** | çº¿æ€§å¯åˆ† | âœ… ç‰¹å¾å¯¹ log-odds å‘ˆçº¿æ€§å…³ç³» |
| **éœ€è¦æ¦‚ç‡** | æ˜¯ | âœ… ç›´æ¥è¾“å‡ºæ¦‚ç‡ï¼Œä¾¿äºé˜ˆå€¼è°ƒæ•´ |
| **ç±»åˆ«å¹³è¡¡** | ä»»æ„ | âœ… å¯é€šè¿‡ class_weight å¤„ç†ä¸å¹³è¡¡ |

**å…¸å‹åº”ç”¨**ï¼š
- åƒåœ¾é‚®ä»¶åˆ†ç±»ï¼ˆç‰¹å¾ â†’ åƒåœ¾/æ­£å¸¸ï¼‰
- ä¿¡ç”¨è¯„åˆ†ï¼ˆç”¨æˆ·ä¿¡æ¯ â†’ è¿çº¦/ä¸è¿çº¦ï¼‰
- ç–¾ç—…è¯Šæ–­ï¼ˆç—‡çŠ¶ â†’ æ‚£ç—…/å¥åº·ï¼‰
- ç‚¹å‡»ç‡é¢„æµ‹ï¼ˆå¹¿å‘Šç‰¹å¾ â†’ ç‚¹å‡»/ä¸ç‚¹å‡»ï¼‰

---

### ğŸ‘ ä¼˜ç‚¹

1. **è¾“å‡ºæ¦‚ç‡** - ç›´æ¥å¾—åˆ°åˆ†ç±»æ¦‚ç‡ï¼Œä¾¿äºä¸šåŠ¡å†³ç­–
2. **å¯è§£é‡Šæ€§å¼º** - ç³»æ•°è¡¨ç¤ºç‰¹å¾å¯¹ log-odds çš„å½±å“
3. **è®­ç»ƒå¿«é€Ÿ** - è®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚åˆå¤§æ•°æ®
4. **æ”¯æŒåœ¨çº¿å­¦ä¹ ** - å¯ä»¥å¢é‡æ›´æ–°æ¨¡å‹
5. **å¤„ç†ä¸å¹³è¡¡** - é€šè¿‡ class_weight è½»æ¾å¤„ç†
6. **å¤šåˆ†ç±»æ‰©å±•** - å®¹æ˜“æ‰©å±•åˆ°å¤šåˆ†ç±»é—®é¢˜

---

### ğŸ‘ ç¼ºç‚¹

1. **åªèƒ½å»ºæ¨¡çº¿æ€§å†³ç­–è¾¹ç•Œ** - æ— æ³•å¤„ç†å¤æ‚éçº¿æ€§å…³ç³»
2. **å®¹æ˜“æ¬ æ‹Ÿåˆ** - å¯¹å¤æ‚æ¨¡å¼å»ºæ¨¡èƒ½åŠ›æœ‰é™
3. **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ** - æç«¯å€¼ä¼šå½±å“ç³»æ•°ä¼°è®¡
4. **éœ€è¦ç‰¹å¾å·¥ç¨‹** - éœ€è¦æ‰‹åŠ¨åˆ›å»ºäº¤äº’ç‰¹å¾æ•æ‰éçº¿æ€§
5. **å¤šé‡å…±çº¿æ€§** - ç‰¹å¾ç›¸å…³ä¼šå¯¼è‡´ç³»æ•°ä¸ç¨³å®š

---

### âš™ï¸ å…³é”®å‚æ•°

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    penalty='l2',           # æ­£åˆ™åŒ–ç±»å‹: 'l1', 'l2', 'elasticnet', 'none'
    C=1.0,                  # æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆè¶Šå°æ­£åˆ™åŒ–è¶Šå¼ºï¼‰
    solver='lbfgs',         # ä¼˜åŒ–ç®—æ³•: 'lbfgs', 'liblinear', 'saga'
    max_iter=100,           # æœ€å¤§è¿­ä»£æ¬¡æ•°
    class_weight=None,      # ç±»åˆ«æƒé‡ï¼Œå¯è®¾ç½®ä¸º'balanced'å¤„ç†ä¸å¹³è¡¡
    random_state=42
)
```

**å‚æ•°è°ƒä¼˜å»ºè®®**ï¼š

| å‚æ•° | è°ƒä¼˜èŒƒå›´ | è¯´æ˜ |
|------|---------|------|
| `C` | [0.001, 0.01, 0.1, 1, 10, 100] | ä»å°åˆ°å¤§å°è¯• |
| `penalty` | ['l1', 'l2'] | L1å¯åšç‰¹å¾é€‰æ‹©ï¼ŒL2æ›´ç¨³å®š |
| `class_weight` | [None, 'balanced'] | ä¸å¹³è¡¡æ•°æ®ç”¨'balanced' |
| `solver` | æ ¹æ®æ•°æ®é€‰æ‹© | å°æ•°æ®ç”¨'liblinear'ï¼Œå¤§æ•°æ®ç”¨'saga' |

---

### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† |
|---------|------|
| è®­ç»ƒé€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (æå¿«) |
| é¢„æµ‹é€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (æå¿«) |
| å†…å­˜å ç”¨ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (å¾ˆå°) |
| å¯è§£é‡Šæ€§ | â­â­â­â­â­ (å®Œç¾) |
| å‡†ç¡®æ€§ | â­â­â­ (ä¸­ç­‰) |
| å­¦ä¹ éš¾åº¦ | â­â­ (ç®€å•) |

---

### ğŸ’¡ ä½¿ç”¨å»ºè®®

**ä½•æ—¶ä½œä¸ºé¦–é€‰**ï¼š
- âœ… éœ€è¦æ¦‚ç‡è¾“å‡ºè€Œéç¡¬åˆ†ç±»
- âœ… éœ€è¦å¿«é€Ÿè®­ç»ƒï¼ˆå¤§æ•°æ®é›†ï¼‰
- âœ… éœ€è¦é«˜åº¦å¯è§£é‡Šæ€§
- âœ… ä½œä¸ºåˆ†ç±»é—®é¢˜çš„ baseline

**ä½•æ—¶ä¸æ¨è**ï¼š
- âŒ æ•°æ®å‘ˆæ˜æ˜¾éçº¿æ€§å¯åˆ†
- âŒ è¿½æ±‚æœ€é«˜ç²¾åº¦ï¼ˆåº”é€‰æ‹© XGBoost ç­‰ï¼‰
- âŒ ç‰¹å¾é—´å­˜åœ¨å¤æ‚äº¤äº’ï¼ˆéœ€è¦å¤§é‡æ‰‹åŠ¨ç‰¹å¾å·¥ç¨‹ï¼‰

**æœ€ä½³å®è·µ**ï¼š
```python
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# 1. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
model = LogisticRegression(
    class_weight='balanced',  # è‡ªåŠ¨è°ƒæ•´æƒé‡
    max_iter=1000
)

# 2. ä¸æ ‡å‡†åŒ–ç»“åˆ
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(C=1.0, max_iter=1000))
])

# 3. è°ƒæ•´å†³ç­–é˜ˆå€¼
y_proba = model.predict_proba(X_test)[:, 1]
y_pred = (y_proba > 0.3).astype(int)  # æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´é˜ˆå€¼

# 4. æŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'coefficient': model.coef_[0]
}).sort_values('coefficient', key=abs, ascending=False)
```

---

## 3. å†³ç­–æ ‘ (Decision Tree)

### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡ä¸€ç³»åˆ— if-else è§„åˆ™é€’å½’åˆ†å‰²æ•°æ®

**åˆ†è£‚æ ‡å‡†**ï¼š
- **åˆ†ç±»**ï¼šGini ä¸çº¯åº¦æˆ–ä¿¡æ¯å¢ç›Šï¼ˆç†µï¼‰
- **å›å½’**ï¼šå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰

**å­¦ä¹ æ–¹å¼**ï¼šè´ªå¿ƒç®—æ³•ï¼Œæ¯æ¬¡é€‰æ‹©æœ€ä¼˜åˆ†è£‚ç‚¹

---

### âœ… é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ˜¯å¦é€‚ç”¨ | è¯´æ˜ |
|------|---------|------|
| **æ•°æ®ç±»å‹** | æ•°å€¼+ç±»åˆ« | âœ… å¯ç›´æ¥å¤„ç†ç±»åˆ«ç‰¹å¾ |
| **ç‰¹å¾å…³ç³»** | éçº¿æ€§ | âœ… å¯æ•æ‰å¤æ‚éçº¿æ€§å…³ç³» |
| **å¯è§£é‡Šæ€§** | éœ€è¦ | âœ… å†³ç­–è·¯å¾„æ¸…æ™°æ˜“æ‡‚ |
| **ç¼ºå¤±å€¼** | æœ‰ | âœ… å¯ä»¥å¤„ç†ç¼ºå¤±å€¼ |
| **æ•°æ®é‡** | å°-ä¸­ | âš ï¸ å¤§æ•°æ®é›†å®¹æ˜“è¿‡æ‹Ÿåˆ |
| **é«˜ç»´æ•°æ®** | ä¸æ¨è | âŒ é«˜ç»´ç©ºé—´è¡¨ç°ä¸ä½³ |

**å…¸å‹åº”ç”¨**ï¼š
- ä¿¡è´·å®¡æ‰¹ï¼ˆè§„åˆ™æ¸…æ™°ï¼Œéœ€è¦è§£é‡Šï¼‰
- åŒ»ç–—è¯Šæ–­ï¼ˆä¸“å®¶è§„åˆ™æå–ï¼‰
- å®¢æˆ·åˆ†ç¾¤ï¼ˆå¯è§†åŒ–å†³ç­–æ ‘ï¼‰

---

### ğŸ‘ ä¼˜ç‚¹

1. **æå¼ºå¯è§£é‡Šæ€§** - å¯ä»¥å¯è§†åŒ–ä¸ºæ ‘çŠ¶å›¾ï¼ŒéæŠ€æœ¯äººå‘˜ä¹Ÿèƒ½ç†è§£
2. **æ— éœ€æ•°æ®é¢„å¤„ç†** - ä¸éœ€è¦æ ‡å‡†åŒ–ã€å½’ä¸€åŒ–
3. **å¤„ç†ç±»åˆ«ç‰¹å¾** - ç›´æ¥å¤„ç†åˆ†ç±»å˜é‡ï¼Œæ— éœ€ç¼–ç 
4. **éçº¿æ€§å»ºæ¨¡** - å¯ä»¥æ•æ‰å¤æ‚çš„éçº¿æ€§å…³ç³»
5. **è‡ªåŠ¨ç‰¹å¾é€‰æ‹©** - åªä½¿ç”¨æœ‰ä¿¡æ¯é‡çš„ç‰¹å¾
6. **å¤„ç†ç¼ºå¤±å€¼** - å¯ä»¥ç”¨ä»£ç†åˆ†è£‚å¤„ç†ç¼ºå¤±
7. **å¿«é€Ÿé¢„æµ‹** - é¢„æµ‹æ—¶é—´ä¸º O(log n)

---

### ğŸ‘ ç¼ºç‚¹

1. **ææ˜“è¿‡æ‹Ÿåˆ** - ä¸åŠ é™åˆ¶ä¼šç”Ÿæˆéå¸¸å¤æ‚çš„æ ‘
2. **ä¸ç¨³å®š** - æ•°æ®å¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´æ ‘ç»“æ„å®Œå…¨ä¸åŒ
3. **é¢„æµ‹ä¸å¹³æ»‘** - å†³ç­–è¾¹ç•Œéƒ½æ˜¯è½´å¹³è¡Œçš„
4. **ä¸é€‚åˆçº¿æ€§å…³ç³»** - å¯¹äºç®€å•çº¿æ€§å…³ç³»ï¼Œè¡¨ç°ä¸å¦‚çº¿æ€§æ¨¡å‹
5. **åå‘å¤šå€¼ç‰¹å¾** - å€¾å‘é€‰æ‹©å–å€¼è¾ƒå¤šçš„ç‰¹å¾
6. **éš¾ä»¥æ•æ‰ç‰¹å¾ç»„åˆ** - éœ€è¦å¾ˆæ·±çš„æ ‘æ‰èƒ½è¡¨ç¤º XOR ç­‰ç»„åˆ

---

### âš™ï¸ å…³é”®å‚æ•°

```python
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor

# åˆ†ç±»æ ‘
model = DecisionTreeClassifier(
    criterion='gini',          # åˆ†è£‚æ ‡å‡†: 'gini' æˆ– 'entropy'
    max_depth=None,            # æ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆé‡è¦ï¼ï¼‰
    min_samples_split=2,       # åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=1,        # å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    max_features=None,         # æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„æœ€å¤§ç‰¹å¾æ•°
    max_leaf_nodes=None,       # æœ€å¤§å¶å­èŠ‚ç‚¹æ•°
    random_state=42
)
```

**é˜²æ­¢è¿‡æ‹Ÿåˆçš„å…³é”®å‚æ•°**ï¼š

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|-------|------|
| `max_depth` | 3-10 | **æœ€é‡è¦**ï¼Œé™åˆ¶æ ‘æ·±åº¦ |
| `min_samples_split` | 2-20 | åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬ |
| `min_samples_leaf` | 1-10 | å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬ |
| `max_leaf_nodes` | 10-100 | é™åˆ¶å¶å­æ€»æ•° |

**è°ƒä¼˜å»ºè®®**ï¼š
```python
# æ–¹æ³•1ï¼šé™åˆ¶æ·±åº¦
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# æ–¹æ³•2ï¼šé¢„å‰ªæï¼ˆæ¨èï¼‰
model = DecisionTreeClassifier(
    max_depth=5,              # é™åˆ¶æ·±åº¦
    min_samples_split=10,     # è‡³å°‘10ä¸ªæ ·æœ¬æ‰åˆ†è£‚
    min_samples_leaf=5        # å¶å­è‡³å°‘5ä¸ªæ ·æœ¬
)
```

---

### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† |
|---------|------|
| è®­ç»ƒé€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ (å¿«) |
| é¢„æµ‹é€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (æå¿«) |
| å†…å­˜å ç”¨ | ğŸš€ğŸš€ğŸš€ (ä¸­ç­‰) |
| å¯è§£é‡Šæ€§ | â­â­â­â­â­ (å®Œç¾) |
| å‡†ç¡®æ€§ | â­â­â­ (ä¸­ç­‰ï¼Œæ˜“è¿‡æ‹Ÿåˆ) |
| å­¦ä¹ éš¾åº¦ | â­â­ (ç®€å•) |

---

### ğŸ’¡ ä½¿ç”¨å»ºè®®

**ä½•æ—¶ä½œä¸ºé¦–é€‰**ï¼š
- âœ… éœ€è¦é«˜åº¦å¯è§£é‡Šæ€§ï¼ˆè§„åˆ™æå–ï¼‰
- âœ… æ•°æ®åŒ…å«å¤§é‡ç±»åˆ«ç‰¹å¾
- âœ… æ¢ç´¢æ€§åˆ†æï¼Œå¿«é€Ÿäº†è§£ç‰¹å¾é‡è¦æ€§
- âœ… ä½œä¸ºé›†æˆæ¨¡å‹çš„åŸºå­¦ä¹ å™¨

**ä½•æ—¶ä¸æ¨è**ï¼š
- âŒ è¿½æ±‚é«˜ç²¾åº¦ï¼ˆå•æ£µæ ‘æ˜“è¿‡æ‹Ÿåˆï¼‰
- âŒ æ•°æ®é‡å¾ˆå¤§ï¼ˆè€ƒè™‘éšæœºæ£®æ—ï¼‰
- âŒ ç‰¹å¾é—´å­˜åœ¨ç®€å•çº¿æ€§å…³ç³»ï¼ˆç”¨çº¿æ€§æ¨¡å‹æ›´å¥½ï¼‰

**æœ€ä½³å®è·µ**ï¼š
```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# 1. è®­ç»ƒæµ…å±‚æ ‘ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
model = DecisionTreeClassifier(
    max_depth=4,
    min_samples_leaf=10,
    random_state=42
)
model.fit(X_train, y_train)

# 2. å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(20, 10))
plot_tree(model,
          feature_names=feature_names,
          class_names=['No', 'Yes'],
          filled=True,
          rounded=True)
plt.show()

# 3. æå–å†³ç­–è§„åˆ™
from sklearn.tree import export_text
tree_rules = export_text(model, feature_names=feature_names)
print(tree_rules)

# 4. ç‰¹å¾é‡è¦æ€§
importances = model.feature_importances_
feature_imp = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)
```

---

## 4. éšæœºæ£®æ— (Random Forest)

### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šé›†æˆå¤šæ£µå†³ç­–æ ‘ï¼Œé€šè¿‡æŠ•ç¥¨æˆ–å¹³å‡è¾“å‡ºç»“æœ

**å…³é”®æœºåˆ¶**ï¼š
1. **Bootstrap é‡‡æ ·**ï¼šæ¯æ£µæ ‘ç”¨ä¸åŒçš„è®­ç»ƒå­é›†
2. **éšæœºç‰¹å¾é€‰æ‹©**ï¼šæ¯æ¬¡åˆ†è£‚éšæœºé€‰æ‹©ç‰¹å¾å­é›†
3. **å¤šæ•°æŠ•ç¥¨/å¹³å‡**ï¼šåˆ†ç±»ç”¨æŠ•ç¥¨ï¼Œå›å½’ç”¨å¹³å‡

---

### âœ… é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ˜¯å¦é€‚ç”¨ | è¯´æ˜ |
|------|---------|------|
| **æ•°æ®é‡** | ä¸­-å¤§ | âœ… 1000+ æ ·æœ¬æ—¶è¡¨ç°ä¼˜å¼‚ |
| **ç‰¹å¾ç»´åº¦** | ä»»æ„ | âœ… å¯å¤„ç†é«˜ç»´æ•°æ® |
| **ç‰¹å¾ç±»å‹** | æ•°å€¼+ç±»åˆ« | âœ… éƒ½èƒ½å¤„ç† |
| **éçº¿æ€§å…³ç³»** | å¤æ‚ | âœ… æ•æ‰å¤æ‚æ¨¡å¼ |
| **è¿‡æ‹Ÿåˆé£é™©** | é™ä½ | âœ… æ¯”å•æ£µæ ‘ç¨³å®šå¾—å¤š |
| **éœ€è¦æ¦‚ç‡** | æ˜¯ | âœ… è¾“å‡ºç±»åˆ«æ¦‚ç‡ |

**å…¸å‹åº”ç”¨**ï¼š
- ä¿¡ç”¨è¯„åˆ†ï¼ˆé«˜å‡†ç¡®æ€§è¦æ±‚ï¼‰
- æ¨èç³»ç»Ÿï¼ˆç‰¹å¾ä¼—å¤šï¼‰
- ç”Ÿç‰©ä¿¡æ¯å­¦ï¼ˆé«˜ç»´åŸºå› æ•°æ®ï¼‰

---

### ğŸ‘ ä¼˜ç‚¹

1. **é«˜å‡†ç¡®æ€§** - é€šå¸¸æ¯”å•æ¨¡å‹å‡†ç¡®ç‡é«˜ 5-10%
2. **é˜²æ­¢è¿‡æ‹Ÿåˆ** - é€šè¿‡é›†æˆå¤§å¹…å‡å°‘è¿‡æ‹Ÿåˆ
3. **ç¨³å®šæ€§å¼º** - å¯¹æ•°æ®æ‰°åŠ¨ä¸æ•æ„Ÿ
4. **å¤„ç†é«˜ç»´** - å¯ä»¥å¤„ç†æ•°åƒä¸ªç‰¹å¾
5. **ç‰¹å¾é‡è¦æ€§** - è‡ªåŠ¨ç»™å‡ºç‰¹å¾æ’å
6. **å¹¶è¡Œè®­ç»ƒ** - æ¯æ£µæ ‘ç‹¬ç«‹è®­ç»ƒï¼Œå¯å¹¶è¡Œ
7. **æ— éœ€è°ƒå‚** - é»˜è®¤å‚æ•°é€šå¸¸è¡¨ç°è‰¯å¥½

---

### ğŸ‘ ç¼ºç‚¹

1. **å¯è§£é‡Šæ€§å·®** - æ— æ³•åƒå•æ£µæ ‘é‚£æ ·å¯è§†åŒ–
2. **è®­ç»ƒæ…¢** - éœ€è¦è®­ç»ƒå¤§é‡æ ‘
3. **é¢„æµ‹æ…¢** - éœ€è¦éå†æ‰€æœ‰æ ‘
4. **å†…å­˜å ç”¨å¤§** - éœ€è¦å­˜å‚¨å¤šæ£µæ ‘
5. **å¯¹æç«¯ä¸å¹³è¡¡æ•°æ®æ•ˆæœä¸€èˆ¬** - éœ€è¦é…åˆ SMOTE

---

### âš™ï¸ å…³é”®å‚æ•°

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

model = RandomForestClassifier(
    n_estimators=100,          # æ ‘çš„æ•°é‡ï¼ˆè¶Šå¤šè¶Šå¥½ï¼Œä½†æ›´æ…¢ï¼‰
    max_depth=None,            # æ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦
    min_samples_split=2,       # åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬
    min_samples_leaf=1,        # å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬
    max_features='sqrt',       # æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„ç‰¹å¾æ•°
    bootstrap=True,            # æ˜¯å¦ä½¿ç”¨bootstrapé‡‡æ ·
    oob_score=False,          # æ˜¯å¦ä½¿ç”¨è¢‹å¤–æ ·æœ¬è¯„ä¼°
    n_jobs=-1,                # å¹¶è¡Œä»»åŠ¡æ•°ï¼ˆ-1=ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒï¼‰
    random_state=42
)
```

**å‚æ•°è°ƒä¼˜å»ºè®®**ï¼š

| å‚æ•° | æ¨èèŒƒå›´ | è¯´æ˜ |
|------|---------|------|
| `n_estimators` | [100, 200, 300, 500] | è¶Šå¤šè¶Šå¥½ï¼Œä½†æ”¶ç›Šé€’å‡ |
| `max_depth` | [10, 20, 30, None] | None=ä¸é™åˆ¶ï¼ˆæ¨èï¼‰ |
| `min_samples_split` | [2, 5, 10] | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| `min_samples_leaf` | [1, 2, 4] | å¶å­èŠ‚ç‚¹æ§åˆ¶ |
| `max_features` | ['sqrt', 'log2', None] | 'sqrt'å¯¹åˆ†ç±»æ•ˆæœå¥½ |

**è°ƒä¼˜ç¤ºä¾‹**ï¼š
```python
from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

rf = RandomForestClassifier(random_state=42)
random_search = RandomizedSearchCV(
    rf, param_dist, n_iter=20, cv=5,
    scoring='roc_auc', n_jobs=-1, random_state=42
)
random_search.fit(X_train, y_train)
```

---

### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† |
|---------|------|
| è®­ç»ƒé€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ (ä¸­ç­‰) |
| é¢„æµ‹é€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ (ä¸­ç­‰) |
| å†…å­˜å ç”¨ | ğŸš€ğŸš€ (è¾ƒå¤§) |
| å¯è§£é‡Šæ€§ | â­â­ (è¾ƒå¼±) |
| å‡†ç¡®æ€§ | â­â­â­â­ (å¾ˆå¥½) |
| å­¦ä¹ éš¾åº¦ | â­â­ (ç®€å•) |

---

### ğŸ’¡ ä½¿ç”¨å»ºè®®

**ä½•æ—¶ä½œä¸ºé¦–é€‰**ï¼š
- âœ… ä¸­ç­‰æ•°æ®é‡ï¼ˆ1000-100000ï¼‰
- âœ… è¿½æ±‚è¾ƒé«˜å‡†ç¡®æ€§
- âœ… ä¸éœ€è¦æè‡´å¯è§£é‡Šæ€§
- âœ… ä½œä¸ºå¼ºbaseline

**ä½•æ—¶ä¸æ¨è**ï¼š
- âŒ æ•°æ®é‡æå¤§ï¼ˆ>100ä¸‡ï¼Œç”¨LightGBMï¼‰
- âŒ éœ€è¦å®æ—¶é¢„æµ‹ï¼ˆå»¶è¿Ÿæ•æ„Ÿï¼‰
- âŒ éœ€è¦å®Œå…¨å¯è§£é‡Šï¼ˆç”¨å†³ç­–æ ‘æˆ–é€»è¾‘å›å½’ï¼‰

**æœ€ä½³å®è·µ**ï¼š
```python
from sklearn.ensemble import RandomForestClassifier

# 1. ä½¿ç”¨OOBè¯„åˆ†ï¼ˆæ— éœ€é¢å¤–éªŒè¯é›†ï¼‰
model = RandomForestClassifier(
    n_estimators=300,
    oob_score=True,    # ä½¿ç”¨è¢‹å¤–æ ·æœ¬è¯„ä¼°
    n_jobs=-1,
    random_state=42
)
model.fit(X_train, y_train)
print(f"OOB Score: {model.oob_score_:.4f}")

# 2. ç‰¹å¾é‡è¦æ€§åˆ†æ
importances = model.feature_importances_
indices = np.argsort(importances)[::-1][:20]

plt.figure(figsize=(10, 6))
plt.barh(range(len(indices)), importances[indices])
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Feature Importance')
plt.title('Top 20 Features - Random Forest')
plt.gca().invert_yaxis()
plt.show()

# 3. ç±»åˆ«ä¸å¹³è¡¡å¤„ç†
model = RandomForestClassifier(
    n_estimators=200,
    class_weight='balanced',  # è‡ªåŠ¨è°ƒæ•´æƒé‡
    random_state=42
)
```

---

## 5. æ”¯æŒå‘é‡æœº (SVM)

### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šå¯»æ‰¾æœ€ä¼˜è¶…å¹³é¢ï¼Œæœ€å¤§åŒ–ä¸åŒç±»åˆ«é—´çš„é—´éš”

**æ•°å­¦è¡¨è¾¾**ï¼šé€šè¿‡æ ¸å‡½æ•°å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œåœ¨é«˜ç»´ç©ºé—´ä¸­çº¿æ€§å¯åˆ†

**å¸¸ç”¨æ ¸å‡½æ•°**ï¼š
- **çº¿æ€§æ ¸**ï¼šé€‚åˆçº¿æ€§å¯åˆ†æ•°æ®
- **RBFæ ¸**ï¼ˆé«˜æ–¯æ ¸ï¼‰ï¼šæœ€å¸¸ç”¨ï¼Œå¯å¤„ç†éçº¿æ€§
- **å¤šé¡¹å¼æ ¸**ï¼šé€‚åˆå¤šé¡¹å¼å…³ç³»
- **Sigmoidæ ¸**ï¼šç±»ä¼¼ç¥ç»ç½‘ç»œ

---

### âœ… é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ˜¯å¦é€‚ç”¨ | è¯´æ˜ |
|------|---------|------|
| **æ•°æ®é‡** | å°-ä¸­ | âœ… <10000 æ ·æœ¬æ—¶æ•ˆæœå¥½ |
| **æ•°æ®é‡** | å¤§ | âŒ >100000 æ ·æœ¬æ—¶éå¸¸æ…¢ |
| **ç‰¹å¾ç»´åº¦** | é«˜ç»´ | âœ… æ–‡æœ¬åˆ†ç±»ç­‰é«˜ç»´é—®é¢˜è¡¨ç°ä¼˜å¼‚ |
| **éçº¿æ€§å…³ç³»** | å¤æ‚ | âœ… RBFæ ¸å¯å¤„ç†å¤æ‚éçº¿æ€§ |
| **ç±»åˆ«å¹³è¡¡** | å¹³è¡¡ | âœ… ä¸å¹³è¡¡éœ€è¦è°ƒæ•´class_weight |

**å…¸å‹åº”ç”¨**ï¼š
- æ–‡æœ¬åˆ†ç±»ï¼ˆé«˜ç»´ç¨€ç–ç‰¹å¾ï¼‰
- å›¾åƒåˆ†ç±»ï¼ˆSVM+HOGç‰¹å¾ï¼‰
- ç”Ÿç‰©ä¿¡æ¯å­¦ï¼ˆè›‹ç™½è´¨åˆ†ç±»ï¼‰

---

### ğŸ‘ ä¼˜ç‚¹

1. **é«˜ç»´ç©ºé—´è¡¨ç°å¥½** - ç‰¹å¾æ•°>æ ·æœ¬æ•°æ—¶ä»ç„¶æœ‰æ•ˆ
2. **å†…å­˜é«˜æ•ˆ** - åªä½¿ç”¨æ”¯æŒå‘é‡ï¼ˆè®­ç»ƒæ ·æœ¬çš„å­é›†ï¼‰
3. **æ ¸æŠ€å·§å¼ºå¤§** - å¯ä»¥å¤„ç†å¤æ‚éçº¿æ€§å…³ç³»
4. **ç¨³å®šæ€§å¥½** - å‡¸ä¼˜åŒ–é—®é¢˜ï¼Œå…¨å±€æœ€ä¼˜è§£
5. **é€‚åˆå°æ•°æ®** - å°æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°ä¼˜äºç¥ç»ç½‘ç»œ

---

### ğŸ‘ ç¼ºç‚¹

1. **è®­ç»ƒææ…¢** - å¤§æ•°æ®é›†è®­ç»ƒæ—¶é—´é•¿ï¼ˆO(nÂ²)åˆ°O(nÂ³)ï¼‰
2. **å¯¹å‚æ•°æ•æ„Ÿ** - C å’Œ gamma éœ€è¦ä»”ç»†è°ƒä¼˜
3. **ä¸ç›´æ¥è¾“å‡ºæ¦‚ç‡** - éœ€è¦Platt Scalingè½¬æ¢
4. **å¯¹ä¸å¹³è¡¡æ•°æ®æ•æ„Ÿ** - éœ€è¦è°ƒæ•´class_weight
5. **éš¾ä»¥è§£é‡Š** - å°¤å…¶æ˜¯ä½¿ç”¨RBFæ ¸æ—¶
6. **å†…å­˜å ç”¨å¤§** - æ ¸çŸ©é˜µå ç”¨å¤§é‡å†…å­˜

---

### âš™ï¸ å…³é”®å‚æ•°

```python
from sklearn.svm import SVC, SVR

# åˆ†ç±»
model = SVC(
    C=1.0,                    # æ­£åˆ™åŒ–å‚æ•°ï¼ˆè¶Šå¤§è¶Šå¤æ‚ï¼‰
    kernel='rbf',             # æ ¸å‡½æ•°: 'linear', 'poly', 'rbf', 'sigmoid'
    gamma='scale',            # RBFæ ¸çš„å‚æ•°ï¼ˆè¶Šå¤§è¶Šå¤æ‚ï¼‰
    class_weight=None,        # ç±»åˆ«æƒé‡
    probability=False,        # æ˜¯å¦å¯ç”¨æ¦‚ç‡ä¼°è®¡ï¼ˆä¼šå˜æ…¢ï¼‰
    random_state=42
)
```

**å…³é”®å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | æ¨èèŒƒå›´ | è¯´æ˜ |
|------|---------|------|
| `C` | [0.1, 1, 10, 100, 1000] | æ§åˆ¶è¯¯åˆ†ç±»æƒ©ç½šï¼Œè¶Šå¤§è¶Šå¤æ‚ |
| `gamma` | [0.001, 0.01, 0.1, 1, 'scale'] | RBFæ ¸å®½åº¦ï¼Œè¶Šå¤§è¶Šå¤æ‚ |
| `kernel` | 'rbf', 'linear' | é»˜è®¤ç”¨'rbf'ï¼Œçº¿æ€§é—®é¢˜ç”¨'linear' |

**è°ƒä¼˜å»ºè®®**ï¼š
```python
from sklearn.model_selection import GridSearchCV

# ç½‘æ ¼æœç´¢Cå’Œgamma
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 'scale'],
    'kernel': ['rbf']
}

grid = GridSearchCV(SVC(), param_grid, cv=5, scoring='roc_auc', n_jobs=-1)
grid.fit(X_train_scaled, y_train)  # æ³¨æ„ï¼šSVMéœ€è¦æ ‡å‡†åŒ–ï¼
```

---

### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† |
|---------|------|
| è®­ç»ƒé€Ÿåº¦ | ğŸš€ (æ…¢ï¼Œå¤§æ•°æ®ææ…¢) |
| é¢„æµ‹é€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ (å¿«) |
| å†…å­˜å ç”¨ | ğŸš€ğŸš€ (è¾ƒå¤§) |
| å¯è§£é‡Šæ€§ | â­â­ (è¾ƒå¼±) |
| å‡†ç¡®æ€§ | â­â­â­â­ (å¾ˆå¥½ï¼Œå°æ•°æ®ä¸Š) |
| å­¦ä¹ éš¾åº¦ | â­â­â­ (ä¸­ç­‰) |

---

### ğŸ’¡ ä½¿ç”¨å»ºè®®

**ä½•æ—¶ä½œä¸ºé¦–é€‰**ï¼š
- âœ… å°æ•°æ®é›†ï¼ˆ<10000æ ·æœ¬ï¼‰
- âœ… é«˜ç»´ç‰¹å¾ï¼ˆç‰¹å¾æ•°>æ ·æœ¬æ•°ï¼‰
- âœ… æ–‡æœ¬åˆ†ç±»é—®é¢˜
- âœ… éœ€è¦ç¨³å®šçš„ç»“æœ

**ä½•æ—¶ä¸æ¨è**ï¼š
- âŒ å¤§æ•°æ®é›†ï¼ˆ>100000æ ·æœ¬ï¼‰
- âŒ è®­ç»ƒæ—¶é—´æ•æ„Ÿ
- âŒ éœ€è¦æ¦‚ç‡è¾“å‡ºï¼ˆè™½ç„¶å¯ä»¥å¼€å¯ä½†ä¸å‡†ç¡®ï¼‰
- âŒ éœ€è¦é«˜åº¦å¯è§£é‡Šæ€§

**æœ€ä½³å®è·µ**ï¼š
```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# 1. å¿…é¡»æ ‡å‡†åŒ–ï¼ï¼ï¼
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(kernel='rbf', C=10, gamma=0.1))
])

# 2. çº¿æ€§æ ¸ç”¨äºçº¿æ€§é—®é¢˜ï¼ˆæ›´å¿«ï¼‰
linear_svm = SVC(kernel='linear', C=1.0)

# 3. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
model = SVC(
    kernel='rbf',
    C=10,
    class_weight='balanced',  # è‡ªåŠ¨è°ƒæ•´
    random_state=42
)

# 4. éœ€è¦æ¦‚ç‡æ—¶
model = SVC(
    kernel='rbf',
    C=10,
    probability=True,  # å¯ç”¨æ¦‚ç‡ä¼°è®¡ï¼ˆè®­ç»ƒä¼šå˜æ…¢ï¼‰
    random_state=42
)
```

---

## 6. Kè¿‘é‚» (KNN)

### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šæ ¹æ®æœ€è¿‘çš„Kä¸ªé‚»å±…è¿›è¡ŒæŠ•ç¥¨æˆ–å¹³å‡

**æ•°å­¦è¡¨è¾¾**ï¼š
- **åˆ†ç±»**ï¼šKä¸ªæœ€è¿‘é‚»ä¸­å‡ºç°æœ€å¤šçš„ç±»åˆ«
- **å›å½’**ï¼šKä¸ªæœ€è¿‘é‚»çš„å¹³å‡å€¼

**è·ç¦»åº¦é‡**ï¼šæ¬§æ°è·ç¦»ã€æ›¼å“ˆé¡¿è·ç¦»ã€é—µå¯å¤«æ–¯åŸºè·ç¦»ç­‰

---

### âœ… é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ˜¯å¦é€‚ç”¨ | è¯´æ˜ |
|------|---------|------|
| **æ•°æ®é‡** | å°-ä¸­ | âœ… <10000æ ·æœ¬æ—¶å¯ç”¨ |
| **æ•°æ®é‡** | å¤§ | âŒ é¢„æµ‹ææ…¢ |
| **ç‰¹å¾ç»´åº¦** | ä½ç»´ | âœ… <20ç»´æ•ˆæœå¥½ |
| **ç‰¹å¾ç»´åº¦** | é«˜ç»´ | âŒ ç»´åº¦ç¾éš¾ |
| **å†³ç­–è¾¹ç•Œ** | ä¸è§„åˆ™ | âœ… å¯ä»¥æ‹Ÿåˆå¤æ‚è¾¹ç•Œ |

**å…¸å‹åº”ç”¨**ï¼š
- æ¨èç³»ç»Ÿï¼ˆåŸºäºç”¨æˆ·ç›¸ä¼¼åº¦ï¼‰
- å¼‚å¸¸æ£€æµ‹ï¼ˆè·ç¦»æ‰€æœ‰ç‚¹éƒ½è¿œï¼‰
- ç¼ºå¤±å€¼å¡«å……ï¼ˆç”¨é‚»å±…å€¼å¡«å……ï¼‰

---

### ğŸ‘ ä¼˜ç‚¹

1. **ç®€å•æ˜“æ‡‚** - ç®—æ³•é€»è¾‘æå…¶ç®€å•
2. **æ— éœ€è®­ç»ƒ** - æ‡’æƒ°å­¦ä¹ ï¼Œè®­ç»ƒå°±æ˜¯å­˜å‚¨æ•°æ®
3. **è‡ªç„¶æ”¯æŒå¤šåˆ†ç±»** - æ— éœ€ç‰¹æ®Šå¤„ç†
4. **éçº¿æ€§å†³ç­–è¾¹ç•Œ** - å¯ä»¥æ‹Ÿåˆä»»æ„å½¢çŠ¶
5. **é€‚åº”æ€§å¼º** - éšç€æ•°æ®å¢åŠ è‡ªåŠ¨æ”¹è¿›

---

### ğŸ‘ ç¼ºç‚¹

1. **é¢„æµ‹ææ…¢** - éœ€è¦è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
2. **å†…å­˜å ç”¨å¤§** - éœ€è¦å­˜å‚¨æ‰€æœ‰è®­ç»ƒæ•°æ®
3. **å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿ** - å¿…é¡»æ ‡å‡†åŒ–
4. **ç»´åº¦ç¾éš¾** - é«˜ç»´ç©ºé—´è·ç¦»å¤±å»æ„ä¹‰
5. **å¯¹ä¸å¹³è¡¡æ•°æ®æ•æ„Ÿ** - å¤šæ•°ç±»ä¼šå ä¸»å¯¼
6. **å¯¹å™ªå£°æ•æ„Ÿ** - å™ªå£°ç‚¹ä¼šå½±å“é¢„æµ‹

---

### âš™ï¸ å…³é”®å‚æ•°

```python
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor

model = KNeighborsClassifier(
    n_neighbors=5,           # Kå€¼ï¼ˆé‚»å±…æ•°é‡ï¼‰
    weights='uniform',       # æƒé‡: 'uniform'æˆ–'distance'
    algorithm='auto',        # ç®—æ³•: 'auto', 'ball_tree', 'kd_tree', 'brute'
    metric='minkowski',      # è·ç¦»åº¦é‡
    p=2,                     # é—µå¯å¤«æ–¯åŸºè·ç¦»çš„på‚æ•°ï¼ˆ2=æ¬§æ°ï¼‰
    n_jobs=-1
)
```

**å‚æ•°è°ƒä¼˜å»ºè®®**ï¼š

| å‚æ•° | æ¨èèŒƒå›´ | è¯´æ˜ |
|------|---------|------|
| `n_neighbors` | [3, 5, 7, 9, 11] | å¥‡æ•°é¿å…å¹³å±€ï¼Œé€šå¸¸5-11 |
| `weights` | ['uniform', 'distance'] | 'distance'ç»™è¿‘é‚»æ›´å¤§æƒé‡ |
| `metric` | 'minkowski', 'manhattan' | æ¬§æ°è·ç¦»æœ€å¸¸ç”¨ |

**è°ƒä¼˜ç¤ºä¾‹**ï¼š
```python
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11, 15],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')
grid.fit(X_train_scaled, y_train)
```

---

### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† |
|---------|------|
| è®­ç»ƒé€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (æå¿«ï¼Œæ— éœ€è®­ç»ƒ) |
| é¢„æµ‹é€Ÿåº¦ | ğŸš€ (ææ…¢) |
| å†…å­˜å ç”¨ | ğŸš€ (å¤§) |
| å¯è§£é‡Šæ€§ | â­â­â­â­ (å®¹æ˜“ç†è§£) |
| å‡†ç¡®æ€§ | â­â­â­ (ä¸­ç­‰) |
| å­¦ä¹ éš¾åº¦ | â­ (æç®€å•) |

---

### ğŸ’¡ ä½¿ç”¨å»ºè®®

**ä½•æ—¶ä½œä¸ºé¦–é€‰**ï¼š
- âœ… å°æ•°æ®é›† + ä½ç»´ç‰¹å¾
- âœ… éœ€è¦æç®€å•çš„baseline
- âœ… å¼‚å¸¸æ£€æµ‹åœºæ™¯
- âœ… æ¨èç³»ç»Ÿï¼ˆååŒè¿‡æ»¤ï¼‰

**ä½•æ—¶ä¸æ¨è**ï¼š
- âŒ å¤§æ•°æ®é›†ï¼ˆ>10000æ ·æœ¬ï¼‰
- âŒ é«˜ç»´ç‰¹å¾ï¼ˆ>20ç»´ï¼‰
- âŒ å®æ—¶é¢„æµ‹è¦æ±‚
- âŒ ç‰¹å¾å°ºåº¦å·®å¼‚å¤§ä¸”æ— æ³•æ ‡å‡†åŒ–

**æœ€ä½³å®è·µ**ï¼š
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# 1. å¿…é¡»æ ‡å‡†åŒ–ï¼
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier(n_neighbors=5, weights='distance'))
])

# 2. ä½¿ç”¨è·ç¦»åŠ æƒï¼ˆæ¨èï¼‰
model = KNeighborsClassifier(
    n_neighbors=7,
    weights='distance',  # è¿‘çš„é‚»å±…æƒé‡æ›´å¤§
    n_jobs=-1
)

# 3. é€‰æ‹©åˆé€‚çš„K
# Kå¤ªå°ï¼šè¿‡æ‹Ÿåˆï¼Œå¯¹å™ªå£°æ•æ„Ÿ
# Kå¤ªå¤§ï¼šæ¬ æ‹Ÿåˆï¼Œå†³ç­–è¾¹ç•Œè¿‡äºå¹³æ»‘
# å»ºè®®ï¼šsqrt(n_samples) æˆ–é€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©
```

---

## 7. XGBoost

### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šæ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGBDTï¼‰çš„é«˜æ•ˆå®ç°

**å…³é”®æŠ€æœ¯**ï¼š
- äºŒé˜¶æ¢¯åº¦ä¿¡æ¯ï¼ˆç‰›é¡¿æ³•ï¼‰
- æ­£åˆ™åŒ–é¡¹ï¼ˆL1+L2ï¼‰
- åˆ—é‡‡æ ·ï¼ˆç±»ä¼¼éšæœºæ£®æ—ï¼‰
- ç¼ºå¤±å€¼è‡ªåŠ¨å¤„ç†

---

### âœ… é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ˜¯å¦é€‚ç”¨ | è¯´æ˜ |
|------|---------|------|
| **æ•°æ®é‡** | ä¸­-å¤§ | âœ… 1000-1000000 æ ·æœ¬ |
| **ç‰¹å¾ç±»å‹** | æ•°å€¼ | âœ… ä¸»è¦ç”¨äºæ•°å€¼ç‰¹å¾ |
| **ç±»åˆ«ç‰¹å¾** | éœ€ç¼–ç  | âš ï¸ éœ€è¦æ‰‹åŠ¨ç¼–ç  |
| **è¿½æ±‚ç²¾åº¦** | æ˜¯ | âœ… Kaggleç«èµ›é¦–é€‰ |
| **ä¸å¹³è¡¡æ•°æ®** | æ˜¯ | âœ… scale_pos_weightå¤„ç† |
| **ç¼ºå¤±å€¼** | æœ‰ | âœ… è‡ªåŠ¨å¤„ç†ç¼ºå¤± |

**å…¸å‹åº”ç”¨**ï¼š
- Kaggleç«èµ›ï¼ˆå‡ ä¹æ‰€æœ‰è¡¨æ ¼æ•°æ®ï¼‰
- é‡‘èé£æ§ï¼ˆé«˜ç²¾åº¦è¦æ±‚ï¼‰
- æ’åºé—®é¢˜ï¼ˆæœç´¢æ’åºã€æ¨èæ’åºï¼‰

---

### ğŸ‘ ä¼˜ç‚¹

1. **æé«˜ç²¾åº¦** - è¡¨æ ¼æ•°æ®ä¸Šé€šå¸¸æ˜¯æœ€ä¼˜ç®—æ³•
2. **é€Ÿåº¦å¿«** - ç›¸æ¯”GBDTå¿«10å€ä»¥ä¸Š
3. **é˜²æ­¢è¿‡æ‹Ÿåˆ** - å†…ç½®æ­£åˆ™åŒ–
4. **å¤„ç†ç¼ºå¤±** - è‡ªåŠ¨å­¦ä¹ ç¼ºå¤±å€¼çš„æœ€ä¼˜åˆ†è£‚æ–¹å‘
5. **å¹¶è¡ŒåŒ–** - ç‰¹å¾å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œ
6. **çµæ´»æ€§å¼º** - æ”¯æŒè‡ªå®šä¹‰æŸå¤±å‡½æ•°
7. **å†…ç½®äº¤å‰éªŒè¯** - è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥early stopping

---

### ğŸ‘ ç¼ºç‚¹

1. **å‚æ•°å¤æ‚** - éœ€è¦ä»”ç»†è°ƒå‚
2. **å¯è§£é‡Šæ€§å·®** - ç›¸æ¯”å•æ£µæ ‘
3. **å¯¹ç±»åˆ«ç‰¹å¾ä¸å‹å¥½** - éœ€è¦æ‰‹åŠ¨ç¼–ç 
4. **å†…å­˜å ç”¨å¤§** - ç›¸æ¯”LightGBM

---

### âš™ï¸ å…³é”®å‚æ•°

```python
from xgboost import XGBClassifier, XGBRegressor

model = XGBClassifier(
    # æ ‘ç»“æ„å‚æ•°
    n_estimators=100,        # æ ‘çš„æ•°é‡
    max_depth=6,             # æ ‘çš„æœ€å¤§æ·±åº¦
    learning_rate=0.3,       # å­¦ä¹ ç‡ï¼ˆetaï¼‰

    # é‡‡æ ·å‚æ•°
    subsample=1.0,           # è¡Œé‡‡æ ·æ¯”ä¾‹
    colsample_bytree=1.0,    # åˆ—é‡‡æ ·æ¯”ä¾‹

    # æ­£åˆ™åŒ–å‚æ•°
    reg_alpha=0,             # L1æ­£åˆ™åŒ–
    reg_lambda=1,            # L2æ­£åˆ™åŒ–
    gamma=0,                 # æœ€å°åˆ†è£‚æŸå¤±

    # å…¶ä»–
    scale_pos_weight=1,      # ç±»åˆ«æƒé‡ï¼ˆå¤„ç†ä¸å¹³è¡¡ï¼‰
    random_state=42,
    n_jobs=-1
)
```

**å‚æ•°è°ƒä¼˜ç­–ç•¥ï¼ˆ4æ­¥ï¼‰**ï¼š

**ç¬¬1æ­¥ï¼šè°ƒæ•´æ ‘çš„æ•°é‡å’Œæ·±åº¦**
```python
param_grid_step1 = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 5, 7, 9]
}
```

**ç¬¬2æ­¥ï¼šè°ƒæ•´å­¦ä¹ ç‡**
```python
# æ‰¾åˆ°æœ€ä¼˜n_estimatorså’Œmax_depthå
param_grid_step2 = {
    'learning_rate': [0.01, 0.05, 0.1, 0.3]
}
```

**ç¬¬3æ­¥ï¼šè°ƒæ•´é‡‡æ ·æ¯”ä¾‹**
```python
param_grid_step3 = {
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]
}
```

**ç¬¬4æ­¥ï¼šè°ƒæ•´æ­£åˆ™åŒ–**
```python
param_grid_step4 = {
    'reg_alpha': [0, 0.1, 1, 10],
    'reg_lambda': [0, 0.1, 1, 10],
    'gamma': [0, 0.1, 1, 5]
}
```

---

### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† |
|---------|------|
| è®­ç»ƒé€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ (å¿«) |
| é¢„æµ‹é€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ (å¿«) |
| å†…å­˜å ç”¨ | ğŸš€ğŸš€ (è¾ƒå¤§) |
| å¯è§£é‡Šæ€§ | â­â­ (è¾ƒå¼±) |
| å‡†ç¡®æ€§ | â­â­â­â­â­ (æé«˜) |
| å­¦ä¹ éš¾åº¦ | â­â­â­â­ (éœ€è¦ç»éªŒ) |

---

### ğŸ’¡ ä½¿ç”¨å»ºè®®

**ä½•æ—¶ä½œä¸ºé¦–é€‰**ï¼š
- âœ… è¿½æ±‚æœ€é«˜ç²¾åº¦ï¼ˆKaggleç«èµ›ï¼‰
- âœ… ä¸­ç­‰æ•°æ®é‡ï¼ˆ1000-100ä¸‡ï¼‰
- âœ… æ•°å€¼ç‰¹å¾ä¸ºä¸»
- âœ… æœ‰æ—¶é—´è°ƒå‚

**ä½•æ—¶ä¸æ¨è**ï¼š
- âŒ éœ€è¦æè‡´å¯è§£é‡Šæ€§
- âŒ æ•°æ®é‡æå¤§ï¼ˆ>1000ä¸‡ï¼Œç”¨LightGBMï¼‰
- âŒ ç‰¹å¾ä»¥ç±»åˆ«ä¸ºä¸»ï¼ˆç”¨LightGBMï¼‰

**æœ€ä½³å®è·µ**ï¼š
```python
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

# 1. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
n_pos = (y_train == 1).sum()
n_neg = (y_train == 0).sum()
scale_pos_weight = n_neg / n_pos

model = XGBClassifier(
    n_estimators=300,
    max_depth=5,
    learning_rate=0.1,
    scale_pos_weight=scale_pos_weight,
    random_state=42
)

# 2. Early Stoppingï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=10,
    verbose=False
)

# 3. ç‰¹å¾é‡è¦æ€§
import matplotlib.pyplot as plt
from xgboost import plot_importance

plot_importance(model, max_num_features=20)
plt.show()

# 4. ç½‘æ ¼æœç´¢ï¼ˆéšæœºæœç´¢æ›´å¿«ï¼‰
from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

random_search = RandomizedSearchCV(
    XGBClassifier(random_state=42),
    param_dist, n_iter=30, cv=5,
    scoring='roc_auc', n_jobs=-1
)
random_search.fit(X_train, y_train)
```

---

## 8. LightGBM

### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šåŸºäºç›´æ–¹å›¾çš„æ¢¯åº¦æå‡å†³ç­–æ ‘

**å…³é”®ä¼˜åŒ–**ï¼š
- **ç›´æ–¹å›¾ç®—æ³•** - è¿ç»­ç‰¹å¾ç¦»æ•£åŒ–ï¼Œå¤§å¹…åŠ é€Ÿ
- **å•è¾¹æ¢¯åº¦é‡‡æ ·(GOSS)** - åªä½¿ç”¨éƒ¨åˆ†æ ·æœ¬
- **äº’æ–¥ç‰¹å¾ç»‘å®š(EFB)** - å‡å°‘ç‰¹å¾æ•°
- **å¶å­ç”Ÿé•¿ç­–ç•¥** - leaf-wise è€Œé level-wise

---

### âœ… é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ˜¯å¦é€‚ç”¨ | è¯´æ˜ |
|------|---------|------|
| **æ•°æ®é‡** | å¤§ | âœ… >10ä¸‡æ ·æœ¬æ—¶ä¼˜åŠ¿æ˜æ˜¾ |
| **ç‰¹å¾ç»´åº¦** | é«˜ç»´ | âœ… å¯å¤„ç†æ•°åƒç‰¹å¾ |
| **ç±»åˆ«ç‰¹å¾** | æœ‰ | âœ… åŸç”Ÿæ”¯æŒï¼Œæ— éœ€ç¼–ç  |
| **è®­ç»ƒé€Ÿåº¦** | è¦æ±‚å¿« | âœ… æ¯”XGBoostå¿«3-10å€ |
| **å†…å­˜é™åˆ¶** | ç´§å¼  | âœ… å†…å­˜å ç”¨å° |

**å…¸å‹åº”ç”¨**ï¼š
- å¤§è§„æ¨¡æ¨èç³»ç»Ÿ
- å¹¿å‘Šç‚¹å‡»ç‡é¢„æµ‹
- å·¥ä¸šç•Œç”Ÿäº§ç¯å¢ƒ

---

### ğŸ‘ ä¼˜ç‚¹

1. **æå¿«é€Ÿåº¦** - æ¯”XGBoostå¿«3-10å€
2. **ä½å†…å­˜å ç”¨** - æ¯”XGBoostçœå†…å­˜50%+
3. **åŸç”Ÿç±»åˆ«æ”¯æŒ** - æ— éœ€æ‰‹åŠ¨ç¼–ç ç±»åˆ«ç‰¹å¾
4. **é«˜ç²¾åº¦** - ä¸XGBoostç›¸å½“ç”šè‡³æ›´å¥½
5. **å¤§æ•°æ®å‹å¥½** - 10ä¸‡+æ ·æœ¬æ—¶ä¼˜åŠ¿æ˜æ˜¾
6. **GPUåŠ é€Ÿ** - æ”¯æŒGPUè®­ç»ƒ

---

### ğŸ‘ ç¼ºç‚¹

1. **å°æ•°æ®æ˜“è¿‡æ‹Ÿåˆ** - leaf-wiseç­–ç•¥åœ¨å°æ•°æ®ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆ
2. **å‚æ•°æ•æ„Ÿ** - éœ€è¦ä»”ç»†è°ƒå‚
3. **å¯è§£é‡Šæ€§å·®** - ä¸XGBoostç±»ä¼¼

---

### âš™ï¸ å…³é”®å‚æ•°

```python
from lightgbm import LGBMClassifier, LGBMRegressor

model = LGBMClassifier(
    # æ ‘ç»“æ„å‚æ•°
    n_estimators=100,        # æ ‘çš„æ•°é‡
    num_leaves=31,           # å¶å­æ•°ï¼ˆæ¯”max_depthæ›´ç›´æ¥ï¼‰
    max_depth=-1,            # æœ€å¤§æ·±åº¦ï¼ˆ-1=ä¸é™åˆ¶ï¼‰
    learning_rate=0.1,       # å­¦ä¹ ç‡

    # é‡‡æ ·å‚æ•°
    subsample=1.0,           # è¡Œé‡‡æ ·ï¼ˆbagging_fractionï¼‰
    colsample_bytree=1.0,    # åˆ—é‡‡æ ·ï¼ˆfeature_fractionï¼‰

    # æ­£åˆ™åŒ–
    reg_alpha=0,             # L1æ­£åˆ™
    reg_lambda=0,            # L2æ­£åˆ™
    min_child_samples=20,    # å¶å­æœ€å°æ ·æœ¬æ•°

    # å…¶ä»–
    class_weight=None,       # ç±»åˆ«æƒé‡
    n_jobs=-1,
    random_state=42,
    verbose=-1               # ä¸è¾“å‡ºè®­ç»ƒæ—¥å¿—
)
```

**LightGBM vs XGBoost å‚æ•°å¯¹åº”**ï¼š

| LightGBM | XGBoost | è¯´æ˜ |
|----------|---------|------|
| `num_leaves` | (2^max_depth) | LightGBMç”¨å¶å­æ•° |
| `min_child_samples` | `min_child_weight` | å¶å­æœ€å°æ ·æœ¬ |
| `subsample` | `subsample` | ç›¸åŒ |
| `colsample_bytree` | `colsample_bytree` | ç›¸åŒ |

**å‚æ•°è°ƒä¼˜å»ºè®®**ï¼š
```python
param_grid = {
    'n_estimators': [100, 200, 300],
    'num_leaves': [31, 50, 100],         # å¶å­æ•°ï¼ˆé‡è¦ï¼ï¼‰
    'learning_rate': [0.01, 0.05, 0.1],
    'min_child_samples': [20, 50, 100],  # é˜²æ­¢è¿‡æ‹Ÿåˆ
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}
```

---

### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† |
|---------|------|
| è®­ç»ƒé€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (æå¿«) |
| é¢„æµ‹é€Ÿåº¦ | ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ (æå¿«) |
| å†…å­˜å ç”¨ | ğŸš€ğŸš€ğŸš€ğŸš€ (å°) |
| å¯è§£é‡Šæ€§ | â­â­ (è¾ƒå¼±) |
| å‡†ç¡®æ€§ | â­â­â­â­â­ (æé«˜) |
| å­¦ä¹ éš¾åº¦ | â­â­â­â­ (éœ€è¦ç»éªŒ) |

---

### ğŸ’¡ ä½¿ç”¨å»ºè®®

**ä½•æ—¶ä½œä¸ºé¦–é€‰**ï¼š
- âœ… å¤§æ•°æ®é›†ï¼ˆ>10ä¸‡æ ·æœ¬ï¼‰
- âœ… é«˜ç»´ç‰¹å¾ï¼ˆ>100ç»´ï¼‰
- âœ… æœ‰ç±»åˆ«ç‰¹å¾
- âœ… è®­ç»ƒæ—¶é—´æ•æ„Ÿ
- âœ… å†…å­˜å—é™

**ä½•æ—¶ä¸æ¨è**ï¼š
- âŒ å°æ•°æ®é›†ï¼ˆ<1ä¸‡æ ·æœ¬ï¼Œæ˜“è¿‡æ‹Ÿåˆï¼‰
- âŒ éœ€è¦æè‡´å¯è§£é‡Šæ€§

**æœ€ä½³å®è·µ**ï¼š
```python
from lightgbm import LGBMClassifier

# 1. åŸç”Ÿå¤„ç†ç±»åˆ«ç‰¹å¾ï¼ˆæ— éœ€ç¼–ç ï¼ï¼‰
categorical_features = ['Contract', 'PaymentMethod', 'InternetService']

model = LGBMClassifier(
    n_estimators=300,
    num_leaves=31,
    learning_rate=0.05,
    min_child_samples=20,
    random_state=42,
    verbose=-1
)

model.fit(
    X_train, y_train,
    categorical_feature=categorical_features  # æŒ‡å®šç±»åˆ«ç‰¹å¾
)

# 2. Early Stopping
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,
    verbose=False
)

# 3. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
model = LGBMClassifier(
    n_estimators=200,
    num_leaves=50,
    class_weight='balanced',  # è‡ªåŠ¨è°ƒæ•´
    random_state=42,
    verbose=-1
)

# 4. é˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆå°æ•°æ®é›†ï¼‰
model = LGBMClassifier(
    n_estimators=100,
    num_leaves=15,            # å‡å°‘å¶å­æ•°
    min_child_samples=50,     # å¢åŠ å¶å­æœ€å°æ ·æœ¬
    subsample=0.8,            # è¡Œé‡‡æ ·
    colsample_bytree=0.8,     # åˆ—é‡‡æ ·
    random_state=42
)
```

---

## ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šæ— ç›‘ç£å­¦ä¹ ç®—æ³•

æ— ç›‘ç£å­¦ä¹ ä¸éœ€è¦æ ‡ç­¾ï¼Œç”¨äºå‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼å’Œç»“æ„ã€‚ä¸»è¦åŒ…æ‹¬ä¸‰å¤§ç±»ï¼š
- **èšç±»ç®—æ³•**ï¼šå°†ç›¸ä¼¼æ ·æœ¬åˆ†ç»„ï¼ˆK-Meansã€DBSCANã€å±‚æ¬¡èšç±»ã€GMMï¼‰
- **é™ç»´ç®—æ³•**ï¼šå‹ç¼©ç‰¹å¾ç»´åº¦ï¼ˆPCAã€t-SNEï¼‰
- **å¼‚å¸¸æ£€æµ‹**ï¼šè¯†åˆ«å¼‚å¸¸æ ·æœ¬ï¼ˆIsolation Forestã€One-Class SVMï¼‰

---

### 9. K-Means èšç±»

#### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ•°æ®åˆ’åˆ†ä¸ºKä¸ªç°‡ï¼Œä½¿ç°‡å†…æ ·æœ¬å°½å¯èƒ½ç›¸ä¼¼ï¼Œç°‡é—´æ ·æœ¬å°½å¯èƒ½ä¸åŒã€‚

**æ•°å­¦è¡¨è¾¾**ï¼š
- ç›®æ ‡å‡½æ•°ï¼šæœ€å°åŒ–ç°‡å†…å¹³æ–¹å’Œ(WCSS)
  ```
  J = Î£(i=1â†’K) Î£(xâˆˆCi) ||x - Î¼i||Â²
  ```
  å…¶ä¸­ Î¼i æ˜¯ç¬¬iä¸ªç°‡çš„ä¸­å¿ƒç‚¹

**ç®—æ³•æµç¨‹**ï¼š
1. éšæœºåˆå§‹åŒ–Kä¸ªä¸­å¿ƒç‚¹
2. å°†æ¯ä¸ªæ ·æœ¬åˆ†é…åˆ°æœ€è¿‘çš„ä¸­å¿ƒç‚¹
3. é‡æ–°è®¡ç®—æ¯ä¸ªç°‡çš„ä¸­å¿ƒç‚¹ï¼ˆå‡å€¼ï¼‰
4. é‡å¤æ­¥éª¤2-3ç›´åˆ°æ”¶æ•›

#### âœ… é€‚ç”¨åœºæ™¯

| æ•°æ®ç‰¹å¾ | é€‚ç”¨æ€§ | è¯´æ˜ |
|---------|-------|------|
| **çƒå½¢åˆ†å¸ƒçš„ç°‡** | âœ… æœ€ä½³ | K-Meanså‡è®¾ç°‡æ˜¯å‡¸å½¢ä¸”å„å‘åŒæ€§çš„ |
| **ç°‡å¤§å°ç›¸è¿‘** | âœ… æ¨è | å¯¹å¤§å°å·®å¼‚å¾ˆå¤§çš„ç°‡æ•ˆæœä¸ä½³ |
| **ä¸­å°è§„æ¨¡æ•°æ®** | âœ… æ¨è | æ—¶é—´å¤æ‚åº¦O(nkt)ï¼Œkæ˜¯ç°‡æ•°ï¼Œtæ˜¯è¿­ä»£æ¬¡æ•° |
| **æ•°å€¼å‹ç‰¹å¾** | âœ… å¿…é¡» | éœ€è¦è®¡ç®—æ¬§æ°è·ç¦» |
| **æ˜ç¡®çš„ç°‡æ•°é‡** | âš ï¸ éœ€è¦ | å¿…é¡»é¢„å…ˆæŒ‡å®šKå€¼ |
| **å¯†åº¦ä¸å‡çš„ç°‡** | âŒ ä¸ä½³ | å¯èƒ½å°†å¯†é›†åŒºåŸŸåˆ†æˆå¤šä¸ªç°‡ |
| **ä»»æ„å½¢çŠ¶çš„ç°‡** | âŒ ä¸ä½³ | æ— æ³•è¯†åˆ«æœˆç‰™å½¢ã€ç¯å½¢ç­‰å¤æ‚å½¢çŠ¶ |
| **å™ªå£°æ•°æ®** | âŒ æ•æ„Ÿ | å¼‚å¸¸å€¼ä¼šä¸¥é‡å½±å“èšç±»ä¸­å¿ƒ |

**å…¸å‹åº”ç”¨**ï¼š
- å®¢æˆ·åˆ†ç¾¤/å¸‚åœºç»†åˆ†
- å›¾åƒå‹ç¼©ï¼ˆé¢œè‰²é‡åŒ–ï¼‰
- æ–‡æ¡£èšç±»
- æ¨èç³»ç»Ÿï¼ˆç”¨æˆ·/ç‰©å“èšç±»ï¼‰

#### ğŸ‘ ä¼˜ç‚¹

1. **ç®€å•æ˜“æ‡‚**ï¼šç®—æ³•é€»è¾‘ç›´è§‚ï¼Œå®¹æ˜“è§£é‡Šç»™éæŠ€æœ¯äººå‘˜
2. **é€Ÿåº¦å¿«**ï¼šçº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®
3. **å¯æ‰©å±•æ€§å¥½**ï¼šMini-Batch K-Meanså¯å¤„ç†æµ·é‡æ•°æ®
4. **æ”¶æ•›ä¿è¯**ï¼šä¿è¯æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£
5. **å†…å­˜å ç”¨å°**ï¼šåªéœ€å­˜å‚¨Kä¸ªèšç±»ä¸­å¿ƒ

#### ğŸ‘ ç¼ºç‚¹

1. **éœ€è¦æŒ‡å®šKå€¼**ï¼šèšç±»æ•°é‡éœ€è¦é¢„å…ˆç¡®å®šï¼ˆå¯ç”¨è‚˜éƒ¨æ³•åˆ™/è½®å»“ç³»æ•°è¾…åŠ©ï¼‰
2. **å¯¹åˆå§‹å€¼æ•æ„Ÿ**ï¼šä¸åŒçš„åˆå§‹ä¸­å¿ƒå¯èƒ½å¯¼è‡´ä¸åŒç»“æœï¼ˆè§£å†³ï¼šK-Means++åˆå§‹åŒ–ï¼‰
3. **åªèƒ½è¯†åˆ«å‡¸å½¢ç°‡**ï¼šæ— æ³•å¤„ç†æœˆç‰™å½¢ã€ç¯å½¢ç­‰å¤æ‚å½¢çŠ¶
4. **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ**ï¼šæç«¯å€¼ä¼šæ‹‰åèšç±»ä¸­å¿ƒ
5. **å‡è®¾ç°‡å¤§å°ç›¸ä¼¼**ï¼šå¯¹ä¸å¹³è¡¡ç°‡æ•ˆæœä¸ä½³
6. **ä»…é€‚ç”¨äºæ•°å€¼æ•°æ®**ï¼šåˆ†ç±»ç‰¹å¾éœ€è¦é¢„å¤„ç†

#### âš™ï¸ å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ | è°ƒä¼˜å»ºè®® |
|------|------|--------|----------|
| `n_clusters` | èšç±»æ•°é‡K | 8 | **æœ€å…³é”®å‚æ•°**<br>ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™æˆ–è½®å»“ç³»æ•°é€‰æ‹© |
| `init` | åˆå§‹åŒ–æ–¹æ³• | 'k-means++' | **æ¨èé»˜è®¤å€¼**<br>'k-means++': æ™ºèƒ½åˆå§‹åŒ–ï¼Œæ•ˆæœæ›´å¥½<br>'random': éšæœºåˆå§‹åŒ–ï¼Œå¯èƒ½æ•ˆæœå·® |
| `n_init` | è¿è¡Œæ¬¡æ•° | 10 | å¤šæ¬¡è¿è¡Œå–æœ€ä¼˜ï¼Œé»˜è®¤å³å¯ |
| `max_iter` | æœ€å¤§è¿­ä»£æ¬¡æ•° | 300 | é€šå¸¸è¶³å¤Ÿï¼Œå¾ˆå°‘éœ€è¦è°ƒæ•´ |
| `random_state` | éšæœºç§å­ | None | **å¿…é¡»è®¾ç½®**ä»¥ä¿è¯å¯å¤ç°æ€§ |

**å‚æ•°è°ƒä¼˜ç¤ºä¾‹**ï¼š

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import numpy as np

# 1. ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä½³Kå€¼
wcss = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)  # ç°‡å†…å¹³æ–¹å’Œ
    silhouette_scores.append(silhouette_score(X, kmeans.labels_))

# ç»˜åˆ¶è‚˜éƒ¨æ›²çº¿
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
ax1.plot(K_range, wcss, 'bo-')
ax1.set_xlabel('èšç±»æ•°é‡ K')
ax1.set_ylabel('ç°‡å†…å¹³æ–¹å’Œ (WCSS)')
ax1.set_title('è‚˜éƒ¨æ³•åˆ™')
ax1.grid(True)

# ç»˜åˆ¶è½®å»“ç³»æ•°
ax2.plot(K_range, silhouette_scores, 'ro-')
ax2.set_xlabel('èšç±»æ•°é‡ K')
ax2.set_ylabel('è½®å»“ç³»æ•°')
ax2.set_title('è½®å»“ç³»æ•°åˆ†æ')
ax2.grid(True)
plt.show()

# 2. ä½¿ç”¨æœ€ä½³Kå€¼å»ºç«‹æœ€ç»ˆæ¨¡å‹
best_k = 4  # æ ¹æ®ä¸Šè¿°å›¾è¡¨ç¡®å®š
kmeans = KMeans(
    n_clusters=best_k,
    init='k-means++',  # æ™ºèƒ½åˆå§‹åŒ–
    n_init=10,         # è¿è¡Œ10æ¬¡å–æœ€ä¼˜
    max_iter=300,
    random_state=42
)
clusters = kmeans.fit_predict(X)

print(f"è½®å»“ç³»æ•°: {silhouette_score(X, clusters):.3f}")
print(f"ç°‡å†…å¹³æ–¹å’Œ: {kmeans.inertia_:.2f}")
```

#### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|---------|------|------|
| **è®­ç»ƒé€Ÿåº¦** | â­â­â­â­â­ | éå¸¸å¿«ï¼Œçº¿æ€§æ—¶é—´å¤æ‚åº¦ |
| **é¢„æµ‹é€Ÿåº¦** | â­â­â­â­â­ | åªéœ€è®¡ç®—åˆ°å„ä¸­å¿ƒçš„è·ç¦» |
| **å†…å­˜å ç”¨** | â­â­â­â­â­ | ä»…å­˜å‚¨Kä¸ªä¸­å¿ƒç‚¹ |
| **å¤„ç†å¤§æ•°æ®** | â­â­â­â­ | Mini-Batchç‰ˆæœ¬å¯å¤„ç†æµ·é‡æ•°æ® |
| **å¤æ‚å½¢çŠ¶è¯†åˆ«** | â­â­ | ä»…é€‚ç”¨äºå‡¸å½¢ç°‡ |
| **å™ªå£°é²æ£’æ€§** | â­â­ | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ |
| **å‚æ•°è°ƒä¼˜éš¾åº¦** | â­â­â­ | ä¸»è¦éœ€è¦ç¡®å®šKå€¼ |
| **å¯è§£é‡Šæ€§** | â­â­â­â­ | èšç±»ä¸­å¿ƒå¯ç›´è§‚è§£é‡Š |

#### ğŸ’¡ ä½¿ç”¨å»ºè®®

**1. æ•°æ®é¢„å¤„ç†æ˜¯å…³é”®**

```python
from sklearn.preprocessing import StandardScaler

# âš ï¸ é”™è¯¯ç¤ºä¾‹ï¼šä¸æ ‡å‡†åŒ–ç›´æ¥èšç±»
# é—®é¢˜ï¼šå°ºåº¦å¤§çš„ç‰¹å¾ä¼šä¸»å¯¼è·ç¦»è®¡ç®—
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X_raw)  # X_rawåŒ…å«[å¹´é¾„: 20-60, æ”¶å…¥: 20000-100000]

# âœ… æ­£ç¡®ç¤ºä¾‹ï¼šå…ˆæ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_scaled)
```

**2. ä½¿ç”¨Mini-Batch K-Meanså¤„ç†å¤§æ•°æ®**

```python
from sklearn.cluster import MiniBatchKMeans

# æ ‡å‡†K-Meansï¼šå†…å­˜å ç”¨å¤§ï¼Œé€Ÿåº¦æ…¢
# kmeans = KMeans(n_clusters=100).fit(X_large)  # 100ä¸‡æ ·æœ¬

# Mini-Batch K-Meansï¼šé€Ÿåº¦å¿«10-100å€
kmeans = MiniBatchKMeans(
    n_clusters=100,
    batch_size=1000,     # æ¯æ‰¹å¤„ç†1000ä¸ªæ ·æœ¬
    random_state=42
)
clusters = kmeans.fit_predict(X_large)
```

**3. ç»“åˆä¸šåŠ¡çŸ¥è¯†è§£é‡Šèšç±»ç»“æœ**

```python
# è·å–èšç±»ä¸­å¿ƒ
centers = kmeans.cluster_centers_
centers_original = scaler.inverse_transform(centers)  # è½¬å›åŸå§‹å°ºåº¦

# åˆ†ææ¯ä¸ªç°‡çš„ç‰¹å¾
import pandas as pd
df['cluster'] = clusters
for i in range(kmeans.n_clusters):
    print(f"\nç°‡ {i} çš„ç‰¹å¾:")
    print(df[df['cluster'] == i].describe())

# ç»™èšç±»å‘½åï¼ˆåŸºäºä¸šåŠ¡ç†è§£ï¼‰
cluster_names = {
    0: "é«˜ä»·å€¼å®¢æˆ·",
    1: "æ½œåŠ›å®¢æˆ·",
    2: "ä½æ´»è·ƒå®¢æˆ·"
}
df['segment'] = df['cluster'].map(cluster_names)
```

**4. ç¨³å®šæ€§æ£€éªŒ**

```python
# å¤šæ¬¡è¿è¡Œæ£€éªŒç¨³å®šæ€§
from sklearn.metrics import adjusted_rand_score

results = []
for seed in range(10):
    km = KMeans(n_clusters=3, random_state=seed)
    results.append(km.fit_predict(X))

# è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„ARIï¼ˆAdjusted Rand Indexï¼‰
ari_scores = [adjusted_rand_score(results[0], results[i]) for i in range(1, 10)]
print(f"å¹³å‡ARI: {np.mean(ari_scores):.3f}")  # æ¥è¿‘1è¡¨ç¤ºç¨³å®š
```

---

### 10. DBSCAN

#### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šåŸºäºå¯†åº¦çš„èšç±»ï¼Œå°†é«˜å¯†åº¦åŒºåŸŸåˆ’åˆ†ä¸ºç°‡ï¼Œä½å¯†åº¦åŒºåŸŸæ ‡è®°ä¸ºå™ªå£°ç‚¹ã€‚

**å…³é”®æ¦‚å¿µ**ï¼š
- **æ ¸å¿ƒç‚¹**ï¼šÎµé‚»åŸŸå†…è‡³å°‘æœ‰MinPtsä¸ªç‚¹
- **è¾¹ç•Œç‚¹**ï¼šä¸æ˜¯æ ¸å¿ƒç‚¹ï¼Œä½†åœ¨æŸæ ¸å¿ƒç‚¹çš„Îµé‚»åŸŸå†…
- **å™ªå£°ç‚¹**ï¼šæ—¢ä¸æ˜¯æ ¸å¿ƒç‚¹ä¹Ÿä¸æ˜¯è¾¹ç•Œç‚¹

**ç®—æ³•æµç¨‹**ï¼š
1. æ ‡è®°æ‰€æœ‰æ ¸å¿ƒç‚¹
2. ä»ä»»ä¸€æ ¸å¿ƒç‚¹å¼€å§‹ï¼Œè¿æ¥å…¶Îµé‚»åŸŸå†…çš„æ‰€æœ‰æ ¸å¿ƒç‚¹å½¢æˆç°‡
3. å°†è¾¹ç•Œç‚¹åˆ†é…åˆ°å¯¹åº”çš„ç°‡
4. å‰©ä½™ç‚¹æ ‡è®°ä¸ºå™ªå£°ï¼ˆlabel=-1ï¼‰

#### âœ… é€‚ç”¨åœºæ™¯

| æ•°æ®ç‰¹å¾ | é€‚ç”¨æ€§ | è¯´æ˜ |
|---------|-------|------|
| **ä»»æ„å½¢çŠ¶çš„ç°‡** | âœ… æœ€ä½³ | å¯è¯†åˆ«æœˆç‰™å½¢ã€ç¯å½¢ç­‰å¤æ‚å½¢çŠ¶ |
| **ä¸åŒå¯†åº¦çš„ç°‡** | âš ï¸ ä¸€èˆ¬ | å•ä¸€Îµå’ŒMinPtséš¾ä»¥åŒæ—¶é€‚åº”æ‰€æœ‰å¯†åº¦ |
| **å«å™ªå£°çš„æ•°æ®** | âœ… ä¼˜ç§€ | è‡ªåŠ¨è¯†åˆ«å¹¶æ ‡è®°å™ªå£°ç‚¹ |
| **ç°‡æ•°æœªçŸ¥** | âœ… æ¨è | æ— éœ€é¢„å…ˆæŒ‡å®šç°‡æ•°é‡ |
| **åœ°ç†ç©ºé—´æ•°æ®** | âœ… æ¨è | å¯å‘ç°åœ°ç†çƒ­ç‚¹åŒºåŸŸ |
| **é«˜ç»´æ•°æ®** | âŒ ä¸ä½³ | è·ç¦»æ¦‚å¿µåœ¨é«˜ç»´ç©ºé—´å¤±æ•ˆ |
| **å¤§è§„æ¨¡æ•°æ®** | âš ï¸ è°¨æ… | æ—¶é—´å¤æ‚åº¦O(nÂ²)æˆ–O(n log n) |

**å…¸å‹åº”ç”¨**ï¼š
- å¼‚å¸¸æ£€æµ‹ï¼ˆè¯†åˆ«å™ªå£°ç‚¹ï¼‰
- åœ°ç†ä½ç½®èšç±»ï¼ˆåŸå¸‚çƒ­ç‚¹åŒºåŸŸï¼‰
- å®¢æˆ·è¡Œä¸ºåˆ†ç¾¤ï¼ˆè¯†åˆ«ç‰¹æ®Šè¡Œä¸ºæ¨¡å¼ï¼‰
- å›¾åƒåˆ†å‰²

#### ğŸ‘ ä¼˜ç‚¹

1. **æ— éœ€æŒ‡å®šç°‡æ•°**ï¼šè‡ªåŠ¨ç¡®å®šç°‡çš„æ•°é‡
2. **è¯†åˆ«ä»»æ„å½¢çŠ¶**ï¼šä¸å±€é™äºå‡¸å½¢ç°‡
3. **é²æ£’æ€§å¼º**ï¼šèƒ½è¯†åˆ«å¹¶è¿‡æ»¤å™ªå£°ç‚¹
4. **ç°‡å¤§å°å¯å˜**ï¼šä¸å‡è®¾ç°‡å¤§å°ç›¸ä¼¼
5. **ç›´è§‚çš„å‚æ•°**ï¼šÎµå’ŒMinPtsæœ‰æ˜ç¡®çš„ç‰©ç†æ„ä¹‰

#### ğŸ‘ ç¼ºç‚¹

1. **å‚æ•°æ•æ„Ÿ**ï¼šÎµå’ŒMinPtséœ€è¦ä»”ç»†è°ƒä¼˜
2. **å¯†åº¦ä¸å‡éš¾å¤„ç†**ï¼šå•ä¸€Îµéš¾ä»¥é€‚åº”å¯†åº¦å˜åŒ–å¤§çš„æ•°æ®
3. **é«˜ç»´æ€§èƒ½å·®**ï¼šè·ç¦»åº¦é‡åœ¨é«˜ç»´ç©ºé—´å¤±æ•ˆï¼ˆç»´åº¦ç¾éš¾ï¼‰
4. **è®¡ç®—æˆæœ¬é«˜**ï¼šO(nÂ²)å¤æ‚åº¦ï¼Œå¤§æ•°æ®é›†æ…¢
5. **å†…å­˜å ç”¨å¤§**ï¼šéœ€è¦è®¡ç®—è·ç¦»çŸ©é˜µ
6. **è¾¹ç•Œç‚¹ä¸ç¡®å®š**ï¼šè¾¹ç•Œç‚¹å¯èƒ½è¢«åˆ†é…åˆ°ä¸åŒçš„ç°‡

#### âš™ï¸ å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ | è°ƒä¼˜å»ºè®® |
|------|------|--------|----------|
| `eps` | é‚»åŸŸåŠå¾„Îµ | 0.5 | **æœ€å…³é”®å‚æ•°**<br>ä½¿ç”¨k-è·ç¦»å›¾ç¡®å®š |
| `min_samples` | æ ¸å¿ƒç‚¹æœ€å°é‚»åŸŸæ ·æœ¬æ•° | 5 | é€šå¸¸è®¾ä¸º2Ã—ç‰¹å¾æ•° |
| `metric` | è·ç¦»åº¦é‡ | 'euclidean' | æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹© |

**å‚æ•°è°ƒä¼˜ç¤ºä¾‹**ï¼š

```python
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import numpy as np

# 1. ä½¿ç”¨k-è·ç¦»å›¾ç¡®å®šeps
# è®¡ç®—æ¯ä¸ªç‚¹åˆ°ç¬¬kè¿‘é‚»çš„è·ç¦»
k = 4  # é€šå¸¸è®¾ä¸ºmin_samples - 1
nbrs = NearestNeighbors(n_neighbors=k).fit(X)
distances, indices = nbrs.kneighbors(X)

# å¯¹k-è·ç¦»æ’åºå¹¶ç»˜å›¾
distances_sorted = np.sort(distances[:, -1])
plt.figure(figsize=(10, 6))
plt.plot(distances_sorted)
plt.xlabel('æ ·æœ¬ç´¢å¼•ï¼ˆæŒ‰è·ç¦»æ’åºï¼‰')
plt.ylabel(f'{k}-è·ç¦»')
plt.title('K-è·ç¦»å›¾ï¼ˆå¯»æ‰¾è‚˜éƒ¨ç¡®å®šepsï¼‰')
plt.grid(True)
plt.show()

# 2. ä»å›¾ä¸­æ‰¾åˆ°è‚˜éƒ¨ç‚¹ï¼Œç¡®å®šeps
eps_optimal = 0.3  # æ ¹æ®ä¸Šå›¾ç¡®å®š

# 3. å»ºç«‹æ¨¡å‹
dbscan = DBSCAN(
    eps=eps_optimal,
    min_samples=4,     # 2Dæ•°æ®: 2Ã—2=4
    metric='euclidean',
    n_jobs=-1          # å¹¶è¡Œè®¡ç®—
)
clusters = dbscan.fit_predict(X)

# 4. åˆ†æç»“æœ
n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
n_noise = list(clusters).count(-1)

print(f"è¯†åˆ«çš„ç°‡æ•°: {n_clusters}")
print(f"å™ªå£°ç‚¹æ•°: {n_noise} ({n_noise/len(X)*100:.1f}%)")
```

#### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|---------|------|------|
| **è®­ç»ƒé€Ÿåº¦** | â­â­ | O(n log n)åˆ°O(nÂ²)ï¼Œè¾ƒæ…¢ |
| **é¢„æµ‹é€Ÿåº¦** | â­â­â­ | éœ€è¦æŸ¥è¯¢æœ€è¿‘é‚» |
| **å†…å­˜å ç”¨** | â­â­ | éœ€è¦å­˜å‚¨è·ç¦»ä¿¡æ¯ |
| **ä»»æ„å½¢çŠ¶è¯†åˆ«** | â­â­â­â­â­ | æœ€å¤§ä¼˜åŠ¿ |
| **å™ªå£°é²æ£’æ€§** | â­â­â­â­â­ | è‡ªåŠ¨è¯†åˆ«å™ªå£° |
| **æ— éœ€æŒ‡å®šç°‡æ•°** | â­â­â­â­â­ | è‡ªåŠ¨ç¡®å®š |
| **å‚æ•°è°ƒä¼˜éš¾åº¦** | â­â­â­â­ | epså’Œmin_sampleséœ€è¦ä»”ç»†è°ƒä¼˜ |
| **é«˜ç»´é€‚ç”¨æ€§** | â­â­ | é«˜ç»´ç©ºé—´æ•ˆæœå·® |

#### ğŸ’¡ ä½¿ç”¨å»ºè®®

**1. å…ˆæ ‡å‡†åŒ–æ•°æ®**

```python
from sklearn.preprocessing import StandardScaler

# âš ï¸ é”™è¯¯ï¼šä¸åŒå°ºåº¦çš„ç‰¹å¾ä¼šå½±å“è·ç¦»è®¡ç®—
dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X_raw)

# âœ… æ­£ç¡®ï¼šæ ‡å‡†åŒ–åå†èšç±»
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)
dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)
```

**2. å™ªå£°ç‚¹åˆ†æ**

```python
# æå–å™ªå£°ç‚¹
noise_mask = (clusters == -1)
noise_points = X[noise_mask]
normal_points = X[~noise_mask]

print(f"å™ªå£°ç‚¹æ¯”ä¾‹: {noise_mask.sum() / len(X) * 100:.1f}%")

# å¯è§†åŒ–å™ªå£°ç‚¹
plt.scatter(normal_points[:, 0], normal_points[:, 1], c=clusters[~noise_mask], cmap='viridis', s=50)
plt.scatter(noise_points[:, 0], noise_points[:, 1], c='red', marker='x', s=100, label='å™ªå£°ç‚¹')
plt.legend()
plt.title('DBSCANèšç±»ç»“æœï¼ˆçº¢å‰ä¸ºå™ªå£°ç‚¹ï¼‰')
plt.show()
```

**3. å¤„ç†å¯†åº¦ä¸å‡çš„æ•°æ®ï¼šHDBSCAN**

```python
# å¯¹äºå¯†åº¦å˜åŒ–å¤§çš„æ•°æ®ï¼Œä½¿ç”¨HDBSCANï¼ˆå±‚æ¬¡DBSCANï¼‰
# pip install hdbscan
import hdbscan

clusterer = hdbscan.HDBSCAN(
    min_cluster_size=5,      # æœ€å°ç°‡å¤§å°
    min_samples=3,           # æ ¸å¿ƒç‚¹æœ€å°é‚»åŸŸæ•°
    cluster_selection_epsilon=0.0
)
clusters = clusterer.fit_predict(X_scaled)
```

**4. åœ°ç†ç©ºé—´èšç±»åº”ç”¨**

```python
# ä½¿ç”¨haversineè·ç¦»å¤„ç†ç»çº¬åº¦æ•°æ®
from sklearn.cluster import DBSCAN
import numpy as np

# ç»çº¬åº¦æ•°æ® (latitude, longitude)
coords = np.array([[39.9, 116.4], [39.91, 116.41], [31.2, 121.5]])

# ä½¿ç”¨haversineè·ç¦»ï¼ˆå•ä½ï¼šå¼§åº¦ï¼‰
# eps=0.01 çº¦ç­‰äº1.11å…¬é‡Œ
dbscan = DBSCAN(
    eps=0.01,          # å¼§åº¦å•ä½
    min_samples=2,
    metric='haversine'
)

# å°†ç»çº¬åº¦è½¬æ¢ä¸ºå¼§åº¦
coords_rad = np.radians(coords)
clusters = dbscan.fit_predict(coords_rad)
```

---

### 11. å±‚æ¬¡èšç±» (Hierarchical Clustering)

#### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šæ„å»ºèšç±»æ ‘ï¼ˆæ ‘çŠ¶å›¾/dendrogramï¼‰ï¼Œé€šè¿‡åˆ‡å‰²æ ‘å¾—åˆ°ä¸åŒå±‚æ¬¡çš„èšç±»ç»“æœã€‚

**ä¸¤ç§ç­–ç•¥**ï¼š
1. **å‡èšå¼ï¼ˆAgglomerativeï¼‰**ï¼šè‡ªåº•å‘ä¸Š
   - åˆå§‹ï¼šæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªç°‡
   - è¿­ä»£ï¼šåˆå¹¶æœ€ç›¸ä¼¼çš„ä¸¤ä¸ªç°‡
   - ç»ˆæ­¢ï¼šæ‰€æœ‰æ ·æœ¬åˆå¹¶ä¸ºä¸€ä¸ªç°‡

2. **åˆ†è£‚å¼ï¼ˆDivisiveï¼‰**ï¼šè‡ªé¡¶å‘ä¸‹ï¼ˆsklearnæœªå®ç°ï¼‰

**é“¾æ¥æ–¹å¼ï¼ˆLinkageï¼‰**ï¼š
- **Singleï¼ˆå•é“¾ï¼‰**ï¼šç°‡é—´æœ€å°è·ç¦»
- **Completeï¼ˆå…¨é“¾ï¼‰**ï¼šç°‡é—´æœ€å¤§è·ç¦»
- **Averageï¼ˆå‡é“¾ï¼‰**ï¼šç°‡é—´å¹³å‡è·ç¦»
- **Ward**ï¼šæœ€å°åŒ–ç°‡å†…æ–¹å·®å¢é‡

#### âœ… é€‚ç”¨åœºæ™¯

| æ•°æ®ç‰¹å¾ | é€‚ç”¨æ€§ | è¯´æ˜ |
|---------|-------|------|
| **å°è§„æ¨¡æ•°æ®** | âœ… æ¨è | æ—¶é—´å¤æ‚åº¦O(nÂ²log n)åˆ°O(nÂ³) |
| **éœ€è¦å±‚æ¬¡ç»“æ„** | âœ… æœ€ä½³ | æ ‘çŠ¶å›¾å±•ç¤ºå±‚æ¬¡å…³ç³» |
| **ç°‡æ•°æœªçŸ¥** | âœ… æ¨è | å¯ä»æ ‘çŠ¶å›¾é€‰æ‹©æœ€ä½³åˆ‡å‰²ç‚¹ |
| **æ¢ç´¢æ€§åˆ†æ** | âœ… æ¨è | æ ‘çŠ¶å›¾æä¾›ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ |
| **ç”Ÿç‰©ä¿¡æ¯å­¦** | âœ… å¸¸ç”¨ | åŸºå› èšç±»ã€ç‰©ç§åˆ†ç±» |
| **å¤§è§„æ¨¡æ•°æ®** | âŒ ä¸æ¨è | æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦å¤ªé«˜ |
| **åœ¨çº¿å­¦ä¹ ** | âŒ ä¸æ”¯æŒ | å¿…é¡»ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ•°æ® |

**å…¸å‹åº”ç”¨**ï¼š
- åŸºå› è¡¨è¾¾æ•°æ®èšç±»
- æ–‡æ¡£å±‚æ¬¡åˆ†ç±»
- å®¢æˆ·åˆ†å±‚ï¼ˆVIPâ†’æ™®é€šâ†’æ½œåœ¨ï¼‰
- åœ°ç†åŒºåŸŸå±‚çº§åˆ’åˆ†

#### ğŸ‘ ä¼˜ç‚¹

1. **æ— éœ€æŒ‡å®šç°‡æ•°**ï¼šå¯ä»æ ‘çŠ¶å›¾çµæ´»é€‰æ‹©èšç±»å±‚æ¬¡
2. **å±‚æ¬¡ç»“æ„å¯è§†åŒ–**ï¼šæ ‘çŠ¶å›¾ç›´è§‚å±•ç¤ºæ•°æ®ç»“æ„
3. **ç¡®å®šæ€§ç»“æœ**ï¼šç›¸åŒå‚æ•°ä¿è¯ç›¸åŒç»“æœï¼ˆK-Meanséšæœºï¼‰
4. **ç°‡å½¢çŠ¶çµæ´»**ï¼šæ ¹æ®é“¾æ¥æ–¹å¼å¯è¯†åˆ«ä¸åŒå½¢çŠ¶
5. **åµŒå¥—ç°‡**ï¼šèƒ½è¡¨ç¤ºå±‚æ¬¡å…³ç³»ï¼ˆå¦‚ç§‘â†’å±â†’ç§ï¼‰

#### ğŸ‘ ç¼ºç‚¹

1. **è®¡ç®—æˆæœ¬é«˜**ï¼šO(nÂ²log n)åˆ°O(nÂ³)ï¼Œä¸é€‚åˆå¤§æ•°æ®
2. **å†…å­˜å ç”¨å¤§**ï¼šéœ€è¦å­˜å‚¨è·ç¦»çŸ©é˜µ
3. **æ— æ³•æ’¤é”€**ï¼šä¸€æ—¦åˆå¹¶æ— æ³•åˆ†ç¦»ï¼ˆå±€éƒ¨æœ€ä¼˜ï¼‰
4. **å¯¹å™ªå£°æ•æ„Ÿ**ï¼šå•é“¾æ¥å®¹æ˜“å½¢æˆé“¾çŠ¶ç°‡
5. **ä¸å¯æ‰©å±•**ï¼šæ— æ³•å¢é‡æ·»åŠ æ–°æ ·æœ¬

#### âš™ï¸ å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ | è°ƒä¼˜å»ºè®® |
|------|------|--------|----------|
| `n_clusters` | ç°‡æ•°é‡ | 2 | å¯é€šè¿‡æ ‘çŠ¶å›¾ç¡®å®šæœ€ä½³å€¼ |
| `linkage` | é“¾æ¥æ–¹å¼ | 'ward' | **æœ€å…³é”®å‚æ•°**<br>'ward': æ–¹å·®æœ€å°åŒ–ï¼Œç°‡ç´§å‡‘<br>'complete': ç°‡ç›´å¾„æœ€å°<br>'average': å¹³è¡¡é€‰æ‹©<br>'single': å¯è¯†åˆ«é“¾çŠ¶ç°‡ |
| `affinity` | è·ç¦»åº¦é‡ | 'euclidean' | 'ward'åªèƒ½ç”¨'euclidean' |

**å‚æ•°è°ƒä¼˜ç¤ºä¾‹**ï¼š

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
import numpy as np

# 1. ç»˜åˆ¶æ ‘çŠ¶å›¾ç¡®å®šç°‡æ•°
# ä½¿ç”¨scipyè®¡ç®—é“¾æ¥çŸ©é˜µ
Z = linkage(X, method='ward', metric='euclidean')

plt.figure(figsize=(12, 6))
dendrogram(Z, truncate_mode='lastp', p=30)  # åªæ˜¾ç¤ºæœ€å30æ¬¡åˆå¹¶
plt.xlabel('æ ·æœ¬ç´¢å¼•æˆ–ç°‡å¤§å°')
plt.ylabel('è·ç¦»')
plt.title('å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾')
plt.axhline(y=10, color='r', linestyle='--', label='åˆ‡å‰²çº¿')
plt.legend()
plt.show()

# 2. æ ¹æ®æ ‘çŠ¶å›¾ç¡®å®šç°‡æ•°
# è§‚å¯Ÿå“ªé‡Œæœ‰æœ€å¤§çš„çºµå‘è·ç¦»å˜åŒ–
best_n_clusters = 4  # æ ¹æ®æ ‘çŠ¶å›¾ç¡®å®š

# 3. å¯¹æ¯”ä¸åŒé“¾æ¥æ–¹å¼
linkages = ['ward', 'complete', 'average', 'single']
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

for idx, link in enumerate(linkages):
    ax = axes[idx // 2, idx % 2]

    # æ³¨æ„ï¼šwardåªèƒ½ç”¨euclideanè·ç¦»
    metric = 'euclidean' if link == 'ward' else 'euclidean'

    model = AgglomerativeClustering(
        n_clusters=best_n_clusters,
        linkage=link,
        affinity=metric
    )
    clusters = model.fit_predict(X)

    ax.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50)
    ax.set_title(f'{link} linkage')
    ax.grid(True)

plt.tight_layout()
plt.show()

# 4. ä½¿ç”¨æœ€ä½³é…ç½®
model = AgglomerativeClustering(
    n_clusters=4,
    linkage='ward',
    affinity='euclidean'
)
clusters = model.fit_predict(X)
```

#### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|---------|------|------|
| **è®­ç»ƒé€Ÿåº¦** | â­â­ | O(nÂ²log n)ï¼Œè¾ƒæ…¢ |
| **å†…å­˜å ç”¨** | â­â­ | éœ€è¦O(nÂ²)è·ç¦»çŸ©é˜µ |
| **å¤§æ•°æ®é€‚ç”¨æ€§** | â­ | ä¸é€‚åˆå¤§è§„æ¨¡æ•°æ® |
| **å±‚æ¬¡ç»“æ„è¡¨è¾¾** | â­â­â­â­â­ | æœ€å¤§ä¼˜åŠ¿ |
| **ç»“æœç¡®å®šæ€§** | â­â­â­â­â­ | ç›¸åŒå‚æ•°ç»“æœç›¸åŒ |
| **ç°‡æ•°çµæ´»æ€§** | â­â­â­â­â­ | å¯ä»æ ‘çŠ¶å›¾é€‰æ‹©ä»»æ„ç°‡æ•° |
| **å¯è§£é‡Šæ€§** | â­â­â­â­ | æ ‘çŠ¶å›¾ç›´è§‚ |

#### ğŸ’¡ ä½¿ç”¨å»ºè®®

**1. é€‰æ‹©åˆé€‚çš„é“¾æ¥æ–¹å¼**

```python
# Wardï¼šç´§å‡‘çš„çƒå½¢ç°‡ï¼ˆæœ€å¸¸ç”¨ï¼‰
model_ward = AgglomerativeClustering(n_clusters=3, linkage='ward')

# Completeï¼šé¿å…é“¾çŠ¶ç°‡ï¼Œç°‡æ›´ç´§å‡‘
model_complete = AgglomerativeClustering(n_clusters=3, linkage='complete')

# Singleï¼šå¯å‘ç°é“¾çŠ¶ç»“æ„ï¼Œä½†å¯¹å™ªå£°æ•æ„Ÿ
model_single = AgglomerativeClustering(n_clusters=3, linkage='single')

# Averageï¼šå¹³è¡¡é€‰æ‹©
model_average = AgglomerativeClustering(n_clusters=3, linkage='average')
```

**2. ä½¿ç”¨è¿æ¥çº¦æŸåŠ é€Ÿå¤§æ•°æ®é›†**

```python
from sklearn.neighbors import kneighbors_graph

# æ„å»ºé‚»æ¥å›¾ï¼šæ¯ä¸ªç‚¹åªä¸kä¸ªæœ€è¿‘é‚»è¿æ¥
connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)

# ä½¿ç”¨è¿æ¥çº¦æŸï¼ˆå¤§å¹…å‡å°‘è®¡ç®—é‡ï¼‰
model = AgglomerativeClustering(
    n_clusters=5,
    connectivity=connectivity,  # åªè€ƒè™‘è¿æ¥çš„ç‚¹
    linkage='ward'
)
clusters = model.fit_predict(X)
```

**3. ç»“åˆä¸šåŠ¡å«ä¹‰è§£è¯»æ ‘çŠ¶å›¾**

```python
import pandas as pd
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

# è®¡ç®—é“¾æ¥çŸ©é˜µï¼ˆå¸¦æ ·æœ¬æ ‡ç­¾ï¼‰
labels = df['customer_name'].values
Z = linkage(X, method='ward')

# ç»˜åˆ¶å¸¦æ ‡ç­¾çš„æ ‘çŠ¶å›¾ï¼ˆå°æ•°æ®é›†ï¼‰
plt.figure(figsize=(15, 8))
dendrogram(Z, labels=labels, leaf_rotation=90)
plt.title('å®¢æˆ·å±‚æ¬¡èšç±»')
plt.xlabel('å®¢æˆ·')
plt.ylabel('è·ç¦»')
plt.show()

# ä»æ ‘çŠ¶å›¾è·å–å¤šå±‚æ¬¡çš„èšç±»
# è·ç¦»é˜ˆå€¼=15æ—¶çš„èšç±»
clusters_15 = fcluster(Z, t=15, criterion='distance')
print(f"è·ç¦»=15: {len(set(clusters_15))} ä¸ªç°‡")

# è·ç¦»é˜ˆå€¼=10æ—¶çš„èšç±»
clusters_10 = fcluster(Z, t=10, criterion='distance')
print(f"è·ç¦»=10: {len(set(clusters_10))} ä¸ªç°‡")
```

---

### 12. é«˜æ–¯æ··åˆæ¨¡å‹ (GMM)

#### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šå‡è®¾æ•°æ®ç”±Kä¸ªé«˜æ–¯åˆ†å¸ƒæ··åˆè€Œæˆï¼Œæ¯ä¸ªç°‡å¯¹åº”ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒã€‚GMMæ˜¯"è½¯èšç±»"ï¼Œæ¯ä¸ªæ ·æœ¬å±äºå„ç°‡çš„æ¦‚ç‡ä¹‹å’Œä¸º1ã€‚

**æ•°å­¦è¡¨è¾¾**ï¼š
```
P(x) = Î£(k=1â†’K) Ï€k Â· N(x | Î¼k, Î£k)
```
- Ï€kï¼šç¬¬kä¸ªåˆ†é‡çš„æ··åˆç³»æ•°ï¼ˆæƒé‡ï¼‰
- N(x | Î¼k, Î£k)ï¼šç¬¬kä¸ªé«˜æ–¯åˆ†å¸ƒ

**ä¸K-Meansçš„å…³ç³»**ï¼š
- K-Meansæ˜¯GMMçš„ç‰¹ä¾‹ï¼ˆåæ–¹å·®ä¸ºå•ä½çŸ©é˜µï¼Œç¡¬åˆ†é…ï¼‰
- GMMæä¾›æ¦‚ç‡åˆ†é…ï¼Œæ›´çµæ´»

**EMç®—æ³•**ï¼š
1. **Eæ­¥**ï¼šè®¡ç®—æ¯ä¸ªæ ·æœ¬å±äºå„ç°‡çš„åéªŒæ¦‚ç‡
2. **Mæ­¥**ï¼šæ ¹æ®åéªŒæ¦‚ç‡æ›´æ–°é«˜æ–¯å‚æ•°(Î¼k, Î£k, Ï€k)
3. è¿­ä»£ç›´åˆ°æ”¶æ•›

#### âœ… é€‚ç”¨åœºæ™¯

| æ•°æ®ç‰¹å¾ | é€‚ç”¨æ€§ | è¯´æ˜ |
|---------|-------|------|
| **æ¤­åœ†å½¢åˆ†å¸ƒçš„ç°‡** | âœ… æœ€ä½³ | å¯è¯†åˆ«ä»»æ„æ–¹å‘å’Œå½¢çŠ¶çš„æ¤­åœ†ç°‡ |
| **éœ€è¦æ¦‚ç‡è¾“å‡º** | âœ… æ¨è | æä¾›æ ·æœ¬å±äºå„ç°‡çš„æ¦‚ç‡ |
| **ç°‡æœ‰é‡å ** | âœ… æ¨è | è½¯èšç±»å¯å¤„ç†è¾¹ç•Œæ¨¡ç³Šçš„æƒ…å†µ |
| **éœ€è¦å¯†åº¦ä¼°è®¡** | âœ… æ¨è | å¯ç”¨äºå¼‚å¸¸æ£€æµ‹ï¼ˆä½æ¦‚ç‡æ ·æœ¬ï¼‰ |
| **ä¸­å°è§„æ¨¡æ•°æ®** | âœ… æ¨è | æ—¶é—´å¤æ‚åº¦O(nkdÂ²)ï¼Œkæ˜¯ç°‡æ•°ï¼Œdæ˜¯ç»´åº¦ |
| **ä»»æ„å½¢çŠ¶ç°‡** | âŒ å±€é™ | ä»å±€é™äºæ¤­åœ†å½¢ï¼ˆä½†æ¯”K-Meansçµæ´»ï¼‰ |
| **é«˜ç»´æ•°æ®** | âš ï¸ è°¨æ… | åæ–¹å·®çŸ©é˜µå‚æ•°è¿‡å¤šï¼Œæ˜“è¿‡æ‹Ÿåˆ |

**å…¸å‹åº”ç”¨**ï¼š
- å›¾åƒåˆ†å‰²
- è¯­éŸ³è¯†åˆ«ï¼ˆå£°å­¦æ¨¡å‹ï¼‰
- å¼‚å¸¸æ£€æµ‹ï¼ˆä½æ¦‚ç‡å¯†åº¦ç‚¹ï¼‰
- åŠç›‘ç£å­¦ä¹ ï¼ˆéƒ¨åˆ†æ ‡ç­¾æ•°æ®ï¼‰

#### ğŸ‘ ä¼˜ç‚¹

1. **è½¯èšç±»**ï¼šæä¾›æ¦‚ç‡åˆ†é…ï¼Œä¿¡æ¯æ›´ä¸°å¯Œ
2. **å½¢çŠ¶çµæ´»**ï¼šå¯è¯†åˆ«ä¸åŒå½¢çŠ¶å’Œæ–¹å‘çš„æ¤­åœ†ç°‡
3. **ç†è®ºåŸºç¡€æ‰å®**ï¼šåŸºäºæ¦‚ç‡æ¨¡å‹ï¼Œå¯ç”¨AIC/BICé€‰æ‹©æ¨¡å‹
4. **å¯ç”¨äºå¯†åº¦ä¼°è®¡**ï¼šä¼°è®¡æ•°æ®åˆ†å¸ƒï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹
5. **ç°‡å¤§å°å¯å˜**ï¼šä¸å‡è®¾ç°‡å¤§å°ç›¸åŒ

#### ğŸ‘ ç¼ºç‚¹

1. **éœ€è¦æŒ‡å®šç°‡æ•°**ï¼šä¸K-MeansåŒæ ·çš„é—®é¢˜
2. **å¯¹åˆå§‹å€¼æ•æ„Ÿ**ï¼šEMç®—æ³•å¯èƒ½æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜
3. **å‚æ•°å¤š**ï¼šé«˜ç»´æ—¶åæ–¹å·®çŸ©é˜µå‚æ•°é‡å¤§ï¼Œæ˜“è¿‡æ‹Ÿåˆ
4. **è®¡ç®—æˆæœ¬é«˜**ï¼šæ¯”K-Meansæ…¢
5. **å‡è®¾é«˜æ–¯åˆ†å¸ƒ**ï¼šæ•°æ®ä¸ç¬¦åˆé«˜æ–¯åˆ†å¸ƒæ—¶æ•ˆæœå·®

#### âš™ï¸ å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ | è°ƒä¼˜å»ºè®® |
|------|------|--------|----------|
| `n_components` | é«˜æ–¯åˆ†é‡æ•°ï¼ˆç°‡æ•°ï¼‰| 1 | ä½¿ç”¨BIC/AICé€‰æ‹© |
| `covariance_type` | åæ–¹å·®ç±»å‹ | 'full' | **å…³é”®å‚æ•°**<br>'full': æ¯ä¸ªç°‡ç‹¬ç«‹çš„åæ–¹å·®çŸ©é˜µï¼ˆæœ€çµæ´»ï¼‰<br>'tied': æ‰€æœ‰ç°‡å…±äº«åæ–¹å·®<br>'diag': å¯¹è§’åæ–¹å·®ï¼ˆç‰¹å¾ç‹¬ç«‹ï¼‰<br>'spherical': çƒå½¢ï¼ˆå„å‘åŒæ€§ï¼‰ |
| `init_params` | åˆå§‹åŒ–æ–¹æ³• | 'kmeans' | é»˜è®¤ç”¨K-Meansåˆå§‹åŒ–ï¼Œæ•ˆæœå¥½ |
| `random_state` | éšæœºç§å­ | None | å¿…é¡»è®¾ç½®ä»¥ä¿è¯å¯å¤ç°æ€§ |

**å‚æ•°è°ƒä¼˜ç¤ºä¾‹**ï¼š

```python
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
import numpy as np

# 1. ä½¿ç”¨BIC/AICé€‰æ‹©æœ€ä½³ç°‡æ•°
n_components_range = range(2, 11)
bic_scores = []
aic_scores = []

for n in n_components_range:
    gmm = GaussianMixture(n_components=n, covariance_type='full', random_state=42)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))
    aic_scores.append(gmm.aic(X))

# ç»˜åˆ¶BIC/AICæ›²çº¿ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
plt.figure(figsize=(10, 6))
plt.plot(n_components_range, bic_scores, 'bo-', label='BIC')
plt.plot(n_components_range, aic_scores, 'rs-', label='AIC')
plt.xlabel('é«˜æ–¯åˆ†é‡æ•°')
plt.ylabel('ä¿¡æ¯å‡†åˆ™å€¼')
plt.title('æ¨¡å‹é€‰æ‹©ï¼ˆBIC/AICè¶Šå°è¶Šå¥½ï¼‰')
plt.legend()
plt.grid(True)
plt.show()

# 2. é€‰æ‹©æœ€ä½³ç°‡æ•°
best_n = n_components_range[np.argmin(bic_scores)]
print(f"BICæ¨èçš„æœ€ä½³ç°‡æ•°: {best_n}")

# 3. å¯¹æ¯”ä¸åŒåæ–¹å·®ç±»å‹
cov_types = ['full', 'tied', 'diag', 'spherical']
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

for idx, cov_type in enumerate(cov_types):
    ax = axes[idx // 2, idx % 2]

    gmm = GaussianMixture(
        n_components=best_n,
        covariance_type=cov_type,
        random_state=42
    )
    clusters = gmm.fit_predict(X)

    ax.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.6)
    ax.set_title(f'{cov_type} covariance (BIC={gmm.bic(X):.0f})')
    ax.grid(True)

plt.tight_layout()
plt.show()

# 4. æœ€ç»ˆæ¨¡å‹
gmm = GaussianMixture(
    n_components=best_n,
    covariance_type='full',
    n_init=10,            # å¤šæ¬¡åˆå§‹åŒ–å–æœ€ä¼˜
    init_params='kmeans',  # ç”¨K-Meansåˆå§‹åŒ–
    random_state=42
)
gmm.fit(X)

# è·å–æ¦‚ç‡åˆ†é…ï¼ˆè½¯èšç±»ï¼‰
proba = gmm.predict_proba(X)
print(f"æ ·æœ¬0å±äºå„ç°‡çš„æ¦‚ç‡: {proba[0]}")
```

#### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|---------|------|------|
| **è®­ç»ƒé€Ÿåº¦** | â­â­â­ | æ¯”K-Meansæ…¢ï¼Œä½†å¯æ¥å— |
| **é¢„æµ‹é€Ÿåº¦** | â­â­â­ | éœ€è¦è®¡ç®—æ¦‚ç‡å¯†åº¦ |
| **å†…å­˜å ç”¨** | â­â­â­ | éœ€è¦å­˜å‚¨åæ–¹å·®çŸ©é˜µ |
| **æ¤­åœ†ç°‡è¯†åˆ«** | â­â­â­â­â­ | æœ€å¤§ä¼˜åŠ¿ |
| **è½¯èšç±»èƒ½åŠ›** | â­â­â­â­â­ | æä¾›æ¦‚ç‡ä¿¡æ¯ |
| **å¯†åº¦ä¼°è®¡** | â­â­â­â­â­ | å¯ç”¨äºå¼‚å¸¸æ£€æµ‹ |
| **é«˜ç»´é€‚ç”¨æ€§** | â­â­ | å‚æ•°è¿‡å¤šæ˜“è¿‡æ‹Ÿåˆ |
| **å¯è§£é‡Šæ€§** | â­â­â­ | æ¦‚ç‡æ¨¡å‹è¾ƒç›´è§‚ |

#### ğŸ’¡ ä½¿ç”¨å»ºè®®

**1. åˆ©ç”¨è½¯èšç±»æ‰¾åˆ°ä¸ç¡®å®šæ ·æœ¬**

```python
# è®­ç»ƒæ¨¡å‹
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(X)

# è·å–æ¦‚ç‡åˆ†é…
proba = gmm.predict_proba(X)

# æ‰¾åˆ°ä¸ç¡®å®šçš„æ ·æœ¬ï¼ˆæ¦‚ç‡åˆ†å¸ƒè¾ƒå‡åŒ€ï¼‰
max_proba = proba.max(axis=1)
uncertain_mask = max_proba < 0.6  # æœ€å¤§æ¦‚ç‡<60%è¡¨ç¤ºä¸ç¡®å®š

print(f"ä¸ç¡®å®šæ ·æœ¬æ•°: {uncertain_mask.sum()} ({uncertain_mask.sum()/len(X)*100:.1f}%)")

# å¯è§†åŒ–ä¸ç¡®å®šæ ·æœ¬
plt.scatter(X[~uncertain_mask, 0], X[~uncertain_mask, 1], c=gmm.predict(X[~uncertain_mask]), s=50, alpha=0.6)
plt.scatter(X[uncertain_mask, 0], X[uncertain_mask, 1], c='red', s=100, marker='x', label='ä¸ç¡®å®šæ ·æœ¬')
plt.legend()
plt.title('GMMè½¯èšç±»ï¼šçº¢å‰ä¸ºè¾¹ç•Œä¸ç¡®å®šæ ·æœ¬')
plt.show()
```

**2. ç”¨äºå¼‚å¸¸æ£€æµ‹**

```python
# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¯¹æ•°æ¦‚ç‡å¯†åº¦
log_prob = gmm.score_samples(X)

# ä½æ¦‚ç‡å¯†åº¦çš„æ ·æœ¬æ˜¯å¼‚å¸¸
threshold = np.percentile(log_prob, 5)  # æœ€ä½5%ä¸ºå¼‚å¸¸
anomalies = log_prob < threshold

print(f"å¼‚å¸¸æ ·æœ¬æ•°: {anomalies.sum()}")

# å¯è§†åŒ–
plt.scatter(X[~anomalies, 0], X[~anomalies, 1], c='blue', s=50, alpha=0.6, label='æ­£å¸¸')
plt.scatter(X[anomalies, 0], X[anomalies, 1], c='red', s=100, marker='x', label='å¼‚å¸¸')
plt.legend()
plt.title('GMMå¼‚å¸¸æ£€æµ‹')
plt.show()
```

**3. é«˜ç»´æ•°æ®ï¼šä½¿ç”¨æ­£åˆ™åŒ–**

```python
# é«˜ç»´æ•°æ®å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨æ­£åˆ™åŒ–
gmm = GaussianMixture(
    n_components=5,
    covariance_type='diag',      # å‡è®¾ç‰¹å¾ç‹¬ç«‹ï¼Œå‡å°‘å‚æ•°
    reg_covar=1e-4,              # æ­£åˆ™åŒ–åæ–¹å·®çŸ©é˜µ
    random_state=42
)
gmm.fit(X_high_dim)
```

---

### 13. PCA (ä¸»æˆåˆ†åˆ†æ)

#### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡çº¿æ€§å˜æ¢å°†æ•°æ®æŠ•å½±åˆ°ä½ç»´ç©ºé—´ï¼ŒåŒæ—¶æœ€å¤§åŒ–ä¿ç•™æ•°æ®æ–¹å·®ï¼ˆä¿¡æ¯ï¼‰ã€‚

**æ•°å­¦è¡¨è¾¾**ï¼š
- æ‰¾åˆ°kä¸ªæ­£äº¤æ–¹å‘ï¼ˆä¸»æˆåˆ†ï¼‰ï¼Œä½¿æŠ•å½±åçš„æ–¹å·®æœ€å¤§
- ä¸»æˆåˆ† = åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡
- æ–¹å·®è§£é‡Šæ¯”ä¾‹ = å¯¹åº”ç‰¹å¾å€¼ / æ‰€æœ‰ç‰¹å¾å€¼ä¹‹å’Œ

**ç®—æ³•æµç¨‹**ï¼š
1. æ•°æ®ä¸­å¿ƒåŒ–ï¼ˆå‡å»å‡å€¼ï¼‰
2. è®¡ç®—åæ–¹å·®çŸ©é˜µ
3. è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
4. é€‰æ‹©å‰kä¸ªæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡
5. å°†æ•°æ®æŠ•å½±åˆ°è¿™kä¸ªæ–¹å‘

#### âœ… é€‚ç”¨åœºæ™¯

| æ•°æ®ç‰¹å¾ | é€‚ç”¨æ€§ | è¯´æ˜ |
|---------|-------|------|
| **é«˜ç»´æ•°æ®é™ç»´** | âœ… æœ€ä½³ | ä»å‡ ç™¾ç»´é™åˆ°å‡ åç»´ |
| **ç‰¹å¾ç›¸å…³æ€§é«˜** | âœ… æ¨è | å¯æ¶ˆé™¤å†—ä½™ç‰¹å¾ |
| **æ•°æ®å¯è§†åŒ–** | âœ… æ¨è | é™åˆ°2D/3Dä¾¿äºå¯è§†åŒ– |
| **åŠ é€Ÿæ¨¡å‹è®­ç»ƒ** | âœ… æ¨è | å‡å°‘ç‰¹å¾æ•°ï¼ŒåŠ å¿«è®­ç»ƒ |
| **å»å™ª** | âœ… æ¨è | ä¿ç•™ä¸»è¦æˆåˆ†ï¼Œè¿‡æ»¤å™ªå£° |
| **åˆ†ç±»ç‰¹å¾** | âŒ ä¸é€‚ç”¨ | ä»…é€‚ç”¨äºæ•°å€¼ç‰¹å¾ |
| **éçº¿æ€§å…³ç³»** | âŒ å±€é™ | ä»…èƒ½æ•æ‰çº¿æ€§å…³ç³»ï¼ˆå¯ç”¨Kernel PCAï¼‰ |

**å…¸å‹åº”ç”¨**ï¼š
- å›¾åƒå‹ç¼©
- äººè„¸è¯†åˆ«ï¼ˆEigenfaceï¼‰
- æ•°æ®å¯è§†åŒ–
- å»é™¤å¤šé‡å…±çº¿æ€§

#### ğŸ‘ ä¼˜ç‚¹

1. **é™ç»´é«˜æ•ˆ**ï¼šçº¿æ€§å˜æ¢ï¼Œè®¡ç®—å¿«é€Ÿ
2. **å»é™¤ç›¸å…³æ€§**ï¼šä¸»æˆåˆ†å½¼æ­¤æ­£äº¤ï¼ˆæ— ç›¸å…³æ€§ï¼‰
3. **ç†è®ºåŸºç¡€æ‰å®**ï¼šåŸºäºæ–¹å·®æœ€å¤§åŒ–
4. **å¯è§£é‡Šæ€§**ï¼šæ–¹å·®è§£é‡Šæ¯”ä¾‹åæ˜ ä¿¡æ¯ä¿ç•™ç¨‹åº¦
5. **æ— ç›‘ç£**ï¼šä¸éœ€è¦æ ‡ç­¾

#### ğŸ‘ ç¼ºç‚¹

1. **åªèƒ½çº¿æ€§é™ç»´**ï¼šæ— æ³•æ•æ‰éçº¿æ€§å…³ç³»
2. **ä¸»æˆåˆ†éš¾è§£é‡Š**ï¼šæ–°ç‰¹å¾æ˜¯åŸç‰¹å¾çš„çº¿æ€§ç»„åˆï¼Œç‰©ç†æ„ä¹‰ä¸æ˜
3. **å¯¹å°ºåº¦æ•æ„Ÿ**ï¼šå¿…é¡»æ ‡å‡†åŒ–æ•°æ®
4. **å¼‚å¸¸å€¼æ•æ„Ÿ**ï¼šæç«¯å€¼ä¼šå½±å“ä¸»æˆåˆ†æ–¹å‘
5. **ä¿¡æ¯æŸå¤±**ï¼šé™ç»´å¿…ç„¶ä¸¢å¤±éƒ¨åˆ†ä¿¡æ¯

#### âš™ï¸ å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ | è°ƒä¼˜å»ºè®® |
|------|------|--------|----------|
| `n_components` | ä¿ç•™çš„ä¸»æˆåˆ†æ•° | None | æ–¹æ³•1: æ•´æ•°ï¼ˆå¦‚10ï¼‰<br>æ–¹æ³•2: æ¯”ä¾‹ï¼ˆå¦‚0.95ä¿ç•™95%æ–¹å·®ï¼‰<br>æ–¹æ³•3: 'mle'è‡ªåŠ¨é€‰æ‹© |
| `whiten` | æ˜¯å¦ç™½åŒ– | False | True: ä½¿ä¸»æˆåˆ†æ–¹å·®ä¸º1ï¼ˆç”¨äºICAç­‰åç»­åˆ†æï¼‰|
| `svd_solver` | SVDæ±‚è§£å™¨ | 'auto' | 'auto': è‡ªåŠ¨é€‰æ‹©<br>'full': æ ‡å‡†SVD<br>'randomized': å¤§æ•°æ®é›†æ›´å¿« |

**å‚æ•°è°ƒä¼˜ç¤ºä¾‹**ï¼š

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# 1. ç¡®å®šä¿ç•™å¤šå°‘ä¸»æˆåˆ†
pca_full = PCA()  # ä¿ç•™æ‰€æœ‰ä¸»æˆåˆ†
pca_full.fit(X)

# ç»˜åˆ¶æ–¹å·®è§£é‡Šæ¯”ä¾‹
explained_var = pca_full.explained_variance_ratio_
cumsum_var = np.cumsum(explained_var)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# å•ä¸ªä¸»æˆåˆ†çš„æ–¹å·®è§£é‡Šæ¯”ä¾‹
ax1.bar(range(1, len(explained_var)+1), explained_var)
ax1.set_xlabel('ä¸»æˆåˆ†')
ax1.set_ylabel('æ–¹å·®è§£é‡Šæ¯”ä¾‹')
ax1.set_title('å„ä¸»æˆåˆ†çš„æ–¹å·®è´¡çŒ®')
ax1.grid(True)

# ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹
ax2.plot(range(1, len(cumsum_var)+1), cumsum_var, 'bo-')
ax2.axhline(y=0.95, color='r', linestyle='--', label='95%é˜ˆå€¼')
ax2.set_xlabel('ä¸»æˆåˆ†æ•°é‡')
ax2.set_ylabel('ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹')
ax2.set_title('ç´¯ç§¯æ–¹å·®æ›²çº¿')
ax2.legend()
ax2.grid(True)
plt.show()

# 2. é€‰æ‹©ä¿ç•™95%æ–¹å·®çš„ä¸»æˆåˆ†æ•°
n_components_95 = np.argmax(cumsum_var >= 0.95) + 1
print(f"ä¿ç•™95%æ–¹å·®éœ€è¦ {n_components_95} ä¸ªä¸»æˆåˆ†")

# 3. ä¸‰ç§æŒ‡å®šn_componentsçš„æ–¹å¼
# æ–¹å¼1ï¼šæŒ‡å®šå…·ä½“æ•°é‡
pca1 = PCA(n_components=10)  # ä¿ç•™10ä¸ªä¸»æˆåˆ†

# æ–¹å¼2ï¼šæŒ‡å®šæ–¹å·®æ¯”ä¾‹
pca2 = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®

# æ–¹å¼3ï¼šè‡ªåŠ¨é€‰æ‹©ï¼ˆMLEï¼‰
pca3 = PCA(n_components='mle', svd_solver='full')

# 4. åº”ç”¨é™ç»´
from sklearn.preprocessing import StandardScaler

# âš ï¸ å¿…é¡»å…ˆæ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# é™ç»´
pca = PCA(n_components=0.95, random_state=42)
X_pca = pca.fit_transform(X_scaled)

print(f"åŸå§‹ç»´åº¦: {X.shape[1]}")
print(f"é™ç»´åç»´åº¦: {X_pca.shape[1]}")
print(f"ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹: {pca.explained_variance_ratio_.sum():.3f}")
```

#### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|---------|------|------|
| **è®­ç»ƒé€Ÿåº¦** | â­â­â­â­ | çº¿æ€§å˜æ¢ï¼Œå¿«é€Ÿ |
| **é¢„æµ‹é€Ÿåº¦** | â­â­â­â­â­ | ä»…éœ€çŸ©é˜µä¹˜æ³• |
| **å†…å­˜å ç”¨** | â­â­â­â­ | ä»…å­˜å‚¨è½¬æ¢çŸ©é˜µ |
| **çº¿æ€§é™ç»´èƒ½åŠ›** | â­â­â­â­â­ | æœ€ä¼˜çº¿æ€§é™ç»´ |
| **éçº¿æ€§é™ç»´** | â­ | éœ€è¦Kernel PCA |
| **å¯è§£é‡Šæ€§** | â­â­â­ | æ–¹å·®è§£é‡Šæ¯”ä¾‹æ¸…æ™° |
| **å¤„ç†å¤§æ•°æ®** | â­â­â­â­ | Randomized SVDå¯å¤„ç†å¤§æ•°æ® |

#### ğŸ’¡ ä½¿ç”¨å»ºè®®

**1. é™ç»´åç”¨äºå¯è§†åŒ–**

```python
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®ï¼ˆ64ç»´ï¼‰
digits = load_digits()
X, y = digits.data, digits.target

# é™åˆ°2ç»´
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X)

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='tab10', s=30, alpha=0.7)
plt.colorbar(scatter, label='æ•°å­—ç±»åˆ«')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
plt.title('æ‰‹å†™æ•°å­—PCAé™ç»´å¯è§†åŒ–')
plt.show()
```

**2. é™ç»´ååŠ é€Ÿæ¨¡å‹è®­ç»ƒ**

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import time

# åŸå§‹é«˜ç»´æ•°æ®
start = time.time()
scores_original = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42),
                                   X, y, cv=5)
time_original = time.time() - start

# PCAé™ç»´å
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X)
start = time.time()
scores_pca = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42),
                              X_pca, y, cv=5)
time_pca = time.time() - start

print(f"åŸå§‹æ•°æ®: {X.shape[1]}ç»´, å‡†ç¡®ç‡={scores_original.mean():.3f}, æ—¶é—´={time_original:.2f}s")
print(f"PCAé™ç»´: {X_pca.shape[1]}ç»´, å‡†ç¡®ç‡={scores_pca.mean():.3f}, æ—¶é—´={time_pca:.2f}s")
print(f"é€Ÿåº¦æå‡: {time_original/time_pca:.1f}å€")
```

**3. åˆ†æä¸»æˆåˆ†å«ä¹‰**

```python
# æŸ¥çœ‹æ¯ä¸ªä¸»æˆåˆ†å¯¹åŸç‰¹å¾çš„æƒé‡
components = pca.components_  # shape: (n_components, n_features)

# å¯¹äºç¬¬ä¸€ä¸»æˆåˆ†ï¼Œæ‰¾å‡ºè´¡çŒ®æœ€å¤§çš„åŸç‰¹å¾
pc1 = components[0]
top_features_idx = np.argsort(np.abs(pc1))[::-1][:5]  # Top 5

print("ç¬¬ä¸€ä¸»æˆåˆ†ä¸»è¦ç”±ä»¥ä¸‹ç‰¹å¾æ„æˆ:")
for idx in top_features_idx:
    print(f"  ç‰¹å¾{idx}: æƒé‡={pc1[idx]:.3f}")
```

**4. å›¾åƒå‹ç¼©åº”ç”¨**

```python
from sklearn.datasets import load_sample_image
import matplotlib.pyplot as plt

# åŠ è½½ç¤ºä¾‹å›¾åƒ
img = load_sample_image('flower.jpg')
img = img / 255.0  # å½’ä¸€åŒ–åˆ°[0,1]

# å±•å¹³å›¾åƒï¼ˆä¿æŒæ¯ä¸ªåƒç´ çš„RGBé€šé“ï¼‰
h, w, c = img.shape
X_img = img.reshape(h * w, c)

# PCAå‹ç¼©
pca = PCA(n_components=2)  # ä»3é€šé“å‹ç¼©åˆ°2é€šé“
X_compressed = pca.fit_transform(X_img)

# é‡å»ºå›¾åƒ
X_reconstructed = pca.inverse_transform(X_compressed)
img_reconstructed = X_reconstructed.reshape(h, w, c)

# å¯¹æ¯”
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
ax1.imshow(img)
ax1.set_title('åŸå§‹å›¾åƒï¼ˆ3é€šé“ï¼‰')
ax1.axis('off')

ax2.imshow(np.clip(img_reconstructed, 0, 1))  # clipåˆ°[0,1]
ax2.set_title(f'PCAå‹ç¼©ï¼ˆ2é€šé“ï¼Œä¿ç•™{pca.explained_variance_ratio_.sum()*100:.1f}%ä¿¡æ¯ï¼‰')
ax2.axis('off')
plt.show()
```

---

### 14. t-SNE

#### ğŸ“Œ ç®—æ³•æ¦‚è¿°

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°ä½ç»´ï¼ˆé€šå¸¸2D/3Dï¼‰ï¼Œä¿æŒæ•°æ®ç‚¹ä¹‹é—´çš„"é‚»è¿‘å…³ç³»"ã€‚ä¸PCAä¸åŒï¼Œt-SNEæ˜¯**éçº¿æ€§é™ç»´**ï¼Œèƒ½æ­ç¤ºå¤æ‚çš„æµå½¢ç»“æ„ã€‚

**æ ¸å¿ƒæœºåˆ¶**ï¼š
1. åœ¨é«˜ç»´ç©ºé—´è®¡ç®—ç‚¹å¯¹ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰
2. åœ¨ä½ç»´ç©ºé—´ä¹Ÿè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆtåˆ†å¸ƒï¼‰
3. é€šè¿‡ä¼˜åŒ–ä½¿ä¸¤ä¸ªç›¸ä¼¼åº¦åˆ†å¸ƒå°½å¯èƒ½æ¥è¿‘ï¼ˆKLæ•£åº¦æœ€å°ï¼‰

**t-SNE vs PCA**ï¼š
- **PCA**ï¼šçº¿æ€§é™ç»´ï¼Œä¿ç•™å…¨å±€ç»“æ„ï¼ˆæ–¹å·®ï¼‰ï¼Œå¿«é€Ÿ
- **t-SNE**ï¼šéçº¿æ€§é™ç»´ï¼Œä¿ç•™å±€éƒ¨ç»“æ„ï¼ˆé‚»è¿‘å…³ç³»ï¼‰ï¼Œæ…¢ä½†æ•ˆæœå¥½

#### âœ… é€‚ç”¨åœºæ™¯

| æ•°æ®ç‰¹å¾ | é€‚ç”¨æ€§ | è¯´æ˜ |
|---------|-------|------|
| **é«˜ç»´æ•°æ®å¯è§†åŒ–** | âœ… æœ€ä½³ | ä»å‡ ç™¾ç»´é™åˆ°2D/3Då¯è§†åŒ– |
| **å‘ç°ç°‡ç»“æ„** | âœ… æ¨è | èƒ½æ­ç¤ºæ•°æ®ä¸­çš„åˆ†ç»„æ¨¡å¼ |
| **éçº¿æ€§æµå½¢** | âœ… æ¨è | å¯å±•å¼€å·æ›²çš„é«˜ç»´æµå½¢ |
| **æ¢ç´¢æ€§åˆ†æ** | âœ… æ¨è | å¿«é€Ÿäº†è§£æ•°æ®ç»“æ„ |
| **å°ä¸­è§„æ¨¡æ•°æ®** | âœ… æ¨è | <10000æ ·æœ¬æ•ˆæœå¥½ |
| **å¤§è§„æ¨¡æ•°æ®** | âš ï¸ æ…ç”¨ | æ—¶é—´å¤æ‚åº¦O(nÂ²)ï¼Œéå¸¸æ…¢ |
| **ç‰¹å¾é™ç»´åå»ºæ¨¡** | âŒ ä¸æ¨è | t-SNEé™ç»´åä¸èƒ½ç”¨äºå»ºæ¨¡ï¼ˆéå‚æ•°æ–¹æ³•ï¼‰ |
| **è·ç¦»åº¦é‡** | âŒ ä¸é€‚ç”¨ | ä½ç»´ç©ºé—´è·ç¦»æ— æ„ä¹‰ï¼Œåªèƒ½çœ‹ç°‡ |

**å…¸å‹åº”ç”¨**ï¼š
- é«˜ç»´æ•°æ®å¯è§†åŒ–ï¼ˆå›¾åƒã€æ–‡æœ¬åµŒå…¥ï¼‰
- å•ç»†èƒæµ‹åºæ•°æ®åˆ†æ
- è¯å‘é‡å¯è§†åŒ–
- æ¢ç´¢æ€§æ•°æ®åˆ†æ

#### ğŸ‘ ä¼˜ç‚¹

1. **å“è¶Šçš„å¯è§†åŒ–æ•ˆæœ**ï¼šèƒ½æ¸…æ™°å±•ç¤ºç°‡ç»“æ„
2. **éçº¿æ€§é™ç»´**ï¼šå¯å¤„ç†å¤æ‚çš„é«˜ç»´æµå½¢
3. **ä¿ç•™å±€éƒ¨ç»“æ„**ï¼šç›¸ä¼¼æ ·æœ¬åœ¨ä½ç»´ç©ºé—´ä»æ¥è¿‘
4. **ç›´è§‚**ï¼š2D/3Då¯è§†åŒ–æ˜“äºç†è§£

#### ğŸ‘ ç¼ºç‚¹

1. **è®¡ç®—æˆæœ¬æé«˜**ï¼šO(nÂ²)å¤æ‚åº¦ï¼Œå¤§æ•°æ®é›†æ…¢
2. **éšæœºæ€§å¼º**ï¼šä¸åŒè¿è¡Œç»“æœå¯èƒ½ä¸åŒ
3. **æ— æ³•ç”¨äºæ–°æ ·æœ¬**ï¼šéå‚æ•°æ–¹æ³•ï¼Œä¸èƒ½transformæ–°æ•°æ®
4. **è¶…å‚æ•°æ•æ„Ÿ**ï¼šperplexityéœ€è¦ä»”ç»†è°ƒæ•´
5. **ä½ç»´è·ç¦»æ— æ„ä¹‰**ï¼šåªèƒ½çœ‹ç°‡ï¼Œä¸èƒ½åº¦é‡è·ç¦»
6. **å…¨å±€ç»“æ„å¯èƒ½å¤±çœŸ**ï¼šè¿‡åº¦å…³æ³¨å±€éƒ¨ï¼Œå¯èƒ½æ‰­æ›²å…¨å±€å…³ç³»

#### âš™ï¸ å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | é»˜è®¤å€¼ | è°ƒä¼˜å»ºè®® |
|------|------|--------|----------|
| `n_components` | ç›®æ ‡ç»´åº¦ | 2 | é€šå¸¸ç”¨2ï¼ˆå¯è§†åŒ–ï¼‰ï¼Œæœ€å¤š3 |
| `perplexity` | å›°æƒ‘åº¦ï¼ˆé‚»å±…æ•°ï¼‰ | 30 | **æœ€å…³é”®å‚æ•°**<br>5-50: å¹³è¡¡å±€éƒ¨/å…¨å±€<br>å°æ•°æ®ç”¨5-15ï¼Œå¤§æ•°æ®ç”¨30-50 |
| `learning_rate` | å­¦ä¹ ç‡ | 200 | 10-1000ï¼Œå¤ªå¤§/å¤ªå°éƒ½ä¸å¥½ |
| `n_iter` | è¿­ä»£æ¬¡æ•° | 1000 | è‡³å°‘1000ï¼Œå¤æ‚æ•°æ®å¯ç”¨5000 |
| `random_state` | éšæœºç§å­ | None | å¿…é¡»è®¾ç½®ä»¥ä¿è¯å¯å¤ç°æ€§ |

**å‚æ•°è°ƒä¼˜ç¤ºä¾‹**ï¼š

```python
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt
import numpy as np

# åŠ è½½æ•°æ®
digits = load_digits()
X, y = digits.data, digits.target

# âš ï¸ é‡è¦ï¼št-SNEå‰å…ˆç”¨PCAé™åˆ°50ç»´å·¦å³ï¼ˆåŠ é€Ÿ+å»å™ªï¼‰
from sklearn.decomposition import PCA
pca = PCA(n_components=50, random_state=42)
X_pca = pca.fit_transform(X)

# 1. å¯¹æ¯”ä¸åŒperplexityçš„æ•ˆæœ
perplexities = [5, 15, 30, 50]
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

for idx, perp in enumerate(perplexities):
    ax = axes[idx // 2, idx % 2]

    tsne = TSNE(
        n_components=2,
        perplexity=perp,
        learning_rate=200,
        n_iter=1000,
        random_state=42
    )
    X_tsne = tsne.fit_transform(X_pca)

    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=20, alpha=0.7)
    ax.set_title(f'perplexity={perp}')
    ax.grid(True)

plt.colorbar(scatter, ax=axes, label='æ•°å­—ç±»åˆ«')
plt.tight_layout()
plt.show()

# 2. æœ€ä½³å‚æ•°é…ç½®
tsne = TSNE(
    n_components=2,
    perplexity=30,        # ä¸­ç­‰å€¼ï¼Œå¹³è¡¡å±€éƒ¨/å…¨å±€
    learning_rate=200,    # é»˜è®¤å€¼é€šå¸¸åˆé€‚
    n_iter=1000,          # è¶³å¤Ÿçš„è¿­ä»£æ¬¡æ•°
    random_state=42,      # ä¿è¯å¯å¤ç°
    verbose=1             # æ˜¾ç¤ºè¿›åº¦
)

X_tsne = tsne.fit_transform(X_pca)

# å¯è§†åŒ–æœ€ç»ˆç»“æœ
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=30, alpha=0.7)
plt.colorbar(scatter, label='æ•°å­—ç±»åˆ«')
plt.title('t-SNEå¯è§†åŒ–ï¼šæ‰‹å†™æ•°å­—æ•°æ®')
plt.xlabel('t-SNE ç»´åº¦1')
plt.ylabel('t-SNE ç»´åº¦2')
plt.show()
```

#### ğŸ“Š æ€§èƒ½è¯„ä¼°

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|---------|------|------|
| **è®­ç»ƒé€Ÿåº¦** | â­ | éå¸¸æ…¢ï¼ŒO(nÂ²)å¤æ‚åº¦ |
| **é¢„æµ‹é€Ÿåº¦** | âŒ | æ— æ³•transformæ–°æ ·æœ¬ |
| **å†…å­˜å ç”¨** | â­â­ | éœ€è¦è®¡ç®—è·ç¦»çŸ©é˜µ |
| **å¯è§†åŒ–æ•ˆæœ** | â­â­â­â­â­ | æœ€å¤§ä¼˜åŠ¿ |
| **éçº¿æ€§é™ç»´** | â­â­â­â­â­ | å“è¶Šçš„æµå½¢å­¦ä¹ èƒ½åŠ› |
| **ç°‡ç»“æ„å±•ç¤º** | â­â­â­â­â­ | æ¸…æ™°çš„ç°‡åˆ†ç¦» |
| **å…¨å±€ç»“æ„ä¿ç•™** | â­â­ | å¯èƒ½æ‰­æ›²å…¨å±€å…³ç³» |
| **å¯å¤ç°æ€§** | â­â­ | éšæœºæ€§å¼º |

#### ğŸ’¡ ä½¿ç”¨å»ºè®®

**1. å…ˆç”¨PCAé™ç»´åŠ é€Ÿ**

```python
# âš ï¸ é”™è¯¯ï¼šç›´æ¥å¯¹é«˜ç»´æ•°æ®t-SNEï¼ˆéå¸¸æ…¢ï¼‰
# tsne = TSNE(n_components=2).fit_transform(X_1000d)  # 1000ç»´

# âœ… æ­£ç¡®ï¼šå…ˆPCAé™åˆ°50ç»´ï¼Œå†t-SNEï¼ˆå¿«10-100å€ï¼‰
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('pca', PCA(n_components=50)),
    ('tsne', TSNE(n_components=2, random_state=42))
])
X_tsne = pipeline.fit_transform(X_1000d)
```

**2. å¤šæ¬¡è¿è¡Œé€‰æ‹©æœ€ä½³ç»“æœ**

```python
# t-SNEæœ‰éšæœºæ€§ï¼Œè¿è¡Œå¤šæ¬¡é€‰æ‹©æœ€å¥½çš„
best_kl = np.inf
best_result = None

for seed in range(5):  # å°è¯•5ä¸ªä¸åŒçš„éšæœºç§å­
    tsne = TSNE(n_components=2, random_state=seed)
    X_tsne = tsne.fit_transform(X_pca)

    if tsne.kl_divergence_ < best_kl:
        best_kl = tsne.kl_divergence_
        best_result = X_tsne

print(f"æœ€ä½³KLæ•£åº¦: {best_kl:.2f}")
X_tsne_final = best_result
```

**3. è°ƒæ•´perplexityè§‚å¯Ÿä¸åŒå°ºåº¦çš„ç»“æ„**

```python
# perplexityæ§åˆ¶å…³æ³¨çš„é‚»å±…æ•°é‡
# - å°perplexity(5-15): å…³æ³¨å±€éƒ¨ç»“æ„ï¼Œç°‡æ›´åˆ†æ•£
# - å¤§perplexity(30-50): å…³æ³¨å…¨å±€ç»“æ„ï¼Œç°‡æ›´ç´§å‡‘

# ç¤ºä¾‹ï¼šå¯¹åŒä¸€æ•°æ®ä½¿ç”¨ä¸åŒperplexity
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
perplexities = [5, 30, 50]

for ax, perp in zip(axes, perplexities):
    tsne = TSNE(n_components=2, perplexity=perp, random_state=42)
    X_tsne = tsne.fit_transform(X_pca)
    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=20)
    ax.set_title(f'perplexity={perp}')

plt.tight_layout()
plt.show()
```

**4. è¯å‘é‡å¯è§†åŒ–åº”ç”¨**

```python
# å¯è§†åŒ–è¯å‘é‡ï¼ˆå¦‚Word2Vecã€GloVeï¼‰
import numpy as np

# å‡è®¾word_vectorsæ˜¯300ç»´çš„è¯å‘é‡ï¼Œshape: (vocab_size, 300)
# wordsæ˜¯å¯¹åº”çš„è¯åˆ—è¡¨

# é€‰æ‹©Top 500é«˜é¢‘è¯
top_words = words[:500]
top_vectors = word_vectors[:500]

# t-SNEé™ç»´
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
vectors_2d = tsne.fit_transform(top_vectors)

# å¯è§†åŒ–
plt.figure(figsize=(15, 12))
plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=10, alpha=0.6)

# æ ‡æ³¨éƒ¨åˆ†è¯ï¼ˆé¿å…æ‹¥æŒ¤ï¼‰
for i in range(0, len(top_words), 20):  # æ¯20ä¸ªæ ‡æ³¨ä¸€ä¸ª
    plt.annotate(top_words[i],
                 xy=(vectors_2d[i, 0], vectors_2d[i, 1]),
                 fontsize=9, alpha=0.7)

plt.title('è¯å‘é‡t-SNEå¯è§†åŒ–')
plt.show()
```

**5. ä¸UMAPå¯¹æ¯”ï¼ˆæ›´å¿«çš„æ›¿ä»£æ–¹æ¡ˆï¼‰**

```python
# UMAPæ˜¯t-SNEçš„æ”¹è¿›ç‰ˆï¼šé€Ÿåº¦æ›´å¿«ï¼Œæ•ˆæœç›¸ä¼¼
# pip install umap-learn

import umap

# UMAPé™ç»´ï¼ˆæ¯”t-SNEå¿«10-100å€ï¼‰
reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = reducer.fit_transform(X_pca)

# å¯¹æ¯”å¯è§†åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# t-SNE
ax1.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=20, alpha=0.7)
ax1.set_title('t-SNEï¼ˆæ…¢ä½†ç»å…¸ï¼‰')

# UMAP
ax2.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', s=20, alpha=0.7)
ax2.set_title('UMAPï¼ˆå¿«ä¸”ç°ä»£ï¼‰')

plt.tight_layout()
plt.show()
```

---

## ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šç®—æ³•é€‰æ‹©å†³ç­–çŸ©é˜µ

### æŒ‰æ•°æ®é‡é€‰æ‹©

| æ ·æœ¬é‡ | ç›‘ç£å­¦ä¹ æ¨è | æ— ç›‘ç£å­¦ä¹ æ¨è | é¿å…ä½¿ç”¨ |
|-------|------------|--------------|---------|
| **å°æ ·æœ¬ (<1K)** | Logisticå›å½’<br>æœ´ç´ è´å¶æ–¯<br>KNN | K-Means<br>å±‚æ¬¡èšç±»<br>PCA | æ·±åº¦å­¦ä¹ <br>XGBoost(æ˜“è¿‡æ‹Ÿåˆ)<br>DBSCAN |
| **ä¸­æ ·æœ¬ (1K-100K)** | æ‰€æœ‰ç®—æ³•é€‚ç”¨<br>æ¨èï¼šéšæœºæ£®æ—<br>XGBoost<br>LightGBM | æ‰€æœ‰ç®—æ³•é€‚ç”¨<br>æ¨èï¼šK-Means<br>DBSCAN<br>GMM | - |
| **å¤§æ ·æœ¬ (100K-1M)** | LightGBM<br>Logisticå›å½’<br>çº¿æ€§SVM | Mini-Batch K-Means<br>PCA<br>DBSCAN(æ…¢) | å±‚æ¬¡èšç±»<br>å®Œæ•´K-Means |
| **è¶…å¤§æ ·æœ¬ (>1M)** | LightGBM<br>çº¿æ€§æ¨¡å‹<br>SGDåˆ†ç±»å™¨ | Mini-Batch K-Means<br>åœ¨çº¿PCA | DBSCAN<br>å±‚æ¬¡èšç±»<br>t-SNE<br>GMM |

### æŒ‰é—®é¢˜ç±»å‹é€‰æ‹©

| é—®é¢˜ç±»å‹ | å€™é€‰ç®—æ³•ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ | å¿«é€ŸBaseline | æœ€ä½³æ€§èƒ½ |
|---------|-------------------|-------------|---------|
| **äºŒåˆ†ç±»** | 1. Logisticå›å½’<br>2. éšæœºæ£®æ—<br>3. XGBoost<br>4. LightGBM | Logisticå›å½’ | XGBoost/LightGBM |
| **å¤šåˆ†ç±»** | 1. éšæœºæ£®æ—<br>2. LightGBM<br>3. Logisticå›å½’(OvR) | Logisticå›å½’ | LightGBM |
| **å›å½’** | 1. çº¿æ€§å›å½’<br>2. éšæœºæ£®æ—<br>3. XGBoost<br>4. LightGBM | Ridgeå›å½’ | XGBoost/LightGBM |
| **èšç±»** | 1. K-Means<br>2. DBSCAN<br>3. å±‚æ¬¡èšç±»<br>4. GMM | K-Means | DBSCAN/GMM |
| **é™ç»´** | 1. PCA<br>2. t-SNE<br>3. UMAP | PCA | t-SNE/UMAP |
| **å¼‚å¸¸æ£€æµ‹** | 1. Isolation Forest<br>2. One-Class SVM<br>3. DBSCAN<br>4. GMM | Isolation Forest | Isolation Forest |

### æŒ‰æ€§èƒ½è¦æ±‚é€‰æ‹©

| éœ€æ±‚ | æ¨èç®—æ³• | ä¸æ¨è |
|------|---------|--------|
| **å¯è§£é‡Šæ€§** | çº¿æ€§å›å½’<br>Logisticå›å½’<br>å†³ç­–æ ‘ | XGBoost<br>ç¥ç»ç½‘ç»œ<br>t-SNE |
| **è®­ç»ƒé€Ÿåº¦** | æœ´ç´ è´å¶æ–¯<br>çº¿æ€§æ¨¡å‹<br>K-Means | SVM(å¤§æ•°æ®)<br>DBSCAN<br>å±‚æ¬¡èšç±»<br>t-SNE |
| **é¢„æµ‹é€Ÿåº¦** | çº¿æ€§æ¨¡å‹<br>æ ‘æ¨¡å‹<br>K-Means | KNN<br>SVM(å¤šæ”¯æŒå‘é‡)<br>DBSCAN |
| **å¤„ç†é«˜ç»´** | çº¿æ€§æ¨¡å‹+L1æ­£åˆ™<br>éšæœºæ£®æ—<br>PCA | KNN<br>DBSCAN<br>t-SNE |
| **å¤„ç†éçº¿æ€§** | SVM(RBFæ ¸)<br>éšæœºæ£®æ—<br>XGBoost<br>DBSCAN | çº¿æ€§å›å½’<br>Logisticå›å½’<br>K-Means |
| **é²æ£’æ€§(æŠ—å¼‚å¸¸)** | éšæœºæ£®æ—<br>LightGBM<br>DBSCAN | çº¿æ€§å›å½’<br>K-Means<br>GMM |

### å¿«é€Ÿå‚è€ƒè¡¨

#### ç›‘ç£å­¦ä¹ ç®—æ³•å¿«é€Ÿå¯¹æ¯”

| ç®—æ³• | è®­ç»ƒé€Ÿåº¦ | é¢„æµ‹é€Ÿåº¦ | å¯è§£é‡Šæ€§ | éçº¿æ€§ | é«˜ç»´é€‚ç”¨ | æœ€ä½³åœºæ™¯ |
|------|---------|---------|---------|--------|---------|---------|
| çº¿æ€§å›å½’ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | âŒ | âœ… | çº¿æ€§å…³ç³»å›å½’ |
| Logisticå›å½’ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | âŒ | âœ… | äºŒåˆ†ç±»baseline |
| å†³ç­–æ ‘ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | âœ… | âš ï¸ | å¯è§£é‡Šæ€§è¦æ±‚é«˜ |
| éšæœºæ£®æ— | â­â­â­ | â­â­â­â­ | â­â­â­ | âœ… | âœ… | é€šç”¨åˆ†ç±»/å›å½’ |
| SVM | â­â­ | â­â­â­ | â­â­ | âœ… | âš ï¸ | å°æ ·æœ¬åˆ†ç±» |
| KNN | â­â­â­â­â­ | â­â­ | â­â­â­ | âœ… | âŒ | å°æ•°æ®ç®€å•åˆ†ç±» |
| XGBoost | â­â­â­ | â­â­â­â­ | â­â­â­ | âœ… | âœ… | ç«èµ›/é«˜æ€§èƒ½éœ€æ±‚ |
| LightGBM | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | âœ… | âœ… | å¤§æ•°æ®/é€Ÿåº¦è¦æ±‚ |

#### æ— ç›‘ç£å­¦ä¹ ç®—æ³•å¿«é€Ÿå¯¹æ¯”

| ç®—æ³• | è®­ç»ƒé€Ÿåº¦ | ç°‡æ•° | ä»»æ„å½¢çŠ¶ | å™ªå£°å¤„ç† | é«˜ç»´é€‚ç”¨ | æœ€ä½³åœºæ™¯ |
|------|---------|------|---------|---------|---------|---------|
| K-Means | â­â­â­â­â­ | éœ€æŒ‡å®š | âŒ | âŒ | âœ… | å¿«é€Ÿèšç±»/å¤§æ•°æ® |
| DBSCAN | â­â­ | è‡ªåŠ¨ | âœ… | âœ… | âŒ | ä»»æ„å½¢çŠ¶/æœ‰å™ªå£° |
| å±‚æ¬¡èšç±» | â­â­ | çµæ´» | âš ï¸ | âŒ | âŒ | å±‚æ¬¡ç»“æ„/å°æ•°æ® |
| GMM | â­â­â­ | éœ€æŒ‡å®š | âš ï¸ | âŒ | âš ï¸ | è½¯èšç±»/å¯†åº¦ä¼°è®¡ |
| PCA | â­â­â­â­ | - | - | - | âœ… | çº¿æ€§é™ç»´/åŠ é€Ÿ |
| t-SNE | â­ | - | - | - | âŒ | å¯è§†åŒ–/æ¢ç´¢ |

---

## ğŸ¯ ä½¿ç”¨å»ºè®®æ€»ç»“

### æ–°æ‰‹å…¥é—¨è·¯å¾„

1. **ç›‘ç£å­¦ä¹ **ï¼š
   - å›å½’ä»»åŠ¡ï¼šä»çº¿æ€§å›å½’å¼€å§‹ â†’ Ridgeæ­£åˆ™åŒ– â†’ éšæœºæ£®æ—
   - åˆ†ç±»ä»»åŠ¡ï¼šä»Logisticå›å½’å¼€å§‹ â†’ å†³ç­–æ ‘ â†’ éšæœºæ£®æ— â†’ XGBoost

2. **æ— ç›‘ç£å­¦ä¹ **ï¼š
   - èšç±»ï¼šä»K-Meanså¼€å§‹ â†’ DBSCANï¼ˆå¤æ‚å½¢çŠ¶ï¼‰ â†’ GMMï¼ˆè½¯èšç±»ï¼‰
   - é™ç»´ï¼šä»PCAå¼€å§‹ â†’ t-SNEï¼ˆå¯è§†åŒ–ï¼‰

### å®é™…é¡¹ç›®å»ºè®®æµç¨‹

1. **å»ºç«‹Baseline**ï¼š
   - åˆ†ç±»ï¼šLogisticå›å½’ / å†³ç­–æ ‘
   - å›å½’ï¼šRidgeå›å½’
   - èšç±»ï¼šK-Means

2. **æ€§èƒ½æå‡**ï¼š
   - ç‰¹å¾å·¥ç¨‹ï¼ˆæœ€é‡è¦ï¼ï¼‰
   - å°è¯•é›†æˆæ–¹æ³•ï¼ˆéšæœºæ£®æ—ã€XGBoostï¼‰
   - è¶…å‚æ•°è°ƒä¼˜

3. **æ¨¡å‹é€‰æ‹©**ï¼š
   - æ ¹æ®å†³ç­–çŸ©é˜µé€‰æ‹©3-5ä¸ªå€™é€‰ç®—æ³•
   - äº¤å‰éªŒè¯å¯¹æ¯”
   - ç»¼åˆè€ƒè™‘æ€§èƒ½ã€é€Ÿåº¦ã€å¯è§£é‡Šæ€§

---

**æ–‡æ¡£çŠ¶æ€**ï¼šâœ… å®Œæ•´ç‰ˆ
- ç¬¬ä¸€éƒ¨åˆ†ï¼šç›‘ç£å­¦ä¹  8ä¸ªç®—æ³• âœ…
- ç¬¬äºŒéƒ¨åˆ†ï¼šæ— ç›‘ç£å­¦ä¹  6ä¸ªç®—æ³• âœ…
- ç¬¬ä¸‰éƒ¨åˆ†ï¼šç®—æ³•é€‰æ‹©å†³ç­–çŸ©é˜µ âœ…

**æ€»è®¡**ï¼š2600+è¡Œï¼Œ14ä¸ªç®—æ³•è¯¦è§£ + å†³ç­–çŸ©é˜µ + ä½¿ç”¨å»ºè®®
