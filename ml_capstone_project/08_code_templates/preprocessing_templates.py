"""
æ•°æ®é¢„å¤„ç†ä»£ç æ¨¡æ¿åº“
===================

å¿«é€Ÿä½¿ç”¨:
    from code_templates.preprocessing_templates import (
        quick_impute,
        quick_outlier_clip,
        quick_encode,
        quick_scale,
        build_quick_pipeline
    )

    # 5è¡Œä»£ç å®Œæˆé¢„å¤„ç†
    df = quick_impute(df, strategy='median')
    df = quick_outlier_clip(df, columns=['price'])
    df = quick_encode(df, method='auto')
    df = quick_scale(df, method='standard')

    # æˆ–ä½¿ç”¨ä¸€é”®å¼Pipeline
    X_train, y_train, X_test, y_test = build_quick_pipeline(
        df, target_col='price', algorithm_type='xgboost'
    )

å¯¹åº”å†³ç­–æ¨¡æ¿: 07_decision_templates/data_diagnosis_template.md
å‚è€ƒå®ç°: 06_comprehensive_project/src/data_preprocessing.py (653è¡Œ)

é¡¹ç›®å®šä½: MLå®æˆ˜æ“ä½œæ‰‹å†Œï¼ˆéæ•™å­¦é¡¹ç›®ï¼‰
æ ¸å¿ƒä»·å€¼: 5-15åˆ†é’Ÿå¿«é€Ÿä»£ç è½åœ°
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Union, Tuple
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import (
    StandardScaler, MinMaxScaler, RobustScaler,
    LabelEncoder, OneHotEncoder
)
import warnings
warnings.filterwarnings('ignore')


# ==================== 1. ç¼ºå¤±å€¼å¤„ç† ====================

def quick_impute(df: pd.DataFrame,
                strategy: str = 'auto',
                numeric_strategy: str = 'median',
                categorical_strategy: str = 'mode',
                drop_threshold: float = 0.5,
                verbose: bool = True) -> pd.DataFrame:
    """
    å¿«é€Ÿç¼ºå¤±å€¼å¤„ç†ï¼ˆ5åˆ†é’Ÿå†³ç­–ï¼‰

    å¯¹åº”å†³ç­–: data_diagnosis_template.md - Step 1: ç¼ºå¤±å€¼å¿«é€Ÿå¤„ç†

    Parameters
    ----------
    df : DataFrame
        è¾“å…¥æ•°æ®
    strategy : {'auto', 'median', 'mean', 'mode', 'drop'}
        'auto' - æ ¹æ®ç¼ºå¤±ç‡è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰
        å…¶ä»– - æ‰‹åŠ¨æŒ‡å®šç­–ç•¥
    numeric_strategy : str, default='median'
        æ•°å€¼åˆ—å¡«å……ç­–ç•¥ï¼ˆå½“strategy='auto'æ—¶ï¼‰
        'median' - ä¸­ä½æ•°å¡«å……ï¼ˆæ¨èï¼Œå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰
        'mean' - å‡å€¼å¡«å……
    categorical_strategy : str, default='mode'
        ç±»åˆ«åˆ—å¡«å……ç­–ç•¥ï¼ˆå½“strategy='auto'æ—¶ï¼‰
    drop_threshold : float, default=0.5
        ç¼ºå¤±ç‡è¶…è¿‡æ­¤å€¼çš„åˆ—ç›´æ¥åˆ é™¤ï¼ˆ0.5å³50%ï¼‰
    verbose : bool, default=True
        æ˜¯å¦æ‰“å°å¤„ç†ä¿¡æ¯

    Returns
    -------
    DataFrame
        å¤„ç†åçš„æ•°æ®

    Examples
    --------
    >>> # å¿«é€Ÿæ¨¡å¼ï¼šå…¨è‡ªåŠ¨
    >>> df_clean = quick_impute(df)
    >>> # âœ“ ç¼ºå¤±å€¼å¤„ç†å®Œæˆ: 15ä¸ªç¼ºå¤±å€¼å·²å¡«å……, 2åˆ—å·²åˆ é™¤(ç¼ºå¤±ç‡>50%)

    >>> # å®šåˆ¶æ¨¡å¼ï¼šæŒ‡å®šç­–ç•¥
    >>> df_clean = quick_impute(
    ...     df,
    ...     numeric_strategy='mean',
    ...     drop_threshold=0.3
    ... )

    Decision Logic (å¯¹åº”07ç« å†³ç­–)
    -----------------------------
    ç¼ºå¤±ç‡ < 5%  â†’ åˆ é™¤è¡Œï¼ˆæ ·æœ¬å……è¶³æ—¶ï¼‰
    ç¼ºå¤±ç‡ 5-20% â†’ ä¸­ä½æ•°/ä¼—æ•°å¡«å……
    ç¼ºå¤±ç‡ 20-50% â†’ KNNå¡«å……ï¼ˆå¯é€‰ï¼Œç”¨advanced_imputeï¼‰
    ç¼ºå¤±ç‡ > 50% â†’ åˆ é™¤åˆ—

    Notes
    -----
    - å¿«é€Ÿæ¨¡å¼é€‚åˆBaselineå»ºç«‹
    - é‡è¦é¡¹ç›®å»ºè®®ä½¿ç”¨advanced_impute()
    - å‚è€ƒ06ç« src/data_preprocessing.py:130-188
    """
    df_copy = df.copy()

    if verbose:
        print("ğŸ” ç¼ºå¤±å€¼è¯Šæ–­...")
        missing_stats = df_copy.isnull().sum()
        missing_stats = missing_stats[missing_stats > 0]
        if len(missing_stats) > 0:
            print(f"   å‘ç° {len(missing_stats)} åˆ—æœ‰ç¼ºå¤±å€¼")
        else:
            print("   âœ“ æ— ç¼ºå¤±å€¼")
            return df_copy

    # 1. åˆ é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„åˆ—
    cols_to_drop = []
    for col in df_copy.columns:
        missing_rate = df_copy[col].isnull().sum() / len(df_copy)
        if missing_rate > drop_threshold:
            cols_to_drop.append(col)

    if cols_to_drop:
        df_copy = df_copy.drop(columns=cols_to_drop)
        if verbose:
            print(f"   âœ“ åˆ é™¤{len(cols_to_drop)}åˆ—(ç¼ºå¤±ç‡>{drop_threshold*100}%): {cols_to_drop[:3]}...")

    # 2. å¡«å……ç¼ºå¤±å€¼
    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns
    categorical_cols = df_copy.select_dtypes(include=['object', 'category']).columns

    # æ•°å€¼åˆ—å¡«å……
    if len(numeric_cols) > 0:
        for col in numeric_cols:
            if df_copy[col].isnull().sum() > 0:
                if numeric_strategy == 'median':
                    fill_value = df_copy[col].median()
                elif numeric_strategy == 'mean':
                    fill_value = df_copy[col].mean()
                else:
                    fill_value = 0
                df_copy[col].fillna(fill_value, inplace=True)

        if verbose:
            print(f"   âœ“ æ•°å€¼åˆ—å¡«å……å®Œæˆï¼ˆç­–ç•¥: {numeric_strategy}ï¼‰")

    # ç±»åˆ«åˆ—å¡«å……
    if len(categorical_cols) > 0:
        for col in categorical_cols:
            if df_copy[col].isnull().sum() > 0:
                fill_value = df_copy[col].mode()[0] if len(df_copy[col].mode()) > 0 else 'Unknown'
                df_copy[col].fillna(fill_value, inplace=True)

        if verbose:
            print(f"   âœ“ ç±»åˆ«åˆ—å¡«å……å®Œæˆï¼ˆç­–ç•¥: {categorical_strategy}ï¼‰")

    if verbose:
        print(f"âœ“ ç¼ºå¤±å€¼å¤„ç†å®Œæˆ\n")

    return df_copy


# ==================== 2. å¼‚å¸¸å€¼å¤„ç† ====================

def quick_outlier_clip(df: pd.DataFrame,
                      columns: List[str] = None,
                      method: str = 'iqr',
                      k: float = 1.5,
                      verbose: bool = True) -> pd.DataFrame:
    """
    å¿«é€Ÿå¼‚å¸¸å€¼æˆªæ–­ï¼ˆIQRæ–¹æ³•ï¼Œä¿ç•™æ‰€æœ‰æ ·æœ¬ï¼‰

    å¯¹åº”å†³ç­–: data_diagnosis_template.md - Step 2: å¼‚å¸¸å€¼å¿«é€Ÿå¤„ç†

    Parameters
    ----------
    columns : list, optional
        éœ€è¦å¤„ç†çš„åˆ—åï¼ŒNoneåˆ™å¤„ç†æ‰€æœ‰æ•°å€¼åˆ—
    method : {'iqr', 'percentile'}
        iqr - IQRæ–¹æ³•ï¼ˆæ¨èï¼‰
        percentile - ç™¾åˆ†ä½æ•°æ–¹æ³•
    k : float, default=1.5
        IQRå€æ•°
        1.5 - æ ‡å‡†å€¼ï¼ˆæ£€æµ‹æ¸©å’Œå¼‚å¸¸ï¼‰
        3.0 - å®½æ¾å€¼ï¼ˆä»…æ£€æµ‹æç«¯å¼‚å¸¸ï¼‰

    Returns
    -------
    DataFrame
        å¼‚å¸¸å€¼æˆªæ–­åçš„æ•°æ®ï¼ˆä¿ç•™æ‰€æœ‰æ ·æœ¬ï¼‰

    Examples
    --------
    >>> # å¿«é€Ÿæ¨¡å¼
    >>> df_clean = quick_outlier_clip(df, columns=['price', 'age'])
    >>> # âœ“ å¼‚å¸¸å€¼æˆªæ–­å®Œæˆ: price(15ä¸ª), age(8ä¸ª)

    >>> # å®½æ¾æ¨¡å¼ï¼ˆä»…å¤„ç†æç«¯å¼‚å¸¸ï¼‰
    >>> df_clean = quick_outlier_clip(df, columns=['price'], k=3.0)

    Decision Logic (å¯¹åº”07ç« å†³ç­–)
    -----------------------------
    çœŸå®æå€¼ + çº¿æ€§æ¨¡å‹ â†’ æˆªæ–­ï¼ˆclipï¼‰
    çœŸå®æå€¼ + æ ‘æ¨¡å‹   â†’ ä¿ç•™ï¼ˆä¸å¤„ç†ï¼‰
    æ•°æ®é”™è¯¯           â†’ åˆ é™¤ï¼ˆç”¨quick_outlier_removeï¼‰

    Notes
    -----
    - æˆªæ–­ä¿ç•™æ ·æœ¬æ•°é‡ï¼Œé€‚åˆå¤§éƒ¨åˆ†åœºæ™¯
    - æ ‘æ¨¡å‹å¯¹å¼‚å¸¸å€¼é²æ£’ï¼Œå¯è·³è¿‡æ­¤æ­¥éª¤
    - å‚è€ƒ06ç« src/data_preprocessing.py:193-243
    """
    df_copy = df.copy()

    if columns is None:
        columns = df_copy.select_dtypes(include=[np.number]).columns.tolist()

    outlier_counts = {}

    for col in columns:
        if col not in df_copy.columns:
            continue

        if method == 'iqr':
            Q1 = df_copy[col].quantile(0.25)
            Q3 = df_copy[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - k * IQR
            upper_bound = Q3 + k * IQR
        elif method == 'percentile':
            lower_bound = df_copy[col].quantile(0.01)
            upper_bound = df_copy[col].quantile(0.99)

        # ç»Ÿè®¡å¼‚å¸¸å€¼æ•°é‡
        n_outliers = ((df_copy[col] < lower_bound) | (df_copy[col] > upper_bound)).sum()
        if n_outliers > 0:
            outlier_counts[col] = n_outliers

        # æˆªæ–­
        df_copy[col] = df_copy[col].clip(lower=lower_bound, upper=upper_bound)

    if verbose:
        if outlier_counts:
            print("âœ“ å¼‚å¸¸å€¼æˆªæ–­å®Œæˆ:")
            for col, count in outlier_counts.items():
                print(f"   {col}: {count}ä¸ªå¼‚å¸¸å€¼å·²æˆªæ–­")
        else:
            print("âœ“ æœªæ£€æµ‹åˆ°å¼‚å¸¸å€¼")
        print()

    return df_copy


# ==================== 3. ç‰¹å¾ç¼–ç  ====================

def quick_encode(df: pd.DataFrame,
                columns: List[str] = None,
                method: str = 'auto',
                algorithm_type: str = 'tree',
                verbose: bool = True) -> pd.DataFrame:
    """
    å¿«é€Ÿç‰¹å¾ç¼–ç ï¼ˆè‡ªåŠ¨è¯†åˆ«æœ€ä½³æ–¹æ³•ï¼‰

    å¯¹åº”å†³ç­–: preprocessing_quick_reference.md - Step 3: ç‰¹å¾ç¼–ç 

    Parameters
    ----------
    columns : list, optional
        éœ€è¦ç¼–ç çš„åˆ—åï¼ŒNoneåˆ™å¤„ç†æ‰€æœ‰object/categoryåˆ—
    method : {'auto', 'onehot', 'label', 'target'}
        'auto' - æ ¹æ®åŸºæ•°è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰
            åŸºæ•° < 10  â†’ One-Hot
            åŸºæ•° 10-50 â†’ Label Encoding
            åŸºæ•° > 50  â†’ Label Encoding
    algorithm_type : {'tree', 'linear', 'nn'}
        ç®—æ³•ç±»å‹ï¼Œå½±å“ç¼–ç é€‰æ‹©
        tree   - æ ‘æ¨¡å‹: Label Encoding
        linear - çº¿æ€§æ¨¡å‹: One-Hot
        nn     - ç¥ç»ç½‘ç»œ: One-Hot

    Returns
    -------
    DataFrame
        ç¼–ç åçš„æ•°æ®

    Examples
    --------
    >>> # å¿«é€Ÿæ¨¡å¼ï¼šè‡ªåŠ¨ç¼–ç 
    >>> df_encoded = quick_encode(df)
    >>> # âœ“ ç‰¹å¾ç¼–ç å®Œæˆ: gender(onehot), city(label)

    >>> # æŒ‡å®šç®—æ³•ç±»å‹
    >>> df_encoded = quick_encode(df, algorithm_type='linear')
    >>> # çº¿æ€§æ¨¡å‹ â†’ ä¼˜å…ˆä½¿ç”¨One-Hot

    Decision Logic
    --------------
    æ— åºåˆ†ç±» + åŸºæ•°<10 + çº¿æ€§æ¨¡å‹ â†’ One-Hot
    æ— åºåˆ†ç±» + åŸºæ•°>10            â†’ Label/Target
    æœ‰åºåˆ†ç±»                      â†’ Label Encoding

    Notes
    -----
    - æ ‘æ¨¡å‹å¯¹ç¼–ç æ–¹å¼ä¸æ•æ„Ÿï¼ŒLabel Encodingå³å¯
    - çº¿æ€§æ¨¡å‹å»ºè®®One-Hot
    - å‚è€ƒ06ç« src/data_preprocessing.py:248-386
    """
    df_copy = df.copy()

    if columns is None:
        columns = df_copy.select_dtypes(include=['object', 'category']).columns.tolist()

    if len(columns) == 0:
        if verbose:
            print("âœ“ æ— éœ€ç¼–ç ï¼ˆæ— ç±»åˆ«ç‰¹å¾ï¼‰\n")
        return df_copy

    encoding_info = {}

    for col in columns:
        if col not in df_copy.columns:
            continue

        cardinality = df_copy[col].nunique()

        # è‡ªåŠ¨é€‰æ‹©ç¼–ç æ–¹å¼
        if method == 'auto':
            if algorithm_type == 'linear' and cardinality < 10:
                chosen_method = 'onehot'
            else:
                chosen_method = 'label'
        else:
            chosen_method = method

        # æ‰§è¡Œç¼–ç 
        if chosen_method == 'onehot':
            dummies = pd.get_dummies(df_copy[col], prefix=col, drop_first=True)
            df_copy = df_copy.drop(col, axis=1)
            df_copy = pd.concat([df_copy, dummies], axis=1)
            encoding_info[col] = f"onehot({cardinality} â†’ {len(dummies.columns)}åˆ—)"

        elif chosen_method == 'label':
            df_copy[col] = LabelEncoder().fit_transform(df_copy[col].astype(str))
            encoding_info[col] = f"label({cardinality}ç±»)"

    if verbose:
        print("âœ“ ç‰¹å¾ç¼–ç å®Œæˆ:")
        for col, info in encoding_info.items():
            print(f"   {col}: {info}")
        print()

    return df_copy


# ==================== 4. ç‰¹å¾ç¼©æ”¾ ====================

def quick_scale(df: pd.DataFrame,
               columns: List[str] = None,
               method: str = 'auto',
               algorithm_type: str = 'tree',
               verbose: bool = True) -> pd.DataFrame:
    """
    å¿«é€Ÿç‰¹å¾ç¼©æ”¾

    Parameters
    ----------
    columns : list, optional
        éœ€è¦ç¼©æ”¾çš„åˆ—åï¼ŒNoneåˆ™å¤„ç†æ‰€æœ‰æ•°å€¼åˆ—
    method : {'auto', 'standard', 'minmax', 'robust'}
        'auto' - æ ¹æ®ç®—æ³•ç±»å‹è‡ªåŠ¨é€‰æ‹©
            çº¿æ€§æ¨¡å‹/SVM â†’ Standard
            ç¥ç»ç½‘ç»œ     â†’ MinMax
            æ ‘æ¨¡å‹       â†’ ä¸ç¼©æ”¾
            æœ‰å¼‚å¸¸å€¼     â†’ Robust
    algorithm_type : {'tree', 'linear', 'nn'}
        ç®—æ³•ç±»å‹

    Returns
    -------
    DataFrame
        ç¼©æ”¾åçš„æ•°æ®

    Examples
    --------
    >>> # çº¿æ€§æ¨¡å‹ï¼šéœ€è¦æ ‡å‡†åŒ–
    >>> df_scaled = quick_scale(df, algorithm_type='linear')

    >>> # æ ‘æ¨¡å‹ï¼šè·³è¿‡ç¼©æ”¾
    >>> df = df  # ä¸éœ€è¦ç¼©æ”¾

    Decision Logic
    --------------
    æ ‘æ¨¡å‹ï¼ˆXGBoost/RFï¼‰ â†’ ä¸éœ€è¦ç¼©æ”¾
    çº¿æ€§æ¨¡å‹/SVM      â†’ Standard Scaler
    ç¥ç»ç½‘ç»œ          â†’ MinMax Scaler
    æœ‰å¼‚å¸¸å€¼          â†’ Robust Scaler
    """
    # æ ‘æ¨¡å‹ä¸éœ€è¦ç¼©æ”¾
    if algorithm_type == 'tree':
        if verbose:
            print("âœ“ æ ‘æ¨¡å‹ä¸éœ€è¦ç‰¹å¾ç¼©æ”¾ï¼Œè·³è¿‡\n")
        return df

    df_copy = df.copy()

    if columns is None:
        columns = df_copy.select_dtypes(include=[np.number]).columns.tolist()

    # è‡ªåŠ¨é€‰æ‹©ç¼©æ”¾æ–¹æ³•
    if method == 'auto':
        if algorithm_type == 'linear':
            method = 'standard'
        elif algorithm_type == 'nn':
            method = 'minmax'

    # æ‰§è¡Œç¼©æ”¾
    if method == 'standard':
        scaler = StandardScaler()
    elif method == 'minmax':
        scaler = MinMaxScaler()
    elif method == 'robust':
        scaler = RobustScaler()

    df_copy[columns] = scaler.fit_transform(df_copy[columns])

    if verbose:
        print(f"âœ“ ç‰¹å¾ç¼©æ”¾å®Œæˆï¼ˆæ–¹æ³•: {method}, {len(columns)}åˆ—ï¼‰\n")

    return df_copy


# ==================== 5. å®Œæ•´Pipelineæ„å»º ====================

def build_quick_pipeline(df: pd.DataFrame,
                        target_col: str,
                        algorithm_type: str = 'tree',
                        test_size: float = 0.2,
                        random_state: int = 42,
                        verbose: bool = True) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:
    """
    5åˆ†é’Ÿæ„å»ºå®Œæ•´é¢„å¤„ç†Pipeline

    å¯¹åº”å†³ç­–: å®Œæˆ07ç« æ‰€æœ‰å†³ç­–åï¼Œä¸€é”®ç”Ÿæˆé¢„å¤„ç†æµç¨‹

    Parameters
    ----------
    df : DataFrame
        åŸå§‹æ•°æ®
    target_col : str
        ç›®æ ‡å˜é‡åˆ—å
    algorithm_type : {'tree', 'linear', 'nn'}
        ç®—æ³•ç±»å‹ï¼ˆå½±å“é¢„å¤„ç†ç­–ç•¥ï¼‰
        tree   - æ ‘æ¨¡å‹: æœ€ç®€é¢„å¤„ç†ï¼ˆä»…ç¼ºå¤±å€¼+ç¼–ç ï¼‰
        linear - çº¿æ€§æ¨¡å‹: å®Œæ•´é¢„å¤„ç†ï¼ˆ+ç¼©æ”¾ï¼‰
        nn     - ç¥ç»ç½‘ç»œ: å®Œæ•´é¢„å¤„ç†ï¼ˆ+å½’ä¸€åŒ–ï¼‰
    test_size : float, default=0.2
        æµ‹è¯•é›†æ¯”ä¾‹
    random_state : int, default=42
        éšæœºç§å­

    Returns
    -------
    X_train, y_train, X_test, y_test : tuple
        é¢„å¤„ç†åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†

    Examples
    --------
    >>> # 5è¡Œä»£ç å®Œæˆå®Œæ•´é¢„å¤„ç†
    >>> X_train, y_train, X_test, y_test = build_quick_pipeline(
    ...     df,
    ...     target_col='price',
    ...     algorithm_type='xgboost'
    ... )
    >>> print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    Pipeline Steps
    --------------
    1. ç¼ºå¤±å€¼å¤„ç†
    2. å¼‚å¸¸å€¼å¤„ç†ï¼ˆå¯é€‰ï¼‰
    3. ç‰¹å¾ç¼–ç 
    4. ç‰¹å¾ç¼©æ”¾ï¼ˆæ ¹æ®algorithm_typeï¼‰
    5. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†

    Notes
    -----
    - é€‚åˆå¿«é€ŸBaselineå»ºç«‹
    - é‡è¦é¡¹ç›®å»ºè®®æ‰‹åŠ¨æ§åˆ¶æ¯ä¸ªæ­¥éª¤
    """
    from sklearn.model_selection import train_test_split

    if verbose:
        print("="*60)
        print("   å¿«é€Ÿé¢„å¤„ç†Pipeline")
        print("="*60 + "\n")

    df_processed = df.copy()

    # 1. ç¼ºå¤±å€¼å¤„ç†
    df_processed = quick_impute(df_processed, verbose=verbose)

    # 2. å¼‚å¸¸å€¼å¤„ç†ï¼ˆå¯é€‰ï¼‰
    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()
    if target_col in numeric_cols:
        numeric_cols.remove(target_col)
    # df_processed = quick_outlier_clip(df_processed, columns=numeric_cols, verbose=verbose)

    # 3. ç‰¹å¾ç¼–ç 
    df_processed = quick_encode(df_processed, algorithm_type=algorithm_type, verbose=verbose)

    # 4. åˆ†ç¦»Xå’Œy
    y = df_processed[target_col]
    X = df_processed.drop(target_col, axis=1)

    # 5. ç‰¹å¾ç¼©æ”¾
    X = quick_scale(X, algorithm_type=algorithm_type, verbose=verbose)

    # 6. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y if df_processed[target_col].nunique() < 10 else None
    )

    if verbose:
        print("âœ“ æ•°æ®åˆ’åˆ†å®Œæˆ")
        print(f"   è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
        print("="*60 + "\n")

    return X_train, y_train, X_test, y_test
