"""
æ¨¡å‹è®­ç»ƒä»£ç æ¨¡æ¿åº“
=================

å¿«é€Ÿä½¿ç”¨:
    from code_templates.modeling_templates import (
        quick_train,
        quick_baseline_comparison,
        quick_tune,
        get_default_param_space
    )

    # 5è¡Œä»£ç å®Œæˆå»ºæ¨¡
    model, metrics = quick_train(X, y, algorithm='xgboost')

    # å¯¹æ¯”å¤šä¸ªç®—æ³•
    results = quick_baseline_comparison(X, y, algorithms=['rf', 'xgb', 'lgb'])

    # è¶…å‚æ•°è°ƒä¼˜
    best_model = quick_tune(X, y, algorithm='xgboost', method='grid')

å¯¹åº”å†³ç­–æ¨¡æ¿: 07_decision_templates/algorithm_selection_template.md
               07_decision_templates/hyperparameter_tuning_template.md
å‚è€ƒå®ç°: 06_comprehensive_project/src/supervised_pipeline.py (475è¡Œ)

é¡¹ç›®å®šä½: MLå®æˆ˜æ“ä½œæ‰‹å†Œï¼ˆéæ•™å­¦é¡¹ç›®ï¼‰
æ ¸å¿ƒä»·å€¼: 5-15åˆ†é’Ÿå¿«é€Ÿä»£ç è½åœ°
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple, Any
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier
from sklearn.svm import SVC, SVR
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥XGBoostå’ŒLightGBMï¼ˆå¦‚æœå·²å®‰è£…ï¼‰
try:
    from xgboost import XGBClassifier, XGBRegressor
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    from lightgbm import LGBMClassifier, LGBMRegressor
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False


# ==================== 1. å¿«é€Ÿè®­ç»ƒ ====================

def quick_train(X: pd.DataFrame,
               y: pd.Series,
               algorithm: str = 'auto',
               problem_type: str = 'auto',
               test_size: float = 0.2,
               random_state: int = 42,
               verbose: bool = True) -> Tuple[Any, Dict[str, float]]:
    """
    å¿«é€Ÿè®­ç»ƒå•ä¸ªæ¨¡å‹ï¼ˆ5åˆ†é’Ÿï¼‰

    å¯¹åº”å†³ç­–: algorithm_selection_template.md - æ¨èç®—æ³•

    Parameters
    ----------
    X : DataFrame
        ç‰¹å¾æ•°æ®
    y : Series
        ç›®æ ‡å˜é‡
    algorithm : str, default='auto'
        ç®—æ³•é€‰æ‹©
        'auto' - è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰
        'rf' - éšæœºæ£®æ—
        'xgboost' - XGBoost
        'lightgbm' - LightGBM
        'logistic' - é€»è¾‘å›å½’
        'svm' - æ”¯æŒå‘é‡æœº
        'knn' - Kè¿‘é‚»
    problem_type : {'auto', 'classification', 'regression'}
        é—®é¢˜ç±»å‹ï¼Œautoè‡ªåŠ¨è¯†åˆ«
    test_size : float, default=0.2
        æµ‹è¯•é›†æ¯”ä¾‹
    random_state : int, default=42
        éšæœºç§å­
    verbose : bool, default=True
        æ˜¯å¦æ‰“å°ä¿¡æ¯

    Returns
    -------
    model, metrics : tuple
        è®­ç»ƒå¥½çš„æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡

    Examples
    --------
    >>> # å¿«é€Ÿæ¨¡å¼ï¼šè‡ªåŠ¨é€‰æ‹©ç®—æ³•
    >>> model, metrics = quick_train(X, y, algorithm='auto')
    >>> # âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ: XGBoost, AUC=0.85, F1=0.78

    >>> # æŒ‡å®šç®—æ³•
    >>> model, metrics = quick_train(X, y, algorithm='xgboost')

    Decision Logic
    --------------
    autoæ¨¡å¼é€‰æ‹©ç­–ç•¥:
    - åˆ†ç±»é—®é¢˜ â†’ XGBoost/RandomForest
    - å›å½’é—®é¢˜ â†’ XGBoost/RandomForest
    - æ ·æœ¬<1000 â†’ KNN/RandomForest
    - æ ·æœ¬>100K â†’ LightGBM

    Notes
    -----
    - é€‚åˆå¿«é€ŸBaselineå»ºç«‹
    - å‚è€ƒ06ç« src/supervised_pipeline.py:85-156
    """
    # è‡ªåŠ¨è¯†åˆ«é—®é¢˜ç±»å‹
    if problem_type == 'auto':
        if y.nunique() <= 10:
            problem_type = 'classification'
        else:
            problem_type = 'regression'

    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    if verbose:
        print("ğŸš€ å¿«é€Ÿè®­ç»ƒæ¨¡å‹...")
        print(f"   é—®é¢˜ç±»å‹: {problem_type}")
        print(f"   è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    # è‡ªåŠ¨é€‰æ‹©ç®—æ³•
    if algorithm == 'auto':
        if len(X_train) < 1000:
            algorithm = 'rf'
        elif XGBOOST_AVAILABLE:
            algorithm = 'xgboost'
        else:
            algorithm = 'rf'

    # åˆ›å»ºæ¨¡å‹
    if problem_type == 'classification':
        if algorithm == 'rf':
            model = RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=-1)
        elif algorithm == 'xgboost' and XGBOOST_AVAILABLE:
            model = XGBClassifier(n_estimators=100, random_state=random_state, n_jobs=-1, verbosity=0)
        elif algorithm == 'lightgbm' and LIGHTGBM_AVAILABLE:
            model = LGBMClassifier(n_estimators=100, random_state=random_state, n_jobs=-1, verbosity=-1)
        elif algorithm == 'logistic':
            model = LogisticRegression(random_state=random_state, max_iter=1000)
        elif algorithm == 'knn':
            model = KNeighborsClassifier()
        else:
            model = RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=-1)

    else:  # regression
        if algorithm == 'rf':
            model = RandomForestRegressor(n_estimators=100, random_state=random_state, n_jobs=-1)
        elif algorithm == 'xgboost' and XGBOOST_AVAILABLE:
            model = XGBRegressor(n_estimators=100, random_state=random_state, n_jobs=-1, verbosity=0)
        elif algorithm == 'lightgbm' and LIGHTGBM_AVAILABLE:
            model = LGBMRegressor(n_estimators=100, random_state=random_state, n_jobs=-1, verbosity=-1)
        elif algorithm == 'ridge':
            model = Ridge(random_state=random_state)
        else:
            model = RandomForestRegressor(n_estimators=100, random_state=random_state, n_jobs=-1)

    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train)

    # è¯„ä¼°æ¨¡å‹
    y_pred = model.predict(X_test)

    if problem_type == 'classification':
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred, average='weighted')
        }
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_test)
            if y_proba.shape[1] == 2:
                metrics['auc'] = roc_auc_score(y_test, y_proba[:, 1])
    else:
        metrics = {
            'mse': mean_squared_error(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'r2': r2_score(y_test, y_pred)
        }

    if verbose:
        print(f"âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ: {algorithm.upper()}")
        print("   æµ‹è¯•é›†æŒ‡æ ‡:")
        for metric_name, value in metrics.items():
            print(f"   {metric_name.upper()}: {value:.4f}")
        print()

    return model, metrics


def quick_baseline_comparison(X: pd.DataFrame,
                              y: pd.Series,
                              algorithms: List[str] = ['rf', 'xgboost', 'lightgbm'],
                              problem_type: str = 'auto',
                              test_size: float = 0.2,
                              random_state: int = 42,
                              verbose: bool = True) -> Dict[str, Dict]:
    """
    å¿«é€Ÿå¯¹æ¯”å¤šä¸ªç®—æ³•ï¼ˆ10-15åˆ†é’Ÿï¼‰

    å¯¹åº”å†³ç­–: algorithm_selection_template.md - æ¨èç®—æ³•Top 3

    Parameters
    ----------
    algorithms : list, default=['rf', 'xgboost', 'lightgbm']
        ç®—æ³•åˆ—è¡¨
    problem_type : {'auto', 'classification', 'regression'}
        é—®é¢˜ç±»å‹
    test_size : float, default=0.2
        æµ‹è¯•é›†æ¯”ä¾‹
    random_state : int, default=42
        éšæœºç§å­
    verbose : bool, default=True
        æ˜¯å¦æ‰“å°å¯¹æ¯”ç»“æœ

    Returns
    -------
    Dict[str, Dict]
        æ¯ä¸ªç®—æ³•çš„æ¨¡å‹å’ŒæŒ‡æ ‡
        {
            'rf': {'model': model, 'metrics': {...}},
            'xgboost': {'model': model, 'metrics': {...}},
            ...
        }

    Examples
    --------
    >>> # å¯¹æ¯”3ä¸ªç®—æ³•
    >>> results = quick_baseline_comparison(
    ...     X, y,
    ...     algorithms=['rf', 'xgboost', 'logistic']
    ... )
    >>> # âœ“ Baselineå¯¹æ¯”å®Œæˆ
    >>> #   RandomForest: AUC=0.85, F1=0.78
    >>> #   XGBoost:      AUC=0.87, F1=0.80  â† æœ€ä½³
    >>> #   Logistic:     AUC=0.75, F1=0.72

    >>> # è·å–æœ€ä½³æ¨¡å‹
    >>> best_algo = max(results.items(), key=lambda x: x[1]['metrics'].get('auc', 0))
    >>> best_model = best_algo[1]['model']

    Notes
    -----
    - é€‚åˆå¿«é€Ÿç­›é€‰æœ€ä½³ç®—æ³•
    - å‚è€ƒ06ç« src/supervised_pipeline.py:160-245
    """
    if verbose:
        print("="*60)
        print("   Baselineç®—æ³•å¯¹æ¯”")
        print("="*60 + "\n")

    results = {}

    for algo in algorithms:
        try:
            model, metrics = quick_train(
                X, y,
                algorithm=algo,
                problem_type=problem_type,
                test_size=test_size,
                random_state=random_state,
                verbose=False
            )
            results[algo] = {
                'model': model,
                'metrics': metrics
            }
        except Exception as e:
            if verbose:
                print(f"âš ï¸  {algo}è®­ç»ƒå¤±è´¥: {str(e)}")

    if verbose:
        print("="*60)
        print("   å¯¹æ¯”ç»“æœ")
        print("="*60)

        # è‡ªåŠ¨è¯†åˆ«é—®é¢˜ç±»å‹
        if problem_type == 'auto':
            if y.nunique() <= 10:
                problem_type = 'classification'
            else:
                problem_type = 'regression'

        # æ‰“å°å¯¹æ¯”è¡¨
        if problem_type == 'classification':
            print(f"{'ç®—æ³•':<15} {'Accuracy':<12} {'F1':<12} {'AUC':<12}")
            print("-"*60)
            for algo, result in results.items():
                metrics = result['metrics']
                print(f"{algo:<15} {metrics.get('accuracy', 0):<12.4f} "
                      f"{metrics.get('f1', 0):<12.4f} {metrics.get('auc', 0):<12.4f}")
        else:
            print(f"{'ç®—æ³•':<15} {'RMSE':<12} {'R2':<12}")
            print("-"*60)
            for algo, result in results.items():
                metrics = result['metrics']
                print(f"{algo:<15} {metrics.get('rmse', 0):<12.4f} "
                      f"{metrics.get('r2', 0):<12.4f}")

        print("="*60 + "\n")

    return results


# ==================== 2. è¶…å‚æ•°è°ƒä¼˜ ====================

def get_default_param_space(algorithm: str) -> Dict[str, List]:
    """
    è·å–é»˜è®¤å‚æ•°ç©ºé—´ï¼ˆåŸºäºç»éªŒå’Œ03ç« ç®—æ³•å¯¹æ¯”è¡¨ï¼‰

    å¯¹åº”å†³ç­–: hyperparameter_tuning_template.md - å‚æ•°ç©ºé—´é€ŸæŸ¥è¡¨

    Parameters
    ----------
    algorithm : str
        ç®—æ³•åç§°

    Returns
    -------
    Dict[str, List]
        å‚æ•°ç©ºé—´

    Examples
    --------
    >>> param_space = get_default_param_space('xgboost')
    >>> # {'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.3], ...}
    """
    param_spaces = {
        'rf': {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        },
        'xgboost': {
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.3],
            'n_estimators': [50, 100, 200],
            'subsample': [0.7, 0.8, 0.9, 1.0],
            'colsample_bytree': [0.7, 0.8, 0.9, 1.0]
        },
        'lightgbm': {
            'max_depth': [3, 5, 7, -1],
            'learning_rate': [0.01, 0.1, 0.3],
            'n_estimators': [50, 100, 200],
            'num_leaves': [15, 31, 63],
            'subsample': [0.7, 0.8, 0.9, 1.0]
        },
        'logistic': {
            'C': [0.001, 0.01, 0.1, 1, 10, 100],
            'penalty': ['l1', 'l2']
        },
        'svm': {
            'C': [0.1, 1, 10, 100],
            'kernel': ['linear', 'rbf'],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]
        }
    }

    return param_spaces.get(algorithm, {})


def quick_tune(X: pd.DataFrame,
              y: pd.Series,
              algorithm: str = 'xgboost',
              method: str = 'grid',
              param_space: Dict = None,
              cv: int = 5,
              n_iter: int = 20,
              random_state: int = 42,
              verbose: bool = True) -> Any:
    """
    å¿«é€Ÿè¶…å‚æ•°è°ƒä¼˜ï¼ˆ15-30åˆ†é’Ÿï¼‰

    å¯¹åº”å†³ç­–: hyperparameter_tuning_template.md - è°ƒä¼˜ç­–ç•¥é€‰æ‹©

    Parameters
    ----------
    algorithm : str
        ç®—æ³•åç§°
    method : {'grid', 'random'}
        è°ƒä¼˜æ–¹æ³•
        grid   - ç½‘æ ¼æœç´¢ï¼ˆå‚æ•°ç©ºé—´å°æ—¶ï¼‰
        random - éšæœºæœç´¢ï¼ˆå‚æ•°ç©ºé—´å¤§æ—¶ï¼‰
    param_space : dict, optional
        å‚æ•°ç©ºé—´ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤
    cv : int, default=5
        äº¤å‰éªŒè¯æŠ˜æ•°
    n_iter : int, default=20
        éšæœºæœç´¢è¿­ä»£æ¬¡æ•°
    random_state : int, default=42
        éšæœºç§å­
    verbose : bool, default=True
        æ˜¯å¦æ‰“å°ä¿¡æ¯

    Returns
    -------
    model
        è°ƒä¼˜åçš„æœ€ä½³æ¨¡å‹

    Examples
    --------
    >>> # ä½¿ç”¨é»˜è®¤å‚æ•°ç©ºé—´
    >>> best_model = quick_tune(X, y, algorithm='xgboost', method='grid')
    >>> # âœ“ è°ƒä¼˜å®Œæˆ: æœ€ä½³å‚æ•° {'max_depth': 5, 'learning_rate': 0.1, ...}

    >>> # è‡ªå®šä¹‰å‚æ•°ç©ºé—´
    >>> param_space = {'max_depth': [3, 5, 7], 'n_estimators': [100, 200]}
    >>> best_model = quick_tune(X, y, param_space=param_space)

    Decision Logic
    --------------
    å‚æ•°æ•°é‡ < 20  â†’ Grid Search
    å‚æ•°æ•°é‡ >= 20 â†’ Random Searchï¼ˆn_iter=20-50ï¼‰

    Notes
    -----
    - Grid Searché€‚åˆå°å‚æ•°ç©ºé—´ï¼ˆ<20ç»„åˆï¼‰
    - Random Searché€‚åˆå¤§å‚æ•°ç©ºé—´
    - å‚è€ƒ06ç« src/supervised_pipeline.py:249-315
    """
    if verbose:
        print("ğŸ”§ è¶…å‚æ•°è°ƒä¼˜...")
        print(f"   ç®—æ³•: {algorithm.upper()}, æ–¹æ³•: {method}")

    # è·å–å‚æ•°ç©ºé—´
    if param_space is None:
        param_space = get_default_param_space(algorithm)

    if not param_space:
        if verbose:
            print(f"âš ï¸  æœªæ‰¾åˆ°{algorithm}çš„é»˜è®¤å‚æ•°ç©ºé—´")
        return None

    # è‡ªåŠ¨è¯†åˆ«é—®é¢˜ç±»å‹
    if y.nunique() <= 10:
        problem_type = 'classification'
        scoring = 'roc_auc'
    else:
        problem_type = 'regression'
        scoring = 'r2'

    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    if problem_type == 'classification':
        if algorithm == 'rf':
            base_model = RandomForestClassifier(random_state=random_state, n_jobs=-1)
        elif algorithm == 'xgboost' and XGBOOST_AVAILABLE:
            base_model = XGBClassifier(random_state=random_state, n_jobs=-1, verbosity=0)
        elif algorithm == 'lightgbm' and LIGHTGBM_AVAILABLE:
            base_model = LGBMClassifier(random_state=random_state, n_jobs=-1, verbosity=-1)
        elif algorithm == 'logistic':
            base_model = LogisticRegression(random_state=random_state, max_iter=1000)
        else:
            if verbose:
                print(f"âš ï¸  ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
            return None
    else:
        if algorithm == 'rf':
            base_model = RandomForestRegressor(random_state=random_state, n_jobs=-1)
        elif algorithm == 'xgboost' and XGBOOST_AVAILABLE:
            base_model = XGBRegressor(random_state=random_state, n_jobs=-1, verbosity=0)
        elif algorithm == 'lightgbm' and LIGHTGBM_AVAILABLE:
            base_model = LGBMRegressor(random_state=random_state, n_jobs=-1, verbosity=-1)
        else:
            if verbose:
                print(f"âš ï¸  ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
            return None

    # æ‰§è¡Œè°ƒä¼˜
    if method == 'grid':
        search = GridSearchCV(
            base_model,
            param_space,
            cv=cv,
            scoring=scoring,
            n_jobs=-1,
            verbose=0
        )
    else:  # random
        search = RandomizedSearchCV(
            base_model,
            param_space,
            n_iter=n_iter,
            cv=cv,
            scoring=scoring,
            random_state=random_state,
            n_jobs=-1,
            verbose=0
        )

    search.fit(X, y)

    if verbose:
        print(f"âœ“ è°ƒä¼˜å®Œæˆ")
        print(f"   æœ€ä½³å¾—åˆ†: {search.best_score_:.4f}")
        print(f"   æœ€ä½³å‚æ•°: {search.best_params_}")
        print()

    return search.best_estimator_


# ==================== 3. æ— ç›‘ç£å­¦ä¹ æ¨¡æ¿ ====================

def quick_kmeans(X: pd.DataFrame,
                n_clusters: int = 3,
                random_state: int = 42,
                verbose: bool = True) -> Tuple[KMeans, np.ndarray]:
    """
    å¿«é€ŸK-Meansèšç±»

    Parameters
    ----------
    X : DataFrame
        ç‰¹å¾æ•°æ®
    n_clusters : int, default=3
        èšç±»æ•°é‡
    random_state : int, default=42
        éšæœºç§å­

    Returns
    -------
    model, labels : tuple
        èšç±»æ¨¡å‹å’Œæ ‡ç­¾

    Examples
    --------
    >>> model, labels = quick_kmeans(X, n_clusters=3)
    >>> # âœ“ K-Meansèšç±»å®Œæˆ: 3ç±»
    """
    model = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
    labels = model.fit_predict(X)

    if verbose:
        print(f"âœ“ K-Meansèšç±»å®Œæˆ: {n_clusters}ç±»")
        for i in range(n_clusters):
            count = (labels == i).sum()
            print(f"   ç±»{i}: {count}ä¸ªæ ·æœ¬")
        print()

    return model, labels


def quick_pca(X: pd.DataFrame,
             n_components: int = 2,
             verbose: bool = True) -> Tuple[PCA, np.ndarray]:
    """
    å¿«é€ŸPCAé™ç»´

    Parameters
    ----------
    X : DataFrame
        ç‰¹å¾æ•°æ®
    n_components : int, default=2
        é™ç»´åçš„ç»´åº¦

    Returns
    -------
    model, X_reduced : tuple
        PCAæ¨¡å‹å’Œé™ç»´åçš„æ•°æ®

    Examples
    --------
    >>> model, X_reduced = quick_pca(X, n_components=2)
    >>> # âœ“ PCAé™ç»´å®Œæˆ: 100ç»´ â†’ 2ç»´, è§£é‡Šæ–¹å·®: 85.3%
    """
    model = PCA(n_components=n_components)
    X_reduced = model.fit_transform(X)

    if verbose:
        variance_ratio = model.explained_variance_ratio_.sum() * 100
        print(f"âœ“ PCAé™ç»´å®Œæˆ: {X.shape[1]}ç»´ â†’ {n_components}ç»´")
        print(f"   è§£é‡Šæ–¹å·®: {variance_ratio:.1f}%")
        print()

    return model, X_reduced
