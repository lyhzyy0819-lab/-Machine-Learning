"""
æ¨¡å‹è¯„ä¼°ä»£ç æ¨¡æ¿åº“
=================

å¿«é€Ÿä½¿ç”¨:
    from code_templates.evaluation_templates import (
        quick_evaluate,
        quick_cross_validate,
        compare_models,
        plot_confusion_matrix,
        plot_roc_curve
    )

    # 3è¡Œä»£ç å®Œæˆè¯„ä¼°
    metrics = quick_evaluate(y_test, y_pred, problem_type='classification')

    # äº¤å‰éªŒè¯
    cv_scores = quick_cross_validate(model, X, y, cv=5)

    # æ¨¡å‹å¯¹æ¯”
    comparison = compare_models(models_dict, X_test, y_test)

å¯¹åº”å†³ç­–æ¨¡æ¿: 07_decision_templates/model_evaluation_template.md
å‚è€ƒå®ç°: 06_comprehensive_project/src/model_evaluation.py (538è¡Œ)

é¡¹ç›®å®šä½: MLå®æˆ˜æ“ä½œæ‰‹å†Œï¼ˆéæ•™å­¦é¡¹ç›®ï¼‰
æ ¸å¿ƒä»·å€¼: 5-15åˆ†é’Ÿå¿«é€Ÿä»£ç è½åœ°
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple, Any
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    mean_squared_error, mean_absolute_error, r2_score,
    roc_curve
)
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')


# ==================== 1. æŒ‡æ ‡è®¡ç®— ====================

def quick_evaluate(y_true: np.ndarray,
                  y_pred: np.ndarray,
                  y_proba: np.ndarray = None,
                  problem_type: str = 'auto',
                  metrics: List[str] = None,
                  verbose: bool = True) -> Dict[str, float]:
    """
    å¿«é€Ÿæ¨¡å‹è¯„ä¼°ï¼ˆ3åˆ†é’Ÿï¼‰

    å¯¹åº”å†³ç­–: model_evaluation_template.md - è¯„ä¼°æŒ‡æ ‡é€‰æ‹©å¡

    Parameters
    ----------
    y_true : array-like
        çœŸå®æ ‡ç­¾
    y_pred : array-like
        é¢„æµ‹æ ‡ç­¾
    y_proba : array-like, optional
        é¢„æµ‹æ¦‚ç‡ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰
    problem_type : {'auto', 'classification', 'regression'}
        é—®é¢˜ç±»å‹ï¼Œautoè‡ªåŠ¨è¯†åˆ«
    metrics : list, optional
        æŒ‡å®šè¯„ä¼°æŒ‡æ ‡ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤
    verbose : bool, default=True
        æ˜¯å¦æ‰“å°è¯„ä¼°ç»“æœ

    Returns
    -------
    Dict[str, float]
        è¯„ä¼°æŒ‡æ ‡å­—å…¸

    Examples
    --------
    >>> # åˆ†ç±»é—®é¢˜
    >>> metrics = quick_evaluate(y_test, y_pred, problem_type='classification')
    >>> # âœ“ æ¨¡å‹è¯„ä¼°å®Œæˆ
    >>> #   Accuracy: 0.8500
    >>> #   Precision: 0.8300
    >>> #   Recall: 0.8700
    >>> #   F1: 0.8500

    >>> # å›å½’é—®é¢˜
    >>> metrics = quick_evaluate(y_test, y_pred, problem_type='regression')
    >>> # âœ“ æ¨¡å‹è¯„ä¼°å®Œæˆ
    >>> #   MSE: 10.25
    >>> #   RMSE: 3.20
    >>> #   MAE: 2.15
    >>> #   R2: 0.85

    Decision Logic
    --------------
    åˆ†ç±»é—®é¢˜:
    - äºŒåˆ†ç±» â†’ Accuracy, Precision, Recall, F1, AUC
    - å¤šåˆ†ç±» â†’ Accuracy, Macro F1, Weighted F1

    å›å½’é—®é¢˜:
    - MAE, MSE, RMSE, R2

    Notes
    -----
    - é€‚åˆå¿«é€Ÿè¯„ä¼°å•ä¸ªæ¨¡å‹
    - å‚è€ƒ06ç« src/model_evaluation.py:45-120
    """
    # è‡ªåŠ¨è¯†åˆ«é—®é¢˜ç±»å‹
    if problem_type == 'auto':
        if len(np.unique(y_true)) <= 10:
            problem_type = 'classification'
        else:
            problem_type = 'regression'

    result = {}

    if problem_type == 'classification':
        # åˆ†ç±»æŒ‡æ ‡
        result['accuracy'] = accuracy_score(y_true, y_pred)
        result['precision'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        result['recall'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        result['f1'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)

        # AUCï¼ˆäºŒåˆ†ç±»ï¼‰
        if y_proba is not None and len(np.unique(y_true)) == 2:
            result['auc'] = roc_auc_score(y_true, y_proba if len(y_proba.shape) == 1 else y_proba[:, 1])

        if verbose:
            print("âœ“ æ¨¡å‹è¯„ä¼°å®Œæˆï¼ˆåˆ†ç±»ï¼‰")
            for metric_name, value in result.items():
                print(f"   {metric_name.upper()}: {value:.4f}")
            print()

    else:
        # å›å½’æŒ‡æ ‡
        result['mae'] = mean_absolute_error(y_true, y_pred)
        result['mse'] = mean_squared_error(y_true, y_pred)
        result['rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))
        result['r2'] = r2_score(y_true, y_pred)

        if verbose:
            print("âœ“ æ¨¡å‹è¯„ä¼°å®Œæˆï¼ˆå›å½’ï¼‰")
            for metric_name, value in result.items():
                print(f"   {metric_name.upper()}: {value:.4f}")
            print()

    return result


# ==================== 2. äº¤å‰éªŒè¯ ====================

def quick_cross_validate(model: Any,
                        X: pd.DataFrame,
                        y: pd.Series,
                        cv: int = 5,
                        scoring: str = 'auto',
                        verbose: bool = True) -> Dict[str, Any]:
    """
    å¿«é€Ÿäº¤å‰éªŒè¯ï¼ˆ5åˆ†é’Ÿï¼‰

    å¯¹åº”å†³ç­–: model_evaluation_template.md - äº¤å‰éªŒè¯ç­–ç•¥è¡¨

    Parameters
    ----------
    model : estimator
        æ¨¡å‹å¯¹è±¡
    X : DataFrame
        ç‰¹å¾æ•°æ®
    y : Series
        ç›®æ ‡å˜é‡
    cv : int, default=5
        äº¤å‰éªŒè¯æŠ˜æ•°
    scoring : str, default='auto'
        è¯„åˆ†æŒ‡æ ‡ï¼Œautoè‡ªåŠ¨é€‰æ‹©
    verbose : bool, default=True
        æ˜¯å¦æ‰“å°ç»“æœ

    Returns
    -------
    Dict[str, Any]
        äº¤å‰éªŒè¯ç»“æœ
        {
            'scores': array,  # å„æŠ˜å¾—åˆ†
            'mean': float,    # å¹³å‡å¾—åˆ†
            'std': float      # æ ‡å‡†å·®
        }

    Examples
    --------
    >>> cv_results = quick_cross_validate(model, X, y, cv=5)
    >>> # âœ“ äº¤å‰éªŒè¯å®Œæˆ
    >>> #   5æŠ˜äº¤å‰éªŒè¯å¾—åˆ†: [0.83, 0.85, 0.84, 0.86, 0.82]
    >>> #   å¹³å‡å¾—åˆ†: 0.84 Â± 0.015

    Decision Logic
    --------------
    åˆ†ç±»é—®é¢˜:
    - äºŒåˆ†ç±» â†’ roc_auc
    - å¤šåˆ†ç±» â†’ accuracy

    å›å½’é—®é¢˜:
    - r2

    Notes
    -----
    - äº¤å‰éªŒè¯æ›´ç¨³å®šå¯é 
    - å‚è€ƒ06ç« src/model_evaluation.py:125-170
    """
    # è‡ªåŠ¨é€‰æ‹©è¯„åˆ†æŒ‡æ ‡
    if scoring == 'auto':
        if len(np.unique(y)) <= 10:
            if len(np.unique(y)) == 2:
                scoring = 'roc_auc'
            else:
                scoring = 'accuracy'
        else:
            scoring = 'r2'

    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)

    result = {
        'scores': scores,
        'mean': scores.mean(),
        'std': scores.std()
    }

    if verbose:
        print(f"âœ“ {cv}æŠ˜äº¤å‰éªŒè¯å®Œæˆ")
        print(f"   è¯„åˆ†æŒ‡æ ‡: {scoring.upper()}")
        print(f"   å„æŠ˜å¾—åˆ†: {[f'{s:.4f}' for s in scores]}")
        print(f"   å¹³å‡å¾—åˆ†: {result['mean']:.4f} Â± {result['std']:.4f}")
        print()

    return result


# ==================== 3. æ¨¡å‹å¯¹æ¯” ====================

def compare_models(models: Dict[str, Any],
                  X_test: pd.DataFrame,
                  y_test: pd.Series,
                  problem_type: str = 'auto',
                  verbose: bool = True) -> pd.DataFrame:
    """
    å¤šæ¨¡å‹å¯¹æ¯”ï¼ˆ10åˆ†é’Ÿï¼‰

    å¯¹åº”å†³ç­–: model_evaluation_template.md - æ¨¡å‹å¯¹æ¯”

    Parameters
    ----------
    models : dict
        æ¨¡å‹å­—å…¸ {'model_name': model_object}
    X_test : DataFrame
        æµ‹è¯•é›†ç‰¹å¾
    y_test : Series
        æµ‹è¯•é›†æ ‡ç­¾
    problem_type : {'auto', 'classification', 'regression'}
        é—®é¢˜ç±»å‹
    verbose : bool, default=True
        æ˜¯å¦æ‰“å°å¯¹æ¯”è¡¨

    Returns
    -------
    DataFrame
        å¯¹æ¯”ç»“æœè¡¨

    Examples
    --------
    >>> models = {
    ...     'Random Forest': rf_model,
    ...     'XGBoost': xgb_model,
    ...     'LightGBM': lgb_model
    ... }
    >>> comparison = compare_models(models, X_test, y_test)
    >>> # âœ“ æ¨¡å‹å¯¹æ¯”å®Œæˆ
    >>> #   Model          Accuracy    F1      AUC
    >>> #   Random Forest  0.8500      0.8300  0.8700
    >>> #   XGBoost        0.8700      0.8500  0.8900  â† æœ€ä½³
    >>> #   LightGBM       0.8600      0.8400  0.8800

    Notes
    -----
    - é€‚åˆæœ€ç»ˆæ¨¡å‹é€‰æ‹©
    - å‚è€ƒ06ç« src/model_evaluation.py:175-235
    """
    # è‡ªåŠ¨è¯†åˆ«é—®é¢˜ç±»å‹
    if problem_type == 'auto':
        if len(np.unique(y_test)) <= 10:
            problem_type = 'classification'
        else:
            problem_type = 'regression'

    results = []

    for model_name, model in models.items():
        y_pred = model.predict(X_test)

        if problem_type == 'classification':
            metrics = {
                'Model': model_name,
                'Accuracy': accuracy_score(y_test, y_pred),
                'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
                'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),
                'F1': f1_score(y_test, y_pred, average='weighted', zero_division=0)
            }

            if hasattr(model, 'predict_proba') and len(np.unique(y_test)) == 2:
                y_proba = model.predict_proba(X_test)[:, 1]
                metrics['AUC'] = roc_auc_score(y_test, y_proba)

        else:
            metrics = {
                'Model': model_name,
                'MAE': mean_absolute_error(y_test, y_pred),
                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
                'R2': r2_score(y_test, y_pred)
            }

        results.append(metrics)

    df_results = pd.DataFrame(results)

    if verbose:
        print("âœ“ æ¨¡å‹å¯¹æ¯”å®Œæˆ\n")
        print(df_results.to_string(index=False))
        print()

        # æ ‡æ³¨æœ€ä½³æ¨¡å‹
        if problem_type == 'classification':
            best_metric = 'AUC' if 'AUC' in df_results.columns else 'F1'
        else:
            best_metric = 'R2'

        best_idx = df_results[best_metric].idxmax() if best_metric == 'R2' else df_results[best_metric].idxmax()
        best_model = df_results.loc[best_idx, 'Model']
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model}\n")

    return df_results


# ==================== 4. å¯è§†åŒ– ====================

def plot_confusion_matrix(y_true: np.ndarray,
                         y_pred: np.ndarray,
                         labels: List[str] = None,
                         figsize: Tuple[int, int] = (8, 6)):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µ

    Parameters
    ----------
    y_true : array-like
        çœŸå®æ ‡ç­¾
    y_pred : array-like
        é¢„æµ‹æ ‡ç­¾
    labels : list, optional
        ç±»åˆ«æ ‡ç­¾åç§°
    figsize : tuple, default=(8, 6)
        å›¾è¡¨å¤§å°

    Examples
    --------
    >>> plot_confusion_matrix(y_test, y_pred, labels=['Class 0', 'Class 1'])
    """
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=figsize)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels)
    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()

    print("âœ“ æ··æ·†çŸ©é˜µå·²ç»˜åˆ¶\n")


def plot_roc_curve(y_true: np.ndarray,
                  y_proba: np.ndarray,
                  model_name: str = 'Model',
                  figsize: Tuple[int, int] = (8, 6)):
    """
    ç»˜åˆ¶ROCæ›²çº¿

    Parameters
    ----------
    y_true : array-like
        çœŸå®æ ‡ç­¾
    y_proba : array-like
        é¢„æµ‹æ¦‚ç‡
    model_name : str, default='Model'
        æ¨¡å‹åç§°
    figsize : tuple, default=(8, 6)
        å›¾è¡¨å¤§å°

    Examples
    --------
    >>> plot_roc_curve(y_test, y_proba, model_name='XGBoost')
    """
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    auc_score = roc_auc_score(y_true, y_proba)

    plt.figure(figsize=figsize)
    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc_score:.4f})')
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Guess')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

    print(f"âœ“ ROCæ›²çº¿å·²ç»˜åˆ¶ (AUC = {auc_score:.4f})\n")


def plot_feature_importance(model: Any,
                           feature_names: List[str],
                           top_n: int = 20,
                           figsize: Tuple[int, int] = (10, 8)):
    """
    ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§

    Parameters
    ----------
    model : estimator
        æ¨¡å‹å¯¹è±¡ï¼ˆå¿…é¡»æœ‰feature_importances_å±æ€§ï¼‰
    feature_names : list
        ç‰¹å¾åç§°åˆ—è¡¨
    top_n : int, default=20
        å±•ç¤ºå‰Nä¸ªé‡è¦ç‰¹å¾
    figsize : tuple, default=(10, 8)
        å›¾è¡¨å¤§å°

    Examples
    --------
    >>> plot_feature_importance(model, X.columns.tolist(), top_n=20)
    """
    if not hasattr(model, 'feature_importances_'):
        print("âš ï¸  æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
        return

    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]

    plt.figure(figsize=figsize)
    plt.barh(range(top_n), importances[indices][::-1])
    plt.yticks(range(top_n), [feature_names[i] for i in indices][::-1])
    plt.xlabel('Feature Importance')
    plt.title(f'Top {top_n} Feature Importances', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

    print(f"âœ“ ç‰¹å¾é‡è¦æ€§å·²ç»˜åˆ¶ (Top {top_n})\n")
